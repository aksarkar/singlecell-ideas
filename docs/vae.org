#+TITLE: VAE for single-cell expression
#+AUTHOR: Abhishek Sarkar
#+EMAIL: aksarkar@uchicago.edu
#+EXCLUDE_TAGS: noexport
#+HTML_CONTAINER: div
#+HTML_DOCTYPE: html-strict
#+LANGUAGE: en
#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline author:t
#+OPTIONS: broken-links:nil c:nil creator:nil d:(not "LOGBOOK") date:t e:t
#+OPTIONS: email:nil f:t inline:t num:t p:nil pri:nil prop:nil stat:t tags:t
#+OPTIONS: html-link-use-abs-url:nil html-postamble:auto html-preamble:t
#+OPTIONS: html-scripts:t html-style:t html5-fancy:nil tex:t
#+OPTIONS: tasks:t tex:t timestamp:t title:t toc:t todo:t |:t

#+PROPERTY: header-args:ipython+ :session kernel-aksarkar.json :results raw drawer :async t

* Setup :noexport:

  #+NAME: do-not-warn
  #+BEGIN_SRC emacs-lisp
    (setq python-shell-prompt-detect-failure-warning nil)
  #+END_SRC

  #+RESULTS:

  #+NAME: ipython3-kernel
  #+BEGIN_SRC shell :dir (concat (file-name-as-directory (getenv "SCRATCH")) "singlecell") :var RESOURCES="--mem=36G --partition=gpu2 --gres=gpu:1"
    sbatch $RESOURCES --job-name=ipython3 --output=ipython3.out
    #!/bin/bash
    source activate singlecell
    rm -f $HOME/.local/share/jupyter/runtime/kernel-aksarkar.json
    ipython3 kernel --ip=$(hostname -i) -f kernel-aksarkar.json
  #+END_SRC

  #+RESULTS: ipython3-kernel
  : Submitted batch job 38809493

  #+NAME: imports
  #+BEGIN_SRC ipython
    %matplotlib inline

    import matplotlib.pyplot as plt
    import numpy as np
    import os
    import pandas as pd
    import scipy.linalg as spla
    import scipy.stats as sps
    import tensorflow as tf
    import tensorflow.contrib.bayesflow as bf
    import tensorflow.contrib.distributions as ds
    import tensorflow.contrib.slim as slim

    st = bf.stochastic_tensor
    vi = bf.variational_inference
  #+END_SRC

  #+RESULTS: imports
  :RESULTS:
  :END:

  #+NAME: list-local-devices
  #+BEGIN_SRC ipython
    from tensorflow.python.client import device_lib as dl
    dl.list_local_devices()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  #+BEGIN_EXAMPLE
  [name: "/cpu:0"
     device_type: "CPU"
     memory_limit: 268435456
     locality {
     }
     incarnation: 3138266557590326484, name: "/gpu:0"
     device_type: "GPU"
     memory_limit: 11324823962
     locality {
       bus_id: 2
     }
     incarnation: 14397799056180715890
     physical_device_desc: "device: 0, name: Tesla K80, pci bus id: 0000:88:00.0"]
  #+END_EXAMPLE
  :END:

* Get the data

  #+BEGIN_SRC shell :dir /home/aksarkar/projects/singlecell/data :async t
    test -f oligodendroma.txt.gz || curl --ftp-pasv 'ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE70nnn/GSE70630/suppl/GSE70630%5FOG%5Fprocessed%5Fdata%5Fv2%2Etxt%2Egz' -o oligodendroma.txt.gz
  #+END_SRC

  #+RESULTS:

  #+NAME: signature-genes
  #+BEGIN_SRC ipython
    signature_genes = pd.read_excel('https://www.nature.com/nature/journal/v539/n7628/extref/nature20123-s1.xlsx', skiprows=8, header=0)
  #+END_SRC

  #+RESULTS: signature-genes
  :RESULTS:
  :END:

  #+NAME: oligodendroma
  #+BEGIN_SRC ipython
    dge = pd.read_table('/home/aksarkar/projects/singlecell/data/oligodendroma.txt.gz', index_col=0, quotechar="'")
    dge = dge.fillna(value=0)
    dge.columns = pd.Series(dge.columns).apply(lambda x: 'MGH{}'.format(x) if 'MGH' not in x else x)
    dge.shape
  #+END_SRC

  #+RESULTS: oligodendroma
  :RESULTS:
  : (23686, 4347)
  :END:

* Replicate Tirosh et al:

  Expression levels were quantified as \(E_{i,j} = \log_2 (TPM_{i,j} /10 +
  1)\), where \(TPM_{i,j}\) refers to transcript-per-million for gene \(i\) in
  sample \(j\), as calculated by RSEM.

  For each cell, we quantified two quality measures: the number of genes for which
  at least one read was mapped, and the average expression level of a curated list of
  housekeeping genes. We then conservatively excluded all cells with either fewer
  than 3,000 detected genes or an average housekeeping expression (E, as defined
  above) below 2.5.

  *Presumably this was already done because the number of cells in the data
  matrix matches the reported number (4,347)*

  For the remaining cells we calculated the aggregate expression of each gene
  as (\log 2 (\mathrm{average}(TPM_{i,1...n})+1)\), and excluded genes with an
  aggregate expression below 4, leaving a set of 8,008 analysed genes.

  #+NAME: pass-aggregate-expression
  #+BEGIN_SRC ipython
    tpm = 10 * (np.exp(dge.values * np.log(2)) - 1)
    pass_aggregate_expression = np.log(tpm.mean(axis=1) + 1) / np.log(2) > 4
    pass_aggregate_expression.sum()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  : 8115
  :END:

  For the remaining cells and genes, we defined relative expression by
  centering the expression levels, \(Er_{i,j} = E_{i,j} -
  \mathrm{average}[E_{i,1...n} ]\). Centering was performed within each tumour
  separately in order to decrease the impact of inter-tumoural variability on
  the combined analysis across tumours.

  #+NAME: relative_expression
  #+BEGIN_SRC ipython
    relative_expression = dge[pass_aggregate_expression].groupby(lambda x: x.split('_')[0], axis=1).transform(lambda x: x - x.mean()).transpose()
    relative_expression.shape
  #+END_SRC

  #+RESULTS: relative_expression
  :RESULTS:
  : (4347, 8115)
  :END:

  The control gene set was defined by first binning all 8,008 analysed genes
  into 25 bins of aggregate expression levels and then, for each gene in the
  lineage gene set, randomly selecting 100 genes from the same expression bin.
  In this way, the control gene set has a comparable distribution of expression
  levels to that of the lineage gene set and the control gene set is 100-fold
  larger, such that its average expression is analogous to averaging over 100
  randomly selected gene sets of the same size as the lineage gene set. The
  final lineage score of each cell was defined as the maximal score over the
  two lineages, \(LIN_i = \max(Lin_i OC, Lin_i AC)\).

  #+NAME: lineage-score
  #+BEGIN_SRC ipython
  
  #+END_SRC

* Model

  \[ q(z \mid x) = N(\mu(x), \sigma(x)) \]

  \[ p(x \mid z) = N(\mu(z), \sigma(z)) \]

  The idea is that \(\mu, \sigma, \pi_0, \lambda\) are outputs of neural
  networks.

  To achieve dimensionality reduction, we want the dimension of \(\mu(x)\) less
  than the dimension of \(x\).

  To visualize the result, we can take \(\mu(x)\).

  #+NAME: vae
  #+BEGIN_SRC ipython
    num_epochs = 100
    minibatch_size = 100
    n, p = relative_expression.shape
    num_minibatch = n // 100
    # Last layer is the target latent dimension
    layer_dim = [512, 2]

    graph = tf.Graph()
    with graph.as_default(), graph.device('/gpu:*'):
      with tf.name_scope('input'):
        x = tf.placeholder(shape=[minibatch_size, p], dtype=tf.float32)
      with slim.arg_scope([slim.fully_connected],
                          normalizer_fn=slim.batch_norm):
        with tf.variable_scope('encoder'):
          # Default activation is relu
          qz = slim.stack(x, slim.fully_connected, layer_dim[:-1], scope='fc')
          q_loc = slim.linear(qz, num_outputs=layer_dim[-1], activation_fn=None,
                              scope='loc')
          q_scale = slim.fully_connected(qz, num_outputs=layer_dim[-1],
                                         activation_fn=tf.nn.softplus, scope='scale')
          with st.value_type(st.SampleValue()):
            # Put minimum scale here because it has to be outside the softplus (bias
            # on sigma is inside the softplus)
            qz = st.StochasticTensor(ds.Normal(loc=q_loc, scale=(1e-6 + q_scale)))
        with tf.variable_scope('decoder'):
          pz = ds.Normal(loc=tf.zeros(layer_dim[-1]), scale=tf.ones(layer_dim[-1]))
          px = slim.stack(qz, slim.fully_connected, list(reversed(layer_dim[:-1])),
                          scope='fc')
          loc = slim.linear(px, num_outputs=p, activation_fn=None, scope='loc')
          scale = slim.linear(px, num_outputs=p, activation_fn=tf.nn.softplus, scope='scale')
          llik = tf.reduce_sum(ds.Normal(loc=loc, scale=(1e-6 + scale)).log_prob(x))

      vi.register_prior(qz, pz)
      elbo = tf.reduce_sum(vi.elbo(llik))
      opt = tf.train.RMSPropOptimizer(learning_rate=5e-5)
      step = tf.get_variable(
        name='step',
        shape=[],
        initializer=tf.constant_initializer(0.0),
        trainable=False)
      train = slim.learning.create_train_op(
        total_loss=-elbo,
        optimizer=opt,
        global_step=step)

    # This needs to be outside graph.device
    sv = tf.train.Supervisor(
      graph=graph,
      logdir=os.path.join(os.getenv('SCRATCH'), 'vae-model'))
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

  #+BEGIN_SRC shell
  rm -f $SCRATCH/vae-model/checkpoint
  #+END_SRC

  #+RESULTS:

  #+NAME: train
  #+BEGIN_SRC ipython
    with sv.managed_session() as sess:
      for i in range(num_epochs * num_minibatch):
        if sv.should_stop():
          break
        start = (i % num_minibatch) * minibatch_size
        _, *loss = sess.run([train, elbo], feed_dict={x: relative_expression.sample(minibatch_size)})
        if np.isnan(loss[0]):
          print(i, *loss)
          raise tf.train.NanLossDuringTrainingError
        if not i % num_minibatch:
          print(i // num_minibatch, *loss)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

  #+NAME: latent
  #+BEGIN_SRC ipython :ipyfile latent.png
    z_proj = np.zeros((relative_expression.shape[0], layer_dim[-1]))
    with sv.managed_session() as sess:
      for i in range(num_minibatch):
        start = i * minibatch_size
        z_proj[start:start + minibatch_size] = sess.run(q_loc, feed_dict={x: relative_expression.iloc[start:start + minibatch_size]})
    plt.clf()
    plt.scatter(z_proj[:,0], z_proj[:,1])
    plt.xlabel('$z_1$')
    plt.ylabel('$z_2$')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  : Text(0,0.5,'$z_2$')
  [[file:latent.png]]
  :END:
