#+TITLE: ZIP VAE for single-cell expression
#+AUTHOR: Abhishek Sarkar
#+EMAIL: aksarkar@uchicago.edu
#+EXCLUDE_TAGS: noexport
#+HTML_CONTAINER: div
#+HTML_DOCTYPE: html-strict
#+LANGUAGE: en
#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline author:t
#+OPTIONS: broken-links:nil c:nil creator:nil d:(not "LOGBOOK") date:t e:t
#+OPTIONS: email:nil f:t inline:t num:t p:nil pri:nil prop:nil stat:t tags:t
#+OPTIONS: html-link-use-abs-url:nil html-postamble:auto html-preamble:t
#+OPTIONS: html-scripts:t html-style:t html5-fancy:nil tex:t
#+OPTIONS: tasks:t tex:t timestamp:t title:t toc:t todo:t |:t

* Setup :noexport:

  #+BEGIN_SRC emacs-lisp
    (setq python-shell-prompt-detect-failure-warning nil)
  #+END_SRC

  #+RESULTS:

  #+BEGIN_SRC shell :dir (concat (file-name-as-directory (getenv "SCRATCH"))) :var RESOURCES="--mem=36G --partition=gpu2 --gres=gpu:1"
    sbatch $RESOURCES --job-name=ipython3 --output=ipython3.out
    #!/bin/bash
    module unload cuda
    module load cuda/8.0
    source activate singlecell
    rm -f $HOME/.local/share/jupyter/runtime/kernel-aksarkar.json
    ipython3 kernel --ip=$(hostname -i) -f kernel-aksarkar.json
  #+END_SRC

  #+RESULTS:
  : Submitted batch job 38019028

  #+BEGIN_SRC shell :dir (concat (file-name-as-directory (getenv "SCRATCH"))) :results raw drawer
  tail ipython3.out
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  NOTE: When using the `ipython kernel` entry point, Ctrl-C will not work.

  To exit, you will have to explicitly quit this process, by either sending
  "quit" from a client, or using Ctrl-\ in UNIX-like environments.

  To read more about this, see https://github.com/ipython/ipython/issues/2049


  To connect another client to this kernel, use:
      --existing kernel-aksarkar.json
  :END:

  #+NAME: imports
  #+BEGIN_SRC ipython :session kernel-aksarkar.json :results raw drawer
    %matplotlib inline

    import matplotlib.pyplot as plt
    import numpy as np
    import os
    import pandas as pd
    import scipy.linalg as spla
    import tensorflow as tf
    import tensorflow.contrib.bayesflow as bf
    import tensorflow.contrib.distributions as ds
    import tensorflow.contrib.slim as slim
    import tensorflow.python.ops.array_ops as array_ops

    st = bf.stochastic_tensor
    vi = bf.variational_inference
  #+END_SRC

* Get the data

  #+BEGIN_SRC ipython :session kernel-aksarkar.json :results raw drawer :async t
    dge = pd.read_table('/home/aksarkar/projects/singlecell/data/mouse-p14-dge.txt.gz', index_col='gene')
    dge = dge.transpose().sample(frac=1).reset_index(drop=True)
    dge.shape
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  f9067002-ac82-4542-84ea-5aaed7671d3c
  :END:

* Gumbel-Softmax distribution
  
  [[https://arxiv.org/abs/1611.00712][Maddison et al ICLR 2017]]

  [[https://arxiv.org/abs/1611.01144][Jang et al ICLR 2017]]

  #+BEGIN_SRC ipython :session kernel-aksarkar.json :result raw drawer
    def sample_gumbel(size=1):
      return -np.log(-np.log(np.random.uniform(size=size)))

    def softmax(logits):
      # This is needed to avoid exp overflow
      p = np.exp(logits - logits.max())
      return p / p.sum()

    def sample_gumbel_softmax(probs, temperature):
      return softmax((np.log(probs) + sample_gumbel(probs.shape)) / temperature)
  #+END_SRC

  #+RESULTS:

  [[file:~/.local/src/tensorflow/tensorflow/contrib/distributions/python/ops/relaxed_onehot_categorical.py::294][RelaxedOneHotCategorical]], [[file:~/.local/src/tensorflow/tensorflow/contrib/distributions/python/ops/relaxed_bernoulli.py::class%20RelaxedBernoulli(transformed_distribution.TransformedDistribution):][RelaxedBernoulli]] in Tensorflow

* Model

  \[ q(z \mid x) = N(\mu(x), \sigma(x)) \]

  \[ p(x \mid z) = \pi_0(z) \delta_0(x) + (1 - \pi_0(z)) \mathrm{Pois}(x; \lambda(z)) \]

  The idea is that \(\mu, \sigma, \pi_0, \lambda\) are outputs of neural
  networks.

  To achieve dimensionality reduction, we want the dimension of \(\mu(x)\) less
  than the dimension of \(x\).

  To visualize the result, we can take \(\mu(x)\).

  #+BEGIN_SRC ipython :session kernel-aksarkar.json :results raw drawer :async t
    num_epochs = 20
    minibatch_size = 100
    n, p = dge.shape
    num_minibatch = n // 100
    # Last layer is the target latent dimension
    layer_dim = [2048, 1024, 512]

    graph = tf.Graph()
    gpu_dev = '/gpu:0'
    with graph.as_default(), graph.device(gpu_dev):
      with tf.name_scope('input'):
        x = tf.placeholder(shape=[minibatch_size, p], dtype=tf.float32)
      with tf.variable_scope('encoder'):
        # Default activation is relu
        qz = slim.stack(x, slim.fully_connected, layer_dim[:-1], scope='fc')
        loc = slim.linear(qz, num_outputs=layer_dim[-1], activation_fn=None,
                          scope='loc')
        scale = slim.fully_connected(qz, num_outputs=layer_dim[-1],
                                     activation_fn=tf.nn.softplus, scope='scale')
        with st.value_type(st.MeanValue()):
          # Put minimum scale here because it has to be outside the softplus (bias
          # on sigma is inside the softplus)
          qz = st.StochasticTensor(ds.Normal(loc=loc, scale=(1e-6 + scale)))
      with tf.variable_scope('decoder'):
        pz = ds.Normal(loc=tf.zeros(layer_dim[-1]), scale=tf.ones(layer_dim[-1]))
        px = slim.stack(qz, slim.fully_connected, list(reversed(layer_dim[:-1])),
                        scope='fc')
        pi_0 = slim.fully_connected(px, num_outputs=1,
                                    activation_fn=tf.nn.sigmoid, scope='pi_0')
        rate = slim.linear(px, num_outputs=p, activation_fn=tf.nn.softplus,
                           scope='lambda')
        # pi_0 + (1 - pi_0) Pois(0; rate), x = 0
        #        (1 - pi_0) Pois(x; rate), x > 0
        #
        # We need minimum rate because otherwise log_prob gives nan
        px = ds.Poisson(1e-6 + rate)
        num_0 = tf.reduce_sum(tf.cast(x == 0, tf.float32))
        llik = tf.reduce_sum(pi_0 * num_0 + (1 - pi_0) * px.log_prob(x))
        reconstructed_x = (1 - pi_0) * rate
        frob = tf.norm(reconstructed_x - x)

      vi.register_prior(qz, pz)
      elbo = tf.reduce_sum(vi.elbo(llik))
      opt = tf.train.AdamOptimizer(learning_rate=1e-5)
      train = opt.minimize(-elbo)

    # This needs to be outside graph.device
    sv = tf.train.Supervisor(
      graph=graph,
      logdir=os.path.join(os.getenv('SCRATCH'), 'zip-vae-model'))
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

  #+BEGIN_SRC ipython :session kernel-aksarkar.json :results raw drawer :async t
    with sv.managed_session() as sess:
      for i in range(num_epochs * dge.shape[0] // minibatch_size):
        if sv.should_stop():
          break
        start = (i % num_minibatch) * minibatch_size
        _, *loss = sess.run([train, elbo, llik, frob], feed_dict={x: dge.iloc[start:start + minibatch_size]})
        if np.isnan(loss[0]):
          raise tf.train.NanLossDuringTrainingError
        if not i % 100:
          print(i, *loss)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

  #+BEGIN_SRC ipython :session kernel-aksarkar.json :results raw drawer :async t
    with sv.managed_session() as sess:
      frob = np.zeros(num_minibatch)
      for i in range(num_minibatch):
        frob[i] = sess.run(frob,
          feed_dict={x: dge.iloc[start:start + minibatch_size]})
      np.square(frob).sum()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  7d14022a-fcef-40ae-a6a9-65bd4a395171
  :END:

  #+BEGIN_SRC ipython :session kernel-aksarkar.json :results raw drawer :ipyfile test.png
  plt.clf()
  plt.scatter(dge.iloc[:,0], x_hat[:,0])
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  : <matplotlib.collections.PathCollection at 0x7ff8b4764080>
  [[file:test.png]]
  :END:

  #+BEGIN_SRC ipython :session kernel-aksarkar.json :results raw drawer :async t
    with sv.managed_session() as sess:
      z = np.zeros((n, layer_dim[-1]))
      for i in range(num_minibatch):
        start = i * minibatch_size
        z[start:start + minibatch_size] = sess.run(qz.value(), feed_dict={x: dge.iloc[start:start + minibatch_size]})
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

  #+BEGIN_SRC ipython :ipyfile pca.png :session kernel-aksarkar.json :results raw drawer :async t
    u, d, v = spla.svd(z, full_matrices=False)
    z_proj = z.dot(v)
    plt.clf()
    plt.scatter(z_proj[:,0], z_proj[:,1])
    plt.xlabel('PC1 $q(z \mid x)$')
    plt.ylabel('PC2 $q(z \mid x)')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  : <matplotlib.text.Text at 0x7fdd150733c8>
  [[file:pca.png]]
  :END:

  #+BEGIN_SRC ipython :ipyfile pi_0.png :session kernel-aksarkar.json :results raw drawer :async t
    pi_0_emp = (dge == 0).sum(axis=1) / dge.shape[1]
    with sv.managed_session() as sess:
      pi_0_hat = np.zeros((n, 1))
      for i in range(num_minibatch):
        start = i * minibatch_size
        pi_0_hat[start:start + minibatch_size] = sess.run(pi_0, feed_dict={x: dge.iloc[start:start + minibatch_size]})

    plt.clf()
    plt.scatter(pi_0_emp, pi_0_hat)
    plt.xlabel('Empirical $\pi_0$')
    plt.ylabel('Posterior mean $\pi_0$')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  : <matplotlib.text.Text at 0x7ff8b1787518>
  [[file:pi_0.png]]
  :END:
