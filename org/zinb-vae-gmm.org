#+TITLE: Deep unsupervised clustering of scRNA-seq data
#+SETUPFILE: setup.org

* Introduction

  Two major strategies for clustering scRNA-seq data are:

  1. Building a \(k\)-nearest neighbor graph on the data, and applying a
     community detection algorithm (e.g.,
     [[https://doi.org/10.1088/1742-5468/2008/10/P10008][Blondel et al. 2008]],
     [[https://arxiv.org/abs/1810.08473][Traag et al. 2018]])
  2. Fitting a topic model to the data
     (e.g., [[https://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1006599][Dey
     et al. 2017]],
     [[https://www.nature.com/articles/s41592-019-0367-1][Gonzáles-Blas et
     al. 2019]])

  The main disadvantage of strategy (1) is that, as commonly applied to
  transformed counts, it does not separate measurement error and biological
  variation of interest. The main disadvantage of strategy (2) is that it does
  not account for transcriptional noise
  ([[https://doi.org/10.1016/j.cell.2008.09.050][Raj 2008]]). We
  [[file:nbmix.org][previously developed a simple mixture model]] which could
  address this issue. Here, we develop an alternative method based on
  [[http://ruishu.io/2016/12/25/gmvae/][GMVAE]], which allows us to explore a
  different way of separating and representing transcriptional noise.

* Setup
  :PROPERTIES:
  :CUSTOM_ID: setup
  :END:

  #+BEGIN_SRC emacs-lisp :exports none
    (add-to-list 'python-shell-completion-native-disabled-interpreters "jupyter")
    (org-babel-lob-ingest "/home/aksarkar/.emacs.d/org-templates/library.org")
  #+END_SRC

  #+RESULTS:
  : 2

  #+CALL: ipython3(venv="singlecell",partition="gpu2",opts="--gres=gpu:1",memory="8G") :exports none :dir /scratch/midway2/aksarkar/singlecell

  #+CALL: tensorboard(venv="singlecell") :dir /scratch/midway2/aksarkar/singlecell

  #+BEGIN_SRC ipython
    import anndata
    import mpebpm
    import numpy as np
    import pandas as pd
    import scanpy as sc
    import scipy.stats as st
    import sklearn.mixture as skm
    import torch
    import torch.utils.data as td
    import torch.utils.tensorboard as tb
    import torchvision
    import umap
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[17]:
  :END:

  #+BEGIN_SRC ipython
    %matplotlib inline
    %config InlineBackend.figure_formats = set(['retina'])
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[2]:
  :END:

  #+BEGIN_SRC ipython
    import colorcet
    import matplotlib.pyplot as plt
    plt.rcParams['figure.facecolor'] = 'w'
    plt.rcParams['font.family'] = 'Nimbus Sans'
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[37]:
  :END:

* Methods
** Variational autoencoder

   In perhaps the simplest case, one assumes \(
   \DeclareMathOperator\Bern{Bernoulli}
   \DeclareMathOperator\E{E}
   \DeclareMathOperator\KL{\mathcal{KL}}
   \DeclareMathOperator\Gam{Gamma}
   \DeclareMathOperator\Mult{Multinomial}
   \DeclareMathOperator\N{\mathcal{N}}
   \DeclareMathOperator\Pois{Poisson}
   \DeclareMathOperator\diag{diag}
   \DeclareMathOperator\KL{\mathcal{KL}}
   \newcommand\kl[2]{\KL(#1\;\Vert\;#2)}
   \newcommand\xiplus{x_{i+}}
   \newcommand\mi{\mathbf{I}}
   \newcommand\va{\mathbf{a}}
   \newcommand\vb{\mathbf{b}}
   \newcommand\vu{\mathbf{u}}
   \newcommand\vx{\mathbf{x}}
   \newcommand\vy{\mathbf{y}}
   \newcommand\vz{\mathbf{z}}
   \newcommand\vlambda{\boldsymbol{\lambda}}
   \newcommand\vmu{\boldsymbol{\mu}}
   \newcommand\vphi{\boldsymbol{\phi}}
   \newcommand\vpi{\boldsymbol{\pi}}
   \)

   \begin{align}
     \vx_i \mid \vz_i, s^2 &\sim \N(f(\vz_i), s^2 \mi)\\
     \vz_i &\sim \N(0, \mi),
   \end{align}

   where

   - \(\vx_i\) denotes a \(p\)-dimensional observation
   - \(\vz_i\) denotes the \(d\)-dimensional latent variable corresponding to
     observation \(\vx_i\), with \(d \ll p\)
   - \(f\) is a (fully connected, say) neural network mapping latent variables
     to the (noiseless, true) mean vector

   This simple case is a generalization of PPCA
   ([[https://dx.doi.org/10.1111/1467-9868.00196][Tipping and Bishop
   1999]]). To perform inference, typically one introduces a variational
   approximation parameterized by an inference network
   ([[https://escholarship.org/uc/item/34j1h7k5][Gershman and Goodman 2014]]).

   \begin{equation}
     q(\vz_i \mid \vx_i) = \N(\mu(\vz_i), \diag(\sigma^2(\vz_i))),
   \end{equation}

   where \(\mu, \sigma^2\) give the mean and diagonal of the covariance matrix
   of the approximate posterior distribution.

   /Remark/ The inference network [[file:vae-unamortized.org][is not strictly
   necessary]].

   #+NAME: dist
   #+BEGIN_SRC ipython
     class FC(torch.nn.Module):
       """Fully connected layers"""
       def __init__(self, input_dim, hidden_dim=128):
         super().__init__()
         self.net = torch.nn.Sequential(
           torch.nn.Linear(input_dim, hidden_dim),
           torch.nn.ReLU(),
           torch.nn.BatchNorm1d(hidden_dim),
         )

       def forward(self, x):
         return self.net(x)

     class DeepGaussian(torch.nn.Module):
       """Gaussian distribution parameterized by FC networks for mean and (diagonal)
     scale"""
       def __init__(self, input_dim, output_dim, hidden_dim=128):
         super().__init__()
         self.net = FC(input_dim, hidden_dim)
         self.mean = torch.nn.Linear(hidden_dim, output_dim)
         self.scale = torch.nn.Sequential(torch.nn.Linear(hidden_dim, output_dim), torch.nn.Softplus())

       def forward(self, x):
         q = self.net(x)
         return self.mean(q), self.scale(q) + 1e-3

     class DeepCategorical(torch.nn.Module):
       """Categorical distribution parameterized by FC network for logits"""
       def __init__(self, input_dim, output_dim, hidden_dim=128):
         super().__init__()
         self.net = FC(input_dim, hidden_dim)
         self.logits = torch.nn.Linear(hidden_dim, output_dim)

       def forward(self, x):
         q = self.net(x)
         return self.logits(q)
   #+END_SRC

   #+RESULTS: dist
   :RESULTS:
   # Out[52]:
   :END:

   Then, the ELBO

   \begin{equation}
     \ell = \sum_i \E_q[\ln p(\vz_i \mid \vz_i)] - \KL(q(\vz_i \mid \vx_i) \Vert p(\vz_i)),
   \end{equation}

   where the second term is analytic (since it is the KL divergence between two
   Gaussians). In practice, one replaces the intractable first term \(\E_q[\ln
   p(\vz_i \mid \vz_i)]\) with a Monte Carlo integral, where samples from \(q\)
   are simulated from a standard distribution transformed by some
   differentiable function \(g\)
   ([[https://openreview.net/forum?id=33X9fd2-9FyZd][Kingma and Welling
   2014]]). Then, it is trivial to generalize this setup to other likelihoods.

** Gaussian GMVAE

   Now assume

   \begin{align}
     \vx_i \mid \vz_i, \sigma^2 &\sim \N(f(\vz_i), \sigma^2\mi)\\
     \vz_i \mid y_i &\sim \N(m(y_i), s^2(y_i))\\
     y_i \mid \va &\sim \Mult(1, \va),
   \end{align}

   where

   - \(y_i \in \{1, \ldots, k\}\) denotes the latent cluster assignment
   - \(m(\cdot), s^2(\cdot)\) are fully-connected neural networks mapping
     latent labels to latent variables (in particular, \(m\) gives the cluster
     centroids)
   - \(\va\) is the \(k\)-vector of prior probabilities for each cluster
     assignment
   
   Now, the (marginal) prior on latent variables \(\vz_i\) is a mixture of
   Gaussians. To perform inference, introduce a variational approximation

   \begin{align}
     q(y_i, \vz_i \mid \vx_i) &= q(y_i \mid \vx_i)\, q(\vz_i \mid \vx_i, y_i)\\
     q(y_i \mid \vx_i) &= \Mult(1, \pi(\vx_i))\\
     q(\vz_i \mid \vx_i, y_i) &= \N(\mu(\vx_i, y_i), \sigma^2(\vx_i, y_i)),
   \end{align}

   where

   - \(\pi(\cdot)\) is a fully-connected neural network mapping observations to
     latent labels (more precisely, posterior probabilities)
   - \(\mu(\cdot), \sigma^2(\cdot)\) are fully-connected neural networks mapping
     observations and latent labels to latent variables.

   Importantly, the approximate posterior distribution of latent variables is
   Gaussian, not a mixture of Gaussians. The ELBO

   \begin{equation}
     \ell = \sum_i \E_q\left[\ln p(\vx_i \mid \vz_i) + \ln\frac{p(y_i)}{q(y_i \mid \vx_i)} + \ln\frac{p(\vz_i \mid y_i)}{q(\vz_i \mid \vx_i, y_i)}\right],
   \end{equation}

   in which the expectation is replaced by a Monte Carlo integral as above. As
   a sanity check (reproducing previous results), implement this case.

   #+NAME: gmvae
   #+BEGIN_SRC ipython
     class GMVAE(torch.nn.Module):
       def __init__(self, input_dim, latent_dim, n_clusters, hidden_dim=128):
         super().__init__()
         self.n_clusters = n_clusters
         self.sigma_raw = torch.nn.Parameter(torch.zeros([1]))
         self.encoder_y = DeepCategorical(input_dim, n_clusters, hidden_dim)
         self.encoder_z = DeepGaussian(input_dim + n_clusters, latent_dim, hidden_dim)
         self.decoder_z = DeepGaussian(n_clusters, latent_dim, hidden_dim)
         self.decoder_x = torch.nn.Sequential(
           FC(latent_dim, hidden_dim),
           torch.nn.Linear(hidden_dim, input_dim)
         )

       def forward(self, x, labels=None, writer=None, global_step=None, eps=1e-16):
         # [batch_size, n_clusters]
         logits = self.encoder_y.forward(x)
         probs = torch.nn.functional.softmax(logits, dim=1)
         # Important: accumulate negative ELBO
         loss = (probs * (torch.log(probs + eps) + torch.log(torch.tensor(self.n_clusters, dtype=torch.float)))).sum()
         assert not torch.isnan(loss)
         if writer is not None and labels is not None:
           # labels is one-hot [batch_size, n_clusters]
           with torch.no_grad():
             if labels.shape[1] == 2:
               writer.add_scalar('loss/cross_entropy_p', torch.nn.functional.binary_cross_entropy(probs, labels), global_step)
               writer.add_scalar('loss/cross_entropy_1p', torch.nn.functional.binary_cross_entropy(1 - probs, labels), global_step)
             writer.add_scalar('loss/cond_entropy', -(probs * torch.log(probs + eps)).mean(), global_step)
         y = torch.eye(self.n_clusters).cuda()
         # [n_clusters, latent_dim]
         prior_mean, prior_scale = self.decoder_z.forward(y)
         # Important: really marginalize over y
         for k in range(self.n_clusters):
           mean, scale = self.encoder_z.forward(torch.cat([x, y[k].repeat(x.shape[0], 1)], dim=1))
           # [batch_size, latent_dim]
           # TODO: n_samples > 1 breaks BatchNorm in decoder
           qz = torch.distributions.Normal(mean, scale).rsample()
           loss += (probs[:,k].reshape(-1, 1) * (torch.distributions.Normal(mean, scale).log_prob(qz) - torch.distributions.Normal(prior_mean[k], prior_scale[k]).log_prob(qz))).sum()
           assert not torch.isnan(loss)
           # [batch_size, input_dim]
           mu = self.decoder_x(qz)
           loss -= (torch.distributions.Normal(mu, torch.nn.functional.softplus(self.sigma_raw)).log_prob(x)).sum()
           assert not torch.isnan(loss)
           assert loss > 0
         if writer is not None:
           writer.add_scalar('loss/neg_elbo', loss, global_step)
         return loss

       def fit(self, data, n_epochs=100, log_dir=None, **kwargs):
         assert torch.cuda.is_available()
         self.cuda()
         if log_dir is not None:
           writer = tb.SummaryWriter(log_dir)
         else:
           writer = None
         opt = torch.optim.RMSprop(self.parameters(), **kwargs)
         global_step = 0
         for epoch in range(n_epochs):
           for batch in data:
             x = batch.pop(0).cuda()
             if batch:
               y = batch.pop(0).cuda()
             opt.zero_grad()
             loss = self.forward(x, labels=y, writer=writer, global_step=global_step)
             if torch.isnan(loss):
               raise RuntimeError('nan loss')
             loss.backward()
             opt.step()
             global_step += 1
         return self

       @property
       @torch.no_grad()
       def sigma(self):
         return torch.nn.functional.softplus(self.sigma_raw).cpu().numpy()

       @torch.no_grad()
       def get_labels(self, data):
         yhat = []
         for x, *_ in data:
           yhat.append(torch.nn.functional.softmax(fit.encoder_y.forward(x), dim=1).cpu().numpy())
         return np.vstack(yhat)

       @torch.no_grad()
       def get_latent(self, data):
         zhat = []
         y = torch.eye(self.n_clusters).cuda()
         for x, *_ in data:
           yhat = torch.nn.functional.softmax(self.encoder_y.forward(x), dim=1)
           zhat.append(torch.stack([yhat[:,k].reshape(-1, 1) * fit.encoder_z.forward(torch.cat([x, y[k].repeat(x.shape[0], 1)], dim=1))[0]
                                    for k in range(self.n_clusters)]).sum(dim=0).cpu().numpy())
         return np.vstack(zhat)

       @torch.no_grad()
       def predict(self, data):
         xhat = []
         y = torch.eye(self.n_clusters).cuda()
         for x, *_ in data:
           yhat = torch.nn.functional.softmax(self.encoder_y.forward(x), dim=1)
           zhat = torch.stack([yhat[:,k].reshape(-1, 1) * fit.encoder_z.forward(torch.cat([x, y[k].repeat(x.shape[0], 1)], dim=1))[0]
                               for k in range(fit.n_clusters)]).sum(dim=0)
           xhat.append(self.decoder_x(zhat).cpu().numpy())
         return np.vstack(xhat)
   #+END_SRC

   #+RESULTS: gmvae
   :RESULTS:
   # Out[181]:
   :END:

** Bernoulli GMVAE

   As a sanity check (applying to MNIST), implement the approach for the
   Bernoulli likelihood.

   \begin{align}
     x_{ij} \mid \vz_i &\sim \Bern((f(\vz_i))_j)\\
     \vz_i \mid y_i &\sim \N(m(y_i), s^2(y_i))\\
     y_i \mid \va &\sim \Mult(1, \va).
   \end{align}

   #+NAME: bgmvae
   #+BEGIN_SRC ipython
    class BGMVAE(torch.nn.Module):
      """GMVAE with Bernoulli likelihood"""
      def __init__(self, input_dim, latent_dim, n_clusters, hidden_dim=128):
        super().__init__()
        self.n_clusters = n_clusters
        self.encoder_y = DeepCategorical(input_dim, n_clusters, hidden_dim)
        self.encoder_z = DeepGaussian(input_dim + n_clusters, latent_dim, hidden_dim)
        self.decoder_z = DeepGaussian(n_clusters, latent_dim, hidden_dim)
        self.decoder_x = torch.nn.Sequential(
          FC(latent_dim, hidden_dim),
          torch.nn.Linear(hidden_dim, input_dim),
          torch.nn.Sigmoid()
        )

      def forward(self, x, labels=None, writer=None, global_step=None):
        # [batch_size, n_clusters]
        logits = self.encoder_y.forward(x)
        probs = torch.nn.functional.softmax(logits, dim=1)
        # Important: Negative ELBO
        loss = (probs * (torch.log(probs + 1e-16) + torch.log(torch.tensor(self.n_clusters, dtype=torch.float)))).sum()
        assert not torch.isnan(loss)
        if writer is not None and labels is not None:
          with torch.no_grad():
            cond_entropy = -(probs * torch.log(probs + 1e-16)).mean()
            writer.add_scalar('loss/cond_entropy', cond_entropy, global_step)
            acc = 0.
            for k in range(labels.shape[1]):
              query = torch.argmax(labels, dim=1) == k
              if query.any():
                idx = torch.argmax(probs[query].sum(dim=0))
                acc += (torch.argmax(probs[query], dim=1) == idx).sum()
            writer.add_scalar('loss/accuracy', acc / x.shape[0], global_step)
        # Assume cuda
        y = torch.eye(self.n_clusters).cuda()
        # [n_clusters, latent_dim]
        prior_mean, prior_scale = self.decoder_z.forward(y)
        for k in range(self.n_clusters):
          mean, scale = self.encoder_z.forward(torch.cat([x, y[k].repeat(x.shape[0], 1)], dim=1))
          # [batch_size, latent_dim]
          # TODO: n_samples > 1 breaks BatchNorm in decoder
          qz = torch.distributions.Normal(mean, scale).rsample()
          loss += (probs[:,k].reshape(-1, 1) * (torch.distributions.Normal(mean, scale).log_prob(qz) - torch.distributions.Normal(prior_mean[k], prior_scale[k]).log_prob(qz))).sum()
          assert not torch.isnan(loss)
          # [batch_size, input_dim]
          px = self.decoder_x(qz)
          loss -= (probs[:,k].reshape(-1, 1) * (x * torch.log(px + 1e-16) + (1 - x) * torch.log(1 - px + 1e-16))).sum()
          assert not torch.isnan(loss)
          assert loss > 0
        if writer is not None:
          writer.add_scalar('loss/neg_elbo', loss, global_step)
        return loss

      def fit(self, data, n_epochs=100, log_dir=None, **kwargs):
        self.cuda()
        if log_dir is not None:
          writer = tb.SummaryWriter(log_dir)
        else:
          writer = None
        opt = torch.optim.RMSprop(self.parameters(), **kwargs)
        global_step = 0
        for epoch in range(n_epochs):
          for x, y in data:
            # TODO: put entire MNIST on GPU in advance
            x = x.cuda()
            y = y.cuda()
            opt.zero_grad()
            loss = self.forward(x, labels=y, writer=writer, global_step=global_step)
            if torch.isnan(loss):
              raise RuntimeError('nan loss')
            loss.backward()
            opt.step()
            global_step += 1
        return self

      @torch.no_grad()
      def get_labels(self, data):
        yhat = []
        for x, *_ in data:
          x = x.cuda()
          yhat.append(torch.nn.functional.softmax(fit.encoder_y.forward(x), dim=1).cpu().numpy())
        return np.vstack(yhat)

      @torch.no_grad()
      def get_latent(self, data):
        zhat = []
        y = torch.eye(self.n_clusters).cuda()
        for x, *_ in data:
          x = x.cuda()
          yhat = torch.nn.functional.softmax(self.encoder_y.forward(x), dim=1)
          zhat.append(torch.stack([yhat[:,k].reshape(-1, 1) * fit.encoder_z.forward(torch.cat([x, y[k].repeat(x.shape[0], 1)], dim=1))[0]
                                   for k in range(fit.n_clusters)]).sum(dim=0).cpu().numpy())
        return np.vstack(zhat)
   #+END_SRC

   #+RESULTS: bgmvae
   :RESULTS:
   # Out[34]:
   :END:

** Poisson GMVAE

   Now implement the case we are interested in for scRNA-seq data, with Poisson
   measurement model (likelihood):

   \begin{align}
     \vx_i \mid \xiplus, \vz_i &\sim \Pois(\xiplus (\lambda(\vz_i))_j)
     \vz_i \mid y_i &\sim \N(m(y_i), s^2(y_i))\\
     y_i \mid \va &\sim \Mult(1, \va),
   \end{align}

   where

   - \(x_{ij}\) denotes the number of molecules of gene \(j = 1, \ldots, p\)
     observed in cell \(i = 1, \ldots, n\)
   - \(\xiplus \triangleq \sum_j x_{ij}\) denotes the total number of molecules
     observed in cell \(i\)
   - \(\lambda(\cdot)\) is a fully-connected neural network mapping latent
     variables to true gene expression

   #+NAME: pgmvae
   #+BEGIN_SRC ipython
     class PGMVAE(torch.nn.Module):
       def __init__(self, input_dim, latent_dim, n_clusters, hidden_dim=128):
         super().__init__()
         self.n_clusters = n_clusters
         self.encoder_y = DeepCategorical(input_dim, n_clusters, hidden_dim)
         self.encoder_z = DeepGaussian(input_dim + n_clusters, latent_dim, hidden_dim)
         self.decoder_z = DeepGaussian(n_clusters, latent_dim, hidden_dim)
         self.decoder_x = torch.nn.Sequential(
           FC(latent_dim, hidden_dim),
           torch.nn.Linear(hidden_dim, input_dim),
           torch.nn.Softplus()
         )

       def forward(self, x, s, labels=None, writer=None, global_step=None):
         # [batch_size, n_clusters]
         logits = self.encoder_y.forward(x)
         probs = torch.nn.functional.softmax(logits, dim=1)
         # Important: Negative ELBO
         loss = (probs * (torch.log(probs + 1e-16) + torch.log(torch.tensor(self.n_clusters, dtype=torch.float)))).sum()
         assert not torch.isnan(loss)
         if writer is not None and labels is not None:
           with torch.no_grad():
             writer.add_scalar('loss/cross_entropy', min(torch.nn.functional.binary_cross_entropy(probs, labels), torch.nn.functional.binary_cross_entropy(1 - probs, labels)), global_step)
             writer.add_scalar('loss/cond_entropy', -(probs * torch.log(probs + 1e-16)).mean(), global_step)
         # Assume cuda
         y = torch.eye(self.n_clusters).cuda()
         # [n_clusters, latent_dim]
         prior_mean, prior_scale = self.decoder_z.forward(y)
         for k in range(self.n_clusters):
           mean, scale = self.encoder_z.forward(torch.cat([x, y[k].repeat(x.shape[0], 1)], dim=1))
           # [batch_size, latent_dim]
           # TODO: n_samples > 1 breaks BatchNorm in decoder
           qz = torch.distributions.Normal(mean, scale).rsample()
           loss += (probs[:,k].reshape(-1, 1) * (torch.distributions.Normal(mean, scale).log_prob(qz) - torch.distributions.Normal(prior_mean[k], prior_scale[k]).log_prob(qz))).sum()
           assert not torch.isnan(loss)
           # [batch_size, input_dim]
           lam = self.decoder_x(qz)
           loss -= (probs[:,k].reshape(-1, 1) * (x * torch.log(s * lam) - s * lam - torch.lgamma(x + 1))).sum()
           assert not torch.isnan(loss)
           assert loss > 0
         if writer is not None:
           writer.add_scalar('loss/neg_elbo', loss, global_step)
         return loss

       def fit(self, data, n_epochs=100, log_dir=None, **kwargs):
         self.cuda()
         if log_dir is not None:
           writer = tb.SummaryWriter(log_dir)
         opt = torch.optim.RMSprop(self.parameters(), **kwargs)
         global_step = 0
         for epoch in range(n_epochs):
           for batch in data:
             assert len(batch) == 3
             x = batch.pop(0)
             s = batch.pop(0)
             if batch:
               y = batch.pop(0)
             else:
               y = None
             opt.zero_grad()
             loss = self.forward(x, s, labels=y, writer=writer, global_step=global_step)
             if torch.isnan(loss):
               raise RuntimeError('nan loss')
             loss.backward()
             opt.step()
             global_step += 1
         return self

       @torch.no_grad()
       def get_labels(self, data):
         yhat = []
         for x, *_ in data:
           yhat.append(torch.nn.functional.softmax(fit.encoder_y.forward(x), dim=1).cpu().numpy())
         return np.vstack(yhat)

       @torch.no_grad()
       def get_latent(self, data):
         zhat = []
         y = torch.eye(self.n_clusters).cuda()
         for x, *_ in data:
           yhat = torch.nn.functional.softmax(self.encoder_y.forward(x), dim=1)
           zhat.append(torch.stack([yhat[:,k].reshape(-1, 1) * fit.encoder_z.forward(torch.cat([x, y[k].repeat(x.shape[0], 1)], dim=1))[0]
                                    for k in range(fit.n_clusters)]).sum(dim=0).cpu().numpy())
         return np.vstack(zhat)
   #+END_SRC

   #+RESULTS: pgmvae
   :RESULTS:
   # Out[165]:
   :END:

* Results
** Pinwheel example

   Use the "pinwheel data" from Johnson et al. 2017
   ([[https://github.com/mattjj/svae/blob/master/experiments/gmm_svae_synth.py#L11][source
   code]] associated with Dilokthanakul et al. 2017).

   #+BEGIN_SRC ipython
     def make_pinwheel_data(radial_std, tangential_std, num_classes, num_per_class, rate, seed=0):
       rng = np.random.default_rng(seed)
       rads = np.linspace(0, 2 * np.pi, num_classes, endpoint=False)
       features = rng.normal(size=(num_classes * num_per_class, 2)) * np.array([radial_std, tangential_std])
       features[:,0] += 1.
       labels = np.repeat(np.arange(num_classes), num_per_class)
       angles = rads[labels] + rate * np.exp(features[:,0])
       rotations = np.stack([np.cos(angles), -np.sin(angles), np.sin(angles), np.cos(angles)])
       rotations = np.reshape(rotations.T, (-1, 2, 2))
       return np.einsum('ti,tij->tj', features, rotations), labels
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[4]:
   :END:

   Generate data following Dilokthanakul et al. 2017.

   #+BEGIN_SRC ipython :ipyfile figure/zinb-vae-gmm.org/pinwheel-ex.png
     n = 10000
     K = 5
     dat, labels = make_pinwheel_data(0.1, 0.05, K, n // K, .5, seed=0)
     cm = plt.get_cmap('Dark2')
     plt.clf()
     plt.gcf().set_size_inches(4, 3)
     for k in range(K):
       plt.scatter(dat[labels == k,0], dat[labels == k,1], s=1, color=cm(k), label=f'C{k}')
     plt.legend(frameon=False, loc='center left', bbox_to_anchor=(1, .5), handletextpad=0, markerscale=4)
     plt.xlabel('$x_1$')
     plt.ylabel('$x_2$')
     plt.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[81]:
   [[file:figure/zinb-vae-gmm.org/pinwheel-ex.png]]
   :END:

   Fit a GMM by EM, with oracle number of components.

   #+BEGIN_SRC ipython :async t
     m0 = skm.GaussianMixture(n_components=K, random_state=1).fit(dat)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[73]:
   :END:

   Plot the fit.

   #+BEGIN_SRC ipython :ipyfile figure/zinb-vae-gmm.org/pinwheel-ex-gmm.png
     cm = plt.get_cmap('Dark2')
     plt.clf()
     plt.gcf().set_size_inches(3.5, 3)
     lim = [-1.5, 1.5]
     x, y = np.meshgrid(np.linspace(*lim), np.linspace(*lim))
     for k in range(K):
       plt.scatter(dat[labels == k,0], dat[labels == k,1], s=1, color=cm(k), label=f'C{k}')
       f = st.multivariate_normal(m0.means_[k], m0.covariances_[k]).pdf(np.stack([x, y]).reshape(2, -1).T).reshape(50, 50)
       plt.contour(x, y, f, linewidths=0.5, levels=5, cmap=colorcet.cm['gray_r'])
     plt.legend(frameon=False, loc='center left', bbox_to_anchor=(1, .5), handletextpad=0, markerscale=4)
     plt.xlabel('$x_1$')
     plt.ylabel('$x_2$')
     plt.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[84]:
   [[file:figure/zinb-vae-gmm.org/pinwheel-ex-gmm.png]]
   :END:

   Fit GMVAE.

   #+BEGIN_SRC ipython :async t
     seed = 9
     batch_size = 64
     lr = 5e-4
     n_epochs = 10
     latent_dim = 2
     torch.manual_seed(seed)
     data = td.DataLoader(
       td.TensorDataset(
         torch.tensor(dat, device='cuda', dtype=torch.float),
         torch.tensor(pd.get_dummies(pd.Categorical(labels)).values, device='cuda', dtype=torch.float)),
       drop_last=True,
       shuffle=True,
       batch_size=batch_size)
     # Fix number of clusters to the ground truth
     fit = (GMVAE(input_dim=2, latent_dim=latent_dim, n_clusters=K)
            .fit(data,
                 lr=lr,
                 n_epochs=n_epochs,
                 # log_dir=f'runs/gmvae/pinwheel-{latent_dim}-{K}-{seed}-{lr:.1g}-{batch_size}-{n_epochs}'
            )
     )
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[182]:
   :END:

   Plot the predicted values.

   #+BEGIN_SRC ipython :ipyfile figure/zinb-vae-gmm.org/pinwheel-ex-gmvae-xhat.png
     data = td.DataLoader(
       td.TensorDataset(
         torch.tensor(dat, device='cuda', dtype=torch.float),
         torch.tensor(pd.get_dummies(pd.Categorical(labels)).values, device='cuda', dtype=torch.float)),
       batch_size=batch_size)
     xhat = fit.predict(data)

     cm = plt.get_cmap('Dark2')
     plt.clf()
     fig, ax = plt.subplots(1, 2, sharey=True)
     fig.set_size_inches(6, 3)
     for k in range(K):
       ax[0].scatter(dat[labels == k,0], dat[labels == k,1], s=1, color=cm(k), label=f'C{k}')
       ax[1].scatter(xhat[labels == k, 0], xhat[labels == k, 1], s=1, color=cm(k), label=f'C{k}')
     ax[0].set_title('Observed data')
     ax[1].set_title('Denoised data')
     ax[1].legend(frameon=False, loc='center left', bbox_to_anchor=(1, .5), handletextpad=0, markerscale=4)
     for a in ax:
       a.set_aspect('equal', adjustable='datalim')
       a.set_xlabel('$x_1$')
     ax[0].set_ylabel('$x_2$')
     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[183]:
   [[file:figure/zinb-vae-gmm.org/pinwheel-ex-gmvae-xhat.png]]
   :END:

   Assign labels to the estimated clusters using the most frequently occurring
   ground truth label, then assess clustering accuracy.

   #+BEGIN_SRC ipython :async t
     data = td.DataLoader(
       td.TensorDataset(
         torch.tensor(dat, device='cuda', dtype=torch.float),
         torch.tensor(pd.get_dummies(pd.Categorical(labels)).values, device='cuda', dtype=torch.float)),
       batch_size=batch_size)
     yhat = fit.get_labels(data)

     acc = 0
     for k in range(K):
       query = np.argmax(pd.get_dummies(pd.Categorical(labels)).values, axis=1) == k
       idx = np.argmax(yhat[query].sum(axis=0))
       acc += (np.argmax(yhat[query], axis=1) == idx).sum()
     acc / n
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[149]:
   : 0.2832
   :END:

   Plot the latent space.

   #+BEGIN_SRC ipython :ipyfile figure/zinb-vae-gmm.org/pinwheel-ex-gmvae.png
     cm = plt.get_cmap('Dark2')
     plt.clf()
     fig, ax = plt.subplots(1, 2, sharey=True)
     fig.set_size_inches(6, 3)

     lim = [-5, 5]
     x, y = np.meshgrid(np.linspace(*lim), np.linspace(*lim))
     with torch.no_grad():
       prior_mean, prior_scale = fit.decoder_z.forward(torch.eye(fit.n_clusters).cuda())
       prior_mean = prior_mean.cpu().numpy()
       prior_scale = prior_scale.cpu().numpy()
     for k in range(K):
       f = st.multivariate_normal(prior_mean[k], np.diag(prior_scale[k] ** 2)).pdf(np.stack([x, y]).reshape(2, -1).T).reshape(50, 50)
       ax[0].scatter(*prior_mean[k], marker='x', s=16, color=cm(k), label=f'C{k}')
       ax[0].contour(x, y, f, cmap=colorcet.cm['blues'])
     ax[0].set_title('Prior $p(z)$')
     ax[0].set_ylabel('$z_2$')

     data = td.DataLoader(
       td.TensorDataset(
         torch.tensor(dat, device='cuda', dtype=torch.float),
         torch.tensor(pd.get_dummies(pd.Categorical(labels)).values, device='cuda', dtype=torch.float)),
       batch_size=batch_size)
     zhat = fit.get_latent(data)
     for k in range(K):
       ax[1].scatter(zhat[labels == k,0], zhat[labels == k,1], s=1, color=cm(k), label=f'C{k}')
     ax[1].legend(frameon=False, loc='center left', bbox_to_anchor=(1, .5), handletextpad=0, markerscale=4)
     ax[1].set_title('Posterior $q(z \mid x)$')
     for a in ax:
       a.set_aspect('equal', adjustable='datalim')
       a.set_xlabel('$z_1$')
     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[185]:
   [[file:figure/zinb-vae-gmm.org/pinwheel-ex-gmvae.png]]
   :END:

** MNIST sanity check

   Replicate Rui Shu's results, modeling grayscale images of handwritten digits
   as binarized, flattened pixel vectors. Assume

   \begin{align}
     x_{ij} \mid \vz_i &\sim \Bern((r(\vz_i))_j)\\
     \vz_i \mid y_i &\sim \N(m(y_i), s^2(y_i))\\
     y_i \mid \va &\sim \Mult(1, \va),
   \end{align}

   where \(r\) is a fully-connected neural network mapping latent variables to
   pixel probabilities.

   #+BEGIN_SRC ipython :async t
     batch_size = 32
     mnist_data = torchvision.datasets.MNIST(
       root='/scratch/midway2/aksarkar/singlecell/',
       transform=lambda x: (np.frombuffer(x.tobytes(), dtype='uint8') > 0).astype(np.float32),
       target_transform=lambda x: np.eye(10)[x])
     data = td.DataLoader(mnist_data, batch_size=batch_size, shuffle=True,
                          pin_memory=True)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[33]:
   :END:

   #+BEGIN_SRC ipython :async t
     k = 10
     seed = 5
     lr = 5e-3
     n_epochs = 2
     latent_dim = 10
     torch.manual_seed(seed)
     fit = (BGMVAE(input_dim=mnist_data[0][0].shape[0], latent_dim=latent_dim, n_clusters=k)
            .fit(data,
                 lr=lr,
                 n_epochs=n_epochs,
                 log_dir=f'runs/pgmvae/mnist-{latent_dim}-{k}-{seed}-{lr:.1g}-{batch_size}-{n_epochs}')
     )
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[35]:
   :END:

   Get the approximate posterior distribution on labels.

   #+BEGIN_SRC ipython :async t
     data = td.DataLoader(mnist_data, batch_size=batch_size, shuffle=False,
                          pin_memory=True)
     yhat = fit.get_labels(data)
     y = np.vstack([label for _, label in mnist_data])
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[36]:
   :END:

   Assign labels to the estimated clusters using the most frequently occurring
   ground truth label, then assess clustering accuracy.

   #+BEGIN_SRC ipython :async t
     acc = 0
     for k in range(y.shape[1]):
       query = np.argmax(y, axis=1) == k
       idx = np.argmax(yhat[query].sum(axis=0))
       acc += (np.argmax(yhat[query], axis=1) == idx).sum()
       print(k, idx)
     acc / y.shape[0]
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[37]:
   : 0.6509666666666667
   :END:

** Two-way example

   Read sorted immune cell scRNA-seq data
   ([[https://dx.doi.org/10.1038/ncomms14049][Zheng et al. 2017]]).

   #+BEGIN_SRC ipython :async t
     dat = anndata.read_h5ad('/scratch/midway2/aksarkar/singlecell/mix4.h5ad')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[39]:
   :END:

   We [[file:nbmix.org::*(Full) 2-way example][previously applied the standard
   methodology]].

   #+BEGIN_SRC ipython :async t
     mix2 = dat[dat.obs['cell_type'].isin(['cytotoxic_t', 'b_cells'])]
     sc.pp.filter_genes(mix2, min_counts=1)
     sc.pp.pca(mix2, zero_center=False)
     sc.pp.neighbors(mix2)
     sc.tl.leiden(mix2)
     sc.tl.umap(mix2)
     mix2
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[40]:
   #+BEGIN_EXAMPLE
     AnnData object with n_obs × n_vars = 20294 × 17808
     obs: 'barcode', 'cell_type', 'leiden'
     var: 'ensg', 'name', 'n_cells', 'n_counts'
     uns: 'leiden', 'neighbors', 'pca', 'umap'
     obsm: 'X_pca', 'X_umap'
     varm: 'PCs'
     obsp: 'connectivities', 'distances'
   #+END_EXAMPLE
   :END:

   [[file:figure/nbmix.org/mix2.png]]

   Fit PGMVAE.

   #+BEGIN_SRC ipython
     assert torch.cuda.is_available()
     batch_size = 32
     x = mix2.X
     y = pd.get_dummies(mix2.obs['cell_type'], sparse=True).sparse.to_coo().tocsr()
     sparse_data = mpebpm.sparse.SparseDataset(
       mpebpm.sparse.CSRTensor(x.data, x.indices, x.indptr, x.shape, dtype=torch.float).cuda(),
       torch.tensor(x.sum(axis=1), dtype=torch.float).cuda(),
       mpebpm.sparse.CSRTensor(y.data, y.indices, y.indptr, y.shape, dtype=torch.float).cuda(),
     )
     collate_fn = getattr(sparse_data, 'collate_fn', td.dataloader.default_collate)
     data = td.DataLoader(sparse_data, batch_size=batch_size, shuffle=True,
                          collate_fn=collate_fn)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[41]:
   :END:

   #+BEGIN_SRC ipython :async t
     k = 2
     seed = 3
     lr = 5e-3
     n_epochs = 2
     latent_dim = 4
     torch.manual_seed(seed)
     fit = (PGMVAE(input_dim=mix2.shape[1], latent_dim=latent_dim, n_clusters=k)
            .fit(data,
                 lr=lr,
                 n_epochs=n_epochs,
                 log_dir=f'runs/pgmvae/mix2-{latent_dim}-{k}-{seed}-{lr:.1g}-{batch_size}-{n_epochs}')
     )
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[60]:
   :END:

   Get the approximate posterior labels.

   #+BEGIN_SRC ipython :async t
     data = td.DataLoader(sparse_data, batch_size=batch_size, shuffle=False,
                          collate_fn=collate_fn)
     yhat = fit.get_labels(data)
     mix2.obs['comp'] = np.argmax(yhat, axis=1)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[61]:
   :END:

   Plot the result.

   #+BEGIN_SRC ipython :ipyfile figure/zinb-vae-gmm.org/mix2-pgmvae.png
     plt.clf()
     fig, ax = plt.subplots(1, 2, sharex=True, sharey=True)
     fig.set_size_inches(5, 3)
     for a, k, t, cm in zip(ax, ['cell_type', 'comp'], ['Ground truth', 'PGMVAE'], ['Paired', 'Dark2']):
       for i, c in enumerate(mix2.obs[k].unique()):
         a.plot(*mix2[mix2.obs[k] == c].obsm["X_umap"].T, c=plt.get_cmap(cm)(i), marker='.', ms=1, lw=0, alpha=0.1, label=f'{k}_{i}')
       leg = a.legend(frameon=False, markerscale=8, handletextpad=0, loc='upper center', bbox_to_anchor=(.5, -.25), ncol=2)
       for h in leg.legendHandles:
         h._legmarker.set_alpha(1)
       a.set_xlabel('UMAP 1')
       a.set_title(t)
     ax[0].set_ylabel('UMAP 2')
     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[62]:
   [[file:figure/zinb-vae-gmm.org/mix2-pgmvae.png]]
   :END:

   Get the latent representation.

   #+BEGIN_SRC ipython :async t
     zhat = fit.get_latent(data)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[63]:
   :END:

   Plot a UMAP of the learned representation.

   #+BEGIN_SRC ipython :async t
     u = umap.UMAP(n_neighbors=5, n_components=2, metric='euclidean').fit_transform(zhat)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[64]:
   :END:

   #+BEGIN_SRC ipython :ipyfile figure/zinb-vae-gmm.org/mix2-zhat.png
     plt.clf()
     fig, ax = plt.subplots(1, 2, sharex=True, sharey=True)
     fig.set_size_inches(5, 3)
     for a, k, t, cm in zip(ax, ['cell_type', 'comp'], ['Ground truth', 'PGMVAE'], ['Paired', 'Dark2']):
       for i, c in enumerate(mix2.obs[k].unique()):
         a.plot(*u[mix2.obs[k] == c].T, c=plt.get_cmap(cm)(i), marker='.', ms=1, lw=0, alpha=0.1, label=f'{k}_{i}')
       leg = a.legend(frameon=False, markerscale=8, handletextpad=0, loc='upper center', bbox_to_anchor=(.5, -.25), ncol=2)
       for h in leg.legendHandles:
         h._legmarker.set_alpha(1)
       a.set_xlabel('UMAP 1 ($\hat{z}$)')
       a.set_title(t)
     ax[0].set_ylabel('UMAP 2 ($\hat{z}$)')
     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[65]:
   [[file:figure/zinb-vae-gmm.org/mix2-zhat.png]]
   :END:

* Related work

   scVI ([[https://dx.doi.org/10.1038/s41592-018-0229-2][Lopez et al. 2018]],
   [[https://www.biorxiv.org/content/10.1101/532895v2][Xu et al. 2020]])
   implements a deep unsupervised (more precisely, semi-supervised) clustering
   model ([[https://arxiv.org/abs/1406.5298][Kingma et al. 2014]],
   [[https://arxiv.org/abs/1611.02648][Dilokthanakul et al. 2016]]).

   \begin{align}
     x_{ij} \mid s_i, \lambda_{ij} &\sim \Pois(s_i \lambda_{ij})\\
     \ln s_i &\sim \N(\cdot)\\
     \lambda_{ij} \mid \vz_i &\sim \Gam(\phi_j^{-1}, (\mu_{\lambda}(\vz_i))_j^{-1} \phi_j^{-1})\\
     \vz_i \mid y_i, \vu_i &\sim \N(\mu_z(\vu_i, y_i), \diag(\sigma^2(\vu_i, y_i)))\\
     y_i \mid \vpi &\sim \Mult(1, \vpi)\\
     \vu_i &\sim \N(0, \mi).
   \end{align}

   where

   - \(y_i\) denotes the cluster assignment for cell \(i\)
   - \(\mu_z(\cdot), \sigma^2(\cdot)\) are neural networks mapping the latent
     cluster variable \(y_i\) and Gaussian noise \(\vu_i\) to the latent variable
     \(\vz_i\)
   - \(\mu_{\lambda}(\cdot)\) is a neural network mapping latent variable \(\vz_i\) to
     latent gene expression \(\vlambda_{i}\)

   To perform variational inference in this model, Lopez et al. introduce
   inference networks

   \begin{align}
     q(\vz_i \mid \vx_i) &= \N(\cdot)\\
     q(y_i \mid \vz_i) &= \Mult(1, \cdot).
   \end{align}

   #+BEGIN_SRC ipython :async t
     import scvi.dataset
     import scvi.inference
     import scvi.models

     expr = scvi.dataset.AnnDatasetFromAnnData(temp)
     m = scvi.models.VAEC(expr.nb_genes, n_batch=0, n_labels=2)
     train = scvi.inference.UnsupervisedTrainer(m, expr, train_size=1, batch_size=32, show_progbar=False, n_epochs_kl_warmup=100)
     train.train(n_epochs=1000, lr=1e-2)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[125]:
   :END:

   # https://scvi.readthedocs.io/en/stable/scvi.models.html#scvi.models.SCANVI
   # https://scvi.readthedocs.io/en/stable/tutorials/scanvi.html?highlight=scanvi#EVF-/-not-EVF

   #+BEGIN_SRC ipython
     post = train.create_posterior(train.model, expr)
     _, _, label = post.get_latent()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[118]:
   :END:

   The main difference in our approach is that we do not make an assumption of
   Gamma perturbations in latent gene expression. Instead, we implicitly assume
   that transcriptional noise is structured, e.g. through the gene-gene
   regulatory network, such that it is reasonable to model it in the low
   dimensional space. This assumption simplifies the model in terms of both
   inference and implementation.
