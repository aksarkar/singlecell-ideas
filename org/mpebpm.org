#+TITLE: Massively Parallel Empirical Bayes Poisson Means
#+SETUPFILE: setup.org

* Introduction

  The /Empirical Bayes Poisson Means/ (EBPM) problem is \( 
  \DeclareMathOperator\Gam{Gamma}
  \DeclareMathOperator\Poi{Poisson}
  \DeclareMathOperator\argmin{arg min}
  \newcommand\mf{\mathbf{F}} 
  \newcommand\ml{\mathbf{L}}
  \newcommand\mx{\mathbf{X}}
  \newcommand\vl{\mathbf{l}}
  \newcommand\vx{\mathbf{x}}
  \)

  \begin{align*}
    x_i \mid s_i, \lambda_i &\sim \Poi(s_i \lambda_i)\\
    \lambda_i &\sim g(\cdot) \in \mathcal{G},
  \end{align*}

  where the (primary) inference goal is to estimate \(g\) by maximizing the
  likelihood. In our prior work
  ([[https://dx.doi.org/10.1371/journal.pgen.1008045][Sarkar et al. 2019]]), we
  used this approach to estimate the mean and variance of gene expression
  across a homogeneous sample of cells within each of a number of donor
  individuals, where we assumed \(\mathcal{G}\) was the family of point-Gamma
  distributions. In total, we solved 537,678 EBPM problems in parallel by
  formulating them as a single factor model

  \begin{align*}
    x_{ij} &\sim \Poi(x_{i+} \lambda_{ij})\\
    \lambda_{ij} &\sim \pi_{ij} \delta_0(\cdot) + (1 - \pi_{ij}) \Gam(\phi_{ij}^{-1}, \mu_{ij}^{-1} \phi_{ij}^{-1})\\
    \ln \mu_{ij} &= (\ml \mf_\mu')_{ij}\\
    \ln \phi_{ij} &= (\ml \mf_\phi')_{ij}\\
    \operatorname{logit} \pi_{ij} &= (\ml \mf_\pi')_{ij},
  \end{align*}

  where 

  - \(x_{ij}\) is the number of molecules of gene \(j = 1, \ldots, p\) observed
    in cell \(i = 1, \ldots, n\)
  - \(x_{i+} \triangleq \sum_j x_{ij}\) is the total number of molecules
    observed in sample \(i\)
  - cells are taken from \(m\) donor individuals, \(\ml\) is \(n \times m\),
    each \(\mf_{(\cdot)}\) is \(p \times m\)
  - assignments of cells to donors (loadings) \(l_{ik} \in \{0, 1\}, k = 1,
    \ldots, m\) are known and fixed.

  We previously implemented maximum likelihood inference of this model via
  batch gradient descent in the Python package
  [[https://www.github.com/aksarkar/scqtl.git][scqtl]]. We have now developed a
  new Python package [[https://www.github.com/aksarkar/mpebpm.git][mpebpm]],
  which should scale inference to much larger data sets. The key improvements
  are optimization using minibatch gradient descent and support for sparse
  matrices. Here, we evaluate the method on simulations.
* Setup
  :PROPERTIES:
  :CUSTOM_ID: setup
  :END:

  #+BEGIN_SRC emacs-lisp :exports none
    (add-to-list 'python-shell-completion-native-disabled-interpreters "jupyter")
    (org-babel-lob-ingest "/home/aksarkar/.emacs.d/org-templates/library.org")
  #+END_SRC

  #+RESULTS:
  : 1

  #+CALL: ipython3(venv="singlecell",partition="gpu2",opts="--gres=gpu:1",memory="16G") :exports none :dir /scratch/midway2/aksarkar/singlecell/

  #+RESULTS:
  : Submitted batch job 66467062

  #+BEGIN_SRC ipython
    import anndata
    import mpebpm
    import numpy as np
    import pandas as pd
    import scipy.sparse as ss
    import scipy.special as sp
    import scipy.stats as st
    import scqtl
    import time
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[2]:
  :END:

  #+BEGIN_SRC ipython
    %matplotlib inline
    %config InlineBackend.figure_formats = set(['retina'])
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[3]:
  :END:

  #+BEGIN_SRC ipython
    import matplotlib.pyplot as plt
    plt.rcParams['figure.facecolor'] = 'w'
    plt.rcParams['font.family'] = 'Nimbus Sans'
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[4]:
  :END:

* Results
** Accuracy of parameter estimation

   We previously evaluated ~scqtl~
   [[https://jdblischak.github.io/singlecell-qtl/zinb.html#orgca44e7f][by
   simulating data from the model]].

   #+BEGIN_SRC ipython
     def evaluate(num_samples, num_mols, num_trials=10, **kwargs):
       # Important: generate all of the samples for each trial in one shot, and use
       # one-hot encoding to get separate estimates
       args = [(num_samples * num_trials, num_mols, log_mu, log_phi, logodds, None, None, None)
               for log_mu in np.linspace(-12, -6, 7)
               for log_phi in np.linspace(-4, 0, 5)
               for logodds in np.linspace(-3, 3, 7)]
       x = np.concatenate([scqtl.simulation.simulate(*a)[0][:,:1] for a in args], axis=1)
       x = ss.csr_matrix(x)
       s = num_mols * np.ones((x.shape[0], 1))
       onehot = np.zeros((num_samples * num_trials, num_trials))
       onehot[np.arange(onehot.shape[0]), np.arange(onehot.shape[0]) // num_samples] = 1
       onehot = ss.csr_matrix(onehot)

       log_mu, neg_log_phi, logodds, _ = mpebpm.ebpm_point_gamma(x, s=s, onehot=onehot, **kwargs)
       result = pd.DataFrame(
         [(a[0] // num_trials, int(a[1]), int(a[2]), int(a[3]), int(a[4]), a[-1], trial)
          for a in args
          for trial in range(num_trials)],
         columns=['num_samples', 'num_mols', 'log_mu', 'log_phi', 'logodds', 'fold', 'trial'])
       result['mean'] = np.exp(result['log_mu'])
       result['var'] = (1 - sp.expit(result['logodds'])) * np.exp(2 * result['log_mu'] + result['log_phi']) + sp.expit(-result['logodds']) * (1 - sp.expit(result['logodds'])) * np.exp(2 * result['log_mu'])

       result['log_mu_hat'] = log_mu.ravel(order='F')
       result['log_phi_hat'] = -neg_log_phi.ravel(order='F')
       result['logodds_hat'] = logodds.ravel(order='F')
       result['mean_hat'] = np.exp(result['log_mu_hat'])
       result['var_hat'] = (1 - sp.expit(result['logodds_hat'])) * np.exp(2 * result['log_mu_hat'] + result['log_phi_hat']) + sp.expit(-result['logodds_hat']) * (1 - sp.expit(result['logodds_hat'])) * np.exp(2 * result['log_mu_hat'])

       diagnostic = []
       for i in range(x.shape[1]):
         for j in range(onehot.shape[1]):
           idx = onehot.A[:,j].astype(bool)
           diagnostic.append(scqtl.diagnostic.diagnostic_test(
             x.A[idx,i].reshape(-1, 1),
             log_mu[j,i],
             -neg_log_phi[j,i],
             -logodds[j,i],
             num_mols,
             np.ones((num_samples, 1))))
       diagnostic = np.array(diagnostic)
       result['ks_d'] = diagnostic[:,0]
       result['ks_p'] = diagnostic[:,1]
       return result
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[53]:
   :END:

   Run the simulation.

   #+BEGIN_SRC ipython :async t
     result = evaluate(num_samples=100, num_mols=int(1e5), batch_size=32, max_epochs=200, verbose=True)
     result.to_csv('/scratch/midway2/aksarkar/ideas/mpebpm-sim.txt.gz', sep='\t')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[94]:
   :END:

   Read the results.

   #+BEGIN_SRC ipython
     result = pd.read_csv('/scratch/midway2/aksarkar/ideas/mpebpm-sim.txt.gz', sep='\t', index_col=0)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[7]:
   :END:

   Plot the estimated values against the ground truth values.

   #+BEGIN_SRC ipython :ipyfile figure/mpebpm.org/sim-params.png
     mu_pass = result['log_mu'] > -10
     pi_pass = result['logodds'] < 0

     plt.clf()
     fig, ax = plt.subplots(2, 3)
     fig.set_size_inches(8, 5)

     subset = result.loc[pi_pass]
     ax[0, 0].scatter(subset['log_mu'], subset['log_mu_hat'], s=2, c='k')
     ax[0, 0].set_xlim(-14, -5)
     ax[0, 0].set_ylim(ax[0, 0].get_xlim())
     ax[0, 0].plot(ax[0, 0].get_xlim(), ax[0, 0].get_xlim(), c='r', ls=':', lw=1)
     ax[0, 0].set_xlabel('True $\ln(\mu)$')
     ax[0, 0].set_ylabel('Estimated $\ln(\mu)$')

     ax[1, 0].set_xscale('log')
     ax[1, 0].set_yscale('log')
     ax[1, 0].scatter(subset['mean'], subset['mean_hat'], s=2, c='k')
     ax[1, 0].set_xlim(1e-6, 1e-2)
     ax[1, 0].set_ylim(ax[1, 0].get_xlim())
     ax[1, 0].plot(ax[1, 0].get_xlim(), ax[1, 0].get_xlim(), c='r', ls=':', lw=1)
     ax[1, 0].set_xlabel('True latent mean')
     ax[1, 0].set_ylabel('Estimated latent mean')

     subset = result.loc[np.logical_and.reduce(np.vstack([mu_pass, pi_pass]))]
     ax[0, 1].scatter(subset['log_phi'], subset['log_phi_hat'], s=2, c='k')
     ax[0, 1].set_xlim(-5, 2)
     ax[0, 1].set_ylim(ax[0, 1].get_xlim())
     ax[0, 1].plot(ax[0, 1].get_xlim(), ax[0, 1].get_xlim(), c='r', ls=':', lw=1)
     ax[0, 1].set_xlabel('True $\ln(\phi)$')
     ax[0, 1].set_ylabel('Estimated $\ln(\phi)$')

     ax[1, 1].set_xscale('log')
     ax[1, 1].set_yscale('log')
     ax[1, 1].scatter(subset['var'], subset['var_hat'], s=2, c='k')
     ax[1, 1].set_xlim(1e-9, 5e-5)
     ax[1, 1].set_ylim(ax[1, 1].get_xlim())
     ax[1, 1].plot(ax[1, 1].get_xlim(), ax[1, 1].get_xlim(), c='r', ls=':', lw=1)
     ax[1, 1].set_xlabel('True latent variance')
     ax[1, 1].set_ylabel('Estimated latent variance')

     subset = result.loc[pi_pass]
     ax[0, 2].scatter(subset['logodds'], subset['logodds_hat'], s=2, c='k')
     ax[0, 2].plot(ax[0, 2].get_xlim(), ax[0, 2].get_xlim(), c='r', ls=':', lw=1)
     ax[0, 2].set_xlabel('True $\mathrm{logit}(\pi)$')
     ax[0, 2].set_ylabel('Estimated $\mathrm{logit}(\pi)$')

     ax[1, 2].set_axis_off()
     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[95]:
   [[file:figure/mpebpm.org/sim-params.png]]
   :END:

** Goodness of fit

   We previously developed a test for goodness of fit, based on the fact that
   if \(x_{ij} \sim F_{ij}\), then \(F_{ij}(x_{ij}) \sim
   \operatorname{Uniform}(0, 1)\). We applied this test to the distributions
   estimated from the simulated data sets. Plot the histogram of
   goodness-of-fit \(p\)-values.

   #+BEGIN_SRC ipython :ipyfile figure/mpebpm.org/mpebpm-sim-gof.png
     plt.clf()
     plt.gcf().set_size_inches(2, 2)
     plt.hist(result['ks_p'], bins=np.linspace(0, 1, 11), density=True, color='0.7')
     plt.axhline(y=1, lw=1, ls=':', c='k')
     plt.xlim(0, 1)
     plt.xlabel('$p$-value')
     plt.ylabel('Density')
     plt.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[96]:
   [[file:figure/mpebpm.org/mpebpm-sim-gof.png]]
   :END:

   Report the number (proportion) of simulation trials where the observed data
   significantly depart from the estimated distribution (\(p < 0.05\) after
   Bonferroni correction).

   #+BEGIN_SRC ipython
     sig = result.loc[result['ks_p'] < 0.05 / result.shape[0]]
     sig.shape[0], sig.shape[0] / result.shape[0]
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[97]:
   : (0, 0.0)
   :END:

** Application to iPSCs

   We previously generated scRNA-seq of 5,597 cells from 54 donors (Sarkar et
   al. 2019). Read the data.

   #+BEGIN_SRC ipython
     x = anndata.read_h5ad('/project2/mstephens/aksarkar/projects/singlecell-ideas/data/ipsc/ipsc.h5ad')
     x.X
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[5]:
   #+BEGIN_EXAMPLE
     <5597x9957 sparse matrix of type '<class 'numpy.float32'>'
     with 39654337 stored elements in Compressed Sparse Row format>
   #+END_EXAMPLE
   :END:

   Prepare the data.

   #+BEGIN_SRC ipython
     # Important: the dense data will fit on the GPU
     y = x.X.A
     s = x.obs['mol_hs'].values.reshape(-1, 1)
     onehot = ss.coo_matrix((np.ones(x.shape[0]), (np.arange(x.shape[0]), pd.Categorical(x.obs['chip_id']).codes))).tocsr()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[7]:
   :END:

   Fit ~mpebpm~, and report the time elapsed in minutes.

   #+BEGIN_SRC ipython :async t
     start = time.time()
     log_mu, neg_log_phi, logodds, _ = mpebpm.ebpm_point_gamma(
       y,
       s=s,
       onehot=onehot,
       batch_size=64,
       lr=1e-2,
       max_epochs=80,
       verbose=True)
     elapsed = time.time() - start
     elapsed / 60
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[8]:
   : 4.631162142753601
   :END:

   For comparison, this analysis previously required 395 minutes using
   ~scqtl~. Estimate the full data log likelihood.

   #+BEGIN_SRC ipython :async t
     mean = s.ravel() * onehot @ np.exp(log_mu)
     inv_disp = onehot @ np.exp(neg_log_phi)
     (y * np.log(mean / inv_disp)
      - y * np.log(1 + mean / inv_disp)
      - inv_disp * np.log(1 + mean / inv_disp)
      # Important: these terms are why we use inverse dispersion
      + sp.gammaln(y + inv_disp)
      - sp.gammaln(inv_disp)
      - sp.gammaln(y + 1)).mean()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[23]:
   : -163.68023324162735
   :END:

   Read the previously estimated parameters, and the full data log likelihood.

   #+BEGIN_SRC ipython
     log_mu0 = pd.read_table("/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design0/zi2-log-mu.txt.gz", index_col=0, sep=' ')
     log_phi0 = pd.read_table("/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design0/zi2-log-phi.txt.gz", index_col=0, sep=' ')
     logodds0 = pd.read_table("/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design0/zi2-logodds.txt.gz", index_col=0, sep=' ')
   #+END_SRC

   #+BEGIN_SRC ipython
     mean0 = s.ravel() * onehot @ np.exp(log_mu0.values.T)
     inv_disp0 = onehot @ np.exp(-log_phi0.values.T)
     (y * np.log(mean0 / inv_disp0)
      - y * np.log(1 + mean0 / inv_disp0)
      - inv_disp0 * np.log(1 + mean0 / inv_disp0)
      + sp.gammaln(y + inv_disp0)
      - sp.gammaln(inv_disp0)
      - sp.gammaln(y + 1)).mean()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[24]:
   : -394.82726690970867
   :END:

   Plot the fitted values of the two methods against each other.

   #+BEGIN_SRC ipython :ipyfile figure/mpebpm.org/mpebpm-ipsc.png
     plt.clf()
     fig, ax = plt.subplots(1, 3)
     fig.set_size_inches(7, 2.5)

     ax[0].scatter(log_mu.ravel(order='F'), log_mu0.values.ravel(), s=1, c='k', alpha=0.1)
     ax[0].set_xlim(ax[0].get_ylim())
     ax[0].plot(ax[0].get_xlim(), ax[0].get_ylim(), lw=1, ls=':', c='r')
     ax[0].set_xlabel('MPEBPM est $\log(\mu)$')
     ax[0].set_ylabel('scqtl est $\log(\mu)$')

     ax[1].scatter(-neg_log_phi.ravel(order='F'), log_phi0.values.ravel(), s=1, c='k', alpha=0.1)
     ax[1].set_xlim(ax[1].get_ylim())
     ax[1].plot(ax[1].get_xlim(), ax[1].get_ylim(), lw=1, ls=':', c='r')
     ax[1].set_xlabel('MPEBPM est $\log(\phi)$')
     ax[1].set_ylabel('scqtl est $\log(\phi)$')

     ax[2].scatter(logodds.ravel(order='F'), logodds0.values.ravel(), s=1, c='k', alpha=0.1)
     ax[2].set_xlim(ax[2].get_ylim())
     ax[2].plot(ax[2].get_xlim(), ax[2].get_ylim(), lw=1, ls=':', c='r')
     ax[2].set_xlabel('MPEBPM est $\mathrm{logit}(\pi)$')
     ax[2].set_ylabel('scqtl est $\mathrm{logit}(\pi)$')

     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[19]:
   [[file:figure/mpebpm.org/mpebpm-ipsc.png]]
   :END:

** Application to Census of Immune Cells

   The
   [[https://data.humancellatlas.org/explore/projects/cc95ff89-2e68-4a08-a234-480eca21ce79][Census
   of Immune Cells]] is part of the Human Cell Atlas. Currently, it comprises
   scRNA-seq data of 593,844 cells from 16 donors. To demonstrate the scalability of
   ~mpebpm~, fit a Gamma distribution to each gene in each donor.

   Read the sparse data and report its dimensions.

   #+BEGIN_SRC ipython :async t
     y_csr = ss.load_npz('/scratch/midway2/aksarkar/modes/immune-cell-census.npz')
     y_csr
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[56]:
   #+BEGIN_EXAMPLE
     <593844x16002 sparse matrix of type '<class 'numpy.int32'>'
     with 551836788 stored elements in Compressed Sparse Row format>
   #+END_EXAMPLE
   :END:

   Read the metadata. Construct the sparse onehot matrix.

   #+BEGIN_SRC ipython
     genes = pd.read_csv('/scratch/midway2/aksarkar/modes/immune-cell-census-genes.txt.gz', sep='\t', index_col=0)
     donor = pd.Categorical(pd.read_csv('/scratch/midway2/aksarkar/modes/immune-cell-census-samples.txt.gz', sep='\t', index_col=0)['0'])
     onehot = ss.coo_matrix((np.ones(donor.shape[0]).astype(np.float32), (np.arange(donor.shape[0]), donor.codes))).tocsr()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[5]:
   :END:

   Fit ~mpebpm~, and report the time elapsed.

   #+BEGIN_SRC ipython :async t
     start = time.time()
     log_mu, neg_log_phi, _ = mpebpm.ebpm_point_gamma(y_csr, onehot=onehot, batch_size=32, lr=1e-3, max_epochs=10)
     elapsed = time.time() - start
     elapsed
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   0 - 46db089c-cf33-486a-b52c-67b9de1b42fa
   :END:
