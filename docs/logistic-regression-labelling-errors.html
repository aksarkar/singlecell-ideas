<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2021-07-21 Wed 21:45 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Logistic regression in the presence of error-prone labels</title>
<meta name="generator" content="Org mode">
<meta name="author" content="Abhishek Sarkar">
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link href="bootstrap.min.css" rel="stylesheet" />
<link rel="stylesheet" type="text/css" href="htmlize.css"/>
<link rel="stylesheet" type="text/css" href="main.css"/>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2017 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Logistic regression in the presence of error-prone labels</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgf22c2b3">Introduction</a></li>
<li><a href="#setup">Setup</a></li>
<li><a href="#org513ce30">Methods</a>
<ul>
<li><a href="#orgc039e51">Maximum likelihood estimation</a></li>
<li><a href="#org34436e8">Approximate Bayesian inference</a></li>
</ul>
</li>
<li><a href="#org0e8fc9e">Results</a>
<ul>
<li><a href="#org18caaf4">Simulation</a></li>
<li><a href="#org7a529a2">Naive analysis</a></li>
<li><a href="#org4fa0a5d">Maximum likelihood estimation</a></li>
<li><a href="#orgfde3581">Approximate Bayesian inference</a></li>
</ul>
</li>
<li><a href="#org858706d">Related work</a></li>
</ul>
</div>
</div>

<div id="outline-container-orgf22c2b3" class="outline-2">
<h2 id="orgf22c2b3">Introduction</h2>
<div class="outline-text-2" id="text-orgf22c2b3">
<p>
Logistic regression can be written
(<a href="https://www.routledge.com/Generalized-Linear-Models/McCullagh-Nelder/p/book/9780412317606">McCullagh and Nelder 1989</a>) \(
  \DeclareMathOperator\Bern{Bernoulli}
  \DeclareMathOperator\E{E}
  \DeclareMathOperator\Exp{Exp}
  \DeclareMathOperator\KL{\mathcal{KL}}
  \DeclareMathOperator\N{\mathcal{N}}
  \DeclareMathOperator\S{sigmoid}
  \DeclareMathOperator\Unif{Uniform}
  \DeclareMathOperator\logit{logit}
  \newcommand\mi{\mathbf{I}}
  \newcommand\mx{\mathbf{X}}
  \newcommand\vb{\mathbf{b}}
  \newcommand\vtheta{\boldsymbol{\theta}}
  \newcommand\vx{\mathbf{x}}
  \newcommand\vy{\mathbf{y}}
  \)
</p>

\begin{align}
  y_i \mid p_i &\sim \Bern(p_i)\\
  \logit(p_i) &\triangleq \eta_i = \vx_i' \vb,
\end{align}

<p>
where \(y_i \in \{0, 1\}\) denotes the label of observation \(i = 1, \ldots,
  n\), \(\vx_i\) denotes the \(p\)-dimensional feature vector for observation
\(i\), and \(\vb\) is a \(p\)-dimensional vector of coefficients. In words,
under this model the features \(x_{ij}\) (\(j = 1, \ldots, p\)) have linear
effects \(b_j\) on the log odds ratio \(\eta_i\) of the label \(y_i\)
equalling 1. One is primarily interested in estimating \(\vb\), in order to
explain which features are important in determining the label \(y_i\), and to
predict labels for future observations. Estimation can be done by a variety
of approaches, such as maximizing the log likelihood
</p>

\begin{equation}
  \ell = \sum_i y_i \ln \S(\vx_i' \vb) + (1 - y_i) \ln \S(-\vx_i' \vb)
\end{equation}

<p>
with respect to \(\vb\), or by assuming a prior \(p(\vb)\) and estimating the
posterior \(p(\vb \mid \mx, \vy)\) using MCMC or variational inference.
</p>

<p>
<i>Remark.</i> Note that the log likelihood equals the negative of the cross
entropy loss.
</p>

<p>
A central assumption in using the model above to analyze data is that the
labels \(y_i\) do not have errors. This is not a safe assumption to make in
some settings, e.g., in associating genetic variants with diseases for which
diagnosis is known to be imperfect
(<a href="https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-020-3387-z">Shafquat
et al. 2020</a>). One approach to account for error-prone labels is to build a
more complex model that relates the observed feature vectors \(\vx_i\), the
true (unobserved) labels \(z_i\), and the observed labels \(y_i\)
</p>

\begin{align}
  y_i \mid z_i = 1, \theta_1 &\sim \Bern(1 - \theta_1)\\
  y_i \mid z_i = 0, \theta_0 &\sim \Bern(\theta_0)\\
  z_i \mid p_i &\sim \Bern(p_i)\\
  \logit(p_i) &= \vx_i' \vb.
\end{align}

<p>
In this model, \(\vtheta = (\theta_0, \theta_1)\) denotes error
probabilities. Specifically, \(\theta_1\) denotes the probability of
observing label \(y_i = 0\) when the true label \(z_i = 1\), and \(\theta_0\)
denotes the probability of observing \(y_i = 1\) when \(z_i = 0\). (One could
simplify by fixing \(\theta_0 = \theta_1\), or potentially build more complex
models where the error rates themselves depend on features in the data.)
</p>

<p>
<i>Remark.</i> This model is identifable up to a sign flip in \(\vb\) and
corresponding &ldquo;reflection&rdquo; of \(\vtheta\) (to mean probability of observing
the correct label). If one places a prior on \(\vtheta\) that does not have
density \(> 0.5\), then the model is identifable.
</p>

<p>
As was the case for logistic regression without error-prone labels, one
estimates \((\vb, \vtheta)\) from observed data by maximizing the log
likelihood, or by assuming a prior \(p(\vb, \vtheta)\) and estimating the
posterior \(p(\vb, \vtheta \mid \mx, \vy)\). However, unlike the simpler
model, one can use these estimates to infer the true labels (and thus
identify mislabelled data points) from the data, e.g., by estimating \(p(z_i
  = 1 \mid \mx, \vy)\).
</p>
</div>
</div>

<div id="outline-container-orgce0ed18" class="outline-2">
<h2 id="setup"><a id="orgce0ed18"></a>Setup</h2>
<div class="outline-text-2" id="text-setup">
<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> numpy <span class="org-keyword">as</span> np
<span class="org-keyword">import</span> pandas <span class="org-keyword">as</span> pd
<span class="org-keyword">import</span> pyro
<span class="org-keyword">import</span> scipy.linalg <span class="org-keyword">as</span> sl
<span class="org-keyword">import</span> scipy.special <span class="org-keyword">as</span> sp
<span class="org-keyword">import</span> scipy.stats <span class="org-keyword">as</span> st
<span class="org-keyword">import</span> sklearn.linear_model <span class="org-keyword">as</span> sklm
<span class="org-keyword">import</span> sklearn.metrics <span class="org-keyword">as</span> skm
<span class="org-keyword">import</span> sklearn.model_selection <span class="org-keyword">as</span> skms
<span class="org-keyword">import</span> sklearn.utils <span class="org-keyword">as</span> sku
<span class="org-keyword">import</span> torch
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">%matplotlib inline
%config <span class="org-variable-name">InlineBackend.figure_formats</span> = <span class="org-builtin">set</span>([<span class="org-string">'retina'</span>])
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> matplotlib.pyplot <span class="org-keyword">as</span> plt
<span class="org-variable-name">plt.rcParams</span>[<span class="org-string">'figure.facecolor'</span>] = <span class="org-string">'w'</span>
<span class="org-variable-name">plt.rcParams</span>[<span class="org-string">'font.family'</span>] = <span class="org-string">'Nimbus Sans'</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-org513ce30" class="outline-2">
<h2 id="org513ce30">Methods</h2>
<div class="outline-text-2" id="text-org513ce30">
</div>
<div id="outline-container-orgc039e51" class="outline-3">
<h3 id="orgc039e51">Maximum likelihood estimation</h3>
<div class="outline-text-3" id="text-orgc039e51">
<p>
One can use an EM algorithm to maximize the log likelihood
(<a href="https://doi.org/10.1007/978-1-4612-5771-4_13">Ekholm and Palmgren 1982</a>,
<a href="https://academic.oup.com/aje/article/146/2/195/165276">Magder and Hughes
1997</a>)
</p>

\begin{equation}
  \ell = \sum_i \ln \left(\sum_{z_i \in \{0, 1\}} p(y_i \mid z_i, \vtheta)\, p(z_i \mid \vx_i, \vb)\right)
\end{equation}

<p>
with respect to \((\vb, \vtheta)\). 
</p>

<p>
<i>Remark.</i> In the case where \(p > n\), one can add an \(\ell_1\) and/or
\(\ell_2\) penalty on \(\vb\) to the log likelihood to regularize the
problem, and modify the M-step update appropriately.
</p>

<p>
We briefly outline the derivation of the algorithm. The joint probability
</p>

\begin{multline}
  \ln p(y_i, z_i \mid \vx_i, \vb, \vtheta) = [z_i = 1]\underbrace{\left(y_i \ln (1 - \theta_1) + (1 - y_i)\ln \theta_1 + \ln \S(\vx_i' \vb)\right)}_{\triangleq \alpha_i}\\
  + [z_i = 0]\underbrace{\left(y_i \ln \theta_0 + (1 - y_i) \ln (1 - \theta_0) + \ln \S(-\vx_i' \vb)\right)}_{\triangleq \beta_i},
\end{multline}

<p>
where \([\cdot]\) denotes the
<a href="https://en.wikipedia.org/wiki/Iverson_bracket">Iverson bracket</a>. The
posterior
</p>

\begin{equation}
  p(z_i = 1 \mid \vx_i, y_i, \vb, \vtheta) \triangleq \phi_i = \frac{\exp(\alpha_i)}{\exp(\alpha_i) + \exp(\beta_i)},
\end{equation}

<p>
and the expected log joint probability with respect to the posterior is
</p>

\begin{multline}
  h = \sum_i \Big(\phi_i \left(y_i \ln (1 - \theta_1) + (1 - y_i)\ln \theta_1 + \ln \S(\vx_i' \vb)\right)\\
  + (1 - \phi_i) \left(y_i \ln \theta_0 + (1 - y_i) \ln (1 - \theta_0) + \ln \S(-\vx_i' \vb)\right) \Big).
\end{multline}

<p>
The EM algorithm maximizes \(h\) by alternating E-steps and M-steps, which
is equivalent to maximizing \(\ell\)
(<a href="https://link.springer.com/chapter/10.1007%2F978-94-011-5014-9_12">Neal
and Hinton 1998</a>). In the E-step, one updates \(\phi_i\) as described
above. In the M-step, one updates
</p>

\begin{align}
  \logit(\theta_1) &:= \ln\left(\frac{\sum_i \phi_i (1 - y_i)}{\sum_i \phi_i y_i}\right)\\
  \logit(\theta_0) &:= \ln\left(\frac{\sum_i (1 - \phi_i) y_i}{\sum_i (1 - \phi_i) (1 - y_i)}\right),
\end{align}

<p>
and updates \(\vb\) by maximizing the expected log joint
probability. Considering only terms of \(h\) that depend on \(\vb\), the
objective function is
</p>

\begin{equation}
  \sum_i \phi_i \ln \S(\vx_i' \vb) + (1 - \phi_i) \ln \S(-\vx_i' \vb)
\end{equation}

<p>
which is the negative cross entropy loss with labels \(\phi_i\). Therefore,
\(\vb\) can be estimated by logistic regression as a subroutine.
</p>

<p>
<i>Remark.</i> In the case that one penalizes the log likelihood, the penalty
terms only enter the M step in the updates to \(\vb\). In this case, the M
step update to \(\vb\) involves a penalized logistic regression, which is
available in standard packages.
</p>

<p>
One iterates the E and M steps alternately until e.g., the change in
objective falls below some threshold. Intuitively, this algorithm alternates
between estimating the true labels given the current estimates of the
regression coefficients and error rates (E step), and estimating the
regression coefficients and error rates given the current estimates of the
true labels (M step). Crucially, all of the data is used to estimate the
regression coefficients and error rates, which makes it possible to detect
that an observed label is unlikely given the observed features and estimated
error rates, by borrowing information from the other observations. From the
estimated \(\hat{\vb}\), one can predict the true label \(\tilde{z}\) for a
new observation \((\tilde{\vx}, \tilde{y})\) as
</p>

\begin{equation}
  \E[\tilde{z}_i] = \S(\tilde{\vx}' \hat{\vb}).
\end{equation}

<div class="org-src-container">
<pre class="src src-ipython" id="org7234a65"><span class="org-keyword">def</span> <span class="org-function-name">logistic_regression_errors_em</span>(x, y, b=<span class="org-constant">None</span>, logits=<span class="org-constant">None</span>, atol=1e-5,
                                  max_epochs=100, update_logits=<span class="org-constant">True</span>,
                                  verbose=<span class="org-constant">True</span>, **kwargs):
  <span class="org-doc">"""Fit logistic regression with error-prone labels by maximum likelihood</span>

<span class="org-doc">  x - features, array-like (n, p)</span>
<span class="org-doc">  y - labels, array-like (n, 1)</span>
<span class="org-doc">  b - initial value for coefficients, array-like (p, 1)</span>
<span class="org-doc">  logits - initial value for error logits, array-like (2, 1)</span>
<span class="org-doc">  atol - tolerance for convergence (&gt; 0)</span>
<span class="org-doc">  max_epochs - maximum number of EM iterations (&gt; 0)</span>
<span class="org-doc">  update_logits - update initial estimates of error logits</span>
<span class="org-doc">  verbose - print EM updates</span>
<span class="org-doc">  kwargs - keyword arguments to logistic regression subroutine</span>

<span class="org-doc">  """</span>
  <span class="org-comment-delimiter"># </span><span class="org-comment">TODO: this needs lots more error checking of args</span>
  <span class="org-keyword">if</span> logits <span class="org-keyword">is</span> <span class="org-constant">None</span>:
    <span class="org-variable-name">logits</span> = np.array([-7, -7], dtype=np.<span class="org-builtin">float</span>)
  <span class="org-keyword">if</span> b <span class="org-keyword">is</span> <span class="org-constant">None</span>:
    <span class="org-variable-name">b</span> = np.zeros((x.shape[1], 1))
  <span class="org-variable-name">z</span> = _logistic_regression_errors_update_z(x, y, b, logits)
  <span class="org-variable-name">obj</span> = _logistic_regression_errors_obj(x, y, z, b, logits)
  <span class="org-keyword">if</span> verbose:
    <span class="org-keyword">print</span>(f<span class="org-string">'{0:&gt;8d} {obj:.10g}'</span>)
  <span class="org-keyword">for</span> i <span class="org-keyword">in</span> <span class="org-builtin">range</span>(max_epochs):
    <span class="org-keyword">if</span> update_logits:
      <span class="org-comment-delimiter"># </span><span class="org-comment">eq. 12-14</span>
      <span class="org-variable-name">logits</span>[0] = np.log(((1 - z) * y).<span class="org-builtin">sum</span>() / ((1 - z) * (1 - y)).<span class="org-builtin">sum</span>())
      <span class="org-variable-name">logits</span>[1] = np.log((z * (1 - y)).<span class="org-builtin">sum</span>() / (z * y).<span class="org-builtin">sum</span>())
    <span class="org-variable-name">theta</span> = sp.expit(logits)
    <span class="org-comment-delimiter"># </span><span class="org-comment">Important: the sklearn solver approximately maximizes the objective,</span>
    <span class="org-comment-delimiter"># </span><span class="org-comment">which leads to weird issues</span>
    <span class="org-variable-name">b</span> = _logistic_regression_irls(x, z)
    <span class="org-variable-name">z</span> = _logistic_regression_errors_update_z(x, y, b, logits)
    <span class="org-variable-name">update</span> = _logistic_regression_errors_obj(x, y, z, b, logits)
    <span class="org-keyword">if</span> verbose:
      <span class="org-keyword">print</span>(f<span class="org-string">'{i + 1:&gt;8d} {update:.10g}'</span>)
    <span class="org-keyword">if</span> <span class="org-keyword">not</span> np.isfinite(update):
      <span class="org-keyword">raise</span> <span class="org-type">RuntimeError</span>(<span class="org-string">'invalid objective function value'</span>)
    <span class="org-keyword">elif</span> <span class="org-builtin">abs</span>(update - obj) &lt; atol:
      <span class="org-keyword">return</span> b, logits
    <span class="org-keyword">else</span>:
      <span class="org-variable-name">obj</span> = update
  <span class="org-keyword">else</span>:
    <span class="org-keyword">raise</span> <span class="org-type">RuntimeError</span>(<span class="org-string">'failed to converge in max_epochs'</span>)

<span class="org-keyword">def</span> <span class="org-function-name">_logistic_regression_errors_obj_comp</span>(x, y, b, logits):
  <span class="org-doc">"""Return 'mixture components' of the log joint probability"""</span>
  <span class="org-variable-name">theta</span> = sp.expit(logits)
  <span class="org-variable-name">alpha</span> = (y * np.log(1 - theta[1])
           + (1 - y) * np.log(theta[1])
           + np.log(sp.expit(x @ b)))
  <span class="org-variable-name">beta</span> = (y * np.log(theta[0])
          + (1 - y) * np.log(1 - theta[0])
          + np.log(sp.expit(-x @ b)))
  <span class="org-keyword">return</span> alpha, beta

<span class="org-keyword">def</span> <span class="org-function-name">_logistic_regression_errors_update_z</span>(x, y, b, logits):
  <span class="org-doc">"""Return new value of E[z]"""</span>
  <span class="org-variable-name">alpha</span>, <span class="org-variable-name">beta</span> = _logistic_regression_errors_obj_comp(x, y, b, logits)
  <span class="org-variable-name">z</span> = sp.softmax(np.hstack([alpha, beta]), axis=1)[:,0].reshape(-1, 1)
  <span class="org-keyword">assert</span> z.shape == alpha.shape
  <span class="org-keyword">return</span> z

<span class="org-keyword">def</span> <span class="org-function-name">_logistic_regression_errors_obj</span>(x, y, z, b, logits):
  <span class="org-doc">"""Return expected log joint probability"""</span>
  <span class="org-variable-name">alpha</span>, <span class="org-variable-name">beta</span> = _logistic_regression_errors_obj_comp(x, y, b, logits)
  <span class="org-variable-name">obj</span> = (z * alpha + (1 - z) * beta).<span class="org-builtin">sum</span>()
  <span class="org-keyword">return</span> obj

<span class="org-keyword">def</span> <span class="org-function-name">_logistic_regression_irls</span>(x, y, b=<span class="org-constant">None</span>, tol=1e-7, max_epochs=1000):
  <span class="org-doc">"""Fit logistic regression by iterative reweighted least squares"""</span>
  <span class="org-variable-name">mu</span> = np.log(y.mean() / (1 - y.mean()))
  <span class="org-keyword">if</span> b <span class="org-keyword">is</span> <span class="org-constant">None</span>:
    <span class="org-variable-name">b</span> = np.zeros((x.shape[1], 1))
  <span class="org-variable-name">llik</span> = -np.inf
  <span class="org-keyword">for</span> i <span class="org-keyword">in</span> <span class="org-builtin">range</span>(max_epochs):
    <span class="org-variable-name">eta</span> = mu + x @ b
    <span class="org-variable-name">mu</span> = sp.expit(eta)
    <span class="org-variable-name">w</span> = mu * (1 - mu)
    <span class="org-variable-name">z</span> = eta + (y - mu) / w
    <span class="org-variable-name">b</span> = sl.solve(x.T @ np.diag(w.ravel()) @ x, x.T @ np.diag(w.ravel()) @ z)
    <span class="org-variable-name">update</span> = (y * np.log(sp.expit(x @ b)) + (1 - y) * np.log(sp.expit(-x @ b))).<span class="org-builtin">sum</span>()
    <span class="org-keyword">if</span> <span class="org-builtin">abs</span>(update - llik) &lt; tol:
      <span class="org-keyword">return</span> b
    <span class="org-keyword">else</span>:
      <span class="org-variable-name">llik</span> = update
  <span class="org-keyword">else</span>:
    <span class="org-keyword">raise</span> <span class="org-type">RuntimeError</span>(<span class="org-string">'failed to converge in max_epochs'</span>)
</pre>
</div>
</div>
</div>

<div id="outline-container-org34436e8" class="outline-3">
<h3 id="org34436e8">Approximate Bayesian inference</h3>
<div class="outline-text-3" id="text-org34436e8">
<p>
If one assumes a prior \(p(\vb, \vtheta)\), then one can estimate the
posterior
</p>

\begin{equation}
  p(\vb, \vtheta \mid \mx, \vy) = \frac{\sum_i \sum_{z_i \in \{0, 1\}} p(y_i \mid z_i, \vtheta) p(z_i \mid \vx_i, \vb) p(\vb, \vtheta)}{\int \sum_i \sum_{z_i \in \{0, 1\}} p(y_i \mid z_i, \vtheta) p(z_i \mid \vx_i, \vb)\, dp(\vb, \vtheta)}.
\end{equation}

<p>
The integral in the denominator is intractable; however, approximate
inference via MCMC can be readily implemented in inference engines such as
<a href="https://mc-stan.org/">Stan</a>, <a href="https://pyro.ai/">pyro</a>, or
<a href="https://www.tensorflow.org/probability">Tensorflow Probability</a>. One
could alternatively use variational inference to find the best approximate
posterior in a tractable family, by minimizing the KL divergence between the
approximate posterior and the true posterior. From the estimated posterior,
one can estimate the posterior distribution of the true label \(\tilde{z}\)
for a new data point \((\tilde{\vx}, \tilde{y})\), given the observed
(training) data, as
</p>

\begin{equation}
  p(\tilde{z} \mid \tilde{\vx}, \tilde{y}, \mx, \vy) = \int p(\tilde{y} \mid \tilde{z}, \vtheta)\, p(\tilde{z} \mid \tilde{\vx}, \vb)\, dp(\vb, \vtheta \mid \mx, \vy).
\end{equation}

<p>
The primary derived quantity of interest for prediction is the posterior
mean \(\E[\tilde{z} \mid \tilde{\vx}, \tilde{y}]\). For simplicity, assume
proper priors
</p>

\begin{align}
  \theta_k &\sim \Unif(0, 0.5)\\
  \vb &\sim \N(\boldsymbol{0}, \lambda \mi)\\
  \lambda &\sim \Exp(1).
\end{align}

<div class="org-src-container">
<pre class="src src-ipython" id="org76d100b"><span class="org-keyword">def</span> <span class="org-function-name">logistic_regression_errors_nuts</span>(x, y, num_samples=100, warmup_steps=100,
                                    verbose=<span class="org-constant">True</span>):
  <span class="org-doc">"""Fit logistic regression with error-prone labels using No U-Turn Sampler</span>

<span class="org-doc">  x - features, array-like (n, p)</span>
<span class="org-doc">  y - labels, array-like (n, 1)</span>
<span class="org-doc">  num_samples - number of posterior samples to return</span>
<span class="org-doc">  warmup_steps - number of initial posterior samples (not returned)</span>
<span class="org-doc">  verbose - report progress</span>

<span class="org-doc">  """</span>

  <span class="org-variable-name">nuts</span> = pyro.infer.mcmc.NUTS(_logistic_regression_errors)
  <span class="org-variable-name">samples</span> = pyro.infer.mcmc.MCMC(nuts, num_samples=num_samples,
                                 warmup_steps=warmup_steps)
  samples.run(torch.tensor(x, dtype=torch.<span class="org-builtin">float</span>),
              torch.tensor(y.ravel(), dtype=torch.<span class="org-builtin">float</span>))
  <span class="org-keyword">return</span> samples

<span class="org-keyword">def</span> <span class="org-function-name">_logistic_regression_errors</span>(x, y):
  <span class="org-doc">"""Return stochastic computation graph for the generative model"""</span>
  <span class="org-variable-name">theta</span> = pyro.sample(<span class="org-string">'theta'</span>, pyro.distributions.Uniform(
    low=torch.zeros(2),
    high=0.5 * torch.ones(2)))
  <span class="org-variable-name">lam</span> = pyro.sample(<span class="org-string">'lam'</span>, pyro.distributions.Exponential(rate=1.))
  <span class="org-variable-name">b</span> = pyro.sample(<span class="org-string">'b'</span>, pyro.distributions.MultivariateNormal(
    loc=torch.zeros(x.shape[1]),
    covariance_matrix=lam * torch.eye(x.shape[1])))
  <span class="org-variable-name">eta</span> = x @ b
  <span class="org-keyword">with</span> pyro.plate(<span class="org-string">'data'</span>, x.shape[0]):
    <span class="org-variable-name">probs</span> = (1 - theta[1]) * torch.sigmoid(eta) + theta[0] * torch.sigmoid(-eta)
    <span class="org-variable-name">y</span> = pyro.sample(<span class="org-string">'y'</span>, pyro.distributions.Bernoulli(probs=probs), obs=y)
  <span class="org-keyword">return</span> y
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-org0e8fc9e" class="outline-2">
<h2 id="org0e8fc9e">Results</h2>
<div class="outline-text-2" id="text-org0e8fc9e">
</div>
<div id="outline-container-org18caaf4" class="outline-3">
<h3 id="org18caaf4">Simulation</h3>
<div class="outline-text-3" id="text-org18caaf4">
<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">simulate</span>(n, p, n_causal, theta0, theta1, seed=0):
  <span class="org-variable-name">rng</span> = np.random.default_rng(seed)
  <span class="org-variable-name">x</span> = rng.normal(size=(n, p))
  <span class="org-variable-name">b</span> = np.zeros((p, 1))
  <span class="org-variable-name">b</span>[:n_causal] = rng.normal(size=(n_causal, 1))
  <span class="org-variable-name">z</span> = rng.uniform(size=(n, 1)) &lt; sp.expit(x @ b)
  <span class="org-variable-name">u</span> = rng.uniform(size=(n, 1))
  <span class="org-variable-name">y</span> = np.where(z, u &lt; (1 - theta1), u &lt; theta0)
  <span class="org-keyword">return</span> x, y, z, b
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">x</span>, <span class="org-variable-name">y</span>, <span class="org-variable-name">z</span>, <span class="org-variable-name">b</span> = simulate(n=1000, p=10, n_causal=3, theta0=0.05, theta1=0.05)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Dark2'</span>)
plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 2, sharey=<span class="org-constant">True</span>)
fig.set_size_inches(5, 2.5)
<span class="org-variable-name">eta</span> = x @ b
<span class="org-keyword">for</span> k <span class="org-keyword">in</span> <span class="org-builtin">range</span>(2):
  <span class="org-variable-name">jitter</span> = np.random.normal(scale=0.01, size=(z == k).<span class="org-builtin">sum</span>())
  ax[0].scatter(eta[z == k], jitter + z[z == k], s=1, c=cm(k), alpha=0.25)
  ax[1].scatter(eta[(z == k) &amp; (y == z)], jitter[(y == z)[z == k]] + y[(z == k) &amp; (y == z)], s=1, c=cm(k), alpha=0.25)
  ax[1].scatter(eta[(z == k) &amp; (y != z)], jitter[(y != z)[z == k]] + y[(z == k) &amp; (y != z)], s=8, marker=<span class="org-string">'x'</span>, c=cm(k), alpha=1)
<span class="org-keyword">for</span> a <span class="org-keyword">in</span> ax:
  a.set_xlabel(<span class="org-string">'Linear predictor'</span>)
ax[0].set_ylabel(<span class="org-string">'True label'</span>)
ax[1].set_ylabel(<span class="org-string">'Observed label'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/logistic-regression-labelling-errors.org/sim-ex.png" alt="sim-ex.png">
</p>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">xt</span>, <span class="org-variable-name">xv</span>, <span class="org-variable-name">yt</span>, <span class="org-variable-name">yv</span>, <span class="org-variable-name">zt</span>, <span class="org-variable-name">zv</span> = skms.train_test_split(x, y, z, test_size=0.1)
</pre>
</div>
</div>
</div>

<div id="outline-container-org7a529a2" class="outline-3">
<h3 id="org7a529a2">Naive analysis</h3>
<div class="outline-text-3" id="text-org7a529a2">
<p>
Fit logistic regression to predict the observed labels from the features,
and report the accuracy in the validation data.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">fit0</span> = _logistic_regression_irls(xt, yt)
<span class="org-variable-name">pv</span> = sp.expit(xv @ fit0)
pd.Series({
  <span class="org-string">'observed'</span>: skm.roc_auc_score(yv, pv),
  <span class="org-string">'true'</span>: skm.roc_auc_score(zv, pv)}, name=<span class="org-string">'roc_auc'</span>)
</pre>
</div>

<pre class="example">
observed    0.852525
true        0.909675
Name: roc_auc, dtype: float64
</pre>
</div>
</div>

<div id="outline-container-org4fa0a5d" class="outline-3">
<h3 id="org4fa0a5d">Maximum likelihood estimation</h3>
<div class="outline-text-3" id="text-org4fa0a5d">
<p>
Fit the model to the simulated data by maximizing the (unpenalized)
likelihood. First, initialize at the oracle values, fix the error rates, and
report the accuracy in the validation data.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">oracle_fit_em</span> = logistic_regression_errors_em(
  xt, yt, b=b, logits=sp.logit([0.05, 0.05]), update_logits=<span class="org-constant">False</span>, tol=1e-7,
  max_epochs=1000)
<span class="org-variable-name">pv</span> = sp.expit(xv @ oracle_fit_em[0])
pd.Series({
  <span class="org-string">'observed'</span>: skm.roc_auc_score(yv, pv),
  <span class="org-string">'true'</span>: skm.roc_auc_score(zv, pv)}, name=<span class="org-string">'roc_auc'</span>)
</pre>
</div>

<pre class="example">
observed    0.856970
true        0.912485
Name: roc_auc, dtype: float64
</pre>

<p>
Report the accuracy of predicting the mislabeled data points in the
validation data.
</p>

<div class="org-src-container">
<pre class="src src-ipython">skm.roc_auc_score(yv != zv, np.where(yv, 1 - pv, pv))
</pre>
</div>

<pre class="example">
0.9219858156028369

</pre>

<p>
Now repeat the analysis, updating the error rates.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">oracle_fit_em</span> = logistic_regression_errors_em(
  xt, yt, b=b, logits=sp.logit([0.05, 0.05]), tol=1e-7, max_epochs=1000)
<span class="org-variable-name">pv</span> = sp.expit(xv @ oracle_fit_em[0])
pd.Series({
  <span class="org-string">'observed'</span>: skm.roc_auc_score(yv, pv),
  <span class="org-string">'true'</span>: skm.roc_auc_score(zv, pv)}, name=<span class="org-string">'roc_auc'</span>)
</pre>
</div>

<pre class="example">
observed    0.854141
true        0.911281
Name: roc_auc, dtype: float64
</pre>

<p>
Report the accuracy of predicting the mislabeled data points in the
validation data.
</p>

<div class="org-src-container">
<pre class="src src-ipython">skm.roc_auc_score(yv != zv, np.where(yv, 1 - pv, pv))
</pre>
</div>

<pre class="example">
0.927304964539007

</pre>

<p>
Now, repeat the analysis, but initialize \(\vb\) at the naive solution, and
initialize \(\vtheta\) at the default \(\logit(-7) = 10^{-3}\).
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">fit_em</span> = logistic_regression_errors_em(xt, yt, b=fit0, tol=1e-7,
                                       max_epochs=1000, verbose=<span class="org-constant">True</span>)
<span class="org-variable-name">pv</span> = sp.expit(xv @ fit_em[0])
pd.Series({
  <span class="org-string">'observed'</span>: skm.roc_auc_score(yv, pv),
  <span class="org-string">'true'</span>: skm.roc_auc_score(zv, pv)}, name=<span class="org-string">'roc_auc'</span>)
</pre>
</div>

<pre class="example">
observed    0.854141
true        0.911281
Name: roc_auc, dtype: float64
</pre>

<p>
Report the point estimates of the error rates.
</p>

<div class="org-src-container">
<pre class="src src-ipython">sp.expit(fit_em[1])
</pre>
</div>

<pre class="example">
array([1.38606116e-02, 1.94704025e-10])

</pre>

<p>
Report the accuracy of predicting the mislabeled data points in the
validation data.
</p>

<div class="org-src-container">
<pre class="src src-ipython">skm.roc_auc_score(yv != zv, np.where(yv, 1 - pv, pv))
</pre>
</div>

<pre class="example">
0.927304964539007

</pre>
</div>
</div>

<div id="outline-container-orgfde3581" class="outline-3">
<h3 id="orgfde3581">Approximate Bayesian inference</h3>
<div class="outline-text-3" id="text-orgfde3581">
<p>
Sample from the posterior (1.5 minutes).
</p>

<div class="org-src-container">
<pre class="src src-ipython">torch.manual_seed(1)
<span class="org-variable-name">samples</span> = logistic_regression_errors_nuts(xt, yt, num_samples=2000, warmup_steps=1000)
</pre>
</div>

<p>
Report the 95% credible intervals for the error rates.
</p>

<div class="org-src-container">
<pre class="src src-ipython">pd.DataFrame(np.percentile(samples.get_samples()[<span class="org-string">'theta'</span>], [2.5, 97.5], axis=0).T)
</pre>
</div>

<pre class="example">
0         1
0  0.044269  0.161897
1  0.015789  0.155854
</pre>

<p>
Report the AUROC of predicting the true labels in the training data.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">xs</span> = torch.tensor(xt, dtype=torch.<span class="org-builtin">float</span>)
<span class="org-variable-name">ys</span> = torch.tensor(yt, dtype=torch.<span class="org-builtin">float</span>)
<span class="org-variable-name">pt</span> = np.stack([(((1 - tt[1]) ** ys * tt[1] ** (1 - ys)).ravel() * torch.sigmoid(xs @ bt)).numpy()
               <span class="org-keyword">for</span> tt, bt <span class="org-keyword">in</span> <span class="org-builtin">zip</span>(samples.get_samples()[<span class="org-string">'theta'</span>],
                                 samples.get_samples()[<span class="org-string">'b'</span>])]).mean(axis=0)
skm.roc_auc_score(zt, pt)
</pre>
</div>

<pre class="example">
0.9751495423583979

</pre>

<p>
Report the AUROC of predicting the true labels in the validation data.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">xs</span> = torch.tensor(xv, dtype=torch.<span class="org-builtin">float</span>)
<span class="org-variable-name">ys</span> = torch.tensor(yv, dtype=torch.<span class="org-builtin">float</span>)
<span class="org-variable-name">pv</span> = np.stack([(((1 - tt[1]) ** ys * tt[1] ** (1 - ys)).ravel() * torch.sigmoid(xs @ bt)).numpy()
               <span class="org-keyword">for</span> tt, bt <span class="org-keyword">in</span> <span class="org-builtin">zip</span>(samples.get_samples()[<span class="org-string">'theta'</span>],
                                 samples.get_samples()[<span class="org-string">'b'</span>])]).mean(axis=0)
skm.roc_auc_score(zv, pv)
</pre>
</div>

<pre class="example">
0.984

</pre>

<p>
Report the accuracy of predicting the mislabeled data points in the
validation data.
</p>

<div class="org-src-container">
<pre class="src src-ipython">skm.roc_auc_score(yv != zv, np.where(yv.ravel(), 1 - pv, pv))
</pre>
</div>

<pre class="example">
0.7057387057387057

</pre>
</div>
</div>
</div>

<div id="outline-container-org858706d" class="outline-2">
<h2 id="org858706d">Related work</h2>
<div class="outline-text-2" id="text-org858706d">
<ul class="org-ul">
<li>Tenenbein, A. (1970). A double sampling scheme for estimating from binomial
data with misclassifications. J. Am. Statist. Assoc. 65. 1350-1361.</li>

<li>Hochberg, Y. (1977). On the use of double sampling schemes in analyzing
categorical data with misclassification
errors. J. Am. Statist. Assoc. 72. 914-921</li>

<li>Chen, T.T. (1979). Log-linear models for categorical data with
misclassification and double sampling. J. Am. Statist. Assoc. 74. 481-488.</li>

<li>Ekholm, A., &amp; Palmgren, J. (1982). A Model for a Binary Response with
Misclassifications. Lecture Notes in Statistics,
128–143. <a href="https://doi.org/10.1007/978-1-4612-5771-4_13">https://doi.org/10.1007/978-1-4612-5771-4_13</a></li>

<li>Espeland, M.A. and Odoroff, C.L. (1985). Log-linear models for doubly sampled
categorical data fitted by the EM
algorithm. J. Am. Statist. Assoc. 80. 663-670</li>

<li>Espeland, M.A. and Hui, S.L. (1987). A general approach to analyzing
epidemiologic data that contain misclassification
errors. Biometrics 43. 1001-1012</li>

<li>Palmgren, J. and Ekholm, A. (1987), Exponential family non-linear models
for categorical data with errors of observation. Appl. Stochastic Models
Data Anal., 3: 111-124. <a href="https://doi.org/10.1002/asm.3150030206">https://doi.org/10.1002/asm.3150030206</a></li>

<li>Zhi Geng &amp; Chooichiro Asano. (1989) Bayesian estimation methods for
categorical data with misclassifications, Communications in Statistics -
Theory and Methods, 18:8,
2935-2954. <a href="https://doi.org/10.1080/03610928908830069">https://doi.org/10.1080/03610928908830069</a></li>

<li>Evans, M., Guttman, I., Haitovsky, Y., and Swartz, T. (1996). Bayesian
analysis of binary data subject to misclassiﬁcation. In Bayesian Analysis
in Statistics and Econometrics: Essays in Honor of Arnold Zellner,
D. Berry, K. Chaloner, and J. Geweke (eds), 67–77. New York: North Holland.</li>

<li>Mendoza-Blanco, J. R., Tu, X. M., and Iyengar, S. (1996). Bayesian
inference on prevalence using a missing-data approach with simulation-based
techniques: Applications to HIV screening. Statistics in Medicine 15,
2161–2176.</li>

<li>Magder LS, Hughes JP. Logistic regression when the outcome is measured with
uncertainty. Am J Epidemiol. 1997;146(2):195–203.</li>

<li>Rekaya, R., Weigel, K. A., and Gianola, D. (2001). Threshold model for
misclassiﬁed binary responses with applications to animal
breeding. Biometrics 57, 1123–1129.</li>

<li>Paulino, C.D., Soares, P. and Neuhaus, J. (2003), Binomial Regression
with Misclassification. Biometrics, 59:
670-675. <a href="https://doi.org/10.1111/1541-0420.00077">https://doi.org/10.1111/1541-0420.00077</a></li>

<li>Duffy SW, Warwick J, Williams AR, Keshavarz H, Kaffashian F, Rohan TE, Nili
F, Sadeghi-Hassanabadi A. A simple model for potential use with a
misclassified binary outcome in epidemiology. J Epidemiol Community
Health. 2004;58(8):712–7.</li>

<li>Prescott GJ, Garthwaite PH. A Bayesian approach to prospective binary
outcome studies with misclassification in a binary risk factor. Stat
Med. 2005;24(22):3463–77.</li>

<li>Hofler M. The effect of misclassification on the estimation of association:
a review. Int J Methods Psychiatr Res. 2005;14(2):92–101.</li>

<li>Smith, S., Hay, E.H., Farhat, N. et al. Genome wide association studies in
presence of misclassified binary responses. BMC Genet 14, 124
(2013). <a href="https://doi.org/10.1186/1471-2156-14-124">https://doi.org/10.1186/1471-2156-14-124</a></li>

<li>Rekaya R, Smith S, Hay el H, Aggrey SE. Misclassification in binary
responses and effect on genome-wide association studies. Poult Sci
2013;92(9):2535–2540.</li>
</ul>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Abhishek Sarkar</p>
<p class="date">Created: 2021-07-21 Wed 21:45</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
