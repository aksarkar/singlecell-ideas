#+TITLE: Logistic regression in the presence of error-prone labels
#+SETUPFILE: setup.org

* Introduction

  Logistic regression can be written \(
  \DeclareMathOperator\Bern{Bernoulli}
  \DeclareMathOperator\E{E}
  \DeclareMathOperator\Exp{Exp}
  \DeclareMathOperator\KL{\mathcal{KL}}
  \DeclareMathOperator\N{\mathcal{N}}
  \DeclareMathOperator\S{sigmoid}
  \DeclareMathOperator\Unif{Uniform}
  \DeclareMathOperator\logit{logit}
  \newcommand\mi{\mathbf{I}}
  \newcommand\mx{\mathbf{X}}
  \newcommand\vb{\mathbf{b}}
  \newcommand\vtheta{\boldsymbol{\theta}}
  \newcommand\vx{\mathbf{x}}
  \newcommand\vy{\mathbf{y}}
  \)

  \begin{align}
    y_i \mid p_i &\sim \Bern(p_i)\\
    \logit(p_i) &\triangleq \eta_i = \vx_i' \vb,
  \end{align}

  where \(y_i \in \{0, 1\}\) denotes the label of observation \(i = 1, \ldots,
  n\), \(\vx_i\) denotes the \(p\)-dimensional feature vector for observation
  \(i\), and \(\vb\) is a \(p\)-dimensional vector of coefficients. In words,
  under this model the features \(x_{ij}\) (\(j = 1, \ldots, p\)) have linear
  effects \(b_j\) on the log odds ratio \(\eta_i\) of the label \(y_i\)
  equalling 1. One is primarily interested in estimating \(\vb\), in order to
  explain which features are important in determining the label \(y_i\), and to
  predict labels for future observations. Estimation can be done by a variety
  of approaches, such as maximizing the log likelihood

  \begin{equation}
    \ell = \sum_i y_i \ln \S(\vx_i' \vb) + (1 - y_i) \ln \S(-\vx_i' \vb)
  \end{equation}

  with respect to \(\vb\), or by assuming a prior \(p(\vb)\) and estimating the
  posterior \(p(\vb \mid \mx, \vy)\) using MCMC or variational inference.

  /Remark./ Note that the log likelihood equals the negative of the cross
  entropy loss.

  A central assumption in using the model above to analyze data is that the
  labels \(y_i\) do not have errors. This is not a safe assumption to make in
  some settings, e.g., in associating genetic variants with diseases for which
  diagnosis is known to be imperfect
  ([[https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-020-3387-z][Shafquat
  et al. 2020]]). One approach to account for error-prone labels is to build a
  more complex model that relates the observed feature vectors \(\vx_i\), the
  true (unobserved) labels \(z_i\), and the observed labels \(y_i\)

  \begin{align}
    y_i \mid z_i = 1, \theta_1 &\sim \Bern(1 - \theta_1)\\
    y_i \mid z_i = 0, \theta_0 &\sim \Bern(\theta_0)\\
    z_i \mid p_i &\sim \Bern(p_i)\\
    \logit(p_i) &= \vx_i \vb.
  \end{align}

  In this model, \(\vtheta = (\theta_0, \theta_1)\) denotes error
  probabilities. Specifically, \(\theta_1\) denotes the probability of
  observing label \(y_i = 0\) when the true label \(z_i = 1\), and \(\theta_0\)
  denotes the probability of observing \(y_i = 1\) when \(z_i = 0\). (One could
  simplify by fixing \(\theta_0 = \theta_1\), or potentially build more complex
  models where the error rates themselves depend on features in the data.) As
  was the case for the simpler model, one estimates \((\vb, \vtheta)\) from
  observed data by maximizing the log likelihood, or by assuming a prior
  \(p(\vb, \vtheta)\) and estimating the posterior \(p(\vb, \vtheta \mid \mx,
  \vy)\). However, unlike the simpler model, one can use these estimates to
  infer the true labels (and thus identify mislabelled data points) from the
  data, e.g., by estimating \(p(z_i = 1 \mid \mx, \vy)\).

* Maximum likelihood estimation

  One can use an EM algorithm to maximize the log likelihood

  \begin{equation}
    \sum_i \ln \left(\sum_{z_i \in \{0, 1\}} p(y_i \mid z_i, \vtheta)\, p(z_i \mid \vx_i, \vb)\right)
  \end{equation}

  with respect to \((\vb, \vtheta)\). 

  /Remark./ In the case where \(p > n\), one can add an \(\ell_1\) and/or
  \(\ell_2\) penalty on \(\vb\) to the log likelihood to regularize the
  problem, and modify the M-step update appropriately.

  We briefly outline the derivation of the algorithm. The joint probability

  \begin{multline}
    \ln p(y_i, z_i \mid \vx_i, \vb, \vtheta) = [z_i = 1]\underbrace{\left(y_i \ln (1 - \theta_1) + (1 - y_i)\ln \theta_1 + \ln \S(\vx_i' \vb)\right)}_{\triangleq \alpha_i}\\
    + [z_i = 0]\underbrace{\left(y_i \ln \theta_0 + (1 - y_i) \ln (1 - \theta_0) + \ln \S(-\vx_i' \vb)\right)}_{\triangleq \beta_i},
  \end{multline}

  where \([\cdot]\) denotes the
  [[https://en.wikipedia.org/wiki/Iverson_bracket][Iverson bracket]]. The
  posterior

  \begin{equation}
    \phi_i \triangleq p(z_i = 1 \mid \vx_i, y_i, \vb, \vtheta) = \frac{\exp(\alpha_i)}{\exp(\alpha_i) + \exp(\beta_i)},
  \end{equation}

  and the expected log joint probability with respect to the posterior is

  \begin{multline}
    \sum_i \Big(\phi_i \left(y_i \ln (1 - \theta_1) + (1 - y_i)\ln \theta_1 + \ln \S(\vx_i' \vb)\right)\\
    + (1 - \phi_i) \left(y_i \ln \theta_0 + (1 - y_i) \ln (1 - \theta_0) + \ln \S(-\vx_i' \vb)\right) \Big).
  \end{multline}

  In the E-step, one updates \(\phi_i\) as described above. In the M-step, one
  updates

  \begin{align}
    \logit(\theta_1) &:= \frac{\sum_i \phi_i (1 - y_i)}{\sum_i \phi_i y_i}\\
    \logit(\theta_0) &:= \frac{\sum_i (1 - \phi_i) y_i}{\sum_i (1 - \phi_i) (1 - y_i)},
  \end{align}

  and updates \(\vb\) using a numerical method to increase the expected log
  joint probability (e.g., gradient ascent). One iterates the E and M steps
  alternately until e.g., the change in objective falls below some
  threshold. Intuitively, this algorithm alternates between estimating the true
  labels given the current estimates of the regression coefficients and error
  rates, and estimating the regression coefficients and error rates given the
  current estimates of the true labels. Crucially, all of the data is used to
  estimate the regression coefficients and error rates, which makes it possible
  to detect that an observed label is unlikely given the observed features and
  estimated error rates, by borrowing information from the other
  observations. From the estimated \(\hat{\vb}\), one can predict the true
  label \(\tilde{z}\) for a new observation \((\tilde{\vx}, \tilde{y})\) as

  \begin{equation}
    \E[\tilde{z}_i] = \S(\tilde{\vx}' \hat{\vb}).
  \end{equation}

* Approximate Bayesian inference

  If one assumes a prior \(p(\vb, \vtheta)\), then one can estimate the
  posterior \(p(\vb, \vtheta \mid \mx, \vy)\). Inference via MCMC can be
  readily implemented in inference engines such as
  [[https://mc-stan.org/][Stan]], [[https://pyro.ai/][pyro]], or
  [[https://www.tensorflow.org/probability][Tensorflow Probability]]. One could
  alternatively use variational inference to find the best approximate
  posterior in a tractable family, by minimizing the KL divergence between the
  approximate posterior and the true posterior. From the estimated posterior,
  one can estimate the posterior distribution of the true label \(\tilde{z}\)
  for a new data point \((\tilde{\vx}, \tilde{y})\), given the observed
  (training) data, as

  \begin{equation}
    p(\tilde{z} \mid \tilde{\vx}, \tilde{y}) = \int p(\tilde{y} \mid \tilde{z}, \vtheta)\, p(\tilde{z} \mid \tilde{\vx}, \vb)\, dp(\vb, \vtheta \mid \mx, \vy).
  \end{equation}

  The primary derived quantity of interest for prediction is the posterior mean
  \(\E[\tilde{z} \mid \tilde{\vx}, \tilde{y}]\).

* Example
** Setup
   :PROPERTIES:
   :CUSTOM_ID: setup
   :END:

   #+BEGIN_SRC emacs-lisp :exports none
    (add-to-list 'python-shell-completion-native-disabled-interpreters "jupyter")
    (org-babel-lob-ingest "/home/aksarkar/.emacs.d/org-templates/library.org")
   #+END_SRC

   #+RESULTS:
   : 2

   #+CALL: ipython3(venv="singlecell",partition="gpu2",opts="--gres=gpu:1") :exports none :dir /scratch/midway2/aksarkar/singlecell

   #+BEGIN_SRC ipython
     import numpy as np
     import pandas as pd
     import pyro
     import scipy.special as sp
     import scipy.stats as st
     import sklearn.linear_model as sklm
     import sklearn.metrics as skm
     import sklearn.model_selection as skms
     import torch
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[11]:
   :END:

   #+BEGIN_SRC ipython
     %matplotlib inline
     %config InlineBackend.figure_formats = set(['retina'])
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[3]:
   :END:

   #+BEGIN_SRC ipython
     import matplotlib.pyplot as plt
     plt.rcParams['figure.facecolor'] = 'w'
     plt.rcParams['font.family'] = 'Nimbus Sans'
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[4]:
   :END:

** Simulation

   #+BEGIN_SRC ipython
     def simulate(n, p, n_causal, theta0, theta1, seed=0):
       rng = np.random.default_rng(seed)
       x = rng.normal(size=(n, p))
       b = np.zeros((p, 1))
       b[:n_causal] = rng.normal(size=(n_causal, 1))
       z = rng.uniform(size=(n, 1)) < sp.expit(x @ b)
       u = rng.uniform(size=(n, 1))
       y = np.where(z, u < (1 - theta1), u < theta0)
       return x, y, z, b
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[5]:
   :END:

   #+BEGIN_SRC ipython
     x, y, z, b = simulate(n=1000, p=10, n_causal=3, theta0=0.05, theta1=0.05)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[6]:
   :END:

   #+BEGIN_SRC ipython :ipyfile figure/logistic-regression-labelling-errors.org/sim-ex.png
     cm = plt.get_cmap('Dark2')
     plt.clf()
     fig, ax = plt.subplots(1, 2, sharey=True)
     fig.set_size_inches(5, 2.5)
     eta = x @ b
     for k in range(2):
       jitter = np.random.normal(scale=0.01, size=(z == k).sum())
       ax[0].scatter(eta[z == k], jitter + z[z == k], s=1, c=cm(k), alpha=0.25)
       ax[1].scatter(eta[(z == k) & (y == z)], jitter[(y == z)[z == k]] + y[(z == k) & (y == z)], s=1, c=cm(k), alpha=0.25)
       ax[1].scatter(eta[(z == k) & (y != z)], jitter[(y != z)[z == k]] + y[(z == k) & (y != z)], s=8, marker='x', c=cm(k), alpha=1)
     for a in ax:
       a.set_xlabel('Linear predictor')
     ax[0].set_ylabel('True label')
     ax[1].set_ylabel('Observed label')
     plt.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[39]:
   [[file:figure/logistic-regression-labelling-errors.org/sim-ex.png]]
   :END:

   #+BEGIN_SRC ipython
     xt, xv, yt, yv, zt, zv = skms.train_test_split(x, y, z, test_size=0.1)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[13]:
   :END:

** Naive analysis

   Fit regularized logistic regression to the data.

   #+BEGIN_SRC ipython :async t
     fit0 = sklm.LogisticRegressionCV(cv=4, n_jobs=-1).fit(xt, yt.ravel())
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[47]:
   :END:

   Report the accuracy in the training and validation data.

   #+BEGIN_SRC ipython
     pd.Series({'observed': fit0.score(xv, yv), 'true': fit0.score(xv, zv)}, name='accuracy')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[53]:
   #+BEGIN_EXAMPLE
     observed    0.70
     true        0.75
     Name: accuracy, dtype: float64
   #+END_EXAMPLE
   :END:

** MCMC

   Assume proper priors \(\theta_k \sim \Unif(0, 1)\), \(\vb \sim
   \N(\boldsymbol{0}, \lambda \mi)\), \(\lambda \sim \Exp(1)\).

   #+BEGIN_SRC ipython
     def _logistic_regression_errors(x, y):
       theta = pyro.sample('theta', pyro.distributions.Uniform(
         low=torch.zeros(2),
         high=torch.ones(2)))
       lam = pyro.sample('lam', pyro.distributions.Exponential(rate=1.))
       b = pyro.sample('b', pyro.distributions.MultivariateNormal(
         loc=torch.zeros(x.shape[1]),
         covariance_matrix=lam * torch.eye(x.shape[1])))
       eta = x @ b
       with pyro.plate('data', x.shape[0]):
         y = pyro.sample('y', pyro.distributions.Bernoulli(
           probs=(1 - theta[1]) * torch.sigmoid(eta) + theta[0] * torch.sigmoid(-eta)), obs=y)
       return y

     def logistic_regression_errors_nuts(x, y, num_samples=100, warmup_steps=100, verbose=True, **kwargs):
       nuts = pyro.infer.mcmc.NUTS(_logistic_regression_errors)
       samples = pyro.infer.mcmc.MCMC(nuts, num_samples=num_samples, warmup_steps=warmup_steps)
       samples.run(torch.tensor(x, dtype=torch.float), torch.tensor(y.ravel(), dtype=torch.float))
       return samples
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[9]:
   :END:

   Sample from the posterior.

   #+BEGIN_SRC ipython :async t
     samples = logistic_regression_errors_nuts(xt, yt, num_samples=2000, warmup_steps=1000)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[60]:
   :END:

   Report the AUROC of predicting the true labels in the training data.

   #+BEGIN_SRC ipython
     pt = np.stack([torch.sigmoid(torch.tensor(xt, dtype=torch.float) @ bt).numpy()
                    for bt in samples.get_samples()['b']]).mean(axis=0)
     skm.roc_auc_score(zt, pt)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[61]:
   : 0.8662639056294336
   :END:

   Report the 95% credible intervals for the error rates.

   #+BEGIN_SRC ipython
     pd.DataFrame(np.percentile(samples.get_samples()['theta'], [2.5, 97.5], axis=0).T)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[63]:
   #+BEGIN_EXAMPLE
     0         1
     0  0.026715  0.155089
     1  0.011076  0.155695
   #+END_EXAMPLE
   :END:

   Report the AUROC of predicting the true labels in the validation data.

   #+BEGIN_SRC ipython
     pv = np.stack([torch.sigmoid(torch.tensor(xv, dtype=torch.float) @ bt).numpy()
                    for bt in samples.get_samples()['b']]).mean(axis=0)
     skm.roc_auc_score(zv, pv)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[79]:
   : 0.8375350140056023
   :END:

   Report the accuracy of predicting the mislabeled data points in the
   validation data.

   #+BEGIN_SRC ipython
     skm.roc_auc_score(yv != zv, np.where(yv.ravel(), 1 - pv, pv))
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[80]:
   : 0.9505208333333334
   :END:

* Related work

  - Tenenbein, A. (1970). A double sampling scheme for estimating from binomial
    data with misclassifications. J. Am. Statist. Assoc. 65. 1350-1361.

  - Hochberg, Y. (1977). On the use of double sampling schemes in analyzing
    categorical data with misclassification
    errors. J. Am. Statist. Assoc. 72. 914-921

  - Chen, T.T. (1979). Log-linear models for categorical data with
    misclassification and double sampling. J. Am. Statist. Assoc. 74. 481-488.

  - Espeland, M.A. and Odoroff, C.L. (1985). Log-linear models for doubly sampled
    categorical data fitted by the EM
    algorithm. J. Am. Statist. Assoc. 80. 663-670

  - Espeland, M.A. and Hui, S.L. (1987). A general approach to analyzing
    epidemiologic data that contain misclassification
    errors. Biometrics 43. 1001-1012

  - Zhi Geng & Chooichiro Asano. (1989) Bayesian estimation methods for
    categorical data with misclassifications, Communications in Statistics -
    Theory and Methods, 18:8, 2935-2954. doi:10.1080/03610928908830069

  - Evans, M., Guttman, I., Haitovsky, Y., and Swartz, T. (1996). Bayesian
    analysis of binary data subject to misclassiﬁcation. In Bayesian Analysis
    in Statistics and Econometrics: Essays in Honor of Arnold Zellner,
    D. Berry, K. Chaloner, and J. Geweke (eds), 67–77. New York: North Holland.

  - Mendoza-Blanco, J. R., Tu, X. M., and Iyengar, S. (1996). Bayesian
    inference on prevalence using a missing-data approach with simulation-based
    techniques: Applications to HIV screening. Statistics in Medicine 15,
    2161–2176.

  - Rekaya, R., Weigel, K. A., and Gianola, D. (2001). Threshold model for
    misclassiﬁed binary responses with applications to animal
    breeding. Biometrics 57, 1123–1129.

  - Paulino, C.D., Soares, P. and Neuhaus, J. (2003), Binomial Regression
    with Misclassification. Biometrics, 59:
    670-675. https://doi.org/10.1111/1541-0420.00077

  - Smith, S., Hay, E.H., Farhat, N. et al. Genome wide association studies in
    presence of misclassified binary responses. BMC Genet 14, 124
    (2013). https://doi.org/10.1186/1471-2156-14-124
