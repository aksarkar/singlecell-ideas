#+TITLE: Logistic regression in the presence of error-prone labels
#+SETUPFILE: setup.org

* Introduction

  Logistic regression can be written \(
  \DeclareMathOperator\Bern{Bernoulli}
  \DeclareMathOperator\E{E}
  \DeclareMathOperator\Exp{Exp}
  \DeclareMathOperator\KL{\mathcal{KL}}
  \DeclareMathOperator\N{\mathcal{N}}
  \DeclareMathOperator\S{sigmoid}
  \DeclareMathOperator\Unif{Uniform}
  \DeclareMathOperator\logit{logit}
  \newcommand\mi{\mathbf{I}}
  \newcommand\mx{\mathbf{X}}
  \newcommand\vb{\mathbf{b}}
  \newcommand\vtheta{\boldsymbol{\theta}}
  \newcommand\vx{\mathbf{x}}
  \newcommand\vy{\mathbf{y}}
  \)

  \begin{align}
    y_i \mid p_i &\sim \Bern(p_i)\\
    \logit(p_i) &\triangleq \eta_i = \vx_i' \vb,
  \end{align}

  where \(y_i \in \{0, 1\}\) denotes the label of observation \(i = 1, \ldots,
  n\), \(\vx_i\) denotes the \(p\)-dimensional feature vector for observation
  \(i\), and \(\vb\) is a \(p\)-dimensional vector of coefficients. In words,
  under this model the features \(x_{ij}\) (\(j = 1, \ldots, p\)) have linear
  effects \(b_j\) on the log odds ratio \(\eta_i\) of the label \(y_i\)
  equalling 1. One is primarily interested in estimating \(\vb\), in order to
  explain which features are important in determining the label \(y_i\), and to
  predict labels for future observations. Estimation can be done by a variety
  of approaches, such as maximizing the log likelihood

  \begin{equation}
    \ell = \sum_i y_i \ln \S(\vx_i' \vb) + (1 - y_i) \ln \S(-\vx_i' \vb)
  \end{equation}

  with respect to \(\vb\), or by assuming a prior \(p(\vb)\) and estimating the
  posterior \(p(\vb \mid \mx, \vy)\) using MCMC or variational inference.

  /Remark./ Note that the log likelihood equals the negative of the cross
  entropy loss.

  A central assumption in using the model above to analyze data is that the
  labels \(y_i\) do not have errors. This is not a safe assumption to make in
  some settings, e.g., in associating genetic variants with diseases for which
  diagnosis is known to be imperfect
  ([[https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-020-3387-z][Shafquat
  et al. 2020]]). One approach to account for error-prone labels is to build a
  more complex model that relates the observed feature vectors \(\vx_i\), the
  true (unobserved) labels \(z_i\), and the observed labels \(y_i\)

  \begin{align}
    y_i \mid z_i = 1, \theta_1 &\sim \Bern(1 - \theta_1)\\
    y_i \mid z_i = 0, \theta_0 &\sim \Bern(\theta_0)\\
    z_i \mid p_i &\sim \Bern(p_i)\\
    \logit(p_i) &= \vx_i \vb.
  \end{align}

  In this model, \(\vtheta = (\theta_0, \theta_1)\) denotes error
  probabilities. Specifically, \(\theta_1\) denotes the probability of
  observing label \(y_i = 0\) when the true label \(z_i = 1\), and \(\theta_0\)
  denotes the probability of observing \(y_i = 1\) when \(z_i = 0\). (One could
  simplify by fixing \(\theta_0 = \theta_1\), or potentially build more complex
  models where the error rates themselves depend on features in the data.) As
  was the case for the simpler model, one estimates \((\vb, \vtheta)\) from
  observed data by maximizing the log likelihood, or by assuming a prior
  \(p(\vb, \vtheta)\) and estimating the posterior \(p(\vb, \vtheta \mid \mx,
  \vy)\). However, unlike the simpler model, one can use these estimates to
  infer the true labels (and thus identify mislabelled data points) from the
  data, e.g., by estimating \(p(z_i = 1 \mid \mx, \vy)\).

* Maximum likelihood estimation

  One can use an EM algorithm to maximize the log likelihood

  \begin{equation}
    \sum_i \ln \left(\sum_{z_i \in \{0, 1\}} p(y_i \mid z_i, \vtheta)\, p(z_i \mid \vx_i, \vb)\right)
  \end{equation}

  with respect to \((\vb, \vtheta)\). 

  /Remark./ In the case where \(p > n\), one can add an \(\ell_1\) and/or
  \(\ell_2\) penalty on \(\vb\) to the log likelihood to regularize the
  problem, and modify the M-step update appropriately.

  We briefly outline the derivation of the algorithm. The joint probability

  \begin{multline}
    \ln p(y_i, z_i \mid \vx_i, \vb, \vtheta) = [z_i = 1]\underbrace{\left(y_i \ln (1 - \theta_1) + (1 - y_i)\ln \theta_1 + \ln \S(\vx_i' \vb)\right)}_{\triangleq \alpha_i}\\
    + [z_i = 0]\underbrace{\left(y_i \ln \theta_0 + (1 - y_i) \ln (1 - \theta_0) + \ln \S(-\vx_i' \vb)\right)}_{\triangleq \beta_i},
  \end{multline}

  where \([\cdot]\) denotes the
  [[https://en.wikipedia.org/wiki/Iverson_bracket][Iverson bracket]]. The
  posterior

  \begin{equation}
    \phi_i \triangleq p(z_i = 1 \mid \vx_i, y_i, \vb, \vtheta) = \frac{\exp(\alpha_i)}{\exp(\alpha_i) + \exp(\beta_i)},
  \end{equation}

  and the expected log joint probability with respect to the posterior is

  \begin{multline}
    \sum_i \Big(\phi_i \left(y_i \ln (1 - \theta_1) + (1 - y_i)\ln \theta_1 + \ln \S(\vx_i' \vb)\right)\\
    + (1 - \phi_i) \left(y_i \ln \theta_0 + (1 - y_i) \ln (1 - \theta_0) + \ln \S(-\vx_i' \vb)\right) \Big).
  \end{multline}

  In the E-step, one updates \(\phi_i\) as described above. In the M-step, one
  updates

  \begin{align}
    \logit(\theta_1) &:= \frac{\sum_i \phi_i (1 - y_i)}{\sum_i \phi_i y_i}\\
    \logit(\theta_0) &:= \frac{\sum_i (1 - \phi_i) y_i}{\sum_i (1 - \phi_i) (1 - y_i)},
  \end{align}

  and updates \(\vb\) using a numerical method to increase the expected log
  joint probability (e.g., gradient ascent). One iterates the E and M steps
  alternately until e.g., the change in objective falls below some
  threshold. Intuitively, this algorithm alternates between estimating the true
  labels given the current estimates of the regression coefficients and error
  rates, and estimating the regression coefficients and error rates given the
  current estimates of the true labels. Crucially, all of the data is used to
  estimate the regression coefficients and error rates, which makes it possible
  to detect that an observed label is unlikely given the observed features and
  estimated error rates, by borrowing information from the other
  observations. From the estimated \(\hat{\vb}\), one can predict the true
  label \(\tilde{z}\) for a new observation \((\tilde{\vx}, \tilde{y})\) as

  \begin{equation}
    \E[\tilde{z}_i] = \S(\tilde{\vx}' \hat{\vb}).
  \end{equation}

* Approximate Bayesian inference

  If one assumes a prior \(p(\vb, \vtheta)\), then one can estimate the
  posterior \(p(\vb, \vtheta \mid \mx, \vy)\). Inference via MCMC can be
  readily implemented in inference engines such as
  [[https://mc-stan.org/][Stan]], [[https://pyro.ai/][pyro]], or
  [[https://www.tensorflow.org/probability][Tensorflow Probability]]. One could
  alternatively use variational inference to find the best approximate
  posterior in a tractable family, by minimizing the KL divergence between the
  approximate posterior and the true posterior. From the estimated posterior,
  one can estimate the posterior distribution of the true label \(\tilde{z}\)
  for a new data point \((\tilde{\vx}, \tilde{y})\), given the observed
  (training) data, as

  \begin{equation}
    p(\tilde{z} \mid \tilde{\vx}, \tilde{y}) = \int p(\tilde{y} \mid \tilde{z}, \vtheta)\, p(\tilde{z} \mid \tilde{\vx}, \vb)\, dp(\vb, \vtheta \mid \mx, \vy).
  \end{equation}

  The primary derived quantity of interest for prediction is the posterior mean
  \(\E[\tilde{z} \mid \tilde{\vx}, \tilde{y}]\). As an example, assume a
  point-Normal prior

  \begin{equation}
    b_j \sim \pi_0 \delta_0(\cdot) + (1 - \pi_0) \N(0, \sigma_b^2),
  \end{equation}

  and assume a conjugate variational family (Carbonetto and Stephens 2012)

  \begin{equation}
    q(b_j) = \alpha_j \delta_0(\cdot) + (1 - \alpha_j) \N(\mu_j, s_j^2).
  \end{equation}

  The evidence lower bound is

  \begin{equation}
    \ell = \sum_i \E\left[\ln \left(\textstyle\sum_{z_i} p(y_i \mid z_i) p(z_i \mid \vx_i, \vb)\right)\right] - \KL(q(\vb) \Vert p(\vb)),
  \end{equation}

  which can be approximated as a Monte Carlo integral (Kingma and Welling 2014,
  Rezende et al. 2014). This yields a differentiable stochastic objective which
  can be optimized via automatic differentiation and gradient descent.

* Example
** Setup
   :PROPERTIES:
   :CUSTOM_ID: setup
   :END:

   #+BEGIN_SRC emacs-lisp :exports none
    (add-to-list 'python-shell-completion-native-disabled-interpreters "jupyter")
    (org-babel-lob-ingest "/home/aksarkar/.emacs.d/org-templates/library.org")
   #+END_SRC

   #+RESULTS:
   : 2

   #+CALL: ipython3(venv="singlecell",partition="gpu2",opts="--gres=gpu:1") :exports none :dir /scratch/midway2/aksarkar/singlecell

   #+BEGIN_SRC ipython
     import numpy as np
     import pandas as pd
     import pyro
     import scipy.special as sp
     import scipy.stats as st
     import torch
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[38]:
   :END:

   #+BEGIN_SRC ipython
     %matplotlib inline
     %config InlineBackend.figure_formats = set(['retina'])
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[4]:
   :END:

   #+BEGIN_SRC ipython
     import matplotlib.pyplot as plt
     plt.rcParams['figure.facecolor'] = 'w'
     plt.rcParams['font.family'] = 'Nimbus Sans'
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[5]:
   :END:

** Simulation

   #+BEGIN_SRC ipython
     def simulate(n, p, n_causal, theta0, theta1, seed=0):
       rng = np.random.default_rng(seed)
       x = rng.normal(size=(n, p))
       b = np.zeros((p, 1))
       b[:n_causal] = rng.normal(size=(n_causal, 1))
       z = rng.uniform(size=(n, 1)) < sp.expit(x @ b)
       u = rng.uniform(size=(n, 1))
       y = np.where(z, u < (1 - theta1), u < theta0)
       return x, y, z, b
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[15]:
   :END:

   #+BEGIN_SRC ipython
     x, y, z, b = simulate(n=500, p=10, n_causal=2, theta0=0.01, theta1=0.01)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[22]:
   :END:

   #+BEGIN_SRC ipython :ipyfile figure/logistic-regression-labelling-errors.org/sim-ex.png
     cm = plt.get_cmap('Dark2')
     plt.clf()
     plt.gcf().set_size_inches(2.5, 2.5)
     eta = x @ b
     for k in range(2):
       jitter = np.random.normal(scale=0.01, size=(z == k).sum())
       plt.scatter(eta[z == k], jitter + z[z == k], s=1, c=cm(k))
     plt.xlabel('Linear predictor')
     plt.ylabel('True label')
     plt.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[39]:
   [[file:figure/logistic-regression-labelling-errors.org/sim-ex.png]]
   :END:

   #+BEGIN_SRC ipython
     idx = y != z
     pd.DataFrame(np.vstack([eta[idx], y[idx], z[idx]]).T, columns=['eta', 'y', 'z'])
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[43]:
   #+BEGIN_EXAMPLE
     eta    y    z
     0 -2.175591  1.0  0.0
     1  2.075414  1.0  0.0
     2 -1.470784  1.0  0.0
     3  3.486473  0.0  1.0
     4 -0.088405  1.0  0.0
   #+END_EXAMPLE
   :END:

** EM

   #+BEGIN_SRC ipython
     def _objective(x, y, z, b, theta):
       eta = x @ b
       alpha = st.bernoulli(1 - theta[1]).pmf(y) + np.log(sp.expit(eta))
       beta = st.bernoulli(theta[0]).pmf(y) + np.log(sp.expit(-eta))
       return (z * alpha + (1 - z) * beta).sum()

     def _objective_b(x, z, b):
       eta = x @ b
       return (z * np.log(sp.expit(eta)) + (1 - z) * np.log(sp.expit(-eta))).sum()

     def _update_bj(x, y, z, b, j, step=1, c=0.5, tau=0.5, max_iters=30):
       """Return regression coefficients

       Use backtracking line search to find the step size for the update

       step - initial step size
       c - control parameter (Armijo-Goldstein condition)
       tau - control parameter (step size update)
       max_iters - maximum number of backtracking steps

       """
       loss = _objective_b(x, z, b)
       eta = x @ b
       d = ((z * (1 - sp.expit(eta)) + (1 - z) * (1 - sp.expit(-eta))) * x[:,j]).sum()
       temp = b[:]
       temp[j] += step * d
       update = _objective_b(x, z, temp)
       while (not np.isfinite(update) or (update > loss + c * step * d).any()) and max_iters > 0:
         step *= tau
         temp = b[:]
         temp[j] += step * d
         update = _objective_b(z, x, temp)
         max_iters -= 1
       if max_iters == 0:
         # Step size is small enough that update can be skipped
         return b, loss
       else:
         return temp, update

     def _update_z_theta(x, y, b, theta):
       eta = x @ b
       alpha = st.bernoulli(1 - theta[1]).pmf(y) + np.log(sp.expit(eta))
       beta = st.bernoulli(theta[0]).pmf(y) + np.log(sp.expit(-eta))
       z = np.exp(alpha) / (np.exp(alpha) + np.exp(beta))
       theta[0] = sp.expit(((1 - z) * y).sum() / ((1 - z) * (1 - y)).sum())
       theta[1] = sp.expit((z * y).sum() / ((1 - z) * y).sum())
       return z, theta

     def logistic_regression_errors(x, y, tol=1e-3, max_iters=10000, verbose=True):
       b = np.zeros(x.shape[1])
       theta = 0.01 * np.ones(2)
       obj = -np.inf
       for i in range(max_iters):
         z, theta = _update_z_theta(x, y, b, theta)
         for j in range(x.shape[1]):
           b[j], _ = _update_bj(x, y, z, b, j)
         obj1 = _objective(x, y, z, b, theta)
         if obj1 < obj:
           raise RuntimeError('objective decreased')
         elif obj1 - obj < tol:
           return b, theta
         else:
           obj = obj1
           print('{i:>6d} {obj:>.9g}')
       else:
         raise RuntimeError('failed to converge in max_iters')
   #+END_SRC

** MCMC

   Assume proper priors \(\theta_k \sim \Unif(0, 1)\), \(\vb \sim \N(0, \lambda
   \mi)\), \(\lambda \sim \Exp(1)\).

   #+BEGIN_SRC ipython
     def _logistic_regression_errors(x, y):
       theta = pyro.sample('theta', pyro.distributions.Uniform(low=torch.zeros(2), high=torch.ones(2)))
       lam = pyro.sample('lam', pyro.distributions.Exponential(rate=torch.ones(1)))
       b = pyro.sample('b', pyro.distributions.MultivariateNormal(loc=torch.zeros(x.shape[1]), scale=torch.sqrt(lam) * torch.eye(x.shape[1])))
       z = pyro.sample('z', pyro.distributions.Bernoulli(logits=x @ b))
       return pyro.sample('y', pyro.distributions.Bernoulli(probs=torch.where(z, 1 - theta[1], theta[0])), obs=y)

     def logistic_regression_errors_nuts(x, y, num_samples=100, warmup_steps=100, verbose=True, **kwargs):
       nuts = pyro.infer.mcmc.NUTS(_logistic_regression_errors)
       samples = pyro.infer.mcmc.MCMC(nuts, num_samples=num_samples, warmup_steps=warmup_steps)
       samples.run(torch.tensor(x), torch.tensor(y))
       return samples
   #+END_SRC

** VEB

   #+BEGIN_SRC ipython
     class SpikeSlab(torch.nn.Module):
       def __init__(self, p):
         super().__init__()
         self.logits = torch.nn.Parameter(torch.randn([p, 1]))
         self.mean = torch.nn.Parameter(torch.zeros([p, 1]))
         self.log_prec = torch.nn.Parameter(torch.zeros([p, 1]))
         self.prior_logodds = torch.nn.Parameter(torch.full([1, 1], -np.log(p)))
         self.log_prior_prec = torch.nn.Parameter(torch.zeros([1, 1]))

       def forward(self):
         finfo = torch.finfo(self.logits.dtype)
         pip = torch.clamp(torch.sigmoid(self.logits), finfo.tiny, 1 - finfo.eps)
         prec = torch.exp(self.log_prec)
         kl = (pip * torch.log(pip / torch.sigmoid(self.prior_logodds)) +
               (1 - pip) * torch.log((1 - pip) / torch.sigmoid(-self.prior_logodds)) +
               .5 * pip * (1 + self.log_prec - self.log_prior_prec + torch.exp(self.log_prior_prec) * (self.mean ** 2 + 1 / prec))).sum()
         mean = pip * self.mean
         var = pip / prec + pip * (1 - pip) * self.mean ** 2
         return mean, var, kl

     class LogisticRegressionWithError(torch.nn.Module):
       def __init__(self, p):
         self.prior = SpikeSlab(p)
         self.err_logits = torch.zeros([2, 1])

       def forward(self, x, y, n_samples=1):
         b_mean, b_var, kl = self.prior.forward()
         eta = torch.distributions.Normal(x @ b_mean, x @ torch.sqrt(b_var)).rsample(n_samples)
         err = (torch.sigmoid(eta) * (y * torch.log(torch.sigmoid(-self.err_logits[1])) + (1 - y) * torch.log(torch.sigmoid(self.err_logits[1]))) +
                torch.sigmoid(-eta) * ((1 - y) * torch.log(torch.sigmoid(self.err_logits[0])) + (1 - y) * torch.log(torch.sigmoid(-self.err_logits[1])))).mean(dim=0).sum()
         elbo = err - kl
         return -elbo

       def fit(self, data, n_samples=10, n_epochs=2000, log_dir=None, **kwargs):
         n_samples = torch.Size([n_samples])
         opt = torch.optim.RMSprop(self.parameters(), **kwargs)
         global_step = 0
         if log_dir is not None:
           writer = tb.SummaryWriter(log_dir)
         else:
           writer = None
         for i in range(n_epochs):
           for x, y in data:
             opt.zero_grad()
             loss = self.forward(x, y, n_samples=n_samples, writer=writer, global_step=global_step)
             if torch.isnan(loss):
               raise RuntimeError('nan loss')
             loss.backward()
             opt.step()
             global_step += 1
         return self
   #+END_SRC

* Related work

  - Tenenbein,A. (1970). A double sampling scheme for estimating from binomial
    data with misclassifications. J. Am. Statist. Assoc. 65. 1350-1361.

  - Hochberg,Y. (1977). On the use of double sampling schemes in analyzing
    categorical data with misclassification
    errors. J. Am. Statist. Assoc. 72. 914-921

  - Chen, T.T. (1979). Log-linear models for categorical data with
    misclassification and double sampling. J. Am. Statist. Assoc. 74. 481-488.

  - Espeland,M.A. and Odoroff,C.L. (1985). Log-linear models for doubly sampled
    categorical data fitted by the EM
    algorithm. J. Am. Statist. Assoc. 80. 663-670

  - Espeland,M.A. and Hui,S.L. (1987). A general approach to analyzing
    epidemiologic data that contain misclassification
    errors. Biometrics 43. 1001-1012

  - Zhi Geng & Chooichiro Asano. (1989) Bayesian estimation methods for
    categorical data with misclassifications, Communications in Statistics -
    Theory and Methods, 18:8, 2935-2954. doi:10.1080/03610928908830069

  - Evans, M., Guttman, I., Haitovsky, Y., and Swartz, T. (1996). Bayesian
    analysis of binary data subject to misclassiﬁcation. In Bayesian Analysis
    in Statistics and Econometrics: Essays in Honor of Arnold Zellner,
    D. Berry, K. Chaloner, and J. Geweke (eds), 67–77. New York: North Holland.

  - Mendoza-Blanco, J. R., Tu, X. M., and Iyengar, S. (1996). Bayesian
    inference on prevalence using a missing-data approach with simulation-based
    techniques: Applications to HIV screening. Statistics in Medicine 15,
    2161–2176.

  - Rekaya, R., Weigel, K. A., and Gianola, D. (2001). Threshold model for
    misclassiﬁed binary responses with applications to animal
    breeding. Biometrics 57, 1123–1129.

  - Daniel Paulino, C., Soares, P. and Neuhaus, J. (2003), Binomial Regression
    with Misclassification. Biometrics, 59:
    670-675. https://doi.org/10.1111/1541-0420.00077

  - Smith, S., Hay, E.H., Farhat, N. et al. Genome wide association studies in
    presence of misclassified binary responses. BMC Genet 14, 124
    (2013). https://doi.org/10.1186/1471-2156-14-124
