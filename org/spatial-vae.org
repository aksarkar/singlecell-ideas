#+TITLE: Convolutional VAE for spatial transcriptomic data
#+SETUPFILE: setup.org

* Introduction

  There is great interest in analyzing transcriptomic profiles in complex
  tissues measured at single (or few) cell resolution jointly with associated
  spatial information
  ([[https://science.sciencemag.org/content/353/6294/78][St책hl et al. 2016]],
  [[https://www.nature.com/articles/s41592-019-0548-y][Vikovic et al. 2019]],
  [[https://science.sciencemag.org/content/353/6294/78][Rodriques et
  al. 2019]]). Such analysis could elucidate the molecular basis for the
  spatial organization of cell types/states
  ([[https://dx.doi.org/10.1016/j.copbio.2017.02.004][Moor and Itzkovitz
  2017]], [[https://science.sciencemag.org/content/358/6359/64][Lein et
  al. 2017]]), and could be useful for genomics-based histology. Existing
  statistical methods for analyzing spatial transcriptomic data are largely
  based on Gaussian processes
  ([[https://dx.doi.org/10.1038/nmeth.4636][Svensson et al. 2018]],
  [[https://www.nature.com/articles/nmeth.4634][Edsg채rd et al. 2018]],
  [[https://doi.org/10.1038/s41592-019-0701-7][Sun et al. 2020]]). These
  methods have proven to be successful in identifying genes that exhibit
  statistically significant spatial patterns of gene expression (compared to a
  null model of no structured spatial variation). However, these methods
  analyze one gene at a time, and therefore lose information on gene
  co-expression within and between samples. Further, they rely on a statistical
  test to prioritize genes that could be relevant to observed spatial
  structures (e.g., in associated imaging data).

  For spatial transcriptomic data that are collected on a regular grid, we can
  instead use convolutional neural networks (CNNs; Denker et al. 1989, LeCun et
  al. 1989) to learn spatial structure. There are several advantages to the
  convolutional approach. 

  1. A single model can be learned on all genes, and therefore can exploit
     covariance of gene expression induced by the gene regulatory network.
  2. The model can learn a lower-dimensional representation of spatial
     expression patterns, which can be used to identify genes with highly
     similar spatial expression patterns
  3. With associated imaging data, the convolutional filters can be jointly
     learned between image features and gene expression features, which can
     more directly associate cell type/state differences at the expression
     level with e.g., differences in morphology.

  The key idea of our approach is to combine a Poisson measurement model
  ([[https://dx.doi.org/10.1101/2020.04.07.030007][Sarkar and Stephens 2020]]),

  \begin{equation}
    \newcommand\Pois{\operatorname{Pois}}
    \newcommand\N{\mathcal{N}}
    \newcommand\vx{\mathbf{x}}
    \newcommand\vz{\mathbf{z}}
    x_{ij} \mid s_i, \lambda_{ij} \sim \Pois(s_i \lambda_{ij})
  \end{equation}

  with a /spatial expression model/,

  \begin{equation}
    \lambda_{ij} \mid \vz_j = (h(\vz_j))_i,
  \end{equation}

  where \(h\) denotes a CNN. To perform approximate Bayesian inference (on
  \(\vz_j\)), we treat \(h\) as a decoder network in a variational autoencoder
  (Kingma and Welling 2014) and introduce an encoder network

  \begin{equation}
    \vz_j \mid \vx_{\cdot j} \sim \N(\mu(\vx_{\cdot j}), \sigma^2(\vx_{\cdot j})),
  \end{equation}

  where \(\mu, \sigma^2\) are also CNNs (Salimans et al. 2014). Unlike existing
  application of VAEs to scRNA-seq data (e.g., Lopez et al. 2018), in the
  spatial transcriptomics setting, inference is done over minibatches of genes
  rather than samples.

  The learned VAE can be used to answer several questions of interest:

  1. /What are the predominant patterns of spatial gene expression?/ These are
     learned as the low dimensional spatial representations of each gene
     \(\vz_j\).
  2. /Which spatial structures define those patterns?/ These can be recovered
     using e.g., saliency maps to associate learned convolutional filters with
     the specific latent variable \(\vz_j\) of interest.
  3. /Which genes co-vary spatially?/ These can be recovered by clustering the
     learned \(\vz_j\).

* Setup
  :PROPERTIES:
  :CUSTOM_ID: setup
  :END:

  #+BEGIN_SRC emacs-lisp :exports none
    (add-to-list 'python-shell-completion-native-disabled-interpreters "jupyter")
    (org-babel-lob-ingest "/home/aksarkar/.emacs.d/org-templates/library.org")
  #+END_SRC

  #+RESULTS:
  : 2

  #+CALL: ipython3(venv="singlecell",partition="gpu2",opts="--gres=gpu:1",memory="32G") :exports none :dir /scratch/midway2/aksarkar/singlecell

  #+CALL: tensorboard(venv="singlecell",partition="mstephens") :dir /scratch/midway2/aksarkar/singlecell

  #+BEGIN_SRC ipython
    import numpy as np
    import torch
    import torch.utils.data as td
    import torch.utils.tensorboard as tb
    import torchvision as tv
    import umap
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[1]:
  :END:

  #+BEGIN_SRC ipython
    %matplotlib inline
    %config InlineBackend.figure_formats = set(['retina'])
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[3]:
  :END:

  #+BEGIN_SRC ipython
    import colorcet
    import matplotlib.pyplot as plt
    plt.rcParams['figure.facecolor'] = 'w'
    plt.rcParams['font.family'] = 'Nimbus Sans'
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[4]:
  :END:

* Methods
** Data

   - [[https://support.10xgenomics.com/spatial-gene-expression/datasets/1.1.0/V1_Breast_Cancer_Block_A_Section_1][Breast
     cancer]] (10X Genomics Visium v1.1.0)
   - [[https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE130682][Mouse
     olfactory bulb]] (HDST;
     [[https://www.nature.com/articles/s41592-019-0548-y][Vikovic et
     al. 2019]])
   - [[https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE120374][Human
     spinal cord]]
     ([[https://science.sciencemag.org/content/364/6435/89.full][Maniatis et
     al. 2019]])
   - Mouse cerebellum (SLIDE-Seq;
     [[https://science.sciencemag.org/content/353/6294/78][Rodriques et
     al. 2019]])
   - Human gingival tissue (ST;
     [[https://www.nature.com/articles/s41598-018-27627-3][Lundmark et
     al. 2018]])
   - Human prostate cancer (ST;
     [[https://www.nature.com/articles/s41467-018-04724-5][Berglund et
     al. 2018]])
   - Human melanoma (ST;
     [[https://cancerres.aacrjournals.org/content/78/20/5970.short][Thrane et
     al. 2018]])
   - Human heart (ST; [[https://www.nature.com/articles/s41598-017-13462-5][Asp
     et al. 2017]])
   - Human breast cancer (ST;
     [[https://science.sciencemag.org/content/353/6294/78][St책hl et al. 2016]])
   - Mouse olfactory bulb
     (ST; [[https://science.sciencemag.org/content/353/6294/78][St책hl et al. 2016]])

* Results
** MNIST

   Implement a VAE using convolutional layers instead of fully connected
   layers. Refer to
   [[https://www.jeremyjordan.me/convnet-architectures/]["Common architectures
   in convolutional neural networks"]].

   # http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html#transposed-convolution-arithmetic

   #+BEGIN_SRC ipython
     class Encoder(torch.nn.Module):
       def __init__(self, input_h, input_w, output_dim):
         super().__init__()
         self.net = torch.nn.Sequential(
           torch.nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1, stride=2),
           torch.nn.ReLU(),
           torch.nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1, stride=2),
           torch.nn.ReLU(),
           torch.nn.Flatten(),
           torch.nn.Linear(32 * input_h * input_w // 16, 1024),
           torch.nn.ReLU(),
           torch.nn.Linear(1024, 512),
           torch.nn.ReLU(),
         )
         self.mean = torch.nn.Linear(512, output_dim)
         self.scale = torch.nn.Sequential(
           torch.nn.Linear(512, output_dim),
           torch.nn.Softplus())

       def forward(self, x):
         h = self.net(x)
         return self.mean(h), self.scale(h)

     class Reshape(torch.nn.Module):
       def __init__(self, shape):
         super().__init__()
         self.shape = shape

       def forward(self, x):
         return torch.reshape(x, self.shape)

     class Decoder(torch.nn.Module):
       def __init__(self, input_dim, output_h, output_w):
         super().__init__()
         self.net = torch.nn.Sequential(
           torch.nn.Linear(input_dim, 512),
           torch.nn.ReLU(),
           torch.nn.Linear(512, 1024),
           torch.nn.ReLU(),
           torch.nn.Linear(1024, 32 * output_h * output_w // 16),
           torch.nn.ReLU(),
           Reshape([-1, 32, output_h // 4, output_w // 4]),
           torch.nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=3, stride=2, padding=1, output_padding=1),
           torch.nn.ReLU(),
           torch.nn.ConvTranspose2d(in_channels=16, out_channels=1, kernel_size=3, stride=2, padding=1, output_padding=1),
           torch.nn.Sigmoid(),
         )
         self.output_h = output_h
         self.output_w = output_w

       def forward(self, x):
         return self.net(x)

     class ConvVAE(torch.nn.Module):
       def __init__(self, input_h, input_w, latent_dim):
         super().__init__()
         self.encoder = Encoder(input_h, input_w, latent_dim)
         self.decoder = Decoder(latent_dim, input_h, input_w)

       def forward(self, x, n_samples, writer=None, global_step=None):
         mean, scale = self.encoder(x)
         kl = torch.sum(.5 * (1 - 2 * torch.log(scale) + (mean * mean + scale * scale)), dim=1)
         qz = torch.distributions.Normal(mean, scale).rsample(n_samples)
         probs = self.decoder.forward(qz)
         error = torch.sum(torch.mean(x * torch.log(probs) + (1 - x) * torch.log(1 - probs), dim=1), dim=[1, 2])
         if writer is not None:
           writer.add_scalar('loss/error', torch.sum(error), global_step)
           writer.add_scalar('loss/kl', torch.sum(kl), global_step)
         loss = -torch.sum(error - kl)
         if torch.isnan(loss):
           raise RuntimeError
         return loss

       def fit(self, data, n_epochs=5, n_samples=None, log_dir=None, **kwargs):
         if log_dir is None:
           writer = None
         else:
           writer = tb.SummaryWriter(log_dir)
         if n_samples is None:
           n_samples = torch.Size([1])
         if torch.cuda.is_available():
           self.cuda()
         opt = torch.optim.Adam(self.parameters(), **kwargs)
         global_step = 0
         for epoch in range(n_epochs):
           for x, y in data:
             if torch.cuda.is_available():
               x = x.cuda()
             opt.zero_grad()
             loss = self.forward(x, n_samples, writer, global_step)
             loss.backward()
             opt.step()
             global_step += 1
         return self
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[5]:
   :END:

   #+BEGIN_SRC ipython
     batch_size = 32
     mnist_train = td.DataLoader(
       tv.datasets.MNIST(
         root='/scratch/midway2/aksarkar/singlecell/',
         transform=lambda x: (np.frombuffer(x.tobytes(), dtype='uint8') > 0).astype(np.float32).reshape((1, x.size[0], x.size[1]))),
       batch_size=batch_size,
       shuffle=True,
       pin_memory=True)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[6]:
   :END:

   #+BEGIN_SRC ipython :async t
     latent_dim = 4
     n_epochs = 2
     run = 3
     torch.manual_seed(run)
     model = ConvVAE(input_h=28, input_w=28, latent_dim=latent_dim)
     model.fit(mnist_train,
               n_epochs=n_epochs,
               lr=1e-4,
               log_dir=f'runs/convvae/mnist/stride_latent_{latent_dim}_epochs_{n_epochs}_run_{run}')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[7]:
   #+BEGIN_EXAMPLE
     ConvVAE(
     (encoder): Encoder(
     (net): Sequential(
     (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
     (1): ReLU()
     (2): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
     (3): ReLU()
     (4): Flatten()
     (5): Linear(in_features=1568, out_features=1024, bias=True)
     (6): ReLU()
     (7): Linear(in_features=1024, out_features=512, bias=True)
     (8): ReLU()
     )
     (mean): Linear(in_features=512, out_features=4, bias=True)
     (scale): Sequential(
     (0): Linear(in_features=512, out_features=4, bias=True)
     (1): Softplus(beta=1, threshold=20)
     )
     )
     (decoder): Decoder(
     (net): Sequential(
     (0): Linear(in_features=4, out_features=512, bias=True)
     (1): ReLU()
     (2): Linear(in_features=512, out_features=1024, bias=True)
     (3): ReLU()
     (4): Linear(in_features=1024, out_features=1568, bias=True)
     (5): ReLU()
     (6): Reshape()
     (7): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
     (8): ReLU()
     (9): ConvTranspose2d(16, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
     (10): Sigmoid()
     )
     )
     )
   #+END_EXAMPLE
   :END:

   Serialize the fitted model parameters.

   #+BEGIN_SRC ipython
     torch.save(model.state_dict(), f'/scratch/midway2/aksarkar/singlecell/mnist-stride_latent_{latent_dim}_epochs_{n_epochs}_run_{run}.pt')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[8]:
   :END:

   Draw some samples.

   #+BEGIN_SRC ipython
     rng = np.random.default_rng(1)
     with torch.no_grad():
       z = rng.normal(size=(10, latent_dim))
       xh = model.decoder.forward(torch.tensor(z, dtype=torch.float, device='cuda')).squeeze().cpu().numpy()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[9]:
   :END:

   #+BEGIN_SRC ipython :ipyfile figure/spatial-vae.org/mnist-ex.png
     plt.clf()
     fig, ax = plt.subplots(2, 5)
     for i, a in enumerate(ax.ravel()):
       a.imshow(xh[i], cmap=colorcet.cm['gray'])
       a.set_xticks([])
       a.set_yticks([])
     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[10]:
   [[file:figure/spatial-vae.org/mnist-ex.png]]
   :END:

   Plot the latent space.

   #+BEGIN_SRC ipython :async t
     with torch.no_grad():
       temp = [(y.numpy(), model.encoder(x.cuda())[0].cpu().numpy()) for x, y in mnist_train]
     labels = np.hstack([y for y, _ in temp])
     embed = np.vstack([z for _, z in temp])
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[11]:
   :END:

   #+BEGIN_SRC ipython
     u, d, vt = np.linalg.svd(embed, full_matrices=False)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[12]:
   :END:

   #+BEGIN_SRC ipython :ipyfile figure/spatial-vae.org/mnist-cvae-latent-pca.png
     cm = plt.get_cmap('Set3')
     plt.clf()
     fig, ax = plt.subplots(1, 3, sharey=True)
     fig.set_size_inches(7.5, 2.5)
     for k, a in enumerate(ax):
       for y in range(10):
         idx = labels == y
         a.scatter(embed[idx,k + 1], embed[idx,0], s=1, alpha=0.25, c=np.array(cm(y)).reshape(1, -1), label=y)
       a.set_xlabel(f'PC {k + 2}')
     ax[0].set_ylabel('PC 1')
     leg = ax[-1].legend(frameon=False, markerscale=4, handletextpad=0, loc='center left', bbox_to_anchor=(1, .5))
     for h in leg.legendHandles:
       h.set_alpha(1)
     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[13]:
   [[file:figure/spatial-vae.org/mnist-cvae-latent-pca.png]]
   :END:

   #+BEGIN_SRC ipython :async t
     umaps = dict()
     for n_neighbors in (5, 10, 25, 50):
       print(f'fitting {n_neighbors}')
       umaps[n_neighbors] = umap.UMAP(n_neighbors=n_neighbors, min_dist=0, metric='euclidean', random_state=1).fit_transform(embed)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[15]:
   :END:

   #+BEGIN_SRC ipython :ipyfile figure/spatial-vae.org/mnist-cvae-latent-umap.png
     cm = plt.get_cmap('Set3')
     plt.clf()
     fig, ax = plt.subplots(1, 4, sharex=True, sharey=True)
     fig.set_size_inches(7.5, 2.5)
     for k, a in zip(umaps, ax):
       for y in range(10):
         idx = labels == y
         a.scatter(umaps[k][idx,0], umaps[k][idx,1], s=1, alpha=0.25, c=np.array(cm(y)).reshape(1, -1), label=y)
       a.set_title(f'k = {k}')
       a.set_xlabel('UMAP 1')
       a.set_xticks([])
       a.set_yticks([])
     ax[0].set_ylabel('UMAP 2')
     leg = ax[-1].legend(frameon=False, markerscale=4, handletextpad=0, loc='center left', bbox_to_anchor=(1, .5))
     for h in leg.legendHandles:
       h.set_alpha(1)
     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[17]:
   [[file:figure/spatial-vae.org/mnist-cvae-latent-umap.png]]
   :END:
