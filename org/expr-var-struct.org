#+TITLE: Learning the structure of RNA expression variation
#+SETUPFILE: setup.org

* Introduction

  Characterizing the extent of RNA expression variation within cell types, cell
  states, donors, and experimental conditions is important to learn about
  evolutionary constraint on gene regulation, modulation of differentiation
  potential, and the mechanisms underlying cell fate decisions. However,
  actually estimating the level of expression variation from scRNA-seq data is
  difficult. Here, we review some of these difficulties. \(
  \DeclareMathOperator\Gam{Gamma}
  \DeclareMathOperator\N{\mathcal{N}}
  \DeclareMathOperator\Pois{Poisson}
  \DeclareMathOperator\V{V}
  \newcommand\mf{\mathbf{F}}
  \newcommand\mh{\mathbf{H}}
  \newcommand\mi{\mathbf{I}}
  \newcommand\ml{\mathbf{L}}
  \newcommand\vl{\mathbf{l}}
  \newcommand\vx{\mathbf{x}}
  \newcommand\vz{\mathbf{z}}
  \newcommand\vmu{\boldsymbol{\mu}}
  \newcommand\vphi{\boldsymbol{\phi}}
  \newcommand\vpi{\boldsymbol{\pi}}
  \)

* Setup
  :PROPERTIES:
  :CUSTOM_ID: setup
  :END:

  #+BEGIN_SRC emacs-lisp :exports none
    (add-to-list 'python-shell-completion-native-disabled-interpreters "jupyter")
    (org-babel-lob-ingest "/home/aksarkar/.emacs.d/org-templates/library.org")
  #+END_SRC

  #+RESULTS:
  : 1

  #+CALL: ipython3(venv="singlecell",partition="gpu2",opts="--gres=gpu:1",memory="4G") :exports none :dir /scratch/midway2/aksarkar/singlecell

  #+CALL: tensorboard(venv="singlecell",partition="mstephens") :dir /scratch/midway2/aksarkar/singlecell

  #+BEGIN_SRC ipython
    import anndata
    import numpy as np
    import mpebpm.topics
    import scanpy as sc
    import scipy.stats as st
    import torch
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[30]:
  :END:

  #+BEGIN_SRC ipython
    %matplotlib inline
    %config InlineBackend.figure_formats = set(['retina'])
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[2]:
  :END:

  #+BEGIN_SRC ipython
    import colorcet
    import matplotlib.pyplot as plt
    plt.rcParams['figure.facecolor'] = 'w'
    plt.rcParams['font.family'] = 'Nimbus Sans'
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[3]:
  :END:

* Main results
** Pre-defined conditions/hard clustering

   The simplest case to consider is analyzing scRNA-seq observations
   from a single gene in one condition. Then, one can fit an observation model
   (Sarkar and Stephens 2020)

   \begin{align}
     x_i \mid s_i, \lambda_i &\sim \Pois(s_i \lambda_i), \qquad i = 1, \ldots, n\\
     \lambda_i &\sim g = \Gam(\phi^{-1}, \mu^{-1}\phi^{-1}).
   \end{align}

   Under this model, it is clear that the variance of true gene expression
   levels is \(\V[g] = \mu^2\phi\). More generally, one could instead assume
   that \(g\) belongs to some other family, but still characterize expression
   variance as \(\V[g]\). There is some evidence that families more flexible
   than Gamma distributions may be required for a large fraction of genes in
   some data sets (Sarkar and Stephens 2020). One goal of this analysis is to
   potentially interpret gene expression variance relative to the mean as
   indicative of
   [[file:diff-var.org::*Mechanistic%20basis%20of%20effects%20on%20variance][the
   underlying kinetic parameters of transcriptional bursting]].

   In the case where \(K\) conditions were pre-defined (e.g., donor) or are
   learned from the data by hard-clustering, one can readily generalize the
   above observation model to multiple genes and conditions (Sarkar et
   al. 2019)

   \begin{align}
     x_{ij} \mid s_i, \lambda_{ij} &\sim \Pois(s_i \lambda_{ij}), \qquad i = 1, \ldots, n; j = 1, \ldots, p\\
     \lambda_{ij} &= \mu_{ij} u_{ij}\\
     \mu_{ij} &= \exp((\ml\mf')_{ij})\\
     u_{ij} \mid \ml, \mh &\sim \Gam(\exp(-(\ml\mh')_{ij}), \exp(-(\ml\mh')_{ij})),
   \end{align}

   where \(\ml\) is an \(n \times K\) fixed, binary matrix corresponding to
   assignment of samples to conditions, \(\mf\) is a \(K \times p\) matrix of
   mean parameters, and \(\mh\) is a \(K \times p\) matrix of dispersion
   parameters. This observation model can be readily fit [[file:mpebpm.org][to
   massive data using stochastic gradient descent]]. Marginalizing,

   \begin{equation}
     \lambda_{ij} \sim g_{ij}(\cdot) = \Gam(\exp(-(\ml\mh')_{ij}), \exp(-(\ml(\mf + \mh)')_{ij})),
   \end{equation}

   where \(g_{ij}\) is identical for observations from the same condition and
   gene. Therefore, the variance of gene expression can still be estimated as
   the distinct \(\V[g_{ij}]\). However, this approach has important
   limitations: (1) it requires pre-defining conditions, which may not be
   justified by the data (in the case of learning clusters), and (2) it treats
   every gene independently, and does not consider propagation of noise in gene
   regulatory networks (e.g.,
   [[https://www.pnas.org/content/98/15/8614][Thattai and van Oudenaarden
   2001]]).

** Generalizing to soft clustering

   Considering the expression model

   \begin{equation}
     \lambda_{ij} \mid \ml, \mf, \mh \sim \Gam(\exp(-(\ml\mh')_{ij}), \exp(-(\ml(\mf + \mh)')_{ij})),
   \end{equation}

   what happens when \(\ml\) is not binary? Suppose \(\ml\) is normalized so
   that its rows sum to 1; then, this model corresponds to a sort of "noisy
   topic model," where topics are characterized by a collection of Gamma
   distributions for each gene, and cells are convex combinations of those
   topics. In some sense then, the topics in this model could be thought of as
   "noisy gene expression programs". One can in principle fit this model by
   automatic differentiation and gradient descent, ensuring the sum constraint
   by re-parameterizing \(\vl_i = \operatorname{softmax}(\tilde{\vl}_i)\). One
   could also amortize inference by introducing an inference network mapping
   \(\vx_i\) to \(\vl_i\).

   /Remark./ When \(\ml\) is binary, using the exponential inverse link
   function makes sense since we are selecting a single log mean parameter and
   a single log dispersion parameter for each observation. However, when
   \(\ml\) is instead normalized to have row sum 1, it is not clear that this
   model makes sense compared to a model with identity link, analogous to
   NMF/pLSI.

   This approach addresses the issues that pre-defined conditions are not
   always available, and hard-clustering data may not be justified. However, it
   complicates the question of what precisely the variance of gene expression
   corresponds to in the model. If we think of samples as being mixtures of
   gene expression programs, then even the question of what "variance of gene
   expression" means is problematic, since it is no longer clear what subset of
   samples the variance is being computed over. Is the variance of each learned
   gene expression program instead the relevant quantity, regardless of which
   samples express it? Is this the correct way to think about asking the
   question when we don't begin from the assumption that there were some number
   of distinct cell types present?

   In this model, we can think of \(\vl_i\) as a latent variable, and the model
   above as a linear latent variable model. But now, expression variation has
   components \(\V[\vl_i]\) and \(\V[\lambda_{ij} \mid \vl_i]\). Which of these
   components corresponds to expression noise? What does it mean for
   \(\V[\lambda_{ij} \mid \vl_i] \neq 0\) when \(\vl_i\) is a latent variable
   for a single observation? When \(\vl_i\) is constrained to lie in the
   simplex, what does \(\V[\vl_i]\) even mean?

   /Remark./ This model is a generalization of [[file:nbmix.org][a simple
   model-based clustering method]], in which hard clusters are defined by a
   collection of Gamma distributions

   \begin{equation}
     \lambda_{ij} \mid \vpi_i, \vmu_k, \vphi_k \sim \sum_{k=1}^{K} \pi_{ik} \Gam(\phi_{kj}^{-1}, \phi_{kj}^{-1}\mu_{kj}^{-1}).
   \end{equation}

   It also generalizes other methods
   ([[https://bmcsystbiol.biomedcentral.com/articles/10.1186/s12918-019-0699-6][scNBMF]],
   [[https://academic.oup.com/bioinformatics/article/36/11/3418/5807606][LDVAE]]),
   which fit expression models of the form

   \begin{align}
     \lambda_{ij} &= \mu_{ij} u_{ij}\\
     \mu_{ij} &= \exp((\ml\mf')_{ij})\\
     u_{ij} \mid \phi_j &\sim \Gam(\phi_j^{-1}, \phi_j^{-1}),
   \end{align}

   i.e., where \(p(u_{ij})\) is constant across conditions. It is also a
   related to [[https://doi.org/10.1038/s41467-018-07931-2][DCA]], which fits a
   similar expression model, but does not constrain \(\vl_i\) to be in the
   simplex and assumes a non-linear mapping from \(\vl_i\) to mean and
   dispersion parameters per cell per gene .

** Generalizing to a non-linear latent variable model

   Now consider the non-linear latent variable model

   \begin{align}
     x_{ij} \mid s_i, \lambda_{ij} &\sim \Pois(s_i \lambda_{ij})\\
     \lambda_{ij} &= (f(\vz_i))_j\\
     \vz_i &\sim \N(0, \mi).
   \end{align}

   Here, \(f\) is a fully-connected neural network, and variability is pushed
   into the \(\vz_i\). In this model, expression noise is completely explained
   by \(\V[\vz_i \mid \vx_i]\). That is, variation in expression levels is
   completely explained by variation in a low-dimensional latent space. Is this
   a reasonable model? One thing that isn't obvious is what exactly expression
   variance corresponds to in this model, since it is not clear which subset of
   samples the variance is being computed over. Naively, one could return to
   hard-clustering the \(\vz_i\), but it is not clear that this is an
   improvement over existing hard-clustering approaches.

   Existing methods ([[https://doi.org/10.1038/s41592-018-0229-2][scVI]],
   [[https://www.nature.com/articles/s41592-020-01050-x][totalVI]]) fit an even
   more complex expression model

   \begin{align}
     \lambda_{ij} &= \mu_{ij} u_{ij}\\
     \mu_{ij} &= (f(\vz_i))_j\\
     u_{ij} &\sim \Gam(\phi_j^{-1}, \phi_j^{-1})\\
     \vz_i &\sim \N(0, \mi).
   \end{align}

   In this model, expression noise depends both on variation in the latent
   space and extra variability. Does this make sense?

* Empirical results                                                :noexport:
** Gamma mixture expression model

   First, simulate from a Gamma mixture expression model. This is simulation
   does not model covariance between genes (due to gene regulation), or the
   relationship between mean and disperesion (due to
   [[file:diff-var.org::#poisson-beta][transcriptional bursting]]).

   #+BEGIN_SRC ipython
     rng = np.random.default_rng(1)
     n = 200
     p = 500
     k = 4
     s = np.full((n, 1), 1e5)
     log_mean = rng.uniform(low=-12, high=-8, size=(k, p))
     log_inv_disp = rng.uniform(low=0, high=4, size=(k, p))
     z = rng.multinomial(1, np.ones(k) / k, size=n)
     lam = rng.gamma(shape=z @ np.exp(log_inv_disp), scale=z @ np.exp(log_mean - log_inv_disp))
     x = rng.poisson(s * lam)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[100]:
   :END:

   Report the oracle log likelihood of the data.

   #+BEGIN_SRC ipython
     st.nbinom(n=z @ np.exp(log_inv_disp), p=1 / (1 + s * z @ np.exp(log_mean - log_inv_disp))).logpmf(x).sum()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[104]:
   : -91002.08432668039
   :END:

   Look at one of the simulated genes.

   #+BEGIN_SRC ipython :ipyfile figure/expr-var-struct.org/gamma-mix-ex-0.png
     cm = plt.get_cmap('Dark2')
     gene = 312
     plt.clf()
     fig, ax = plt.subplots(2, 1)
     fig.set_size_inches(4, 4)
     grid = np.arange(x[:,gene].max() + 1)
     ax[0].hist(x[:,gene], bins=grid, color='0.7')
     # for t in range(k):
     #   ax[0].plot(grid + 0.5, st.nbinom(n=np.exp(log_inv_disp[t,gene]), p=1 / (1 + s[0] * np.exp(log_mean[t,gene] - log_inv_disp[t,gene]))).pmf(grid), lw=1, c=cm(t), label=f'C{t}')
     # ax[0].legend(frameon=False)
     ax[0].set_xlabel('Num mols')
     ax[0].set_ylabel('Density')

     grid = np.linspace(0, (x[:,gene] / s).max(), 1000)
     for t in range(k):
       ax[1].plot(grid, st.gamma(a=np.exp(log_inv_disp[t,gene]), scale=np.exp(log_mean[t,gene] - log_inv_disp[t,gene])).cdf(grid), lw=1, c=cm(t), label=f'C{t}')
     ax[1].legend(frameon=False)
     ax[1].set_xlabel('True gene expr level')
     ax[1].set_ylabel('Density')
     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[114]:
   [[file:figure/expr-var-struct.org/gamma-mix-ex-0.png]]
   :END:

   Look at what the standard methodology does on the simulated data.

   #+BEGIN_SRC ipython :async t
     dat = anndata.AnnData(x)
     sc.pp.filter_genes(dat, min_counts=1)
     sc.pp.pca(dat, n_comps=10)
     sc.pp.neighbors(dat, n_neighbors=25)
     sc.tl.leiden(dat)
     sc.tl.umap(dat, random_state=1)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[56]:
   :END:

   #+BEGIN_SRC ipython :ipyfile figure/expr-var-struct.org/gamma-mix-ex.png
     plt.clf()
     fig, ax = plt.subplots(1, 2)
     fig.set_size_inches(6, 4.5)
     for t in range(k):
       ax[0].scatter(dat.obsm['X_umap'][z[:,t].astype(bool),0], dat.obsm['X_umap'][z[:,t].astype(bool),1], c=plt.get_cmap('Dark2')(t), s=1, label=f'C{t}')
     ax[0].legend(frameon=False, handletextpad=0, markerscale=8, ncol=2, loc='upper center', bbox_to_anchor=(.5, -.25))
     ax[0].set_title('Ground truth')
     for t in dat.obs['leiden'].values.categories:
       ax[1].scatter(dat.obsm['X_umap'][dat.obs['leiden'].values == t,0],
                     dat.obsm['X_umap'][dat.obs['leiden'].values == t,1], c=colorcet.cm['glasbey'](int(t)), s=1, label=f'C{t}')
     ax[1].legend(frameon=False, handletextpad=0, markerscale=8, ncol=4, loc='upper center', bbox_to_anchor=(.5, -.25))
     ax[1].set_title('Leiden')
     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[86]:
   [[file:figure/expr-var-struct.org/gamma-mix-ex.png]]
   :END:

   Fit the noisy topic model.

   #+BEGIN_SRC ipython :async t
     import imp; imp.reload(mpebpm.topics)
     run = 1
     lr = 1e-3
     batch_size = 20
     num_epochs = 1280
     oracle_init = False
     fix_decoder = False

     torch.manual_seed(run)
     m = mpebpm.topics.NoisyTopicModel(input_dim=p, latent_dim=k)
     if oracle_init:
       m.F.data = torch.tensor(log_mean, dtype=torch.float)
       m.H.data = torch.tensor(log_inv_disp, dtype=torch.float)
     if fix_decoder:
       m.F.requires_grad = False
       m.H.requires_grad = False
     m.fit(x, s, batch_size=batch_size, lr=lr, num_epochs=num_epochs, shuffle=batch_size < n,
           log_dir=f'/scratch/midway2/aksarkar/singlecell/runs/ntm-gamma-mix-ex-{oracle_init}-{fix_decoder}-{batch_size}-{lr}-{num_epochs}-{run}')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[75]:
   #+BEGIN_EXAMPLE
     NoisyTopicModel(
     (encoder): Encoder(
     (net): Sequential(
     (0): Log1p()
     (1): Linear(in_features=500, out_features=128, bias=True)
     (2): ReLU()
     (3): Linear(in_features=128, out_features=128, bias=True)
     (4): ReLU()
     (5): Linear(in_features=128, out_features=4, bias=True)
     (6): Softmax(dim=1)
     )
     )
     )
   #+END_EXAMPLE
   :END:

   #+BEGIN_SRC ipython
     with torch.no_grad():
       zhat = m.encoder.forward(torch.tensor(x, dtype=torch.float, device='cuda')).cpu().numpy()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[66]:
   :END:

   #+BEGIN_SRC ipython :ipyfile figure/expr-var-struct.org/gamma-mix-ex-ntm.png
     plt.clf()
     fig, ax = plt.subplots(1, 2)
     fig.set_size_inches(6, 4)
     for t in range(k):
       ax[0].scatter(dat.obsm['X_umap'][z[:,t].astype(bool),0],
                     dat.obsm['X_umap'][z[:,t].astype(bool),1],
                     c=plt.get_cmap('Dark2')(t), s=1, label=f'C{t}')
     ax[0].legend(frameon=False, handletextpad=0, markerscale=8, ncol=2, loc='upper center', bbox_to_anchor=(.5, -.25))
     ax[0].set_title('Ground truth')
     for t in range(k):
       ax[1].scatter(dat.obsm['X_umap'][zhat[:,t] > 0.5,0],
                     dat.obsm['X_umap'][zhat[:,t] > 0.5,1],
                     c=plt.get_cmap('Set2')(t), s=1, label=f'C{t}')
     ax[1].legend(frameon=False, handletextpad=0, markerscale=8, ncol=2, loc='upper center', bbox_to_anchor=(.5, -.25))
     ax[1].set_title('Noisy topic model')
     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[67]:
   [[file:figure/expr-var-struct.org/gamma-mix-ex-ntm.png]]
   :END:
