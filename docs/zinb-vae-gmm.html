<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2021-09-06 Mon 12:06 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Deep unsupervised clustering of scRNA-seq data</title>
<meta name="generator" content="Org mode">
<meta name="author" content="Abhishek Sarkar">
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link href="bootstrap.min.css" rel="stylesheet" />
<link rel="stylesheet" type="text/css" href="htmlize.css"/>
<link rel="stylesheet" type="text/css" href="main.css"/>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2017 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Deep unsupervised clustering of scRNA-seq data</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgf40b2f7">Introduction</a></li>
<li><a href="#setup">Setup</a></li>
<li><a href="#org1b5c4f4">Methods</a>
<ul>
<li><a href="#org0949e82">Variational autoencoder</a></li>
<li><a href="#org9ebf0d0">Gaussian GMVAE</a></li>
<li><a href="#org6cecaaf">Bernoulli GMVAE</a></li>
<li><a href="#orgdbd1073">Poisson GMVAE</a></li>
</ul>
</li>
<li><a href="#org6e57704">Results</a>
<ul>
<li><a href="#org3d1099e">Pinwheel example</a></li>
<li><a href="#orgda964e9">MNIST sanity check</a></li>
<li><a href="#org0bb3b13">Two-way example</a></li>
</ul>
</li>
<li><a href="#orga7f9d99">Related work</a></li>
</ul>
</div>
</div>

<div id="outline-container-orgf40b2f7" class="outline-2">
<h2 id="orgf40b2f7">Introduction</h2>
<div class="outline-text-2" id="text-orgf40b2f7">
<p>
Two major strategies for clustering scRNA-seq data are:
</p>

<ol class="org-ol">
<li>Building a \(k\)-nearest neighbor graph on the data, and applying a
community detection algorithm (e.g.,
<a href="https://doi.org/10.1088/1742-5468/2008/10/P10008">Blondel et al. 2008</a>,
<a href="https://arxiv.org/abs/1810.08473">Traag et al. 2018</a>)</li>
<li>Fitting a topic model to the data
(e.g., <a href="https://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1006599">Dey
et al. 2017</a>,
<a href="https://www.nature.com/articles/s41592-019-0367-1">Gonzáles-Blas et
al. 2019</a>)</li>
</ol>

<p>
The main disadvantage of strategy (1) is that, as commonly applied to
transformed counts, it does not separate measurement error and biological
variation of interest. The main disadvantage of strategy (2) is that it does
not account for transcriptional noise
(<a href="https://doi.org/10.1016/j.cell.2008.09.050">Raj 2008</a>). We
<a href="nbmix.html">previously developed a simple mixture model</a> which could
address this issue. Here, we develop an alternative method based on
<a href="http://ruishu.io/2016/12/25/gmvae/">GMVAE</a>, which allows us to explore a
different way of separating and representing transcriptional noise.
</p>
</div>
</div>

<div id="outline-container-orgbbadc60" class="outline-2">
<h2 id="setup"><a id="orgbbadc60"></a>Setup</h2>
<div class="outline-text-2" id="text-setup">
<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> anndata
<span class="org-keyword">import</span> mpebpm
<span class="org-keyword">import</span> numpy <span class="org-keyword">as</span> np
<span class="org-keyword">import</span> pandas <span class="org-keyword">as</span> pd
<span class="org-keyword">import</span> scanpy <span class="org-keyword">as</span> sc
<span class="org-keyword">import</span> scipy.stats <span class="org-keyword">as</span> st
<span class="org-keyword">import</span> sklearn.mixture <span class="org-keyword">as</span> skm
<span class="org-keyword">import</span> torch
<span class="org-keyword">import</span> torch.utils.data <span class="org-keyword">as</span> td
<span class="org-keyword">import</span> torch.utils.tensorboard <span class="org-keyword">as</span> tb
<span class="org-keyword">import</span> torchvision
<span class="org-keyword">import</span> umap
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">%matplotlib inline
%config <span class="org-variable-name">InlineBackend.figure_formats</span> = <span class="org-builtin">set</span>([<span class="org-string">'retina'</span>])
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> colorcet
<span class="org-keyword">import</span> matplotlib.pyplot <span class="org-keyword">as</span> plt
<span class="org-variable-name">plt.rcParams</span>[<span class="org-string">'figure.facecolor'</span>] = <span class="org-string">'w'</span>
<span class="org-variable-name">plt.rcParams</span>[<span class="org-string">'font.family'</span>] = <span class="org-string">'Nimbus Sans'</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-org1b5c4f4" class="outline-2">
<h2 id="org1b5c4f4">Methods</h2>
<div class="outline-text-2" id="text-org1b5c4f4">
</div>
<div id="outline-container-org0949e82" class="outline-3">
<h3 id="org0949e82">Variational autoencoder</h3>
<div class="outline-text-3" id="text-org0949e82">
<p>
In perhaps the simplest case, one assumes \(
   \DeclareMathOperator\Bern{Bernoulli}
   \DeclareMathOperator\E{E}
   \DeclareMathOperator\KL{\mathcal{KL}}
   \DeclareMathOperator\Gam{Gamma}
   \DeclareMathOperator\Mult{Multinomial}
   \DeclareMathOperator\N{\mathcal{N}}
   \DeclareMathOperator\Pois{Poisson}
   \DeclareMathOperator\diag{diag}
   \DeclareMathOperator\KL{\mathcal{KL}}
   \newcommand\kl[2]{\KL(#1\;\Vert\;#2)}
   \newcommand\xiplus{x_{i+}}
   \newcommand\mi{\mathbf{I}}
   \newcommand\va{\mathbf{a}}
   \newcommand\vb{\mathbf{b}}
   \newcommand\vu{\mathbf{u}}
   \newcommand\vx{\mathbf{x}}
   \newcommand\vy{\mathbf{y}}
   \newcommand\vz{\mathbf{z}}
   \newcommand\vlambda{\boldsymbol{\lambda}}
   \newcommand\vmu{\boldsymbol{\mu}}
   \newcommand\vphi{\boldsymbol{\phi}}
   \newcommand\vpi{\boldsymbol{\pi}}
   \)
</p>

\begin{align}
  \vx_i \mid \vz_i, s^2 &\sim \N(f(\vz_i), s^2 \mi)\\
  \vz_i &\sim \N(0, \mi),
\end{align}

<p>
where
</p>

<ul class="org-ul">
<li>\(\vx_i\) denotes a \(p\)-dimensional observation</li>
<li>\(\vz_i\) denotes the \(d\)-dimensional latent variable corresponding to
observation \(\vx_i\), with \(d \ll p\)</li>
<li>\(f\) is a (fully connected, say) neural network mapping latent variables
to the (noiseless, true) mean vector</li>
</ul>

<p>
This simple case is a generalization of PPCA
(<a href="https://dx.doi.org/10.1111/1467-9868.00196">Tipping and Bishop
1999</a>). To perform inference, typically one introduces a variational
approximation parameterized by an inference network
(<a href="https://escholarship.org/uc/item/34j1h7k5">Gershman and Goodman 2014</a>).
</p>

\begin{equation}
  q(\vz_i \mid \vx_i) = \N(\mu(\vz_i), \diag(\sigma^2(\vz_i))),
\end{equation}

<p>
where \(\mu, \sigma^2\) give the mean and diagonal of the covariance matrix
of the approximate posterior distribution.
</p>

<p>
<i>Remark</i> The inference network <a href="vae-unamortized.html">is not strictly
necessary</a>.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="orgec24408"><span class="org-keyword">class</span> <span class="org-type">FC</span>(torch.nn.Module):
  <span class="org-doc">"""Fully connected layers"""</span>
  <span class="org-keyword">def</span> <span class="org-function-name">__init__</span>(<span class="org-keyword">self</span>, input_dim, hidden_dim=128):
    <span class="org-builtin">super</span>().__init__()
    <span class="org-keyword">self</span>.net = torch.nn.Sequential(
      torch.nn.Linear(input_dim, hidden_dim),
      torch.nn.ReLU(),
      torch.nn.BatchNorm1d(hidden_dim),
    )

  <span class="org-keyword">def</span> <span class="org-function-name">forward</span>(<span class="org-keyword">self</span>, x):
    <span class="org-keyword">return</span> <span class="org-keyword">self</span>.net(x)

<span class="org-keyword">class</span> <span class="org-type">DeepGaussian</span>(torch.nn.Module):
  <span class="org-doc">"""Gaussian distribution parameterized by FC networks for mean and (diagonal)</span>
<span class="org-doc">scale"""</span>
  <span class="org-keyword">def</span> <span class="org-function-name">__init__</span>(<span class="org-keyword">self</span>, input_dim, output_dim, hidden_dim=128):
    <span class="org-builtin">super</span>().__init__()
    <span class="org-keyword">self</span>.net = FC(input_dim, hidden_dim)
    <span class="org-keyword">self</span>.mean = torch.nn.Linear(hidden_dim, output_dim)
    <span class="org-keyword">self</span>.scale = torch.nn.Sequential(torch.nn.Linear(hidden_dim, output_dim), torch.nn.Softplus())

  <span class="org-keyword">def</span> <span class="org-function-name">forward</span>(<span class="org-keyword">self</span>, x):
    <span class="org-variable-name">q</span> = <span class="org-keyword">self</span>.net(x)
    <span class="org-keyword">return</span> <span class="org-keyword">self</span>.mean(q), <span class="org-keyword">self</span>.scale(q) + 1e-3

<span class="org-keyword">class</span> <span class="org-type">DeepCategorical</span>(torch.nn.Module):
  <span class="org-doc">"""Categorical distribution parameterized by FC network for logits"""</span>
  <span class="org-keyword">def</span> <span class="org-function-name">__init__</span>(<span class="org-keyword">self</span>, input_dim, output_dim, hidden_dim=128):
    <span class="org-builtin">super</span>().__init__()
    <span class="org-keyword">self</span>.net = FC(input_dim, hidden_dim)
    <span class="org-keyword">self</span>.logits = torch.nn.Linear(hidden_dim, output_dim)

  <span class="org-keyword">def</span> <span class="org-function-name">forward</span>(<span class="org-keyword">self</span>, x):
    <span class="org-variable-name">q</span> = <span class="org-keyword">self</span>.net(x)
    <span class="org-keyword">return</span> <span class="org-keyword">self</span>.logits(q)
</pre>
</div>

<p>
Then, the ELBO
</p>

\begin{equation}
  \ell = \sum_i \E_q[\ln p(\vz_i \mid \vz_i)] - \KL(q(\vz_i \mid \vx_i) \Vert p(\vz_i)),
\end{equation}

<p>
where the second term is analytic (since it is the KL divergence between two
Gaussians). In practice, one replaces the intractable first term \(\E_q[\ln
   p(\vz_i \mid \vz_i)]\) with a Monte Carlo integral, where samples from \(q\)
are simulated from a standard distribution transformed by some
differentiable function \(g\)
(<a href="https://openreview.net/forum?id=33X9fd2-9FyZd">Kingma and Welling
2014</a>). Then, it is trivial to generalize this setup to other likelihoods.
</p>
</div>
</div>

<div id="outline-container-org9ebf0d0" class="outline-3">
<h3 id="org9ebf0d0">Gaussian GMVAE</h3>
<div class="outline-text-3" id="text-org9ebf0d0">
<p>
Now assume
</p>

\begin{align}
  \vx_i \mid \vz_i, \sigma^2 &\sim \N(f(\vz_i), \sigma^2\mi)\\
  \vz_i \mid y_i &\sim \N(m(y_i), s^2(y_i))\\
  y_i \mid \va &\sim \Mult(1, \va),
\end{align}

<p>
where
</p>

<ul class="org-ul">
<li>\(y_i \in \{1, \ldots, k\}\) denotes the latent cluster assignment</li>
<li>\(m(\cdot), s^2(\cdot)\) are fully-connected neural networks mapping
latent labels to latent variables (in particular, \(m\) gives the cluster
centroids)</li>
<li>\(\va\) is the \(k\)-vector of prior probabilities for each cluster
assignment</li>
</ul>

<p>
Now, the (marginal) prior on latent variables \(\vz_i\) is a mixture of
Gaussians. To perform inference, introduce a variational approximation
</p>

\begin{align}
  q(y_i, \vz_i \mid \vx_i) &= q(y_i \mid \vx_i)\, q(\vz_i \mid \vx_i, y_i)\\
  q(y_i \mid \vx_i) &= \Mult(1, \pi(\vx_i))\\
  q(\vz_i \mid \vx_i, y_i) &= \N(\mu(\vx_i, y_i), \sigma^2(\vx_i, y_i)),
\end{align}

<p>
where
</p>

<ul class="org-ul">
<li>\(\pi(\cdot)\) is a fully-connected neural network mapping observations to
latent labels (more precisely, posterior probabilities)</li>
<li>\(\mu(\cdot), \sigma^2(\cdot)\) are fully-connected neural networks mapping
observations and latent labels to latent variables.</li>
</ul>

<p>
Importantly, the approximate posterior distribution of latent variables is
Gaussian, not a mixture of Gaussians. The ELBO
</p>

\begin{equation}
  \ell = \sum_i \E_q\left[\ln p(\vx_i \mid \vz_i) + \ln\frac{p(y_i)}{q(y_i \mid \vx_i)} + \ln\frac{p(\vz_i \mid y_i)}{q(\vz_i \mid \vx_i, y_i)}\right],
\end{equation}

<p>
in which the expectation is replaced by a Monte Carlo integral as above. As
a sanity check (reproducing previous results), implement this case.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="orgdcc3b93"><span class="org-keyword">class</span> <span class="org-type">GMVAE</span>(torch.nn.Module):
  <span class="org-keyword">def</span> <span class="org-function-name">__init__</span>(<span class="org-keyword">self</span>, input_dim, latent_dim, n_clusters, hidden_dim=128):
    <span class="org-builtin">super</span>().__init__()
    <span class="org-keyword">self</span>.n_clusters = n_clusters
    <span class="org-keyword">self</span>.sigma_raw = torch.nn.Parameter(torch.zeros([1]))
    <span class="org-keyword">self</span>.encoder_y = DeepCategorical(input_dim, n_clusters, hidden_dim)
    <span class="org-keyword">self</span>.encoder_z = DeepGaussian(input_dim + n_clusters, latent_dim, hidden_dim)
    <span class="org-keyword">self</span>.decoder_z = DeepGaussian(n_clusters, latent_dim, hidden_dim)
    <span class="org-keyword">self</span>.decoder_x = torch.nn.Sequential(
      FC(latent_dim, hidden_dim),
      torch.nn.Linear(hidden_dim, input_dim)
    )

  <span class="org-keyword">def</span> <span class="org-function-name">forward</span>(<span class="org-keyword">self</span>, x, labels=<span class="org-constant">None</span>, writer=<span class="org-constant">None</span>, global_step=<span class="org-constant">None</span>, eps=1e-16):
    <span class="org-comment-delimiter"># </span><span class="org-comment">[batch_size, n_clusters]</span>
    <span class="org-variable-name">logits</span> = <span class="org-keyword">self</span>.encoder_y.forward(x)
    <span class="org-variable-name">probs</span> = torch.nn.functional.softmax(logits, dim=1)
    <span class="org-comment-delimiter"># </span><span class="org-comment">Important: accumulate negative ELBO</span>
    <span class="org-variable-name">loss</span> = (probs * (torch.log(probs + eps) + torch.log(torch.tensor(<span class="org-keyword">self</span>.n_clusters, dtype=torch.<span class="org-builtin">float</span>)))).<span class="org-builtin">sum</span>()
    <span class="org-keyword">assert</span> <span class="org-keyword">not</span> torch.isnan(loss)
    <span class="org-keyword">if</span> writer <span class="org-keyword">is</span> <span class="org-keyword">not</span> <span class="org-constant">None</span> <span class="org-keyword">and</span> labels <span class="org-keyword">is</span> <span class="org-keyword">not</span> <span class="org-constant">None</span>:
      <span class="org-comment-delimiter"># </span><span class="org-comment">labels is one-hot [batch_size, n_clusters]</span>
      <span class="org-keyword">with</span> torch.no_grad():
        <span class="org-keyword">if</span> labels.shape[1] == 2:
          writer.add_scalar(<span class="org-string">'loss/cross_entropy_p'</span>, torch.nn.functional.binary_cross_entropy(probs, labels), global_step)
          writer.add_scalar(<span class="org-string">'loss/cross_entropy_1p'</span>, torch.nn.functional.binary_cross_entropy(1 - probs, labels), global_step)
        writer.add_scalar(<span class="org-string">'loss/cond_entropy'</span>, -(probs * torch.log(probs + eps)).mean(), global_step)
    <span class="org-variable-name">y</span> = torch.eye(<span class="org-keyword">self</span>.n_clusters).cuda()
    <span class="org-comment-delimiter"># </span><span class="org-comment">[n_clusters, latent_dim]</span>
    <span class="org-variable-name">prior_mean</span>, <span class="org-variable-name">prior_scale</span> = <span class="org-keyword">self</span>.decoder_z.forward(y)
    <span class="org-comment-delimiter"># </span><span class="org-comment">Important: really marginalize over y</span>
    <span class="org-keyword">for</span> k <span class="org-keyword">in</span> <span class="org-builtin">range</span>(<span class="org-keyword">self</span>.n_clusters):
      <span class="org-variable-name">mean</span>, <span class="org-variable-name">scale</span> = <span class="org-keyword">self</span>.encoder_z.forward(torch.cat([x, y[k].repeat(x.shape[0], 1)], dim=1))
      <span class="org-comment-delimiter"># </span><span class="org-comment">[batch_size, latent_dim]</span>
      <span class="org-comment-delimiter"># </span><span class="org-comment">TODO: n_samples &gt; 1 breaks BatchNorm in decoder</span>
      <span class="org-variable-name">qz</span> = torch.distributions.Normal(mean, scale).rsample()
      <span class="org-variable-name">loss</span> += (probs[:,k].reshape(-1, 1) * (torch.distributions.Normal(mean, scale).log_prob(qz) - torch.distributions.Normal(prior_mean[k], prior_scale[k]).log_prob(qz))).<span class="org-builtin">sum</span>()
      <span class="org-keyword">assert</span> <span class="org-keyword">not</span> torch.isnan(loss)
      <span class="org-comment-delimiter"># </span><span class="org-comment">[batch_size, input_dim]</span>
      <span class="org-variable-name">mu</span> = <span class="org-keyword">self</span>.decoder_x(qz)
      <span class="org-variable-name">loss</span> -= (torch.distributions.Normal(mu, torch.nn.functional.softplus(<span class="org-keyword">self</span>.sigma_raw)).log_prob(x)).<span class="org-builtin">sum</span>()
      <span class="org-keyword">assert</span> <span class="org-keyword">not</span> torch.isnan(loss)
      <span class="org-keyword">assert</span> loss &gt; 0
    <span class="org-keyword">if</span> writer <span class="org-keyword">is</span> <span class="org-keyword">not</span> <span class="org-constant">None</span>:
      writer.add_scalar(<span class="org-string">'loss/neg_elbo'</span>, loss, global_step)
    <span class="org-keyword">return</span> loss

  <span class="org-keyword">def</span> <span class="org-function-name">fit</span>(<span class="org-keyword">self</span>, data, n_epochs=100, log_dir=<span class="org-constant">None</span>, **kwargs):
    <span class="org-keyword">assert</span> torch.cuda.is_available()
    <span class="org-keyword">self</span>.cuda()
    <span class="org-keyword">if</span> log_dir <span class="org-keyword">is</span> <span class="org-keyword">not</span> <span class="org-constant">None</span>:
      <span class="org-variable-name">writer</span> = tb.SummaryWriter(log_dir)
    <span class="org-keyword">else</span>:
      <span class="org-variable-name">writer</span> = <span class="org-constant">None</span>
    <span class="org-variable-name">opt</span> = torch.optim.RMSprop(<span class="org-keyword">self</span>.parameters(), **kwargs)
    <span class="org-variable-name">global_step</span> = 0
    <span class="org-keyword">for</span> epoch <span class="org-keyword">in</span> <span class="org-builtin">range</span>(n_epochs):
      <span class="org-keyword">for</span> batch <span class="org-keyword">in</span> data:
        <span class="org-variable-name">x</span> = batch.pop(0).cuda()
        <span class="org-keyword">if</span> batch:
          <span class="org-variable-name">y</span> = batch.pop(0).cuda()
        opt.zero_grad()
        <span class="org-variable-name">loss</span> = <span class="org-keyword">self</span>.forward(x, labels=y, writer=writer, global_step=global_step)
        <span class="org-keyword">if</span> torch.isnan(loss):
          <span class="org-keyword">raise</span> <span class="org-type">RuntimeError</span>(<span class="org-string">'nan loss'</span>)
        loss.backward()
        opt.step()
        <span class="org-variable-name">global_step</span> += 1
    <span class="org-keyword">return</span> <span class="org-keyword">self</span>

  <span class="org-type">@property</span>
  <span class="org-type">@torch.no_grad</span>()
  <span class="org-keyword">def</span> <span class="org-function-name">sigma</span>(<span class="org-keyword">self</span>):
    <span class="org-keyword">return</span> torch.nn.functional.softplus(<span class="org-keyword">self</span>.sigma_raw).cpu().numpy()

  <span class="org-type">@torch.no_grad</span>()
  <span class="org-keyword">def</span> <span class="org-function-name">get_labels</span>(<span class="org-keyword">self</span>, data):
    <span class="org-variable-name">yhat</span> = []
    <span class="org-keyword">for</span> x, *_ <span class="org-keyword">in</span> data:
      yhat.append(torch.nn.functional.softmax(fit.encoder_y.forward(x), dim=1).cpu().numpy())
    <span class="org-keyword">return</span> np.vstack(yhat)

  <span class="org-type">@torch.no_grad</span>()
  <span class="org-keyword">def</span> <span class="org-function-name">get_latent</span>(<span class="org-keyword">self</span>, data):
    <span class="org-variable-name">zhat</span> = []
    <span class="org-variable-name">y</span> = torch.eye(<span class="org-keyword">self</span>.n_clusters).cuda()
    <span class="org-keyword">for</span> x, *_ <span class="org-keyword">in</span> data:
      <span class="org-variable-name">yhat</span> = torch.nn.functional.softmax(<span class="org-keyword">self</span>.encoder_y.forward(x), dim=1)
      zhat.append(torch.stack([yhat[:,k].reshape(-1, 1) * fit.encoder_z.forward(torch.cat([x, y[k].repeat(x.shape[0], 1)], dim=1))[0]
                               <span class="org-keyword">for</span> k <span class="org-keyword">in</span> <span class="org-builtin">range</span>(<span class="org-keyword">self</span>.n_clusters)]).<span class="org-builtin">sum</span>(dim=0).cpu().numpy())
    <span class="org-keyword">return</span> np.vstack(zhat)

  <span class="org-type">@torch.no_grad</span>()
  <span class="org-keyword">def</span> <span class="org-function-name">predict</span>(<span class="org-keyword">self</span>, data):
    <span class="org-variable-name">xhat</span> = []
    <span class="org-variable-name">y</span> = torch.eye(<span class="org-keyword">self</span>.n_clusters).cuda()
    <span class="org-keyword">for</span> x, *_ <span class="org-keyword">in</span> data:
      <span class="org-variable-name">yhat</span> = torch.nn.functional.softmax(<span class="org-keyword">self</span>.encoder_y.forward(x), dim=1)
      <span class="org-variable-name">zhat</span> = torch.stack([yhat[:,k].reshape(-1, 1) * fit.encoder_z.forward(torch.cat([x, y[k].repeat(x.shape[0], 1)], dim=1))[0]
                          <span class="org-keyword">for</span> k <span class="org-keyword">in</span> <span class="org-builtin">range</span>(fit.n_clusters)]).<span class="org-builtin">sum</span>(dim=0)
      xhat.append(<span class="org-keyword">self</span>.decoder_x(zhat).cpu().numpy())
    <span class="org-keyword">return</span> np.vstack(xhat)
</pre>
</div>
</div>
</div>

<div id="outline-container-org6cecaaf" class="outline-3">
<h3 id="org6cecaaf">Bernoulli GMVAE</h3>
<div class="outline-text-3" id="text-org6cecaaf">
<p>
As a sanity check (applying to MNIST), implement the approach for the
Bernoulli likelihood.
</p>

\begin{align}
  x_{ij} \mid \vz_i &\sim \Bern((f(\vz_i))_j)\\
  \vz_i \mid y_i &\sim \N(m(y_i), s^2(y_i))\\
  y_i \mid \va &\sim \Mult(1, \va).
\end{align}

<div class="org-src-container">
<pre class="src src-ipython" id="orgf28a8c5"><span class="org-keyword">class</span> <span class="org-type">BGMVAE</span>(torch.nn.Module):
  <span class="org-doc">"""GMVAE with Bernoulli likelihood"""</span>
  <span class="org-keyword">def</span> <span class="org-function-name">__init__</span>(<span class="org-keyword">self</span>, input_dim, latent_dim, n_clusters, hidden_dim=128):
    <span class="org-builtin">super</span>().__init__()
    <span class="org-keyword">self</span>.n_clusters = n_clusters
    <span class="org-keyword">self</span>.encoder_y = DeepCategorical(input_dim, n_clusters, hidden_dim)
    <span class="org-keyword">self</span>.encoder_z = DeepGaussian(input_dim + n_clusters, latent_dim, hidden_dim)
    <span class="org-keyword">self</span>.decoder_z = DeepGaussian(n_clusters, latent_dim, hidden_dim)
    <span class="org-keyword">self</span>.decoder_x = torch.nn.Sequential(
      FC(latent_dim, hidden_dim),
      torch.nn.Linear(hidden_dim, input_dim),
      torch.nn.Sigmoid()
    )

  <span class="org-keyword">def</span> <span class="org-function-name">forward</span>(<span class="org-keyword">self</span>, x, labels=<span class="org-constant">None</span>, writer=<span class="org-constant">None</span>, global_step=<span class="org-constant">None</span>):
    <span class="org-comment-delimiter"># </span><span class="org-comment">[batch_size, n_clusters]</span>
    <span class="org-variable-name">logits</span> = <span class="org-keyword">self</span>.encoder_y.forward(x)
    <span class="org-variable-name">probs</span> = torch.nn.functional.softmax(logits, dim=1)
    <span class="org-comment-delimiter"># </span><span class="org-comment">Important: Negative ELBO</span>
    <span class="org-variable-name">loss</span> = (probs * (torch.log(probs + 1e-16) + torch.log(torch.tensor(<span class="org-keyword">self</span>.n_clusters, dtype=torch.<span class="org-builtin">float</span>)))).<span class="org-builtin">sum</span>()
    <span class="org-keyword">assert</span> <span class="org-keyword">not</span> torch.isnan(loss)
    <span class="org-keyword">if</span> writer <span class="org-keyword">is</span> <span class="org-keyword">not</span> <span class="org-constant">None</span> <span class="org-keyword">and</span> labels <span class="org-keyword">is</span> <span class="org-keyword">not</span> <span class="org-constant">None</span>:
      <span class="org-keyword">with</span> torch.no_grad():
        <span class="org-variable-name">cond_entropy</span> = -(probs * torch.log(probs + 1e-16)).mean()
        writer.add_scalar(<span class="org-string">'loss/cond_entropy'</span>, cond_entropy, global_step)
        <span class="org-variable-name">acc</span> = 0.
        <span class="org-keyword">for</span> k <span class="org-keyword">in</span> <span class="org-builtin">range</span>(labels.shape[1]):
          <span class="org-variable-name">query</span> = torch.argmax(labels, dim=1) == k
          <span class="org-keyword">if</span> query.<span class="org-builtin">any</span>():
            <span class="org-variable-name">idx</span> = torch.argmax(probs[query].<span class="org-builtin">sum</span>(dim=0))
            <span class="org-variable-name">acc</span> += (torch.argmax(probs[query], dim=1) == idx).<span class="org-builtin">sum</span>()
        writer.add_scalar(<span class="org-string">'loss/accuracy'</span>, acc / x.shape[0], global_step)
    <span class="org-comment-delimiter"># </span><span class="org-comment">Assume cuda</span>
    <span class="org-variable-name">y</span> = torch.eye(<span class="org-keyword">self</span>.n_clusters).cuda()
    <span class="org-comment-delimiter"># </span><span class="org-comment">[n_clusters, latent_dim]</span>
    <span class="org-variable-name">prior_mean</span>, <span class="org-variable-name">prior_scale</span> = <span class="org-keyword">self</span>.decoder_z.forward(y)
    <span class="org-keyword">for</span> k <span class="org-keyword">in</span> <span class="org-builtin">range</span>(<span class="org-keyword">self</span>.n_clusters):
      <span class="org-variable-name">mean</span>, <span class="org-variable-name">scale</span> = <span class="org-keyword">self</span>.encoder_z.forward(torch.cat([x, y[k].repeat(x.shape[0], 1)], dim=1))
      <span class="org-comment-delimiter"># </span><span class="org-comment">[batch_size, latent_dim]</span>
      <span class="org-comment-delimiter"># </span><span class="org-comment">TODO: n_samples &gt; 1 breaks BatchNorm in decoder</span>
      <span class="org-variable-name">qz</span> = torch.distributions.Normal(mean, scale).rsample()
      <span class="org-variable-name">loss</span> += (probs[:,k].reshape(-1, 1) * (torch.distributions.Normal(mean, scale).log_prob(qz) - torch.distributions.Normal(prior_mean[k], prior_scale[k]).log_prob(qz))).<span class="org-builtin">sum</span>()
      <span class="org-keyword">assert</span> <span class="org-keyword">not</span> torch.isnan(loss)
      <span class="org-comment-delimiter"># </span><span class="org-comment">[batch_size, input_dim]</span>
      <span class="org-variable-name">px</span> = <span class="org-keyword">self</span>.decoder_x(qz)
      <span class="org-variable-name">loss</span> -= (probs[:,k].reshape(-1, 1) * (x * torch.log(px + 1e-16) + (1 - x) * torch.log(1 - px + 1e-16))).<span class="org-builtin">sum</span>()
      <span class="org-keyword">assert</span> <span class="org-keyword">not</span> torch.isnan(loss)
      <span class="org-keyword">assert</span> loss &gt; 0
    <span class="org-keyword">if</span> writer <span class="org-keyword">is</span> <span class="org-keyword">not</span> <span class="org-constant">None</span>:
      writer.add_scalar(<span class="org-string">'loss/neg_elbo'</span>, loss, global_step)
    <span class="org-keyword">return</span> loss

  <span class="org-keyword">def</span> <span class="org-function-name">fit</span>(<span class="org-keyword">self</span>, data, n_epochs=100, log_dir=<span class="org-constant">None</span>, **kwargs):
    <span class="org-keyword">self</span>.cuda()
    <span class="org-keyword">if</span> log_dir <span class="org-keyword">is</span> <span class="org-keyword">not</span> <span class="org-constant">None</span>:
      <span class="org-variable-name">writer</span> = tb.SummaryWriter(log_dir)
    <span class="org-keyword">else</span>:
      <span class="org-variable-name">writer</span> = <span class="org-constant">None</span>
    <span class="org-variable-name">opt</span> = torch.optim.RMSprop(<span class="org-keyword">self</span>.parameters(), **kwargs)
    <span class="org-variable-name">global_step</span> = 0
    <span class="org-keyword">for</span> epoch <span class="org-keyword">in</span> <span class="org-builtin">range</span>(n_epochs):
      <span class="org-keyword">for</span> x, y <span class="org-keyword">in</span> data:
        <span class="org-comment-delimiter"># </span><span class="org-comment">TODO: put entire MNIST on GPU in advance</span>
        <span class="org-variable-name">x</span> = x.cuda()
        <span class="org-variable-name">y</span> = y.cuda()
        opt.zero_grad()
        <span class="org-variable-name">loss</span> = <span class="org-keyword">self</span>.forward(x, labels=y, writer=writer, global_step=global_step)
        <span class="org-keyword">if</span> torch.isnan(loss):
          <span class="org-keyword">raise</span> <span class="org-type">RuntimeError</span>(<span class="org-string">'nan loss'</span>)
        loss.backward()
        opt.step()
        <span class="org-variable-name">global_step</span> += 1
    <span class="org-keyword">return</span> <span class="org-keyword">self</span>

  <span class="org-type">@torch.no_grad</span>()
  <span class="org-keyword">def</span> <span class="org-function-name">get_labels</span>(<span class="org-keyword">self</span>, data):
    <span class="org-variable-name">yhat</span> = []
    <span class="org-keyword">for</span> x, *_ <span class="org-keyword">in</span> data:
      <span class="org-variable-name">x</span> = x.cuda()
      yhat.append(torch.nn.functional.softmax(fit.encoder_y.forward(x), dim=1).cpu().numpy())
    <span class="org-keyword">return</span> np.vstack(yhat)

  <span class="org-type">@torch.no_grad</span>()
  <span class="org-keyword">def</span> <span class="org-function-name">get_latent</span>(<span class="org-keyword">self</span>, data):
    <span class="org-variable-name">zhat</span> = []
    <span class="org-variable-name">y</span> = torch.eye(<span class="org-keyword">self</span>.n_clusters).cuda()
    <span class="org-keyword">for</span> x, *_ <span class="org-keyword">in</span> data:
      <span class="org-variable-name">x</span> = x.cuda()
      <span class="org-variable-name">yhat</span> = torch.nn.functional.softmax(<span class="org-keyword">self</span>.encoder_y.forward(x), dim=1)
      zhat.append(torch.stack([yhat[:,k].reshape(-1, 1) * fit.encoder_z.forward(torch.cat([x, y[k].repeat(x.shape[0], 1)], dim=1))[0]
                               <span class="org-keyword">for</span> k <span class="org-keyword">in</span> <span class="org-builtin">range</span>(fit.n_clusters)]).<span class="org-builtin">sum</span>(dim=0).cpu().numpy())
    <span class="org-keyword">return</span> np.vstack(zhat)
</pre>
</div>
</div>
</div>

<div id="outline-container-orgdbd1073" class="outline-3">
<h3 id="orgdbd1073">Poisson GMVAE</h3>
<div class="outline-text-3" id="text-orgdbd1073">
<p>
Now implement the case we are interested in for scRNA-seq data, with Poisson
measurement model (likelihood):
</p>

\begin{align}
  \vx_i \mid \xiplus, \vz_i &\sim \Pois(\xiplus (\lambda(\vz_i))_j)
  \vz_i \mid y_i &\sim \N(m(y_i), s^2(y_i))\\
  y_i \mid \va &\sim \Mult(1, \va),
\end{align}

<p>
where
</p>

<ul class="org-ul">
<li>\(x_{ij}\) denotes the number of molecules of gene \(j = 1, \ldots, p\)
observed in cell \(i = 1, \ldots, n\)</li>
<li>\(\xiplus \triangleq \sum_j x_{ij}\) denotes the total number of molecules
observed in cell \(i\)</li>
<li>\(\lambda(\cdot)\) is a fully-connected neural network mapping latent
variables to true gene expression</li>
</ul>

<div class="org-src-container">
<pre class="src src-ipython" id="org712d798"><span class="org-keyword">class</span> <span class="org-type">PGMVAE</span>(torch.nn.Module):
  <span class="org-keyword">def</span> <span class="org-function-name">__init__</span>(<span class="org-keyword">self</span>, input_dim, latent_dim, n_clusters, hidden_dim=128):
    <span class="org-builtin">super</span>().__init__()
    <span class="org-keyword">self</span>.n_clusters = n_clusters
    <span class="org-keyword">self</span>.encoder_y = DeepCategorical(input_dim, n_clusters, hidden_dim)
    <span class="org-keyword">self</span>.encoder_z = DeepGaussian(input_dim + n_clusters, latent_dim, hidden_dim)
    <span class="org-keyword">self</span>.decoder_z = DeepGaussian(n_clusters, latent_dim, hidden_dim)
    <span class="org-keyword">self</span>.decoder_x = torch.nn.Sequential(
      FC(latent_dim, hidden_dim),
      torch.nn.Linear(hidden_dim, input_dim),
      torch.nn.Softplus()
    )

  <span class="org-keyword">def</span> <span class="org-function-name">forward</span>(<span class="org-keyword">self</span>, x, s, labels=<span class="org-constant">None</span>, writer=<span class="org-constant">None</span>, global_step=<span class="org-constant">None</span>):
    <span class="org-comment-delimiter"># </span><span class="org-comment">[batch_size, n_clusters]</span>
    <span class="org-variable-name">logits</span> = <span class="org-keyword">self</span>.encoder_y.forward(x)
    <span class="org-variable-name">probs</span> = torch.nn.functional.softmax(logits, dim=1)
    <span class="org-comment-delimiter"># </span><span class="org-comment">Important: Negative ELBO</span>
    <span class="org-variable-name">loss</span> = (probs * (torch.log(probs + 1e-16) + torch.log(torch.tensor(<span class="org-keyword">self</span>.n_clusters, dtype=torch.<span class="org-builtin">float</span>)))).<span class="org-builtin">sum</span>()
    <span class="org-keyword">assert</span> <span class="org-keyword">not</span> torch.isnan(loss)
    <span class="org-keyword">if</span> writer <span class="org-keyword">is</span> <span class="org-keyword">not</span> <span class="org-constant">None</span> <span class="org-keyword">and</span> labels <span class="org-keyword">is</span> <span class="org-keyword">not</span> <span class="org-constant">None</span>:
      <span class="org-keyword">with</span> torch.no_grad():
        writer.add_scalar(<span class="org-string">'loss/cross_entropy'</span>, <span class="org-builtin">min</span>(torch.nn.functional.binary_cross_entropy(probs, labels), torch.nn.functional.binary_cross_entropy(1 - probs, labels)), global_step)
        writer.add_scalar(<span class="org-string">'loss/cond_entropy'</span>, -(probs * torch.log(probs + 1e-16)).mean(), global_step)
    <span class="org-comment-delimiter"># </span><span class="org-comment">Assume cuda</span>
    <span class="org-variable-name">y</span> = torch.eye(<span class="org-keyword">self</span>.n_clusters).cuda()
    <span class="org-comment-delimiter"># </span><span class="org-comment">[n_clusters, latent_dim]</span>
    <span class="org-variable-name">prior_mean</span>, <span class="org-variable-name">prior_scale</span> = <span class="org-keyword">self</span>.decoder_z.forward(y)
    <span class="org-keyword">for</span> k <span class="org-keyword">in</span> <span class="org-builtin">range</span>(<span class="org-keyword">self</span>.n_clusters):
      <span class="org-variable-name">mean</span>, <span class="org-variable-name">scale</span> = <span class="org-keyword">self</span>.encoder_z.forward(torch.cat([x, y[k].repeat(x.shape[0], 1)], dim=1))
      <span class="org-comment-delimiter"># </span><span class="org-comment">[batch_size, latent_dim]</span>
      <span class="org-comment-delimiter"># </span><span class="org-comment">TODO: n_samples &gt; 1 breaks BatchNorm in decoder</span>
      <span class="org-variable-name">qz</span> = torch.distributions.Normal(mean, scale).rsample()
      <span class="org-variable-name">loss</span> += (probs[:,k].reshape(-1, 1) * (torch.distributions.Normal(mean, scale).log_prob(qz) - torch.distributions.Normal(prior_mean[k], prior_scale[k]).log_prob(qz))).<span class="org-builtin">sum</span>()
      <span class="org-keyword">assert</span> <span class="org-keyword">not</span> torch.isnan(loss)
      <span class="org-comment-delimiter"># </span><span class="org-comment">[batch_size, input_dim]</span>
      <span class="org-variable-name">lam</span> = <span class="org-keyword">self</span>.decoder_x(qz)
      <span class="org-variable-name">loss</span> -= (probs[:,k].reshape(-1, 1) * (x * torch.log(s * lam) - s * lam - torch.lgamma(x + 1))).<span class="org-builtin">sum</span>()
      <span class="org-keyword">assert</span> <span class="org-keyword">not</span> torch.isnan(loss)
      <span class="org-keyword">assert</span> loss &gt; 0
    <span class="org-keyword">if</span> writer <span class="org-keyword">is</span> <span class="org-keyword">not</span> <span class="org-constant">None</span>:
      writer.add_scalar(<span class="org-string">'loss/neg_elbo'</span>, loss, global_step)
    <span class="org-keyword">return</span> loss

  <span class="org-keyword">def</span> <span class="org-function-name">fit</span>(<span class="org-keyword">self</span>, data, n_epochs=100, log_dir=<span class="org-constant">None</span>, **kwargs):
    <span class="org-keyword">self</span>.cuda()
    <span class="org-keyword">if</span> log_dir <span class="org-keyword">is</span> <span class="org-keyword">not</span> <span class="org-constant">None</span>:
      <span class="org-variable-name">writer</span> = tb.SummaryWriter(log_dir)
    <span class="org-variable-name">opt</span> = torch.optim.RMSprop(<span class="org-keyword">self</span>.parameters(), **kwargs)
    <span class="org-variable-name">global_step</span> = 0
    <span class="org-keyword">for</span> epoch <span class="org-keyword">in</span> <span class="org-builtin">range</span>(n_epochs):
      <span class="org-keyword">for</span> batch <span class="org-keyword">in</span> data:
        <span class="org-keyword">assert</span> <span class="org-builtin">len</span>(batch) == 3
        <span class="org-variable-name">x</span> = batch.pop(0)
        <span class="org-variable-name">s</span> = batch.pop(0)
        <span class="org-keyword">if</span> batch:
          <span class="org-variable-name">y</span> = batch.pop(0)
        <span class="org-keyword">else</span>:
          <span class="org-variable-name">y</span> = <span class="org-constant">None</span>
        opt.zero_grad()
        <span class="org-variable-name">loss</span> = <span class="org-keyword">self</span>.forward(x, s, labels=y, writer=writer, global_step=global_step)
        <span class="org-keyword">if</span> torch.isnan(loss):
          <span class="org-keyword">raise</span> <span class="org-type">RuntimeError</span>(<span class="org-string">'nan loss'</span>)
        loss.backward()
        opt.step()
        <span class="org-variable-name">global_step</span> += 1
    <span class="org-keyword">return</span> <span class="org-keyword">self</span>

  <span class="org-type">@torch.no_grad</span>()
  <span class="org-keyword">def</span> <span class="org-function-name">get_labels</span>(<span class="org-keyword">self</span>, data):
    <span class="org-variable-name">yhat</span> = []
    <span class="org-keyword">for</span> x, *_ <span class="org-keyword">in</span> data:
      yhat.append(torch.nn.functional.softmax(fit.encoder_y.forward(x), dim=1).cpu().numpy())
    <span class="org-keyword">return</span> np.vstack(yhat)

  <span class="org-type">@torch.no_grad</span>()
  <span class="org-keyword">def</span> <span class="org-function-name">get_latent</span>(<span class="org-keyword">self</span>, data):
    <span class="org-variable-name">zhat</span> = []
    <span class="org-variable-name">y</span> = torch.eye(<span class="org-keyword">self</span>.n_clusters).cuda()
    <span class="org-keyword">for</span> x, *_ <span class="org-keyword">in</span> data:
      <span class="org-variable-name">yhat</span> = torch.nn.functional.softmax(<span class="org-keyword">self</span>.encoder_y.forward(x), dim=1)
      zhat.append(torch.stack([yhat[:,k].reshape(-1, 1) * fit.encoder_z.forward(torch.cat([x, y[k].repeat(x.shape[0], 1)], dim=1))[0]
                               <span class="org-keyword">for</span> k <span class="org-keyword">in</span> <span class="org-builtin">range</span>(fit.n_clusters)]).<span class="org-builtin">sum</span>(dim=0).cpu().numpy())
    <span class="org-keyword">return</span> np.vstack(zhat)
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-org6e57704" class="outline-2">
<h2 id="org6e57704">Results</h2>
<div class="outline-text-2" id="text-org6e57704">
</div>
<div id="outline-container-org3d1099e" class="outline-3">
<h3 id="org3d1099e">Pinwheel example</h3>
<div class="outline-text-3" id="text-org3d1099e">
<p>
Use the &ldquo;pinwheel data&rdquo; from Johnson et al. 2017
(<a href="https://github.com/mattjj/svae/blob/master/experiments/gmm_svae_synth.py#L11">source
code</a> associated with Dilokthanakul et al. 2017).
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">make_pinwheel_data</span>(radial_std, tangential_std, num_classes, num_per_class, rate, seed=0):
  <span class="org-variable-name">rng</span> = np.random.default_rng(seed)
  <span class="org-variable-name">rads</span> = np.linspace(0, 2 * np.pi, num_classes, endpoint=<span class="org-constant">False</span>)
  <span class="org-variable-name">features</span> = rng.normal(size=(num_classes * num_per_class, 2)) * np.array([radial_std, tangential_std])
  <span class="org-variable-name">features</span>[:,0] += 1.
  <span class="org-variable-name">labels</span> = np.repeat(np.arange(num_classes), num_per_class)
  <span class="org-variable-name">angles</span> = rads[labels] + rate * np.exp(features[:,0])
  <span class="org-variable-name">rotations</span> = np.stack([np.cos(angles), -np.sin(angles), np.sin(angles), np.cos(angles)])
  <span class="org-variable-name">rotations</span> = np.reshape(rotations.T, (-1, 2, 2))
  <span class="org-keyword">return</span> np.einsum(<span class="org-string">'ti,tij-&gt;tj'</span>, features, rotations), labels
</pre>
</div>

<p>
Generate data following Dilokthanakul et al. 2017.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">n</span> = 10000
<span class="org-variable-name">K</span> = 5
<span class="org-variable-name">dat</span>, <span class="org-variable-name">labels</span> = make_pinwheel_data(0.1, 0.05, K, n // K, .5, seed=0)
<span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Dark2'</span>)
plt.clf()
plt.gcf().set_size_inches(4, 3)
<span class="org-keyword">for</span> k <span class="org-keyword">in</span> <span class="org-builtin">range</span>(K):
  plt.scatter(dat[labels == k,0], dat[labels == k,1], s=1, color=cm(k), label=f<span class="org-string">'C{k}'</span>)
plt.legend(frameon=<span class="org-constant">False</span>, loc=<span class="org-string">'center left'</span>, bbox_to_anchor=(1, .5), handletextpad=0, markerscale=4)
plt.xlabel(<span class="org-string">'$x_1$'</span>)
plt.ylabel(<span class="org-string">'$x_2$'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/zinb-vae-gmm.org/pinwheel-ex.png" alt="pinwheel-ex.png">
</p>
</div>

<p>
Fit a GMM by EM, with oracle number of components.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">m0</span> = skm.GaussianMixture(n_components=K, random_state=1).fit(dat)
</pre>
</div>

<p>
Plot the fit.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Dark2'</span>)
plt.clf()
plt.gcf().set_size_inches(3.5, 3)
<span class="org-variable-name">lim</span> = [-1.5, 1.5]
<span class="org-variable-name">x</span>, <span class="org-variable-name">y</span> = np.meshgrid(np.linspace(*lim), np.linspace(*lim))
<span class="org-keyword">for</span> k <span class="org-keyword">in</span> <span class="org-builtin">range</span>(K):
  plt.scatter(dat[labels == k,0], dat[labels == k,1], s=1, color=cm(k), label=f<span class="org-string">'C{k}'</span>)
  <span class="org-variable-name">f</span> = st.multivariate_normal(m0.means_[k], m0.covariances_[k]).pdf(np.stack([x, y]).reshape(2, -1).T).reshape(50, 50)
  plt.contour(x, y, f, linewidths=0.5, levels=5, cmap=colorcet.cm[<span class="org-string">'gray_r'</span>])
plt.legend(frameon=<span class="org-constant">False</span>, loc=<span class="org-string">'center left'</span>, bbox_to_anchor=(1, .5), handletextpad=0, markerscale=4)
plt.xlabel(<span class="org-string">'$x_1$'</span>)
plt.ylabel(<span class="org-string">'$x_2$'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/zinb-vae-gmm.org/pinwheel-ex-gmm.png" alt="pinwheel-ex-gmm.png">
</p>
</div>

<p>
Fit GMVAE.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">seed</span> = 9
<span class="org-variable-name">batch_size</span> = 64
<span class="org-variable-name">lr</span> = 5e-4
<span class="org-variable-name">n_epochs</span> = 10
<span class="org-variable-name">latent_dim</span> = 2
torch.manual_seed(seed)
<span class="org-variable-name">data</span> = td.DataLoader(
  td.TensorDataset(
    torch.tensor(dat, device=<span class="org-string">'cuda'</span>, dtype=torch.<span class="org-builtin">float</span>),
    torch.tensor(pd.get_dummies(pd.Categorical(labels)).values, device=<span class="org-string">'cuda'</span>, dtype=torch.<span class="org-builtin">float</span>)),
  drop_last=<span class="org-constant">True</span>,
  shuffle=<span class="org-constant">True</span>,
  batch_size=batch_size)
<span class="org-comment-delimiter"># </span><span class="org-comment">Fix number of clusters to the ground truth</span>
<span class="org-variable-name">fit</span> = (GMVAE(input_dim=2, latent_dim=latent_dim, n_clusters=K)
       .fit(data,
            lr=lr,
            n_epochs=n_epochs,
            <span class="org-comment-delimiter"># </span><span class="org-comment">log_dir=f'runs/gmvae/pinwheel-{latent_dim}-{K}-{seed}-{lr:.1g}-{batch_size}-{n_epochs}'</span>
       )
)
</pre>
</div>

<p>
Plot the predicted values.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">data</span> = td.DataLoader(
  td.TensorDataset(
    torch.tensor(dat, device=<span class="org-string">'cuda'</span>, dtype=torch.<span class="org-builtin">float</span>),
    torch.tensor(pd.get_dummies(pd.Categorical(labels)).values, device=<span class="org-string">'cuda'</span>, dtype=torch.<span class="org-builtin">float</span>)),
  batch_size=batch_size)
<span class="org-variable-name">xhat</span> = fit.predict(data)

<span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Dark2'</span>)
plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 2, sharey=<span class="org-constant">True</span>)
fig.set_size_inches(6, 3)
<span class="org-keyword">for</span> k <span class="org-keyword">in</span> <span class="org-builtin">range</span>(K):
  ax[0].scatter(dat[labels == k,0], dat[labels == k,1], s=1, color=cm(k), label=f<span class="org-string">'C{k}'</span>)
  ax[1].scatter(xhat[labels == k, 0], xhat[labels == k, 1], s=1, color=cm(k), label=f<span class="org-string">'C{k}'</span>)
ax[0].set_title(<span class="org-string">'Observed data'</span>)
ax[1].set_title(<span class="org-string">'Denoised data'</span>)
ax[1].legend(frameon=<span class="org-constant">False</span>, loc=<span class="org-string">'center left'</span>, bbox_to_anchor=(1, .5), handletextpad=0, markerscale=4)
<span class="org-keyword">for</span> a <span class="org-keyword">in</span> ax:
  a.set_aspect(<span class="org-string">'equal'</span>, adjustable=<span class="org-string">'datalim'</span>)
  a.set_xlabel(<span class="org-string">'$x_1$'</span>)
ax[0].set_ylabel(<span class="org-string">'$x_2$'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/zinb-vae-gmm.org/pinwheel-ex-gmvae-xhat.png" alt="pinwheel-ex-gmvae-xhat.png">
</p>
</div>

<p>
Assign labels to the estimated clusters using the most frequently occurring
ground truth label, then assess clustering accuracy.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">data</span> = td.DataLoader(
  td.TensorDataset(
    torch.tensor(dat, device=<span class="org-string">'cuda'</span>, dtype=torch.<span class="org-builtin">float</span>),
    torch.tensor(pd.get_dummies(pd.Categorical(labels)).values, device=<span class="org-string">'cuda'</span>, dtype=torch.<span class="org-builtin">float</span>)),
  batch_size=batch_size)
<span class="org-variable-name">yhat</span> = fit.get_labels(data)

<span class="org-variable-name">acc</span> = 0
<span class="org-keyword">for</span> k <span class="org-keyword">in</span> <span class="org-builtin">range</span>(K):
  <span class="org-variable-name">query</span> = np.argmax(pd.get_dummies(pd.Categorical(labels)).values, axis=1) == k
  <span class="org-variable-name">idx</span> = np.argmax(yhat[query].<span class="org-builtin">sum</span>(axis=0))
  <span class="org-variable-name">acc</span> += (np.argmax(yhat[query], axis=1) == idx).<span class="org-builtin">sum</span>()
acc / n
</pre>
</div>

<pre class="example">
0.2832

</pre>

<p>
Plot the latent space.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Dark2'</span>)
plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 2, sharey=<span class="org-constant">True</span>)
fig.set_size_inches(6, 3)

<span class="org-variable-name">lim</span> = [-5, 5]
<span class="org-variable-name">x</span>, <span class="org-variable-name">y</span> = np.meshgrid(np.linspace(*lim), np.linspace(*lim))
<span class="org-keyword">with</span> torch.no_grad():
  <span class="org-variable-name">prior_mean</span>, <span class="org-variable-name">prior_scale</span> = fit.decoder_z.forward(torch.eye(fit.n_clusters).cuda())
  <span class="org-variable-name">prior_mean</span> = prior_mean.cpu().numpy()
  <span class="org-variable-name">prior_scale</span> = prior_scale.cpu().numpy()
<span class="org-keyword">for</span> k <span class="org-keyword">in</span> <span class="org-builtin">range</span>(K):
  <span class="org-variable-name">f</span> = st.multivariate_normal(prior_mean[k], np.diag(prior_scale[k] ** 2)).pdf(np.stack([x, y]).reshape(2, -1).T).reshape(50, 50)
  ax[0].scatter(*prior_mean[k], marker=<span class="org-string">'x'</span>, s=16, color=cm(k), label=f<span class="org-string">'C{k}'</span>)
  ax[0].contour(x, y, f, cmap=colorcet.cm[<span class="org-string">'blues'</span>])
ax[0].set_title(<span class="org-string">'Prior $p(z)$'</span>)
ax[0].set_ylabel(<span class="org-string">'$z_2$'</span>)

<span class="org-variable-name">data</span> = td.DataLoader(
  td.TensorDataset(
    torch.tensor(dat, device=<span class="org-string">'cuda'</span>, dtype=torch.<span class="org-builtin">float</span>),
    torch.tensor(pd.get_dummies(pd.Categorical(labels)).values, device=<span class="org-string">'cuda'</span>, dtype=torch.<span class="org-builtin">float</span>)),
  batch_size=batch_size)
<span class="org-variable-name">zhat</span> = fit.get_latent(data)
<span class="org-keyword">for</span> k <span class="org-keyword">in</span> <span class="org-builtin">range</span>(K):
  ax[1].scatter(zhat[labels == k,0], zhat[labels == k,1], s=1, color=cm(k), label=f<span class="org-string">'C{k}'</span>)
ax[1].legend(frameon=<span class="org-constant">False</span>, loc=<span class="org-string">'center left'</span>, bbox_to_anchor=(1, .5), handletextpad=0, markerscale=4)
ax[1].set_title(<span class="org-string">'Posterior $q(z \mid x)$'</span>)
<span class="org-keyword">for</span> a <span class="org-keyword">in</span> ax:
  a.set_aspect(<span class="org-string">'equal'</span>, adjustable=<span class="org-string">'datalim'</span>)
  a.set_xlabel(<span class="org-string">'$z_1$'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/zinb-vae-gmm.org/pinwheel-ex-gmvae.png" alt="pinwheel-ex-gmvae.png">
</p>
</div>
</div>
</div>

<div id="outline-container-orgda964e9" class="outline-3">
<h3 id="orgda964e9">MNIST sanity check</h3>
<div class="outline-text-3" id="text-orgda964e9">
<p>
Replicate Rui Shu&rsquo;s results, modeling grayscale images of handwritten digits
as binarized, flattened pixel vectors. Assume
</p>

\begin{align}
  x_{ij} \mid \vz_i &\sim \Bern((r(\vz_i))_j)\\
  \vz_i \mid y_i &\sim \N(m(y_i), s^2(y_i))\\
  y_i \mid \va &\sim \Mult(1, \va),
\end{align}

<p>
where \(r\) is a fully-connected neural network mapping latent variables to
pixel probabilities.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">batch_size</span> = 32
<span class="org-variable-name">mnist_data</span> = torchvision.datasets.MNIST(
  root=<span class="org-string">'/scratch/midway2/aksarkar/singlecell/'</span>,
  transform=<span class="org-keyword">lambda</span> x: (np.frombuffer(x.tobytes(), dtype=<span class="org-string">'uint8'</span>) &gt; 0).astype(np.float32),
  target_transform=<span class="org-keyword">lambda</span> x: np.eye(10)[x])
<span class="org-variable-name">data</span> = td.DataLoader(mnist_data, batch_size=batch_size, shuffle=<span class="org-constant">True</span>,
                     pin_memory=<span class="org-constant">True</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">k</span> = 10
<span class="org-variable-name">seed</span> = 5
<span class="org-variable-name">lr</span> = 5e-3
<span class="org-variable-name">n_epochs</span> = 2
<span class="org-variable-name">latent_dim</span> = 10
torch.manual_seed(seed)
<span class="org-variable-name">fit</span> = (BGMVAE(input_dim=mnist_data[0][0].shape[0], latent_dim=latent_dim, n_clusters=k)
       .fit(data,
            lr=lr,
            n_epochs=n_epochs,
            log_dir=f<span class="org-string">'runs/pgmvae/mnist-{latent_dim}-{k}-{seed}-{lr:.1g}-{batch_size}-{n_epochs}'</span>)
)
</pre>
</div>

<p>
Get the approximate posterior distribution on labels.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">data</span> = td.DataLoader(mnist_data, batch_size=batch_size, shuffle=<span class="org-constant">False</span>,
                     pin_memory=<span class="org-constant">True</span>)
<span class="org-variable-name">yhat</span> = fit.get_labels(data)
<span class="org-variable-name">y</span> = np.vstack([label <span class="org-keyword">for</span> _, label <span class="org-keyword">in</span> mnist_data])
</pre>
</div>

<p>
Assign labels to the estimated clusters using the most frequently occurring
ground truth label, then assess clustering accuracy.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">acc</span> = 0
<span class="org-keyword">for</span> k <span class="org-keyword">in</span> <span class="org-builtin">range</span>(y.shape[1]):
  <span class="org-variable-name">query</span> = np.argmax(y, axis=1) == k
  <span class="org-variable-name">idx</span> = np.argmax(yhat[query].<span class="org-builtin">sum</span>(axis=0))
  <span class="org-variable-name">acc</span> += (np.argmax(yhat[query], axis=1) == idx).<span class="org-builtin">sum</span>()
  <span class="org-keyword">print</span>(k, idx)
acc / y.shape[0]
</pre>
</div>

<pre class="example">
0.6509666666666667

</pre>
</div>
</div>

<div id="outline-container-org0bb3b13" class="outline-3">
<h3 id="org0bb3b13">Two-way example</h3>
<div class="outline-text-3" id="text-org0bb3b13">
<p>
Read sorted immune cell scRNA-seq data
(<a href="https://dx.doi.org/10.1038/ncomms14049">Zheng et al. 2017</a>).
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">dat</span> = anndata.read_h5ad(<span class="org-string">'/scratch/midway2/aksarkar/singlecell/mix4.h5ad'</span>)
</pre>
</div>

<p>
We <a href="nbmix.html#org59c95a8">previously applied the standard
methodology</a>.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">mix2</span> = dat[dat.obs[<span class="org-string">'cell_type'</span>].isin([<span class="org-string">'cytotoxic_t'</span>, <span class="org-string">'b_cells'</span>])]
sc.pp.filter_genes(mix2, min_counts=1)
sc.pp.pca(mix2, zero_center=<span class="org-constant">False</span>)
sc.pp.neighbors(mix2)
sc.tl.leiden(mix2)
sc.tl.umap(mix2)
mix2
</pre>
</div>

<pre class="example">
AnnData object with n_obs × n_vars = 20294 × 17808
obs: 'barcode', 'cell_type', 'leiden'
var: 'ensg', 'name', 'n_cells', 'n_counts'
uns: 'leiden', 'neighbors', 'pca', 'umap'
obsm: 'X_pca', 'X_umap'
varm: 'PCs'
obsp: 'connectivities', 'distances'
</pre>


<div class="figure">
<p><img src="figure/nbmix.org/mix2.png" alt="mix2.png">
</p>
</div>

<p>
Fit PGMVAE.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">assert</span> torch.cuda.is_available()
<span class="org-variable-name">batch_size</span> = 32
<span class="org-variable-name">x</span> = mix2.X
<span class="org-variable-name">y</span> = pd.get_dummies(mix2.obs[<span class="org-string">'cell_type'</span>], sparse=<span class="org-constant">True</span>).sparse.to_coo().tocsr()
<span class="org-variable-name">sparse_data</span> = mpebpm.sparse.SparseDataset(
  mpebpm.sparse.CSRTensor(x.data, x.indices, x.indptr, x.shape, dtype=torch.<span class="org-builtin">float</span>).cuda(),
  torch.tensor(x.<span class="org-builtin">sum</span>(axis=1), dtype=torch.<span class="org-builtin">float</span>).cuda(),
  mpebpm.sparse.CSRTensor(y.data, y.indices, y.indptr, y.shape, dtype=torch.<span class="org-builtin">float</span>).cuda(),
)
<span class="org-variable-name">collate_fn</span> = <span class="org-builtin">getattr</span>(sparse_data, <span class="org-string">'collate_fn'</span>, td.dataloader.default_collate)
<span class="org-variable-name">data</span> = td.DataLoader(sparse_data, batch_size=batch_size, shuffle=<span class="org-constant">True</span>,
                     collate_fn=collate_fn)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">k</span> = 2
<span class="org-variable-name">seed</span> = 3
<span class="org-variable-name">lr</span> = 5e-3
<span class="org-variable-name">n_epochs</span> = 2
<span class="org-variable-name">latent_dim</span> = 4
torch.manual_seed(seed)
<span class="org-variable-name">fit</span> = (PGMVAE(input_dim=mix2.shape[1], latent_dim=latent_dim, n_clusters=k)
       .fit(data,
            lr=lr,
            n_epochs=n_epochs,
            log_dir=f<span class="org-string">'runs/pgmvae/mix2-{latent_dim}-{k}-{seed}-{lr:.1g}-{batch_size}-{n_epochs}'</span>)
)
</pre>
</div>

<p>
Get the approximate posterior labels.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">data</span> = td.DataLoader(sparse_data, batch_size=batch_size, shuffle=<span class="org-constant">False</span>,
                     collate_fn=collate_fn)
<span class="org-variable-name">yhat</span> = fit.get_labels(data)
<span class="org-variable-name">mix2.obs</span>[<span class="org-string">'comp'</span>] = np.argmax(yhat, axis=1)
</pre>
</div>

<p>
Plot the result.
</p>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 2, sharex=<span class="org-constant">True</span>, sharey=<span class="org-constant">True</span>)
fig.set_size_inches(5, 3)
<span class="org-keyword">for</span> a, k, t, cm <span class="org-keyword">in</span> <span class="org-builtin">zip</span>(ax, [<span class="org-string">'cell_type'</span>, <span class="org-string">'comp'</span>], [<span class="org-string">'Ground truth'</span>, <span class="org-string">'PGMVAE'</span>], [<span class="org-string">'Paired'</span>, <span class="org-string">'Dark2'</span>]):
  <span class="org-keyword">for</span> i, c <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(mix2.obs[k].unique()):
    a.plot(*mix2[mix2.obs[k] == c].obsm[<span class="org-string">"X_umap"</span>].T, c=plt.get_cmap(cm)(i), marker=<span class="org-string">'.'</span>, ms=1, lw=0, alpha=0.1, label=f<span class="org-string">'{k}_{i}'</span>)
  <span class="org-variable-name">leg</span> = a.legend(frameon=<span class="org-constant">False</span>, markerscale=8, handletextpad=0, loc=<span class="org-string">'upper center'</span>, bbox_to_anchor=(.5, -.25), ncol=2)
  <span class="org-keyword">for</span> h <span class="org-keyword">in</span> leg.legendHandles:
    h._legmarker.set_alpha(1)
  a.set_xlabel(<span class="org-string">'UMAP 1'</span>)
  a.set_title(t)
ax[0].set_ylabel(<span class="org-string">'UMAP 2'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/zinb-vae-gmm.org/mix2-pgmvae.png" alt="mix2-pgmvae.png">
</p>
</div>

<p>
Get the latent representation.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">zhat</span> = fit.get_latent(data)
</pre>
</div>

<p>
Plot a UMAP of the learned representation.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">u</span> = umap.UMAP(n_neighbors=5, n_components=2, metric=<span class="org-string">'euclidean'</span>).fit_transform(zhat)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 2, sharex=<span class="org-constant">True</span>, sharey=<span class="org-constant">True</span>)
fig.set_size_inches(5, 3)
<span class="org-keyword">for</span> a, k, t, cm <span class="org-keyword">in</span> <span class="org-builtin">zip</span>(ax, [<span class="org-string">'cell_type'</span>, <span class="org-string">'comp'</span>], [<span class="org-string">'Ground truth'</span>, <span class="org-string">'PGMVAE'</span>], [<span class="org-string">'Paired'</span>, <span class="org-string">'Dark2'</span>]):
  <span class="org-keyword">for</span> i, c <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(mix2.obs[k].unique()):
    a.plot(*u[mix2.obs[k] == c].T, c=plt.get_cmap(cm)(i), marker=<span class="org-string">'.'</span>, ms=1, lw=0, alpha=0.1, label=f<span class="org-string">'{k}_{i}'</span>)
  <span class="org-variable-name">leg</span> = a.legend(frameon=<span class="org-constant">False</span>, markerscale=8, handletextpad=0, loc=<span class="org-string">'upper center'</span>, bbox_to_anchor=(.5, -.25), ncol=2)
  <span class="org-keyword">for</span> h <span class="org-keyword">in</span> leg.legendHandles:
    h._legmarker.set_alpha(1)
  a.set_xlabel(<span class="org-string">'UMAP 1 ($\hat{z}$)'</span>)
  a.set_title(t)
ax[0].set_ylabel(<span class="org-string">'UMAP 2 ($\hat{z}$)'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/zinb-vae-gmm.org/mix2-zhat.png" alt="mix2-zhat.png">
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-orga7f9d99" class="outline-2">
<h2 id="orga7f9d99">Related work</h2>
<div class="outline-text-2" id="text-orga7f9d99">
<p>
scVI (<a href="https://dx.doi.org/10.1038/s41592-018-0229-2">Lopez et al. 2018</a>,
<a href="https://www.biorxiv.org/content/10.1101/532895v2">Xu et al. 2020</a>)
implements a deep unsupervised (more precisely, semi-supervised) clustering
model (<a href="https://arxiv.org/abs/1406.5298">Kingma et al. 2014</a>,
<a href="https://arxiv.org/abs/1611.02648">Dilokthanakul et al. 2016</a>).
</p>

\begin{align}
  x_{ij} \mid s_i, \lambda_{ij} &\sim \Pois(s_i \lambda_{ij})\\
  \ln s_i &\sim \N(\cdot)\\
  \lambda_{ij} \mid \vz_i &\sim \Gam(\phi_j^{-1}, (\mu_{\lambda}(\vz_i))_j^{-1} \phi_j^{-1})\\
  \vz_i \mid y_i, \vu_i &\sim \N(\mu_z(\vu_i, y_i), \diag(\sigma^2(\vu_i, y_i)))\\
  y_i \mid \vpi &\sim \Mult(1, \vpi)\\
  \vu_i &\sim \N(0, \mi).
\end{align}

<p>
where
</p>

<ul class="org-ul">
<li>\(y_i\) denotes the cluster assignment for cell \(i\)</li>
<li>\(\mu_z(\cdot), \sigma^2(\cdot)\) are neural networks mapping the latent
cluster variable \(y_i\) and Gaussian noise \(\vu_i\) to the latent variable
\(\vz_i\)</li>
<li>\(\mu_{\lambda}(\cdot)\) is a neural network mapping latent variable \(\vz_i\) to
latent gene expression \(\vlambda_{i}\)</li>
</ul>

<p>
To perform variational inference in this model, Lopez et al. introduce
inference networks
</p>

\begin{align}
  q(\vz_i \mid \vx_i) &= \N(\cdot)\\
  q(y_i \mid \vz_i) &= \Mult(1, \cdot).
\end{align}

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> scvi.dataset
<span class="org-keyword">import</span> scvi.inference
<span class="org-keyword">import</span> scvi.models

<span class="org-variable-name">expr</span> = scvi.dataset.AnnDatasetFromAnnData(temp)
<span class="org-variable-name">m</span> = scvi.models.VAEC(expr.nb_genes, n_batch=0, n_labels=2)
<span class="org-variable-name">train</span> = scvi.inference.UnsupervisedTrainer(m, expr, train_size=1, batch_size=32, show_progbar=<span class="org-constant">False</span>, n_epochs_kl_warmup=100)
train.train(n_epochs=1000, lr=1e-2)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">post</span> = train.create_posterior(train.model, expr)
<span class="org-variable-name">_</span>, <span class="org-variable-name">_</span>, <span class="org-variable-name">label</span> = post.get_latent()
</pre>
</div>

<p>
The main difference in our approach is that we do not make an assumption of
Gamma perturbations in latent gene expression. Instead, we implicitly assume
that transcriptional noise is structured, e.g. through the gene-gene
regulatory network, such that it is reasonable to model it in the low
dimensional space. This assumption simplifies the model in terms of both
inference and implementation.
</p>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Abhishek Sarkar</p>
<p class="date">Created: 2021-09-06 Mon 12:06</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
