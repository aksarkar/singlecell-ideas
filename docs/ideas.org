#+TITLE: Latent representation of single cell experiments
#+AUTHOR: Abhishek Sarkar
#+EMAIL: aksarkar@uchicago.edu
#+LANGUAGE: en
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline author:t
#+OPTIONS: broken-links:nil c:nil creator:nil d:(not "LOGBOOK") date:t e:t
#+OPTIONS: email:nil f:t inline:t num:t p:nil pri:nil prop:nil stat:t tags:t
#+OPTIONS: tasks:t tex:t timestamp:t title:t toc:t todo:t |:t
#+OPTIONS: html-link-use-abs-url:nil html-postamble:auto html-preamble:t
#+OPTIONS: html-scripts:t html-style:nil html5-fancy:nil tex:t
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="main.css" />
#+HTML_CONTAINER: div
#+DESCRIPTION:
#+KEYWORDS:

* Introduction

  We are interested in learning latent representations of single cell sequencing
  data. There are a number of potential questions which we could pursue:

  1. Can we make quantitative claims about clustering of cells in latent
     spaces?

  2. Can we learn hierarchical representations of cells?

  3. How should we think about differential expression between groups of
     single cells?

  4. How should we handle replicate data in latent variable models?

* Cell clustering

  Currently, the state of the art is to generate a visualization of
  cells in the latent space (e.g. principal components) and then make
  qualitative claims about the distances between points of interest.

  [[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4481139/][Macosko et al 2015]] developed Drop-Seq, a microfluidics platform which can now
  assay enough cells to start applying deep learning-based approaches.

  #+BEGIN_QUOTE
  We analyzed transcriptomes from 44,808 mouse retinal cells and identified
  39 transcriptionally distinct cell populations, creating a molecular atlas
  of gene expression for known retinal cell classes and novel candidate cell
  subtypes.
  #+END_QUOTE

  For comparison, the well-studied [[https://www.cs.toronto.edu/~kriz/cifar.html][CIFAR10 data set]] is 60,000 images.

  A number of analyses in the paper could be improved upon using modeling-based
  approaches. *However, most important is to first clarify what exactly the end
  goal of the analysis is.*

  #+BEGIN_QUOTE
  We projected the remaining 36,145 cells in the data into the tSNE analysis.
  We then combined a density clustering approach with post hoc differential
  expression analysis to divide 44,808 cells among 39 transcriptionally
  distinct clusters (Supplemental Experimental Procedures) ranging from 50 to
  29,400 cells in size (Figures 5B and 5C).
  #+END_QUOTE

  *Can we say anything more than things we already know from low-dimensional
  visualization?*

  The most obvious thing to do is fit a variational auto-encoder to the
  data. This is already being done ([[http://dx.doi.org/10.1101/178624][Ding et al. 2017]]), but the goal is to
  make a better visualization.

  #+BEGIN_QUOTE
  For all the scRNA-seq datasets, we used principal component analysis (as
  a noise-reduction preprocessing step) to project the cells into a
  100-dimensional space, and used the projected coordinates in the
  100-dimensional spaces as inputs to scvis.
  #+END_QUOTE

  This is the one obvious thing to improve on.

  #+BEGIN_QUOTE
  To explicitly encourage cells with similar expression profiles to be
  proximal (and those with dissimilar profiles to be distal) in the latent
  space, we add the t-SNE objective function on the latent z distribution as
  a constraint.
  #+END_QUOTE

  Improving the visualization in this way seems counterproductive when the
  goal is to use the visualization to learn something new. In particular, in
  unsupervised learning of images the goal is actually the opposite: to put
  the examples on a dense manifold. The canonical example of useful
  representations learned like this is to trace along dimensions of this
  manifold, generating images which are smooth interpolations between the
  observations.

  #+BEGIN_QUOTE
  We next analyzed the scvis learned probabilistic mapping from a training
  single-cell dataset, and tested how it performed on unseen data. We first
  trained a model on the mouse bipolar cell of the retina dataset, and then
  used the learned model to map the independently generated mouse retina
  dataset
  #+END_QUOTE

  *How to make this quantitative?*

  *What exactly are we trying to learn from low-dimensional visualization,
  and how do we validate any novel insights?*

  In the machine learning community, latent representations are instead
  typically evaluated by showing that the latent states (say, clusters) can
  be used to predict independent labeled data (say, cell type). With DropSeq,
  it might be feasible to hold out data points, although we might have to
  worry about which points get held out. Better would be to generate
  replicate data, but if we fail to predict in replicate data, can we really
  say that was a failure of the model or a failure of the data?

  We might have to tackle the more basic problem of correcting for
  confounders in single cell data.

  The most basic approach is to fit a variational autoencoder ([[https://arxiv.org/abs/1312.6114][Kingma and
  Welling 2013]]) to the cells.

  A more complicated approach would be to use an adversarial autoencoder
  [[https://arxiv.org/abs/1511.05644][(Makhzani et al., 2015]]) to include prior information about the latent
  space. For example, if we knew that the sample was a heterogenous mixture of
  some number of cell types, we could try to use a GMM prior on the latent
  states. This example was already pursued in the AAE paper. 

  We could try a DP prior on number of mixture components. Would AAE beat
  hierarchical Dirichlet process ([[https://people.eecs.berkeley.edu/~jordan/papers/hdp.pdf][Teh et al. 2005]]) in that case? However, this
  might be overcomplicated, and not help with the end inference goal.

* Hierarchical representations

  Ideally, we should learn not just individual genes, but gene
  sets/pathways/processes which change between cell states. We could
  do this either for cell cycle at one time point, or differentiation
  across several time points.

  *Can we learn the analogue of "high-level image features" from scRNA-Seq?*

  By analogy to computer vision, we need the equivalent of edge detectors, line
  detectors, etc. In simpler terms, we need the AlexNet ([[https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf][Krizhevsky et al 2012]])
  of gene expression. One of the key advances in this area which we should
  build off of is DCGAN ([[https://arxiv.org/abs/1511.06434][Radford et al 2016]]).

* Differential expression

  In a bulk-sequencing DE analysis, we expose cells to different conditions,
  take an average over expression in those conditions, then ask about
  difference in the means.

  But in single cells, different clusters in latent space don't really
  correspond to different conditions. Instead, the single cells are on
  trajectories between the states represented by the clusters.

  *Can we write this as regression over the latent dimensions instead of just
  comparing means between two groups?*

* Replicate data

  A natural idea is to require that replicate observations are generated from
  the same latent state, which leads to an interesting modeling problem. Again
  from [[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4481139/][Macosko et al 2015]]:

  #+BEGIN_QUOTE
  The retinal STAMPs were generated on 4 different days (weeks apart),
  utilizing different litters and multiple runs in several sessions, for a
  total of seven replicates.
  #+END_QUOTE
     
  How should we think about replicates in the context of deep implicit models
  (or really, any kind of latent variable model)?

  Do biological replicates really come from the same latent state? If we
  learn a hierarchical representation of latent states, can we require that
  they share some higher-level representation (assuming unknown variability
  is lower down)?

  Should we try to learn a latent representation of technical artifacts also?
  Or should we try to simply correct for technical artifacts (like [[https://www.nature.com/nbt/journal/v33/n2/full/nbt.3102.html][Buettner et
  al 2015]]).

* Useful datasets

  - Mouse retina: [[https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE63473][GSE63473]]
  - Mouse bipolar neurons: [[https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE81905][GSE81905]]
  - oligodendroglioma: [[https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE70630][GSE70630]]
  - metastatic melanoma: [[https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE72056][GSE72056]]
