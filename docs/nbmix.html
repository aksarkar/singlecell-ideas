<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2020-05-17 Sun 15:19 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Model-based clustering of scRNA-seq data</title>
<meta name="generator" content="Org mode">
<meta name="author" content="Abhishek Sarkar">
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link href="bootstrap.min.css" rel="stylesheet" />
<link rel="stylesheet" type="text/css" href="htmlize.css"/>
<link rel="stylesheet" type="text/css" href="main.css"/>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2017 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Model-based clustering of scRNA-seq data</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#introdcution">Introduction</a></li>
<li><a href="#setup">Setup</a></li>
<li><a href="#method">Method</a></li>
<li><a href="#results">Results</a>
<ul>
<li><a href="#example">Example</a></li>
<li><a href="#leiden">Leiden algorithm</a></li>
<li><a href="#mpebpm">MPEBPM</a></li>
<li><a href="#em">EM</a></li>
<li><a href="#amortized">Amortized inference</a></li>
</ul>
</li>
<li><a href="#related-work">Related work</a></li>
</ul>
</div>
</div>

<div id="outline-container-org34e8d5e" class="outline-2">
<h2 id="introdcution"><a id="org34e8d5e"></a>Introduction</h2>
<div class="outline-text-2" id="text-introdcution">
<p>
Two major strategies for clustering scRNA-seq data are:
</p>

<ol class="org-ol">
<li>Building a \(k\)-nearest neighbor graph on the data, and applying a
community detection algorithm (e.g.,
<a href="https://doi.org/10.1088/1742-5468/2008/10/P10008">Blondel et al. 2008</a>,
<a href="https://arxiv.org/abs/1810.08473">Traag et al. 2018</a>)</li>
<li>Fitting a topic model to the data
(e.g., <a href="https://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1006599">Dey
et al. 2017</a>,
<a href="https://www.nature.com/articles/s41592-019-0367-1">Gonzáles-Blas et
al. 2019</a>)</li>
</ol>

<p>
The main disadvantage of strategy (1) is that, as commonly applied to
transformed counts, it does not separate measurement error and biological
variation of interest. The main disadvantage of strategy (2) is that it does
not account for transcriptional noise
(<a href="https://doi.org/10.1016/j.cell.2008.09.050">Raj 2008</a>). Here, we develop
a simple model-based clustering algorithm which addresses both of these
issues.
</p>
</div>
</div>

<div id="outline-container-orgb4dd078" class="outline-2">
<h2 id="setup"><a id="orgb4dd078"></a>Setup</h2>
<div class="outline-text-2" id="text-setup">
<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> anndata
<span class="org-keyword">import</span> mpebpm.gam_mix
<span class="org-keyword">import</span> mpebpm.sgd
<span class="org-keyword">import</span> numpy <span class="org-keyword">as</span> np
<span class="org-keyword">import</span> pandas <span class="org-keyword">as</span> pd
<span class="org-keyword">import</span> scanpy <span class="org-keyword">as</span> sc
<span class="org-keyword">import</span> scipy.optimize <span class="org-keyword">as</span> so
<span class="org-keyword">import</span> scipy.special <span class="org-keyword">as</span> sp
<span class="org-keyword">import</span> scipy.stats <span class="org-keyword">as</span> st
<span class="org-keyword">import</span> scmodes
<span class="org-keyword">import</span> torch
<span class="org-keyword">import</span> torch.utils.data <span class="org-keyword">as</span> td
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">%matplotlib inline
%config <span class="org-variable-name">InlineBackend.figure_formats</span> = <span class="org-builtin">set</span>([<span class="org-string">'retina'</span>])
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> matplotlib
<span class="org-keyword">import</span> matplotlib.pyplot <span class="org-keyword">as</span> plt
<span class="org-variable-name">plt.rcParams</span>[<span class="org-string">'figure.facecolor'</span>] = <span class="org-string">'w'</span>
<span class="org-variable-name">plt.rcParams</span>[<span class="org-string">'font.family'</span>] = <span class="org-string">'Nimbus Sans'</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-org5ee032a" class="outline-2">
<h2 id="method"><a id="org5ee032a"></a>Method</h2>
<div class="outline-text-2" id="text-method">
<p>
We assume \(
  \DeclareMathOperator\Gam{Gamma}
  \DeclareMathOperator\Mult{Multinomial}
  \DeclareMathOperator\N{\mathcal{N}}
  \DeclareMathOperator\Pois{Poisson}
  \DeclareMathOperator\diag{diag}
  \newcommand\xiplus{x_{i+}}
  \newcommand\mi{\mathbf{I}}
  \newcommand\vu{\mathbf{u}}
  \newcommand\vx{\mathbf{x}}
  \newcommand\vz{\mathbf{z}}
  \newcommand\vlambda{\boldsymbol{\lambda}}
  \newcommand\vmu{\boldsymbol{\mu}}
  \newcommand\vphi{\boldsymbol{\phi}}
  \newcommand\vpi{\boldsymbol{\pi}}
  \)
</p>

\begin{align}
  x_{ij} \mid \xiplus, \lambda_{ij} &\sim \Pois(\xiplus \lambda_{ij})\\
  \lambda_{ij} \mid \vpi_i, \vmu_k, \vphi_k &\sim \sum_{k=1}^{K} \pi_{ik} \Gam(\phi_{kj}^{-1}, \phi_{kj}^{-1}\mu_{kj}^{-1}),
\end{align}

<p>
where
</p>

<ul class="org-ul">
<li>\(x_{ij}\) denotes the number of molecules of gene \(j\) observed in cell \(i\)</li>
<li>\(\xiplus \triangleq \sum_j x_{ij}\) denotes the total number of molecules
observed in cell \(i\)</li>
<li>\(\vpi_i\) denotes cluster assignment probabilities for cell \(i\)</li>
<li>\(\vmu_k\) denotes the cluster &ldquo;centroid&rdquo; for cluster \(k\), and
\(\vphi_k\) describes stochastic perturbations within each cluster</li>
</ul>

<p>
The intuition behind this model is that each cluster \(k\) is defined by a
collection of independent Gamma distributions, one per gene \(j\). These
Gamma distributions describe the distribution of true gene expression for
each gene in each cluster
(<a href="https://dx.doi.org/10.1101/2020.04.07.030007">Sarkar and Stephens
2020</a>). (In this parameterization, each has mean \(\mu_{kj}\) and variance
\(\mu_{kj}^2\phi_{kj}\).)
</p>

<p>
Under this model, the marginal likelihood is a mixture of negative
binomials. We can estimate \(\vpi, \vmu, \vphi\) by maximizing the likelihood
using an EM algorithm. Letting \(z_{ik} \in \{0, 1\}\) indicate whether cell
\(i\) is assigned to cluster \(k\), in the E step
</p>

\begin{equation}
  E[z_{ik} \mid \vx_i, \xiplus, \vpi_i, \vmu_k, \vphi_k] \propto \sum_j \int_0^{\infty} \Pois(x_{ij}; \xiplus\lambda) \Gam(\lambda; \phi_{kj}^{-1}, \phi_{kj}^{-1}\mu_{kj}^{-1})\, d\lambda.
\end{equation}

<p>
In the M step, we improve the likelihood by <a href="mepbpm.html">(batch)
gradient descent</a>, implemented in the Python package <code>mpebpm</code>. To make the
model amenable to stochastic gradient descent, we can instead maximize the
evidence lower bound and amortize inference
(<a href="https://escholarship.org/content/qt34j1h7k5/qt34j1h7k5.pdf">Gershman and
Goodman 2014</a>, <a href="https://arxiv.org/abs/1312.6114">Kingma and Welling 2014</a>,
<a href="http://proceedings.mlr.press/v32/rezende14.html">Rezende et al. 2014</a>),
learning a neural network \(f_z\) mapping \(\vx_i \rightarrow \vz_i\)
</p>

\begin{align}
  p(z_{i1}, \ldots, z_{iK}) &= \Mult(1, \vpi)\\
  q(z_{i1}, \ldots, z_{iK} \mid \vx_i) &= \Mult(1, f_z(\vx_i)).
\end{align}
</div>
</div>

<div id="outline-container-orgdca968c" class="outline-2">
<h2 id="results"><a id="orgdca968c"></a>Results</h2>
<div class="outline-text-2" id="text-results">
</div>
<div id="outline-container-org56f4812" class="outline-3">
<h3 id="example"><a id="org56f4812"></a>Example</h3>
<div class="outline-text-3" id="text-example">
<p>
Read sorted immune cell scRNA-seq data
(<a href="https://dx.doi.org/10.1038/ncomms14049">Zheng et al. 2017</a>).
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">dat</span> = anndata.read_h5ad(<span class="org-string">'/scratch/midway2/aksarkar/ideas/zheng-10-way.h5ad'</span>)
</pre>
</div>

<p>
Get 256 B cells and 256 cytotoxic T cells.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">b_cells</span> = dat[dat.obs[<span class="org-string">'cell_type'</span>] == <span class="org-string">'b_cells'</span>]
sc.pp.subsample(b_cells, n_obs=256, random_state=0)
<span class="org-variable-name">t_cells</span> = dat[dat.obs[<span class="org-string">'cell_type'</span>] == <span class="org-string">'cytotoxic_t'</span>]
sc.pp.subsample(t_cells, n_obs=256)
<span class="org-variable-name">temp</span> = b_cells.concatenate(t_cells)
sc.pp.filter_genes(temp, min_counts=1)
</pre>
</div>

<p>
Plot a UMAP embedding of the data, coloring points by the ground truth
labels.
</p>

<div class="org-src-container">
<pre class="src src-ipython">sc.pp.pca(temp)
sc.pp.neighbors(temp)
sc.tl.umap(temp)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Dark2'</span>)
plt.clf()
plt.gcf().set_size_inches(3, 3)
<span class="org-keyword">for</span> i, c <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(temp.obs[<span class="org-string">'cell_type'</span>].unique()):
  plt.plot(*temp[temp.obs[<span class="org-string">'cell_type'</span>] == c].obsm[<span class="org-string">"X_umap"</span>].T, c=cm(i), marker=<span class="org-string">'.'</span>, ms=2, lw=0, label=f<span class="org-string">'{c}'</span>)
plt.legend(frameon=<span class="org-constant">False</span>, markerscale=4, handletextpad=0)
plt.xlabel(<span class="org-string">'UMAP 1'</span>)
plt.ylabel(<span class="org-string">'UMAP 2'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/nbmix.org/sim-ex.png" alt="sim-ex.png">
</p>
</div>
</div>
</div>

<div id="outline-container-orgca14b7d" class="outline-3">
<h3 id="leiden"><a id="orgca14b7d"></a>Leiden algorithm</h3>
<div class="outline-text-3" id="text-leiden">
<p>
Apply the Leiden algorithm (<a href="https://arxiv.org/abs/1810.08473">Traag et
al. 2018</a>) to the data (&lt;1 s).
</p>

<div class="org-src-container">
<pre class="src src-ipython">sc.tl.leiden(temp, random_state=0)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Dark2'</span>)
plt.clf()
plt.gcf().set_size_inches(3, 3)
<span class="org-keyword">for</span> i, c <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(temp.obs[<span class="org-string">'leiden'</span>].unique()):
  plt.plot(*temp[temp.obs[<span class="org-string">'leiden'</span>] == c].obsm[<span class="org-string">"X_umap"</span>].T, c=cm(i), marker=<span class="org-string">'.'</span>, ms=2, lw=0, label=f<span class="org-string">'Cluster {i}'</span>)
plt.legend(frameon=<span class="org-constant">False</span>, markerscale=4, handletextpad=0)
plt.xlabel(<span class="org-string">'UMAP 1'</span>)
plt.ylabel(<span class="org-string">'UMAP 2'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/nbmix.org/leiden-ex.png" alt="leiden-ex.png">
</p>
</div>
</div>
</div>

<div id="outline-container-org97f5a8f" class="outline-3">
<h3 id="mpebpm"><a id="org97f5a8f"></a>MPEBPM</h3>
<div class="outline-text-3" id="text-mpebpm">
<p>
First, start from the ground truth \(z\) (labels), and estimate the Gamma
expression models.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">fit0</span> = mpebpm.sgd.ebpm_gamma(
  temp.X,
  onehot=pd.get_dummies(temp.obs[<span class="org-string">'cell_type'</span>]).values,
  batch_size=32,
  num_epochs=320,
  shuffle=<span class="org-constant">True</span>)
</pre>
</div>

<p>
Look at the differences in the estimated mean parameter for each gene, to
see how many genes are informative about the labels.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">query</span> = np.sort(np.diff(fit0[0], axis=0).ravel())
plt.clf()
plt.gcf().set_size_inches(4, 2)
plt.plot(query, lw=1, c=<span class="org-string">'k'</span>)
plt.axhline(y=0, lw=1, ls=<span class="org-string">':'</span>, c=<span class="org-string">'k'</span>)
plt.xlabel(<span class="org-string">'Gene'</span>)
plt.ylabel(r<span class="org-string">'Diff $\ln(\mu_j)$'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/nbmix.org/mepbpm-log-mu.png" alt="mepbpm-log-mu.png">
</p>
</div>

<p>
Estimate the cluster weights.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">L</span> = mpebpm.gam_mix._nb_mix_llik(
  x=torch.tensor(temp.X.A, dtype=torch.<span class="org-builtin">float</span>), 
  s=torch.tensor(temp.X.<span class="org-builtin">sum</span>(axis=1), dtype=torch.<span class="org-builtin">float</span>),
  log_mean=torch.tensor(fit0[0], dtype=torch.<span class="org-builtin">float</span>),
  log_inv_disp=torch.tensor(fit0[1], dtype=torch.<span class="org-builtin">float</span>))
<span class="org-variable-name">zhat</span> = torch.nn.functional.softmax(L, dim=1)
</pre>
</div>

<p>
Plot the log likelihood difference between the two components for each data
point.
</p>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
plt.gcf().set_size_inches(4, 2)
plt.plot(np.diff(L).ravel(), lw=0, marker=<span class="org-string">'.'</span>, c=<span class="org-string">'k'</span>, ms=2)
plt.axhline(y=0, lw=1, ls=<span class="org-string">':'</span>, c=<span class="org-string">'k'</span>)
plt.xlabel(<span class="org-string">'Cell'</span>)
plt.ylabel(<span class="org-string">'Diff log lik'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/nbmix.org/mpebpm-llik-diff.png" alt="mpebpm-llik-diff.png">
</p>
</div>

<p>
Compute the cross entropy between the estimated \(\hat{z}\) and the ground
truth.
</p>

<div class="org-src-container">
<pre class="src src-ipython">torch.nn.functional.binary_cross_entropy(
  zhat,
  torch.tensor(pd.get_dummies(temp.obs[<span class="org-string">'cell_type'</span>]).values, dtype=torch.<span class="org-builtin">float</span>))
</pre>
</div>

<pre class="example">
tensor(0.)

</pre>

<p>
Compute a weighted log likelihood.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">w</span> = torch.rand([512, 2])
<span class="org-variable-name">w</span> /= w.<span class="org-builtin">sum</span>(dim=1).unsqueeze(-1)
<span class="org-variable-name">m</span>, <span class="org-variable-name">_</span> = L.<span class="org-builtin">max</span>(dim=1, keepdim=<span class="org-constant">True</span>)
(m + torch.log(w * torch.exp(L - m) + 1e-8)).mean()
</pre>
</div>

<pre class="example">
tensor(-1872.5645)

</pre>

<p>
Try fitting the model from a random initialization (49 s).
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> imp; imp.<span class="org-builtin">reload</span>(mpebpm.gam_mix)
torch.manual_seed(0)
<span class="org-variable-name">fit</span> = mpebpm.gam_mix.ebpm_gam_mix_em(
  x=temp.X.A,
  s=temp.X.<span class="org-builtin">sum</span>(axis=1),
  y=torch.tensor(pd.get_dummies(temp.obs[<span class="org-string">'cell_type'</span>]).values, dtype=torch.<span class="org-builtin">float</span>).cuda(),
  k=2,
  num_epochs=400,
  max_em_iters=10,
  log_dir=<span class="org-string">'runs/nbmix/mpebpm-random-init0-iter10'</span>)
</pre>
</div>

<p>
Compute the cross entropy between the estimated \(\hat{z}\) and the ground
truth.
</p>

<div class="org-src-container">
<pre class="src src-ipython">torch.nn.functional.binary_cross_entropy(
  torch.tensor(fit[-1], dtype=torch.<span class="org-builtin">float</span>),
  torch.tensor(pd.get_dummies(temp.obs[<span class="org-string">'cell_type'</span>]).values, dtype=torch.<span class="org-builtin">float</span>))
</pre>
</div>

<pre class="example">
tensor(0.)

</pre>

<p>
Plot the UMAP, colored by the fitted clusters.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Dark2'</span>)
plt.clf()
plt.gcf().set_size_inches(3, 3)
<span class="org-keyword">for</span> i <span class="org-keyword">in</span> <span class="org-builtin">range</span>(fit[-1].shape[1]):
  plt.plot(*temp[fit[-1][:,i].astype(<span class="org-builtin">bool</span>)].obsm[<span class="org-string">"X_umap"</span>].T, c=cm(i), marker=<span class="org-string">'.'</span>, ms=2, lw=0, label=f<span class="org-string">'Cluster {i}'</span>)
plt.legend(frameon=<span class="org-constant">False</span>, markerscale=4, handletextpad=0)
plt.xlabel(<span class="org-string">'UMAP 1'</span>)
plt.ylabel(<span class="org-string">'UMAP 2'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/nbmix.org/sim-ex-fit.png" alt="sim-ex-fit.png">
</p>
</div>
</div>
</div>

<div id="outline-container-orgebb4a14" class="outline-3">
<h3 id="em"><a id="orgebb4a14"></a>EM</h3>
<div class="outline-text-3" id="text-em">
<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">_nb_mix_llik</span>(theta, x, s):
  <span class="org-doc">"""Return log likelihood matrix</span>

<span class="org-doc">  theta - array-like [2, k]</span>
<span class="org-doc">  x - array-like [n, 1]</span>
<span class="org-doc">  s - array-like [n, 1]</span>

<span class="org-doc">  """</span>
  <span class="org-comment-delimiter"># </span><span class="org-comment">Important: so.minimize flattens theta</span>
  <span class="org-variable-name">theta</span> = theta.reshape(2, -1)
  <span class="org-variable-name">mean</span> = np.exp(theta[0]).reshape(1, -1)
  <span class="org-comment-delimiter"># </span><span class="org-comment">Important: this can blow up</span>
  <span class="org-variable-name">inv_disp</span> = np.exp(np.clip(theta[1], -15, 15)).reshape(1, -1)
  <span class="org-comment-delimiter"># </span><span class="org-comment">[n, k]</span>
  <span class="org-variable-name">L</span> = st.nbinom(n=inv_disp, p=1 / (1 + s * mean / inv_disp)).logpmf(x.reshape(-1, 1))
  <span class="org-keyword">assert</span> L.shape == (x.shape[0], theta.shape[1])
  <span class="org-keyword">assert</span> np.isfinite(L).<span class="org-builtin">all</span>()
  <span class="org-keyword">return</span> L

<span class="org-keyword">def</span> <span class="org-function-name">_nb_mix_obj</span>(theta, x, z, s):
  <span class="org-doc">"""Return negative log likelihood</span>

<span class="org-doc">  theta - array-like [2, k]</span>
<span class="org-doc">  x - array-like [n, 1]</span>
<span class="org-doc">  z - array-like [n, k]</span>
<span class="org-doc">  s - array-like [n, 1]</span>

<span class="org-doc">  """</span>
  <span class="org-variable-name">L</span> = _nb_mix_llik(theta, x, s)
  <span class="org-variable-name">m</span> = L.<span class="org-builtin">max</span>(axis=1, keepdims=<span class="org-constant">True</span>)
  <span class="org-keyword">return</span> -(np.log(z) + L).mean()

<span class="org-keyword">def</span> <span class="org-function-name">ebpm_gamma_mix</span>(x, s, k, max_iters=100, tol=1e-3, verbose=<span class="org-constant">False</span>, seed=1):
  <span class="org-keyword">assert</span> x.ndim == 2
  <span class="org-variable-name">n</span>, <span class="org-variable-name">p</span> = x.shape
  <span class="org-keyword">assert</span> s.shape == (n, 1)
  <span class="org-keyword">assert</span> k &gt; 1
  <span class="org-variable-name">rng</span> = np.random.default_rng(seed)
  <span class="org-variable-name">log_mean</span> = rng.normal(size=(p, k))
  <span class="org-variable-name">log_inv_disp</span> = rng.normal(size=(p, k))
  <span class="org-variable-name">z</span> = np.ones((n, k)) / k
  <span class="org-variable-name">loss</span> = np.array([_nb_mix_obj(np.vstack([log_mean[j], log_inv_disp[j]]), x[:,j], z, s) <span class="org-keyword">for</span> j <span class="org-keyword">in</span> <span class="org-builtin">range</span>(p)]).<span class="org-builtin">sum</span>()
  <span class="org-keyword">if</span> verbose:
    <span class="org-keyword">print</span>(f<span class="org-string">'epoch 0: {loss:.4g}'</span>)
  <span class="org-keyword">for</span> t <span class="org-keyword">in</span> <span class="org-builtin">range</span>(max_iters):
    <span class="org-keyword">for</span> j <span class="org-keyword">in</span> <span class="org-builtin">range</span>(p):
      <span class="org-variable-name">opt</span> = so.minimize(_nb_mix_obj, x0=np.vstack([log_mean[j], log_inv_disp[j]]), args=(x[:,j], z, s), method=<span class="org-string">'nelder-mead'</span>, options={<span class="org-string">'maxiter'</span>:1000})
      <span class="org-keyword">if</span> <span class="org-keyword">not</span> opt.success:
        <span class="org-keyword">raise</span> <span class="org-type">RuntimeError</span>(f<span class="org-string">'M step failed to converge: {opt.message}'</span>)
      <span class="org-variable-name">log_mean</span>[j] = opt.x[0]
      <span class="org-variable-name">log_inv_disp</span>[j] = opt.x[1]
    <span class="org-variable-name">L</span> = np.array([_nb_mix_llik(np.vstack(log_mean[j], log_inv_disp[j]), x[:,j], s) <span class="org-keyword">for</span> j <span class="org-keyword">in</span> <span class="org-builtin">range</span>(p)]).<span class="org-builtin">sum</span>(axis=0)
    <span class="org-variable-name">z</span> = sp.softmax(L, axis=1)
    <span class="org-variable-name">update</span> = np.array([_nb_mix_obj(np.vstack(log_mean[j], log_inv_disp[j]), x[:,j], z, s) <span class="org-keyword">for</span> j <span class="org-keyword">in</span> <span class="org-builtin">range</span>(p)]).<span class="org-builtin">sum</span>()
    <span class="org-keyword">if</span> update &gt; loss:
      <span class="org-keyword">raise</span> <span class="org-type">RuntimeError</span>(<span class="org-string">'objective increased'</span>)
    <span class="org-keyword">elif</span> loss - update &lt; tol:
      <span class="org-keyword">return</span> z, log_mean, log_inv_disp
    <span class="org-keyword">else</span>:
      <span class="org-keyword">if</span> verbose:
        <span class="org-keyword">print</span>(f<span class="org-string">'epoch {t + 1}: {loss:.4g}'</span>)
      <span class="org-variable-name">loss</span> = update
  <span class="org-keyword">raise</span> <span class="org-type">RuntimeError</span>(<span class="org-string">'failed to converge in max_iters'</span>)
</pre>
</div>
</div>
</div>

<div id="outline-container-orge0613b0" class="outline-3">
<h3 id="amortized"><a id="orge0613b0"></a>Amortized inference</h3>
<div class="outline-text-3" id="text-amortized">
<p>
Fit the amortized inference model, initializing \(\vmu_k, \vphi_k\) from the
MLE starting from the ground-truth labels.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">query</span> = torch.tensor(temp.X.A)
<span class="org-variable-name">s</span> = torch.tensor(temp.X.<span class="org-builtin">sum</span>(axis=1))
<span class="org-variable-name">fit</span> = mpebpm.gam_mix.EBPMGammaMix(
  p=temp.shape[1],
  k=2,
  log_mean=fit0[0],
  log_inv_disp=fit0[1])
</pre>
</div>

<p>
Look at the initial loss.
</p>

<div class="org-src-container">
<pre class="src src-ipython">(
  mpebpm.gam_mix._nb_mix_loss(
    fit.encoder.forward(query),
    query,
    s,
    fit.log_mean,
    fit.log_inv_disp),
  mpebpm.gam_mix._nb_mix_loss(
    torch.tensor(pd.get_dummies(temp.obs[<span class="org-string">'cell_type'</span>]).values),
    query,
    s,
    fit.log_mean,
    fit.log_inv_disp)
)
</pre>
</div>

<pre class="example">
(tensor(1927253.3750, grad_fn=&lt;NegBackward&gt;),
tensor(1926631.7500, grad_fn=&lt;NegBackward&gt;))
</pre>

<p>
Look at the gradients with respect to the encoder network weights.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">temp_loss</span> = mpebpm.gam_mix._nb_mix_loss(
  fit.encoder.forward(query),
  query,
  torch.tensor(temp.X.<span class="org-builtin">sum</span>(axis=1)),
  fit.log_mean,
  fit.log_inv_disp)
temp_loss.retain_grad()
temp_loss.backward()
torch.norm(fit.encoder[0].weight.grad)
</pre>
</div>

<pre class="example">
tensor(31154.1895)

</pre>

<p>
Perform amortized inference, initializing \(\vmu_k, \vphi_k\) from the MLE
starting from the ground-truth labels.
</p>

<div class="org-src-container">
<pre class="src src-ipython">torch.manual_seed(1)
<span class="org-variable-name">fit1</span> = mpebpm.gam_mix.EBPMGammaMix(
  p=temp.shape[1],
  k=2,
  log_mean=fit0[0],
  log_inv_disp=fit0[1]).fit(
    x=temp.X.A,
    s=temp.X.<span class="org-builtin">sum</span>(axis=1),
    y=pd.get_dummies(temp.obs[<span class="org-string">'cell_type'</span>]).values,
    lr=1e-3,
    batch_size=64,
    shuffle=<span class="org-constant">True</span>,
    num_pretrain=80,
    num_epochs=80,
    log_dir=<span class="org-string">'runs/nbmix/ai-inittruth-epoch80'</span>)
</pre>
</div>

<p>
Compute the cross entropy loss over the posterior mean cluster assignments.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">zhat</span> = fit1.forward(query.cuda()).detach().cpu().numpy()
torch.nn.functional.binary_cross_entropy(
  torch.tensor(zhat, dtype=torch.<span class="org-builtin">float</span>),
  torch.tensor(pd.get_dummies(temp.obs[<span class="org-string">'cell_type'</span>]).values, dtype=torch.<span class="org-builtin">float</span>))
</pre>
</div>

<pre class="example">
tensor(0.2482)

</pre>

<p>
Plot the approximate posterior over cluster assignments for each point.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Dark2'</span>)
plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 2, sharex=<span class="org-constant">True</span>, sharey=<span class="org-constant">True</span>)
fig.set_size_inches(4.5, 2.5)
<span class="org-keyword">for</span> i, a <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(ax):
  a.scatter(*temp.obsm[<span class="org-string">"X_umap"</span>].T, s=4, c=np.hstack((np.tile(np.array(cm(i)[:3]), zhat.shape[0]).reshape(-1, 3), zhat[:,i].reshape(-1, 1))))
  a.set_xlabel(<span class="org-string">'UMAP 1'</span>)
ax[0].set_ylabel(<span class="org-string">'UMAP 2'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/nbmix.org/am-inf-ex.png" alt="am-inf-ex.png">
</p>
</div>

<p>
Threshold cluster assignments, and compute the cross entropy loss.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">zhat_thresh</span> = pd.get_dummies(np.argmax(zhat, axis=1))
torch.nn.functional.binary_cross_entropy(
  torch.tensor(zhat_thresh.values, dtype=torch.<span class="org-builtin">float</span>),
  torch.tensor(pd.get_dummies(temp.obs[<span class="org-string">'cell_type'</span>]).values, dtype=torch.<span class="org-builtin">float</span>))
</pre>
</div>

<pre class="example">
tensor(0.)

</pre>

<p>
Now, fit the model starting from a random initialization.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> imp; imp.<span class="org-builtin">reload</span>(mpebpm.gam_mix)
torch.manual_seed(1)
<span class="org-variable-name">fit2</span> = mpebpm.gam_mix.EBPMGammaMix(
  p=temp.shape[1],
  k=2).fit(
    x=temp.X.A,
    s=temp.X.<span class="org-builtin">sum</span>(axis=1),
    y=pd.get_dummies(temp.obs[<span class="org-string">'cell_type'</span>]).values,
    lr=1e-2,
    batch_size=64,
    shuffle=<span class="org-constant">True</span>,
    num_pretrain=1,
    num_epochs=120,
    log_dir=<span class="org-string">'runs/nbmix/ai-init1-pretrain1'</span>)
</pre>
</div>

<p>
Compare the estimated \(\ln\mu\) from the clustering model to the estimates
using the ground truth labels.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">lim</span> = [-10, 10]
plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 2, sharey=<span class="org-constant">True</span>)
fig.set_size_inches(4.5, 2.5)
<span class="org-keyword">for</span> i, a <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(ax):
  a.scatter(fit1.log_mean[i].detach().cpu().numpy(), fit2.log_mean[i].detach().cpu().numpy(), s=1, c=<span class="org-string">'k'</span>, alpha=0.2)
  a.plot(lim, lim, c=<span class="org-string">'r'</span>, lw=1, ls=<span class="org-string">':'</span>)
  a.set_title(f<span class="org-string">'Cluster {i}'</span>)
  a.set_ylim(lim)
  a.set_xlabel(<span class="org-string">'Ground truth initialization'</span>)
ax[0].set_ylabel(<span class="org-string">'Random initialization'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/nbmix.org/am-inf-vs-mpebpm.png" alt="am-inf-vs-mpebpm.png">
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org089f29e" class="outline-2">
<h2 id="related-work"><a id="org089f29e"></a>Related work</h2>
<div class="outline-text-2" id="text-related-work">
<p>
scVI (<a href="https://dx.doi.org/10.1038/s41592-018-0229-2">Lopez et al. 2018</a>,
<a href="https://www.biorxiv.org/content/10.1101/532895v2">Xu et al. 2020</a>)
implements a related deep unsupervised (more precisely, semi-supervised)
clustering model (<a href="https://arxiv.org/abs/1406.5298">Kingma et al. 2014</a>,
<a href="https://arxiv.org/abs/1611.02648">Dilokthanakul et al. 2016</a>).
</p>

\begin{align}
  x_{ij} \mid s_i, \lambda_{ij} &\sim \Pois(s_i \lambda_{ij})\\
  \ln s_i &\sim \N(\cdot)\\
  \lambda_{ij} \mid \vz_i &\sim \Gam(\phi_j^{-1}, (\mu_{\lambda}(\vz_i))_j^{-1} \phi_j^{-1})\\
  \vz_i \mid y_i, \vu_i &\sim \N(\mu_z(\vu_i, y_i), \diag(\sigma^2(\vu_i, y_i)))\\
  y_i \mid \vpi &\sim \Mult(1, \vpi)\\
  \vu_i &\sim \N(0, \mi).
\end{align}

<p>
where
</p>

<ul class="org-ul">
<li>\(y_i\) denotes the cluster assignment for cell \(i\)</li>
<li>\(\mu_z(\cdot), \sigma^2(\cdot)\) are neural networks mapping the latent
cluster variable \(y_i\) and Gaussian noise \(\vu_i\) to the latent variable
\(\vz_i\)</li>
<li>\(\mu_{\lambda}(\cdot)\) is a neural network mapping latent variable \(\vz_i\) to
latent gene expression \(\vlambda_{i}\)</li>
</ul>

<p>
The intuition behind this model is that, marginalizing over \(y_i\), the
prior \(p(z_i)\) is a mixture of Gaussians, and therefore the model embeds
examples in a space which makes clustering easy, and maps examples to those
clusters simultaneously. To perform variational inference in this model,
Lopez et al. introduce inference networks
</p>

\begin{align}
  q(\vz_i \mid \vx_i) &= \N(\cdot)\\
  q(y_i \mid \vz_i) &= \Mult(1, \cdot).
\end{align}

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> scvi.dataset
<span class="org-keyword">import</span> scvi.inference
<span class="org-keyword">import</span> scvi.models

<span class="org-variable-name">expr</span> = scvi.dataset.AnnDatasetFromAnnData(temp)
<span class="org-variable-name">m</span> = scvi.models.VAEC(expr.nb_genes, n_batch=0, n_labels=2)
<span class="org-variable-name">train</span> = scvi.inference.UnsupervisedTrainer(m, expr, train_size=1, batch_size=32, show_progbar=<span class="org-constant">False</span>, n_epochs_kl_warmup=100)
train.train(n_epochs=1000, lr=1e-2)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">post</span> = train.create_posterior(train.model, expr)
<span class="org-variable-name">_</span>, <span class="org-variable-name">_</span>, <span class="org-variable-name">label</span> = post.get_latent()
</pre>
</div>

<p>
The fundamental difference between scVI and our approach is that scVI
clusters points in the low-dimensional latent space, and we cluster points in
the space of latent gene expression (which has equal dimension to the
observations).
</p>

<p>
Rui Shu <a href="http://ruishu.io/2016/12/25/gmvae/">proposed an alternative
generative model</a>, which has some practical benefits and can be adapted to
this problem
</p>

\begin{align}
  x_{ij} \mid \xiplus, \lambda_{ij} &\sim \Pois(\xiplus \lambda_{ij})\\
  \lambda_{ij} \mid \vz_i &\sim \Gam(\phi_j^{-1}, (\mu_{\lambda}(\vz_i))_j^{-1} \phi_j^{-1})\\
  \vz_i \mid y_i &\sim \N(\mu_z(y_i), \sigma^2(y_i))\\
  y_i \mid \vpi &\sim \Mult(1, \vpi)\\
  q(y_i, \vz_i \mid \vx_i) &= q(y_i \mid \vx_i)\, q(\vz_i \mid y_i, \vx_i).
\end{align}

<p>
There are additional deep unsupervised learning methods, which could
potentially be adapted to this setting (reviewed in
<a href="https://dx.doi.org/10.1109/ACCESS.2018.2855437">Min et al. 2018</a>).
</p>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Abhishek Sarkar</p>
<p class="date">Created: 2020-05-17 Sun 15:19</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
