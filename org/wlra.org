#+TITLE: Weighted low rank approximation
#+SETUPFILE: setup.org
#+OPTIONS: toc:2

* Setup                                                            :noexport:

  #+BEGIN_SRC emacs-lisp
    (org-babel-lob-ingest "/home/aksarkar/.emacs.d/org-templates/library.org")
  #+END_SRC

  #+RESULTS:
  : 1

  #+CALL: ipython3(memory="4G",venv="singlecell") :dir /scratch/midway2/aksarkar/singlecell

  #+RESULTS:
  : Submitted batch job 50207591

  #+BEGIN_SRC ipython
    %matplotlib inline
    %config InlineBackend.figure_formats = set(['retina'])
    import colorcet
    import matplotlib.pyplot as plt
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[1]:
  :END:

  #+NAME: imports
  #+BEGIN_SRC ipython
    import numpy as np
    import pandas as pd
    import scipy.special as sp
    import scipy.stats as st
    import sklearn.decomposition as skd
    import wlra
  #+END_SRC

  #+RESULTS: imports
  :RESULTS:
  # Out[68]:
  :END:

* Introduction

  We are interested in solving the /weighted low-rank approximation problem/:

  \[ \min_{\mathbf{Z}} \sum_{i,j} w_{ij} \left(x_{ij} - z_{ij} \right)^2 \]

  where \(n \times p\) target matrix \(\mathbf{X}\) and \(n \times p\) weight
  matrix \(\mathbf{W}\) are given, and \(\mathbf{Z}\) is constrained to some
  rank.

  Solving WLRA allows us to solve two main problems. First, we can learn low
  rank structure in non-Gaussian data. Using Taylor expansion of non-Gaussian
  likelihoods, we can rewrite the MLE of factor models as the solution to
  WLRA. The key idea is that the Taylor expansion is performed around a
  different value for each observation, naturally leading to an iterative
  approach.

  Prior work has tried to learn low rank structure to solve the /collaborative
  filtering/ problem: given user \(i\) interacted with item \(j\), predict what
  other items ([[http://yifanhu.net/PUB/cf.pdf][Hu et al. 2001]]). Low rank structure in this context corresponds
  to learning patterns of user preferences (factors), and the specific
  preferences of individual users (loadings). 

  One special feature of the problem is that only /implicit feedback/ is
  assumed: we only have observations \(x_{ij} = 1\) when an interaction is
  recorded, and \(x_{ij} = 0\) could either reflect negative feedback or
  missing data. However, this assumption does not make sense for scRNA-Seq
  data, where observations of 0 are not missing.

  The second problem we can solve usign LRA is true imputation. By setting
  weights to zero, we can code missing data. This approach works even in
  settings where observations can also take the value zero, such as single cell
  RNA sequencing data.

  The methods are implemented in the Python package [[https://www.github.com/aksarkar/wlra][wlra]].

* Methods
** EM algorithm 

   [[https://www.aaai.org/Papers/ICML/2003/ICML03-094.pdf][Srebro and Jaakkola 2003]] propose an EM algorithm to solve WLRA. The algorithm
   is EM in the following sense: suppose the weights \(w_{ij} \in \{0, 1\}\),
   corresponding to presence/absence, and suppose \(\mathbf{X} = \mathbf{Z} +
   \mathbf{E}\), where \(\mathbf{Z}\) is low-rank and elements of \(\mathbf{E}\)
   are Gaussian.

   Then, \(E[x_{ij} \mid w_{ij} = 0] = z_{ij}\), naturally giving an EM
   algorithm. The E-step fills in \(x_{ij}\) with \(z_{ij}\), and the M-step
   estimates \(\mathbf{Z}\) from the filled in \(\mathbf{X}\). The solution to
   the M-step is given by the optimal unweighted rank \(k\) approximation,
   i.e. truncated SVD, because all the non-zero weights are equal to 1.

   Conceptually, the method for arbitrary weights is to suppose instead that we
   have rational \(w_{ij} \in \{0, 1/N, \ldots, 1\}\). 

   Then, we can reduce this problem to a problem in 0/1 weights by supposing we
   have \(X^{(k)} = Z + E^{(k)}\), \(k \in 1, \ldots, N\), and each entry is
   observed in only \(N w_{ij}\) of the \(X^{(k)}\).

   Then, the M-step becomes:

   \[ \mathbf{Z}^{(t + 1)} = \mathrm{LRA}_k(\mathbf{W} \circ \mathbf{X} +
   (\mathbf{1} - \mathbf{W}) \circ \mathbf{Z}^{(t)}) \]

   where \(\mathrm{LRA}_k\) is the unweighted rank \(k\) approximation and
   \(\circ\) denotes Hadamard product.

   Intuitively, the implied E-step corresponds to taking the expectation of
   \(z_{ij}^{(k)}\) over the targets \(k\). Clearly, the algorithm generalizes
   to any weight matrix where \(0 \leq w_{ij} \leq 1\) are stored in finite
   precision.

   The weights must be constrained to be between zero and one in the EM
   algorithm, so we scale them by the maximum weight.

** Alternative derivation

   Suppose \(\mathbf{X} = \mathbf{M} + \mathbf{Z} + \mathbf{E}\) where
   \(\mathbf{M}\) is low rank, and:

   \[ z_{ij} \sim N(0, s_{ij}^2) \]

   \[ e_{ij} \sim N(0, \tau^{-1}) \]

   Let \(\mathbf{R} = \mathbf{X} - \mathbf{M}\). Then:

   \[ r_{ij} \mid z_{ij} \sim N(r_{ij}, \tau^{-1}) \]

   \[ z_{ij} \mid \cdot \sim N(\mu_1, \tau_1^{-1}) \]

   \[ \mu_1 = r_{ij} \tau / \tau_1 \]

   \[ \tau_1 = \tau + 1 / s_{ij}^2 \]

   Therefore,

   \[ E[z_{ij} \mid x_{ij}, m_{ij}, s_{ij}] = \frac{\tau}{\tau + 1 /
   s_{ij}^2} (x_{ij} - m_{ij}) = w_{ij} (x_{ij} - m_{ij}) \]

   and the solution to the E step is:

   \[ \mathbf{Z}^{(t + 1)} = \mathbf{W} \circ (\mathbf{X} - \mathbf{M}^{(t)}) \]

   Given \(\mathbf{Z}^{(t)}\), the solution to the M step is PCA with
   homoscedastic errors, i.e. truncated SVD.

   \[ \mathbf{M}^{(t + 1)} = \mathrm{LRA}_k(\mathbf{X} - \mathbf{Z}) \]

   \[ = \mathrm{LRA}_k((1 - \mathbf{W}) \circ \mathbf{X} + \mathbf{W} \circ
   \mathbf{M^{(t)}}) \]

   To recover the algorithm of Srebro and Jaakkola, we make several
   observations:

   1. Taking \(\tau \rightarrow 0\), we have \(w_{ij} = 1 / s_{ij}^2\).
   2. To mark missing values, we set \(s_{ij}^2 = \infty\), in which case
      \(w_{ij} = 1\), and the M-step takes the value of \(m_{ij}\) instead of
      \(x_{ij}\).
   3. Srebro and Jaakkola explicitly require weights to be between zero and
      one; this derivation only suggests that they should be scaled (because
      the M step involves the form of a linear interpolation).

** Maximizing non-Gaussian likelihoods

   Suppose \(l(\theta) = \ln p(x \mid \theta)\). Then, taking second-order
   Taylor expansion about \(\theta_0\):

   \[ l(\theta) \approx l(\theta_0) + (\theta - \theta_0)\,l'(\theta_0) +
   \frac{(\theta - \theta)^2}{2}\,l''(\theta_0) \]

   \[ = \frac{l''(\theta_0)}{2} \left[ \theta - \left(\theta_0 -
   \frac{l'(\theta_0)}{l''(\theta_0)}\right)\right]^2 + \mathrm{const}\]

   where the constant does not depend on \(\theta\). Now, maximizing the
   objective is equivalent to maximizing a Gaussian likelihood:

   \[= \mathcal{N}\left(\theta; \theta_0 -
   \frac{l'(\theta_0)}{l''(\theta_0)}, -\frac{1}{l''(\theta_0)}\right) +
   \mathrm{const} \]

   Equivalently, minimizing the negative of the objective function is
   WLRA. This result makes sense because for fixed \(\sigma^2\), maximizing the
   Gaussian likelihood is the same as minimizing the Frobenius norm.

   The result suggests we can improve \(l\) by optimizing this objective
   instead, and suggests an iterative algorithm where we alternate updates
   between \(\theta_0\) and \(\theta\).

   Because this new objective is written in terms of derivatives of the log
   likelihood, we can readily write down the required quantities for relevant
   distributions:

   #+ATTR_HTML: :class table text-left
   | Distribution | Parameter \(\theta\)  | Target (mean)                                               | Weight (precision)                     |
   |--------------+-----------------------+-------------------------------------------------------------+----------------------------------------|
   | Bernoulli    | \(\mathrm{logit}(p)\) | \(\theta + \frac{x - S(\theta)}{S(\theta)(1 - S(\theta))}\) | \(\frac{1}{S(\theta)(1 - S(\theta))}\) |
   | Poisson      | \(\ln\lambda\)        | \(1 + \theta + x\exp(-\theta)\)                             | \(\exp(-\theta)\)                      |

   where \(S(\cdot)\) denotes the sigmoid function.

** Imputing missing values

   The EM algorithm for WLRA supports missing values by setting \(s_{ij}^2 =
   \infty\).

   When maximizing non-Gaussian likelihoods using WLRA, we need to introduce
   external weights \(\tilde{w}_{ij} \in \{0, 1\}\) to denote
   presence/absence. We can do this by incorporating the weights
   \(\tilde{\mathbf{W}} \circ \mathbf{W}\) in each outer iteration.

* Results
** Recovering low rank structure

   We first consider the problem of recovering a planted low rank matrix after
   convolving with Gaussian noise, assuming we know the rank.

   \[ l_{ik} \sim \mathcal{N}(0, 1) \]
   \[ f_{kj} \sim \mathcal{N}(0, 1) \]
   \[ \mu_{ij} = (\mathbf{l}\mathbf{f})_{ij} \]
   \[ \sigma^2_{ij} \sim \mathrm{Uniform}(1, \sigma^2_0) \]
   \[ x_{ij} \sim \mathcal{N}(\mu_{ij}, \sigma^2_{ij}) \]

   We assume the noise variances for each observation are known, and use the
   inverse variances as the weights.

   #+NAME: gaussian-reconstruction
   #+BEGIN_SRC ipython
     def wnorm(x, w):
       return (w * np.square(x)).sum()

     def simulate_gaussian(n, p, rank, s0=10, seed=0):
       np.random.seed(seed)
       l = np.random.normal(size=(n, rank))
       f = np.random.normal(size=(rank, p))
       eta = l.dot(f)
       noise = np.random.uniform(1, s0, size=eta.shape)
       w = 1 / noise
       x = np.random.normal(loc=eta, scale=noise)
       return x, w, eta

     def rrmse(pred, true):
       return np.sqrt(np.linalg.norm(pred - true) / np.linalg.norm(true))

     def score_wlra(x, w, eta, rank):
       res = wlra.wlra(x, w, rank=rank, max_iters=1000)
       return rrmse(res, eta)

     def score_lra(x, eta, rank):
       res = wlra.lra(x, rank)
       return rrmse(res, eta)

     def score_hsvd(x, w, eta, rank):
       res = wlra.hsvd(x, w, rank=rank, max_iters=1000)
       return rrmse(res, eta)

     def evaluate_gaussian_known_rank(num_trials=10):
       result = []
       for trial in range(num_trials):
         x, w, eta = simulate_gaussian(n=100, p=1000, rank=1, s0=10, seed=trial)
         result.append([trial,
                        score_lra(x, eta, rank=1),
                        score_wlra(x, w, eta, rank=1),
                        score_hsvd(x, 1 / w, eta, rank=1)])
       result = pd.DataFrame(result)
       result.columns = ['trial', 'LRA', 'WLRA', 'HSVD']
       return result
   #+END_SRC

   #+RESULTS: gaussian-reconstruction
   :RESULTS:
   # Out[3]:
   :END:

   #+BEGIN_SRC ipython :eval never :noweb tangle :tangle /project2/mstephens/aksarkar/projects/singlecell-ideas/code/gaussian-known-rank.py
     <<imports>>
     <<gaussian-reconstruction>>
     res = evaluate_gaussian_known_rank(num_trials=100)
     res.to_csv('/project2/mstephens/aksarkar/projects/singlecell-ideas/data/wlra/gaussian-known-rank.txt.gz', compression='gzip', sep='\t')
   #+END_SRC

   #+BEGIN_SRC sh :dir /scratch/midway2/aksarkar/ideas/
     sbatch --partition=broadwl --job-name=gaussian-wlra
     #!/bin/bash
     source activate singlecell
     python /project2/mstephens/aksarkar/projects/singlecell-ideas/code/gaussian-known-rank.py
   #+END_SRC

   #+RESULTS:
   : Submitted batch job 50149976

   Read the results.

   #+BEGIN_SRC ipython
     results_gaussian_known_rank = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-ideas/data/wlra/gaussian-known-rank.txt.gz', index_col=0)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[88]:
   :END:

   Plot the performance.

   #+BEGIN_SRC ipython :ipyfile figure/wlra.org/gaussian-known-rank.png
     plt.clf()
     plt.gcf().set_size_inches(3, 3)
     plt.boxplot(x=results_gaussian_known_rank[['LRA', 'WLRA']].T, positions=range(2), medianprops={'color': 'k'})
     plt.xticks(range(2), ['LRA', 'WLRA'])
     plt.xlabel('Method')
     _ = plt.ylabel('RRMSE')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[89]:
   [[file:figure/wlra.org/gaussian-known-rank.png]]
   :END:

** Imputing missing values

   We generate a Poisson data matrix with planted row rank structure.

   \[ \eta_{ij} = (\mathbf{l f})_{ij} \]
   \[ x_{ij} \sim \mathrm{Poisson}(\exp(\eta_{ij})) \]

   We then hold out entries using numpy masked arrays, and impute them using
   the Poisson low rank approximation. We compare the imputation accuracy
   against non-negative matrix factorization ([[https://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf][Lee and Seung 2001]],
   [[https://arxiv.org/abs/1010.1763][Févotte and Idier 2011]]). We consider several loss functions:

   - Frobenius norm (\(\beta=2\) divergence)
   - KL divergence (after normalizing the factors and loadings; \(\beta=1\)
     divergence)
   - \(\beta=0.5\) divergence

   #+NAME: poisson-imputation
   #+BEGIN_SRC ipython
     def simulate_pois(n, p, rank, seed=0):
       np.random.seed(seed)
       l = np.random.normal(size=(n, rank))
       f = np.random.normal(size=(rank, p))
       eta = l.dot(f)
       x = np.random.poisson(lam=np.exp(eta))
       return x, eta

     def rmse(pred, true):
       return np.sqrt(np.square(pred - true).mean())

     def imputation_score_nmf(x, rank, **kwargs):
       """Non-negative matrix factorization"""
       if 'max_iter' not in kwargs:
         kwargs['max_iter'] = 1000
       if 'tol' not in kwargs:
         kwargs['tol'] = 1e-3
       if 'init' not in kwargs:
         kwargs['init'] = 'random'
       m = skd.NMF(n_components=rank, **kwargs).fit(x)
       res = m.transform(x).dot(m.components_)
       return rmse(res[x.mask], x.data[x.mask])

     def imputation_score_pois_lra(x, rank):
       try:
         res = wlra.pois_lra(x, rank=rank, max_outer_iters=50)
         return rmse(res[x.mask], x.data[x.mask])
       except RuntimeError:
         return np.nan

     def evaluate_pois_imputation(rank=3, num_trials=10):
       result = []
       for trial in range(num_trials):
         x, eta = simulate_pois(n=200, p=300, rank=rank, seed=trial)
         mask = np.random.uniform(size=(200, 300)) < 0.25
         x = np.ma.masked_array(x, mask=mask)
         result.append([trial,
                        imputation_score_nmf(x, rank, beta_loss='frobenius'),
                        imputation_score_nmf(x, rank, solver='mu', beta_loss='kullback-leibler'),
                        imputation_score_nmf(x, rank, solver='mu', beta_loss=0.5),
                        imputation_score_pois_lra(x, rank)])
       result = pd.DataFrame(result)
       result.columns = ['trial', 'Poisson LRA', 'NMF-Frob', 'NMF-KL', 'NMF-β=0.5']
       return result
   #+END_SRC

   #+BEGIN_SRC ipython :eval never :noweb tangle :tangle /project2/mstephens/aksarkar/projects/singlecell-ideas/code/pois-imputation.py
     <<imports>>
     <<poisson-imputation>>
     res = evaluate_pois_imputation(num_trials=100)
     res.to_csv('/project2/mstephens/aksarkar/projects/singlecell-ideas/data/wlra/pois-imputation.txt.gz', compression='gzip', sep='\t')
   #+END_SRC

   #+BEGIN_SRC sh :dir /scratch/midway2/aksarkar/ideas/
     sbatch --partition=broadwl --job-name=pois-imputation
     #!/bin/bash
     source activate singlecell
     python /project2/mstephens/aksarkar/projects/singlecell-ideas/code/pois-imputation.py
   #+END_SRC

   #+RESULTS:
   : Submitted batch job 50209831

   Read the results.

   #+BEGIN_SRC ipython
     results_pois_imputation = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-ideas/data/wlra/pois-imputation.txt.gz', index_col=0)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[55]:
   :END:

   Plot the performance.

   #+BEGIN_SRC ipython :ipyfile figure/wlra.org/pois-imputation.png
     T = results_pois_imputation.dropna()

     plt.clf()
     plt.gcf().set_size_inches(3, 3)
     plt.yscale('symlog', linthreshy=10)
     keys = ['NMF-Frob', 'NMF-KL', 'NMF-β=0.5']
     for i, k in enumerate(keys):
       y = T[k] - T['Poisson LRA']
       f = st.gaussian_kde(y)
       py = f(y)
       x = i + .1 / py.max() * np.random.uniform(-py, py)
       plt.scatter(x, y, c='k', s=4)
     plt.axhline(y=0, c='k', ls=':', lw=1)
     plt.xticks(range(len(keys)), keys)
     plt.xlabel('Method')
     _ = plt.ylabel('Difference in RMSE (Poisson LRA)')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[82]:
   [[file:figure/wlra.org/pois-imputation.png]]
   :END:

** Explaining held out data

   A number of methods have been proposed to estimate low rank structure from
   count data, including scRNA-Seq data:

   - Non-negative matrix factorization ([[https://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf][Lee and Seung 2001]], [[https://arxiv.org/abs/1010.1763][Févotte and Idier 2011]])
   - Hierarchical Bayesian Poisson Factorization ([[http://www.cs.columbia.edu/~blei/papers/GopalanHofmanBlei2015.pdf][Gopalan et al. 2015]])
   - ZINB-WAVE ([[https://www.nature.com/articles/s41467-017-02554-5][Risso et al. 2018]])
   - scVI ([[https://www.biorxiv.org/content/early/2018/03/30/292037][Lopez et al 2018]])

   These methods typically make the /implicit feedback/ assumption of
   collaborative filtering, and treat zeros in the input data as missing
   data. Clearly, this assumption does not hold for scRNA-Seq data, and
   therefore we cannot evaluate all of the methods on the true imputation task
   above.

   Instead, we evaluate the methods on their ability to generalize to new
   data. We assume:

   \[ x_{ij} \sim \mathrm{Poisson}(\lambda_{ij}) \]

   and hold out molecules by randomly thinning the observed counts:

   \[ y_{ij} \sim \mathrm{Binomial}(x_{ij}, 0.5) \]

   \[ \tilde{y}_{ij} = x_{ij} - y_{ij} \]

   This approach leaves the relative abundance of the transcripts unchanged in
   expectation, implying that the low rank structure learned in \(\mathbf{Y}\)
   should explain the data in \(\tilde\mathbf{Y}\). Our metric is then the
   likelihood of the data under the Poisson likelihood.

   #+BEGIN_SRC ipython
     
   #+END_SRC
