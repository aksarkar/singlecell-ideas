<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2021-05-10 Mon 14:42 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Logistic regression in the presence of error-prone labels</title>
<meta name="generator" content="Org mode">
<meta name="author" content="Abhishek Sarkar">
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link href="bootstrap.min.css" rel="stylesheet" />
<link rel="stylesheet" type="text/css" href="htmlize.css"/>
<link rel="stylesheet" type="text/css" href="main.css"/>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2017 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Logistic regression in the presence of error-prone labels</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgf22c2b3">Introduction</a></li>
<li><a href="#org0eb2264">Maximum likelihood estimation</a></li>
<li><a href="#orgcbac8fb">Approximate Bayesian inference</a></li>
<li><a href="#orgc166d0f">Example</a>
<ul>
<li><a href="#setup">Setup</a></li>
<li><a href="#org18caaf4">Simulation</a></li>
<li><a href="#orga8eb9df">EM</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-orgf22c2b3" class="outline-2">
<h2 id="orgf22c2b3">Introduction</h2>
<div class="outline-text-2" id="text-orgf22c2b3">
<p>
Logistic regression can be written \(
  \DeclareMathOperator\Bern{Bernoulli}
  \DeclareMathOperator\E{E}
  \DeclareMathOperator\S{sigmoid}
  \DeclareMathOperator\logit{logit}
  \newcommand\vx{\mathbf{x}}
  \newcommand\vb{\mathbf{b}}
  \newcommand\vy{\mathbf{y}}
  \newcommand\vtheta{\boldsymbol{\theta}}
  \newcommand\mx{\mathbf{X}}
  \)
</p>

\begin{align}
  y_i \mid p_i &\sim \Bern(p_i)\\
  \logit(p_i) &\triangleq \eta_i = \vx_i' \vb,
\end{align}

<p>
where \(y_i \in \{0, 1\}\) denotes the label of observation \(i = 1, \ldots,
  n\), \(\vx_i\) denotes the \(p\)-dimensional feature vector for observation
\(i\), and \(\vb\) is a \(p\)-dimensional vector of coefficients. In words,
under this model the features \(x_{ij}\) (\(j = 1, \ldots, p\)) have linear
effects \(b_j\) on the log odds ratio \(\eta_i\) of the label \(y_i\)
equalling 1. One is primarily interested in estimating \(\vb\), in order to
explain which features are important in determining the label \(y_i\), and to
predict labels for future observations. Estimation can be done by a variety
of approaches, such as maximizing the log likelihood
</p>

\begin{equation}
  \ell = \sum_i y_i \ln \S(\vx_i' \vb) + (1 - y_i) \ln \S(-\vx_i' \vb)
\end{equation}

<p>
with respect to \(\vb\), or by assuming a prior \(p(\vb)\) and estimating the
posterior \(p(\vb \mid \mx, \vy)\) using MCMC or variational inference.
</p>

<p>
A central assumption in using the model above to analyze data is that the
labels \(y_i\) do not have errors. This is not a safe assumption to make in
some settings, e.g., in associating genetic variants with diseases for which
diagnosis is known to be imperfect
(<a href="https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-020-3387-z">Shafquat
et al. 2020</a>). One approach to account for error-prone labels is to build a
more complex model that relates the observed feature vectors \(\vx_i\), the
true (unobserved) labels \(z_i\), and the observed labels \(y_i\)
</p>

\begin{align}
  y_i \mid z_i = 1, \theta_1 &\sim \Bern(1 - \theta_1)\\
  y_i \mid z_i = 0, \theta_0 &\sim \Bern(\theta_0)\\
  z_i \mid p_i &\sim \Bern(p_i)\\
  \logit(p_i) &= \vx_i \vb.
\end{align}

<p>
In this model, \(\vtheta = (\theta_0, \theta_1)\) denotes error
probabilities. Specifically, \(\theta_1\) denotes the probability of
observing label \(y_i = 0\) when the true label \(z_i = 1\), and \(\theta_0\)
denotes the probability of observing \(y_i = 1\) when \(z_i = 0\). (One could
simplify by fixing \(\theta_0 = \theta_1\), or potentially build more complex
models where the error rates themselves depend on features in the data.) As
was the case for the simpler model, one estimates \((\vb, \vtheta)\) from
observed data by maximizing the log likelihood, or by assuming a prior
\(p(\vb, \vtheta)\) and estimating the posterior \(p(\vb, \vtheta \mid \mx,
  \vy)\). However, unlike the simpler model, one can use these estimates to
infer the true labels (and thus identify mislabelled data points) from the
data, e.g., by estimating \(p(z_i = 1 \mid \mx, \vy)\).
</p>
</div>
</div>

<div id="outline-container-org0eb2264" class="outline-2">
<h2 id="org0eb2264">Maximum likelihood estimation</h2>
<div class="outline-text-2" id="text-org0eb2264">
<p>
One can use an EM algorithm to maximize the log likelihood
</p>

\begin{equation}
  \ell = \sum_i \ln \left(\sum_{z_i \in \{0, 1\}} p(y_i \mid z_i, \vtheta)\, p(z_i \mid \vx_i, \vb)\right)
\end{equation}

<p>
with respect to \((\vb, \vtheta)\). We briefly outline the derivation of the
algorithm. The joint probability
</p>

\begin{multline}
  \ln p(y_i, z_i \mid \vx_i, \vb, \vtheta) = [z_i = 1]\underbrace{(y_i \ln (1 - \theta_1) + (1 - y_i)\ln \theta_1 + \ln \S(\vx_i' \vb))}_{\triangleq \alpha_i}\\
  + [z_i = 0]\underbrace{(y_i \ln \theta_0 + (1 - y_i) \ln (1 - \theta_0) + \ln \S(-\vx_i' \vb))}_{\triangleq \beta_i},
\end{multline}

<p>
where \([\cdot]\) denotes the
<a href="https://en.wikipedia.org/wiki/Iverson_bracket">Iverson bracket</a>. The
posterior
</p>

\begin{equation}
  \phi_i \triangleq p(z_i = 1 \mid \vx_i, y_i, \vb, \vtheta) = \frac{\exp(\alpha_i)}{\exp(\alpha_i) + \exp(\beta_i)},
\end{equation}

<p>
and the expected log joint probability with respect to the posterior is
</p>

\begin{multline}
  h = \sum_i \Big(\phi_i (y_i \ln (1 - \theta_1) + (1 - y_i)\ln \theta_1 + \ln \S(\vx_i' \vb))\\
  + (1 - \phi_i) (y_i \ln \theta_0 + (1 - y_i) \ln (1 - \theta_0) + \ln \S(-\vx_i' \vb)) \Big).
\end{multline}

<p>
In the E-step, one updates \(\phi_i\) as described above. In the M-step, one
updates
</p>

\begin{align}
  \logit(\theta_1) &:= \frac{\sum_i \phi_i (1 - y_i)}{\sum_i \phi_i y_i}\\
  \logit(\theta_0) &:= \frac{\sum_i (1 - \phi_i) y_i}{\sum_i (1 - \phi_i) (1 - y_i)},
\end{align}

<p>
and updates \(\vb\) using a numerical method to increase the expected log
joint probability (e.g., gradient ascent). One iterates the E and M steps
alternately until e.g., the change in log likelihood falls below some
threshold. Intuitively, this algorithm alternates between estimating the
true labels given the current estimates of the regression coefficients and
error rates, and estimating the regression coefficients and error rates
given the current estimates of the true labels. Crucially, all of the data
is used to estimate the regression coefficients and error rates, which makes
it possible to detect that an observed label is unlikely given the observed
features and estimated error rates, by borrowing information from the other
observations. From the estimated \(\hat{\vb}\), one can predict the true
label \(\tilde{z}\) for a new observation \((\tilde{\vx}, \tilde{y})\) as
</p>

\begin{equation}
  \E[\tilde{z}_i] = \sigmoid(\tilde{\vx}' \hat{\vb}).
\end{equation}
</div>
</div>

<div id="outline-container-orgcbac8fb" class="outline-2">
<h2 id="orgcbac8fb">Approximate Bayesian inference</h2>
<div class="outline-text-2" id="text-orgcbac8fb">
<p>
If one assumes a prior \(p(\vb, \vtheta)\), then one can estimate the
posterior \(p(\vb, \vtheta \mid \mx, \vy)\). Inference via MCMC can be
readily implemented in inference engines such as
<a href="https://mc-stan.org/">Stan</a>, <a href="https://pyro.ai/">pyro</a>, or
<a href="https://www.tensorflow.org/probability">Tensorflow Probability</a>. From the
estimated posterior, one can estimate the posterior distribution of the true
label \(\tilde{z}\) for a new data point \((\tilde{\vx}, \tilde{y})\), given
the observed (training) data, as
</p>

\begin{equation}
  p(\tilde{z} \mid \tilde{\vx}, \tilde{y}) = \int p(\tilde{y} \mid \tilde{z}, \vtheta)\, p(\tilde{z} \mid \tilde{\vx}, \vb)\, dp(\vb, \vtheta \mid \mx, \vy).
\end{equation}

<p>
One could alternatively use variational inference to find the best
approximate posterior in a tractable family, by minimizing the KL divergence
between the approximate posterior and the true posterior. In either case,
the primary derived quantity of interest for prediction would be the
posterior mean \(\E[\tilde{z} \mid \tilde{\vx}, \tilde{y}]\).
</p>
</div>
</div>

<div id="outline-container-orgc166d0f" class="outline-2">
<h2 id="orgc166d0f">Example</h2>
<div class="outline-text-2" id="text-orgc166d0f">
</div>
<div id="outline-container-orgce0ed18" class="outline-3">
<h3 id="setup"><a id="orgce0ed18"></a>Setup</h3>
<div class="outline-text-3" id="text-setup">
<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> numpy <span class="org-keyword">as</span> np
<span class="org-keyword">import</span> pandas <span class="org-keyword">as</span> pd
<span class="org-keyword">import</span> scipy.special <span class="org-keyword">as</span> sp
<span class="org-keyword">import</span> scipy.stats <span class="org-keyword">as</span> st
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">%matplotlib inline
%config <span class="org-variable-name">InlineBackend.figure_formats</span> = <span class="org-builtin">set</span>([<span class="org-string">'retina'</span>])
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> matplotlib.pyplot <span class="org-keyword">as</span> plt
<span class="org-variable-name">plt.rcParams</span>[<span class="org-string">'figure.facecolor'</span>] = <span class="org-string">'w'</span>
<span class="org-variable-name">plt.rcParams</span>[<span class="org-string">'font.family'</span>] = <span class="org-string">'Nimbus Sans'</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-org18caaf4" class="outline-3">
<h3 id="org18caaf4">Simulation</h3>
<div class="outline-text-3" id="text-org18caaf4">
<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">simulate</span>(n, p, n_causal, theta0, theta1, seed=0):
  <span class="org-variable-name">rng</span> = np.random.default_rng(seed)
  <span class="org-variable-name">x</span> = rng.normal(size=(n, p))
  <span class="org-variable-name">b</span> = np.zeros((p, 1))
  <span class="org-variable-name">b</span>[:n_causal] = rng.normal(size=(n_causal, 1))
  <span class="org-variable-name">z</span> = rng.uniform(size=(n, 1)) &lt; sp.expit(x @ b)
  <span class="org-variable-name">u</span> = rng.uniform(size=(n, 1))
  <span class="org-variable-name">y</span> = np.where(z, u &lt; (1 - theta1), u &lt; theta0)
  <span class="org-keyword">return</span> x, y, z, b
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">x</span>, <span class="org-variable-name">y</span>, <span class="org-variable-name">z</span>, <span class="org-variable-name">b</span> = simulate(n=500, p=10, n_causal=2, theta0=0.01, theta1=0.01)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Dark2'</span>)
plt.clf()
plt.gcf().set_size_inches(2.5, 2.5)
<span class="org-variable-name">eta</span> = x @ b
<span class="org-keyword">for</span> k <span class="org-keyword">in</span> <span class="org-builtin">range</span>(2):
  <span class="org-variable-name">jitter</span> = np.random.normal(scale=0.01, size=(z == k).<span class="org-builtin">sum</span>())
  plt.scatter(eta[z == k], jitter + z[z == k], s=1, c=cm(k))
plt.xlabel(<span class="org-string">'Linear predictor'</span>)
plt.ylabel(<span class="org-string">'True label'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/logistic-regression-labelling-errors.org/sim-ex.png" alt="sim-ex.png">
</p>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">idx</span> = y != z
pd.DataFrame(np.vstack([eta[idx], y[idx], z[idx]]).T, columns=[<span class="org-string">'eta'</span>, <span class="org-string">'y'</span>, <span class="org-string">'z'</span>])
</pre>
</div>

<pre class="example">
eta    y    z
0 -2.175591  1.0  0.0
1  2.075414  1.0  0.0
2 -1.470784  1.0  0.0
3  3.486473  0.0  1.0
4 -0.088405  1.0  0.0
</pre>
</div>
</div>

<div id="outline-container-orga8eb9df" class="outline-3">
<h3 id="orga8eb9df">EM</h3>
<div class="outline-text-3" id="text-orga8eb9df">
<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">_objective</span>(x, y, z, b, theta):
  <span class="org-variable-name">eta</span> = x @ b
  <span class="org-variable-name">alpha</span> = st.bernoulli(1 - theta[1]).pmf(y) + np.log(sp.expit(eta))
  <span class="org-variable-name">beta</span> = st.bernoulli(theta[0]).pmf(y) + np.log(sp.expit(-eta))
  <span class="org-keyword">return</span> (z * alpha + (1 - z) * beta).<span class="org-builtin">sum</span>()

<span class="org-keyword">def</span> <span class="org-function-name">_objective_b</span>(x, z, b):
  <span class="org-variable-name">eta</span> = x @ b
  <span class="org-keyword">return</span> (z * np.log(sp.expit(eta)) + (1 - z) * np.log(sp.expit(-eta))).<span class="org-builtin">sum</span>()

<span class="org-keyword">def</span> <span class="org-function-name">_update_bj</span>(x, y, z, b, j, step=1, c=0.5, tau=0.5, max_iters=30):
  <span class="org-doc">"""Return regression coefficients</span>

<span class="org-doc">  Use backtracking line search to find the step size for the update</span>

<span class="org-doc">  step - initial step size</span>
<span class="org-doc">  c - control parameter (Armijo-Goldstein condition)</span>
<span class="org-doc">  tau - control parameter (step size update)</span>
<span class="org-doc">  max_iters - maximum number of backtracking steps</span>

<span class="org-doc">  """</span>
  <span class="org-variable-name">loss</span> = _objective_b(x, z, b)
  <span class="org-variable-name">eta</span> = x @ b
  <span class="org-variable-name">d</span> = ((z * (1 - sp.expit(eta)) + (1 - z) * (1 - sp.expit(-eta))) * x[:,j]).<span class="org-builtin">sum</span>()
  <span class="org-variable-name">temp</span> = b[:]
  <span class="org-variable-name">temp</span>[j] += step * d
  <span class="org-variable-name">update</span> = _objective_b(x, z, temp)
  <span class="org-keyword">while</span> (<span class="org-keyword">not</span> np.isfinite(update) <span class="org-keyword">or</span> (update &gt; loss + c * step * d).<span class="org-builtin">any</span>()) <span class="org-keyword">and</span> max_iters &gt; 0:
    <span class="org-variable-name">step</span> *= tau
    <span class="org-variable-name">temp</span> = b[:]
    <span class="org-variable-name">temp</span>[j] += step * d
    <span class="org-variable-name">update</span> = _objective_b(z, x, temp)
    <span class="org-variable-name">max_iters</span> -= 1
  <span class="org-keyword">if</span> max_iters == 0:
    <span class="org-comment-delimiter"># </span><span class="org-comment">Step size is small enough that update can be skipped</span>
    <span class="org-keyword">return</span> b, loss
  <span class="org-keyword">else</span>:
    <span class="org-keyword">return</span> temp, update

<span class="org-keyword">def</span> <span class="org-function-name">_update_z_theta</span>(x, y, b, theta):
  <span class="org-variable-name">eta</span> = x @ b
  <span class="org-variable-name">alpha</span> = st.bernoulli(1 - theta[1]).pmf(y) + np.log(sp.expit(eta))
  <span class="org-variable-name">beta</span> = st.bernoulli(theta[0]).pmf(y) + np.log(sp.expit(-eta))
  <span class="org-variable-name">z</span> = np.exp(alpha) / (np.exp(alpha) + np.exp(beta))
  <span class="org-variable-name">theta</span>[0] = sp.expit(((1 - z) * y).<span class="org-builtin">sum</span>() / ((1 - z) * (1 - y)).<span class="org-builtin">sum</span>())
  <span class="org-variable-name">theta</span>[1] = sp.expit((z * y).<span class="org-builtin">sum</span>() / ((1 - z) * y).<span class="org-builtin">sum</span>())
  <span class="org-keyword">return</span> z, theta

<span class="org-keyword">def</span> <span class="org-function-name">logistic_regression_errors</span>(x, y, tol=1e-3, max_iters=10000, verbose=<span class="org-constant">True</span>):
  <span class="org-variable-name">b</span> = np.zeros(x.shape[1])
  <span class="org-variable-name">theta</span> = 0.01 * np.ones(2)
  <span class="org-variable-name">obj</span> = -np.inf
  <span class="org-keyword">for</span> i <span class="org-keyword">in</span> <span class="org-builtin">range</span>(max_iters):
    <span class="org-variable-name">z</span>, <span class="org-variable-name">theta</span> = _update_z_theta(x, y, b, theta)
    <span class="org-keyword">for</span> j <span class="org-keyword">in</span> <span class="org-builtin">range</span>(x.shape[1]):
      b[j], <span class="org-variable-name">_</span> = _update_bj(x, y, z, b, j)
    <span class="org-variable-name">obj1</span> = _objective(x, y, z, b, theta)
    <span class="org-keyword">if</span> obj1 &lt; obj:
      <span class="org-keyword">raise</span> <span class="org-type">RuntimeError</span>(<span class="org-string">'objective decreased'</span>)
    <span class="org-keyword">elif</span> obj1 - obj &lt; tol:
      <span class="org-keyword">return</span> b, theta
    <span class="org-keyword">else</span>:
      <span class="org-variable-name">obj</span> = obj1
      <span class="org-keyword">print</span>(<span class="org-string">'{i:&gt;6d} {obj:&gt;.9g}'</span>)
  <span class="org-keyword">else</span>:
    <span class="org-keyword">raise</span> <span class="org-type">RuntimeError</span>(<span class="org-string">'failed to converge in max_iters'</span>)
</pre>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Abhishek Sarkar</p>
<p class="date">Created: 2021-05-10 Mon 14:42</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
