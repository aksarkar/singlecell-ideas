#+TITLE: Fully unsupervised topic models of scRNA-seq time course data
#+SETUPFILE: setup.org

* Introduction

  In our prior work ([[https://dx.doi.org/10.1371/journal.pgen.1008045][Sarkar
  et al. 2019]]), we introduced a factor model to capture between donor
  individual variation in the mean and variance of gene expression in a single
  cell type \( 
  \DeclareMathOperator\Gam{Gamma}
  \DeclareMathOperator\Poi{Poisson}
  \DeclareMathOperator\argmin{arg min}
  \newcommand\mf{\mathbf{F}} 
  \newcommand\ml{\mathbf{L}}
  \newcommand\mx{\mathbf{X}}
  \newcommand\vl{\mathbf{l}}
  \newcommand\vx{\mathbf{x}}
  \)

  \begin{align*}
    x_{ij} &\sim \Poi(x_i^+ \lambda_{ij})\\
    \lambda_{ij} &\sim \pi_{ij} \delta_0(\cdot) + (1 - \pi_{ij}) \Gam(\phi_{ij}^{-1}, \mu_{ij}^{-1} \phi_{ij}^{-1})\\
    \ln \mu_{ij} &= (\ml \mf_\mu')_{ij}\\
    \ln \phi_{ij} &= (\ml \mf_\phi')_{ij}\\
    \operatorname{logit} \pi_{ij} &= (\ml \mf_\pi')_{ij}\\
  \end{align*}

  where 

  - \(x_{ij}\) is the number of molecules of gene \(j = 1, \ldots, p\) observed
    in cell \(i = 1, \ldots, n\)
  - cells are taken from \(m\) donor individuals, \(\ml\) is \(n \times m\),
    each \(\mf_{\cdot}\) is \(p \times m\)
  - assignments of cells to donors (loadings) \(l_{ik} \in \{0, 1\}\) are
    known and fixed.

  We are now interested in several lines of questions:

  1. If we analyze this kind of data in a fully unsupervised manner, can we
     recover the assignments of cells to donors? This approach has been
     previously proposed in our specific factor model
     ([[doi:10.1038/s41467-017-02554-5][Risso et al. 2018]]). If not, what do
     we recover?
  2. Can we generalize this analysis approach to data which additionally has
     multiple cell types, and then multiple time points?
  3. Can we implement this approach without forming entire products
     \(\ml\mf'\)? Can we implement each update without looking at the entire
     data \(\mx\)? How much faster can we get than existing methods? Can we
     analyze datasets which are currently impossible to analyze (due to size),
     e.g. Human Cell Atlas? As references, compare against
     [[doi:10.1038/s41592-018-0229-2][scVI]],
     [[https://rawcdn.githack.com/aertslab/cisTopic/8d15fa2813312aa0b20c1042604079558829e947/vignettes/10X_workflow.html#building_the_models][cisTopic]],
     [[https://github.com/stephenslab/fastTopics][fastTopics]]

* Setup
  :PROPERTIES:
  :CUSTOM_ID: setup
  :END:

  #+BEGIN_SRC emacs-lisp :exports none
    (add-to-list 'python-shell-completion-native-disabled-interpreters "jupyter")
    (org-babel-lob-ingest "/home/aksarkar/.emacs.d/org-templates/library.org")
  #+END_SRC

  #+RESULTS:
  : 1

  #+CALL: ipython3(venv="singlecell",partition="gpu2",opts="--gres=gpu:1",memory="8G") :exports none :dir /scratch/midway2/aksarkar/ideas/

  #+CALL: ipython3(venv="singlecell",partition="mstephens",memory="32G") :exports none :dir /scratch/midway2/aksarkar/ideas/

  #+RESULTS:
  : Submitted batch job 66391170

  #+BEGIN_SRC ipython
    import anmf
    import anndata
    import numpy as np
    import pandas as pd
    import scanpy as sc
    import scmodes
    import scipy.io as si
    import scipy.sparse as ss
    import scipy.stats as st
    import sklearn.decomposition as skd
    import time
    import torch
    import torch.utils.data as td
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[1]:
  :END:

  #+BEGIN_SRC ipython
    %matplotlib inline
    %config InlineBackend.figure_formats = set(['retina'])
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[2]:
  :END:

  #+BEGIN_SRC ipython
    import colorcet
    import matplotlib.pyplot as plt
    plt.rcParams['figure.facecolor'] = 'w'
    plt.rcParams['font.family'] = 'Nimbus Sans'
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[3]:
  :END:

* Methods
** Amortized NMF

   Amortized inference
   ([[http://ruishu.io/2017/11/07/amortized-optimization/][Shu 2017]],
   [[https://papers.nips.cc/paper/7692-amortized-inference-regularization.pdf][Shu
   et al. 2018]]) refers to a strategy to efficiently solve a large (possibly
   infinite) collection of optimization problems:

   \[ \theta^* = \argmin_{\theta} f(\theta, \phi), \]

   where \(\phi \in \Phi\) is some context variable. Rather than solving one
   problem for each \(\phi \in \Phi\), we learn a function \(h_\alpha\)
   parameterized by \(\alpha\) which predicts \(\theta^*\) from \(\phi\)

   \[ \alpha^* = \argmin_\alpha E_{\phi \sim \hat{p}(\phi)}[f(h_\alpha(\phi), \phi)], \]

   where \(\hat{p}(\phi)\) denotes the empirical distribution of \(\phi\) in
   the training data. The function \(h_\alpha\) amortizes inference over the
   training data examples by coupling the optimization problems together, and
   indeed amortizes inference over unseen examples also.

   As a concrete example, NMF
   ([[http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization][Lee
   and Seung 2001]], [[https://dx.doi.org/10.1155/2009/785152][Cemgil 2009]])
   can be written

   \begin{align*}
     \vl_i^* &= \argmin_{\vl_i} f(\vl_i, \vx_i, \mf)\\
     f(\vl_i, \vx_i, \mf) &\triangleq \sum_j \operatorname{Poisson}(x_{ij}; \sum_k l_{ik} f_{jk}),
   \end{align*}

   where \(\vx_i\) is the context variable, and the goal is to learn
   \(h_\alpha\) which maps observations \(\vx_i\) to loadings \(\vl_i\). (Here,
   we are simplifying by holding \(\mf\) fixed. In this setup, we treat the
   \(\vl_i\) as local latent variables, and \(\mf\) as a global latent
   variable. These can be optimized in alternating phases.) The resulting
   optimization problem can be solved by introducing an /encoder network/
   \(h_\alpha\), which has been previously proposed
   ([[doi:10.1038/s41592-018-0229-2][Lopez et al. 2018]],
   [[doi:10.1038/s41467-018-07931-2][Eraslan et al. 2018]]).

   Existing methods have been introduced with the motivation that auto-encoding
   networks can represent non-linear mappings into latent spaces; however, the
   gain in explanatory power of such methods is unclear. Amortized inference
   suggests a simpler, more compelling motivation to explore these methods:
   they enable the use of stochastic optimization methods to analyze large data
   sets (for example, those which do not fit in memory). However, existing
   software implementations have a major limitation: they do not support sparse
   matrices on the GPU, making it either impossible to analyze complete data
   sets, or introducing a major bottleneck in moving minibatches to the GPU. We
   implement this functionality in the Python package ~anmf~.

** Amortized LDA

   NMF is intimately connected to LDA via the Multinomial-Poisson transform
   ([[http://www.jstor.org/stable/2348134][Baker 1994]]). Briefly, if we have
   an MLE for NMF and scale \(\ml\) and \(\mf\) to satisfy the constraints
   \(\sum_k l_{ik} = 1\), \(\sum_k f_{jk} = 1\), we recover an MLE for the
   Multinomial likelihood underlying LDA. This fact suggests an amortized
   inference scheme for maximum likelihood estimation of topic models:

   \begin{align*}
     x_{ij} \mid s_i, \vl_i, \mf &\sim \operatorname{Poisson}(s_i \sum_k l_{ik} f_{jk})\\
     \vl_i &= h(\vx_i)
   \end{align*}

   where \(h\) is a neural network with softmax output. Unlike previous
   approaches for amortized inference in topic models
   ([[https://arxiv.org/pdf/1703.01488][Srivastava et al. 2017]]), we are not
   concerned with recovering an approximate posterior over \(\mathbf{L},
   \mathbf{F}\), simplifying the problem.

** iPSC data

   Read the data.

   #+BEGIN_SRC ipython :async t
     keep_samples = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/quality-single-cells.txt', index_col=0, header=None)
     keep_genes = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/genes-pass-filter.txt', index_col=0, header=None)
     annotations = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-annotation.txt')
     annotations = annotations.loc[keep_samples.values.ravel()]
     header = sorted(set(annotations['chip_id']))
     umi = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-counts.txt.gz', index_col=0).loc[keep_genes.values.ravel(),keep_samples.values.ravel()]
     gene_info = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-genes.txt.gz', index_col=0)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[7]:
   :END:

   Convert to sparse ~h5ad~.

   #+BEGIN_SRC ipython :async t
     del annotations["index"]
     x = anndata.AnnData(ss.csr_matrix(umi.values.T), obs=annotations, var=gene_info.loc[umi.index])
     x.write('/project2/mstephens/aksarkar/projects/singlecell-ideas/data/ipsc/ipsc.h5ad', compression=None, force_dense=False)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[33]:
   :END:

** 10-way mixture

   Read the FACS sorted data sets from
   [[https://dx.doi.org/10.1038/ncomms14049][Zheng et al 2017]].

   #+BEGIN_SRC ipython
     keys = (
       'b_cells',
       'cd34',
       'cd4_t_helper',
       'cd56_nk',
       'cytotoxic_t',
       'memory_t',
       'naive_cytotoxic',
       'naive_t',
       'regulatory_t',
       'cd14_monocytes',
     )
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[71]:
   :END:

   #+BEGIN_SRC ipython :async t
     data = {k: scmodes.dataset.read_10x(f'/project2/mstephens/aksarkar/projects/singlecell-ideas/data/10xgenomics/{k}/filtered_matrices_mex/hg19/', return_adata=True, min_detect=0)
             for k in keys}
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[72]:
   :END:

   Concatenate the data, then take genes with observations in at least 0.1% of
   cells.

   #+BEGIN_SRC ipython :async t
     x = data[keys[0]].concatenate(*[data[k] for k in keys[1:]], join='inner', batch_key='cell_type', batch_categories=keys)
     sc.pp.filter_genes(x, min_cells=1)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[73]:
   :END:

   Report the dimensions.

   #+BEGIN_SRC ipython
     x.shape
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[74]:
   : (94655, 21952)
   :END:

   Write out the data.

   #+BEGIN_SRC ipython
     x.obs = x.obs.rename({0: 'barcode'}, axis=1)
     x.var = x.var.rename({0: 'ensg', 1: 'name'}, axis=1)
     x.write('/scratch/midway2/aksarkar/ideas/zheng-10-way.h5ad')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[75]:
   :END:

** Census of Immune Cells

   We previously processed the Census of Immune Cells.

   Read the sparse data. (20 seconds)

   #+BEGIN_SRC ipython :async t
     y_csr = ss.load_npz('/scratch/midway2/aksarkar/modes/immune-cell-census.npz')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[46]:
   :END:

   Read the metadata.

   #+BEGIN_SRC ipython
     genes = pd.read_csv('/scratch/midway2/aksarkar/modes/immune-cell-census-genes.txt.gz', sep='\t', index_col=0)
     donor = pd.Categorical(pd.read_csv('/scratch/midway2/aksarkar/modes/immune-cell-census-samples.txt.gz', sep='\t', index_col=0)['0'])
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[47]:
   :END:

** Mouse Organogenesis Cell Atlas

   [[https://www.nature.com/articles/s41586-019-0969-x][Cao et al. 2019]]
   generated an atlas of 2M mouse cells, analyzed by
   [[https://www.biorxiv.org/content/10.1101/737601v1.full.pdf][Svensson and
   Pacther 2019]]. Download the data.

   #+BEGIN_SRC sh :dir /scratch/midway2/aksarkar/ideas/
     sbatch --partition=build
     #!/bin/bash
     set -e
     CURL="curl -sfOL"
     $CURL "https://shendure-web.gs.washington.edu/content/members/cao1025/public/mouse_embryo_atlas/gene_count.txt"
     $CURL "https://shendure-web.gs.washington.edu/content/members/cao1025/public/mouse_embryo_atlas/gene_annotate.csv"
     $CURL "https://shendure-web.gs.washington.edu/content/members/cao1025/public/mouse_embryo_atlas/cell_annotate.csv"
   #+END_SRC

   #+RESULTS:
   : Submitted batch job 66386315

   Read the data. (31 mins)

   #+BEGIN_SRC ipython :async t
     x = si.mmread('/scratch/midway2/aksarkar/ideas/gene_count.txt')
     var = pd.read_csv('/scratch/midway2/aksarkar/ideas/gene_annotate.csv')
     obs = pd.read_csv('/scratch/midway2/aksarkar/ideas/cell_annotate.csv')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[2]:
   :END:

   #+BEGIN_SRC ipython
     keep_genes = (x != 0).sum(axis=1).A > 0
   #+END_SRC

   #+BEGIN_SRC ipython :async t
     y = anndata.AnnData(x.T.tocsr(), var=var, obs=obs)
     y.write_h5ad('/scratch/midway2/aksarkar/ideas/moca.h5ad')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   0 - 1e2ea9ee-c7ed-4efe-9eac-5a61f80ef91e
   :END:

   Report the dimensions of the data.

   #+BEGIN_SRC ipython
     y.shape
   #+END_SRC

* Results
** Simulation

   Simulate a simple example.

   #+BEGIN_SRC ipython
     np.random.seed(0)
     n = 512
     p = 1000
     k = 10
     l = np.random.lognormal(sigma=0.5, size=(n, k))
     f = np.random.lognormal(sigma=0.5, size=(p, k))
     x = np.random.poisson(l @ f.T)
     s = x.sum(axis=1)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[4]:
   :END:

   Fit NMF via EM. Report the time elapsed (minutes).

   #+BEGIN_SRC ipython :async t
     start = time.time()
     lhat0, fhat0, loss0 = scmodes.lra.nmf(x, rank=10, tol=1e-3, max_iters=50000, verbose=True)
     elapsed = time.time() - start
     elapsed / 60
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[69]:
   : 13.413497316837312
   :END:

   Peter Carbonetto previously derived the KKT conditions for the NMF
   objective. Report the maximum absolute KKT residual.

   #+BEGIN_SRC ipython :async t
     # Important: our NMF implementation does not remove the size factor
     A = x / (lhat0 @ fhat0.T)
     l_resid = abs(fhat0 * ((1 - A).T @ lhat0)).max()
     f_resid = abs(lhat0 * ((1 - A) @ fhat0)).max()
     l_resid, f_resid
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[70]:
   : (0.05866832853326247, 0.09743250873968519)
   :END:

   #+BEGIN_SRC ipython :ipyfile figure/time-course.org/sim-em-fhat.png
     r = np.corrcoef(f.T, fhat0.T)[:10,10:]
     order = np.argmax(r, axis=1)
     plt.clf()
     plt.gcf().set_size_inches(3, 3)
     plt.imshow(r[:,order], cmap=colorcet.cm['coolwarm'], vmin=-1, vmax=1)
     plt.xticks(np.arange(10))
     plt.yticks(np.arange(10))
     plt.xlabel('True factor')
     plt.ylabel('Estimated factor')
     plt.title('EM')
     plt.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[138]:
   [[file:figure/time-course.org/sim-em-fhat.png]]
   :END:

   Fit ANMF. Report the time elapsed (seconds).

   #+BEGIN_SRC ipython
     assert torch.cuda.is_available()
     xt = torch.tensor(x, dtype=torch.float).cuda()
     dense_data = td.TensorDataset(xt, torch.tensor(s, dtype=torch.float).cuda())
     data = td.DataLoader(dense_data, batch_size=x.shape[0], shuffle=False)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[153]:
   :END:

   #+BEGIN_SRC ipython :async t
     start = time.time()
     fit = (anmf.modules.ANMF(input_dim=x.shape[1])
            .fit(data, max_epochs=6400, trace=True, lr=5e-4))
     elapsed = time.time() - start
     elapsed
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[154]:
   : 49.469866037368774
   :END:

   Plot the optimization trace, focusing on the last 100 minibatches.

   #+BEGIN_SRC ipython :ipyfile figure/time-course.org/sim-trace.png
     plt.clf()
     plt.gcf().set_size_inches(4, 2)
     plt.plot(np.array(fit.trace).ravel()[-400:] / (n * p), lw=1, c='k')
     plt.xticks(np.arange(0, 500, 100), np.arange(-400, 100, 100))
     plt.xlabel('Minibatch before end')
     plt.ylabel('Per obs neg log lik')
     plt.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[190]:
   [[file:figure/time-course.org/sim-trace.png]]
   :END:

   Recover the loadings and factors. Convert the factors to topics and
   correlate them to each other.

   #+BEGIN_SRC ipython
     d = f / f.sum(axis=0, keepdims=True)
     t = l * f.sum(axis=0, keepdims=True)
     t /= t.sum(axis=1, keepdims=True)

     lhat = fit.loadings(xt)
     fhat = fit.factors()
     dhat = (fhat / fhat.sum(axis=1, keepdims=True)).T
     that = lhat * fhat.sum(axis=1)
     that /= that.sum(axis=1, keepdims=True)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[177]:
   :END:

   #+BEGIN_SRC ipython :ipyfile figure/time-course.org/sim-topics.png
     r = np.corrcoef(t.T, that.T)[:10,10:]
     order = np.argmax(r, axis=1)
     plt.clf()
     plt.gcf().set_size_inches(3, 3)
     plt.imshow(r[order], cmap=colorcet.cm['coolwarm'], vmin=-1, vmax=1)
     plt.xticks(np.arange(10))
     plt.yticks(np.arange(10))
     plt.xlabel('True factor')
     plt.ylabel('Estimated factor')
     plt.title('ANMF')
     plt.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[187]:
   [[file:figure/time-course.org/sim-fhat.png]]
   :END:

   Report the maximum absolute KKT residual.

   #+BEGIN_SRC ipython :async t
     A = x / (s.reshape(-1, 1) * lhat.dot(fhat))
     l_resid = abs(fhat.T * ((1 - A).T @ lhat)).max()
     f_resid = abs(lhat * ((1 - A) @ fhat.T)).max()
     l_resid, f_resid
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[108]:
   : (0.001169713130727257, 0.020623888958659735)
   :END:

   Report the loss achieved by each algorithm.

   #+BEGIN_SRC ipython
     pd.Series({
       'oracle': -st.poisson(mu=l @ f.T).logpmf(x).mean(),
       'em': loss0 / np.prod(x.shape),
       'anmf': -st.poisson(mu=s.reshape(-1, 1) * lhat @ fhat).logpmf(x).mean(),
     })
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[189]:
   #+BEGIN_EXAMPLE
     oracle    2.664753
     em        2.649209
     anmf      2.704351
     dtype: float64
   #+END_EXAMPLE
   :END:

   The log likelihood achieved by ANMF is lower than the oracle. Recent results
   ([[https://arxiv.org/abs/1801.03558][Cremer et al 2020]]) suggest this is to
   be expected: there is an /amortization gap/ due to the fact that we are not
   directly optimizing the likelihood with respect to the local latent variable
   \(\vl_i\) for each observation \(\vx_i\). To investigate this gap, fix the
   factors to the ground truth, and see whether ANMF recovers the loadings.

   #+BEGIN_SRC ipython :async t
     start = time.time()
     fit = anmf.modules.ANMF(input_dim=x.shape[1], latent_dim=10)
     # y = ln(1 + exp(x))
     # exp(y) - 1 = exp(x)
     # ln(exp(y) - 1) = x
     fit.decoder._f.data = torch.tensor(np.log(np.exp(f.T) - 1), dtype=torch.float)
     fit.decoder._f.requires_grad = False
     fit.fit(data, max_epochs=1600, trace=True, lr=1e-3)
     elapsed = time.time() - start
     elapsed
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[63]:
   : 30.25163698196411
   :END:

   Compare the fit fixing the ground truth \(\mf\) to the oracle.

   #+BEGIN_SRC ipython
     lhat = fit.loadings(xt)
     fhat = fit.factors()
     pd.Series({
       'oracle': -st.poisson(mu=l @ f.T).logpmf(x).mean(),
       'anmf': -st.poisson(mu=s.reshape(-1, 1) * lhat @ fhat).logpmf(x).mean(),
     })
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[64]:
   #+BEGIN_EXAMPLE
     oracle    2.664753
     anmf      2.676659
     dtype: float64
   #+END_EXAMPLE
   :END:

   Plot the estimated loadings against the true loadings.

   #+BEGIN_SRC ipython :ipyfile figure/time-course.org/sim-lhat.png
     plt.clf()
     fig, ax = plt.subplots(2, 5, sharex=True, sharey=True)
     fig.set_size_inches(8, 4)
     lim = [0, 3]
     for i, a in enumerate(ax.ravel()):
       a.scatter(np.sqrt(l[:,i]), np.sqrt(s * lhat[:,i]), s=1, c='k')
       a.plot(lim, lim, lw=1, ls=':', c='r')
       a.set_title(f'Factor {i}')
       a.set_xlim(lim)
       a.set_ylim(lim)
     for a in ax:
       a[0].set_ylabel('Sqrt est loading')
     for a in ax.T:
       a[-1].set_xlabel('Sqrt true loading')
     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[65]:
   [[file:figure/time-course.org/sim-lhat.png]]
   :END:

   Check whether this is an algorithmic problem by trying batch gradient
   descent instead.

   #+BEGIN_SRC ipython :async t
     data = td.DataLoader(dense_data, batch_size=x.shape[0], shuffle=False)
     fit = anmf.modules.ANMF(input_dim=x.shape[1], latent_dim=10)
     fit.decoder._f.data = torch.tensor(np.log(np.exp(f.T) - 1), dtype=torch.float)
     fit.decoder._f.requires_grad = False
     fit.fit(data, max_epochs=6400, trace=True, lr=1e-3)

     lhat = fit.loadings(xt)
     fhat = fit.factors()
     pd.Series({
       'oracle': -st.poisson(mu=l @ f.T).logpmf(x).mean(),
       'anmf': -st.poisson(mu=s.reshape(-1, 1) * lhat @ fhat).logpmf(x).mean(),
     })
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[68]:
   #+BEGIN_EXAMPLE
     oracle    2.664753
     anmf      2.675015
     dtype: float64
   #+END_EXAMPLE
   :END:

   Try batch gradient descent without amortization.

   #+BEGIN_SRC ipython :async t
     m = (anmf.modules.GNMF(n=x.shape[0], p=x.shape[1], k=10)
          .cuda()
          .fit(xt, max_epochs=1000, lr=1e-2, verbose=True, trace=True))
     m.loss(xt).detach().cpu().numpy() / (n * p)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   0 - 8cdb718e-f62b-4d43-960f-7301e2568059
   :END:

** ANMF on 68K PBMCs

   Read the data, restricting to genes with non-zero observations in at least 1
   cell.

   #+BEGIN_SRC ipython :async t
     x = scmodes.dataset.read_10x('/project2/mstephens/aksarkar/projects/singlecell-ideas/data/10xgenomics/fresh_68k_pbmc_donor_a/filtered_matrices_mex/hg19/', min_detect=0, return_adata=True)
     sc.pp.filter_genes(x, min_cells=1)
     s = x.X.sum(axis=1)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[64]:
   :END:

   Report the dimensions.

   #+BEGIN_SRC ipython
     x.shape
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[65]:
   : (68579, 20387)
   :END:

   Fit rank 10 ANMF.

   #+BEGIN_SRC ipython
     sparse_data = anmf.dataset.ExprDataset(x.X, s.A)
     data = td.DataLoader(sparse_data, batch_size=128, shuffle=False, collate_fn=sparse_data.collate_fn)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[66]:
   :END:

   #+BEGIN_SRC ipython :async t
     start = time.time()
     fit = (anmf.modules.ANMF(input_dim=x.shape[1], latent_dim=10)
            .fit(data, max_epochs=15, trace=True, verbose=True, lr=1e-2))
     elapsed = time.time() - start
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[67]:
   :END:

   Report how long the optimization took (minutes).

   #+BEGIN_SRC ipython
     elapsed / 60
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[69]:
   : 5.1033944328626
   :END:

   Plot the optimization trace, focusing on the tail.

   #+BEGIN_SRC ipython :ipyfile figure/time-course.org/pbmc-trace.png
     plt.clf()
     plt.gcf().set_size_inches(4, 2)
     plt.plot(np.array(fit.trace).ravel()[-2000:] / (128 * x.shape[1]), lw=1, c='k')
     plt.xticks(np.arange(0, 2500, 500), np.arange(-2000, 500, 500))
     plt.xlabel('Minibatches before maximum')
     plt.ylabel('Neg log lik per obs')
     plt.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[70]:
   [[file:figure/time-course.org/pbmc-trace.png]]
   :END:

** ANMF on 10-way mixture

   Read the data.

   #+BEGIN_SRC ipython
     x = anndata.read_h5ad('/scratch/midway2/aksarkar/ideas/zheng-10-way.h5ad')
     s = x.X.sum(axis=1)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[4]:
   :END:

   Fit ANMF. Report how long the optimization took (minutes).

   #+BEGIN_SRC ipython
     sparse_data = anmf.dataset.ExprDataset(x.X, s.A)
     data = td.DataLoader(sparse_data, batch_size=64, shuffle=False, collate_fn=sparse_data.collate_fn)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[5]:
   :END:

   #+BEGIN_SRC ipython :async t
     start = time.time()
     fit = (anmf.modules.ANMF(input_dim=x.shape[1])
            .fit(data, max_epochs=10, trace=True, verbose=True, lr=5e-3))
     elapsed = time.time() - start
     elapsed / 60
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[9]:
   : 5.154516808191935
   :END:

   Save the fitted model.

   #+BEGIN_SRC ipython
     torch.save(fit.state_dict(), '/scratch/midway2/aksarkar/ideas/zheng-10-way-anmf.pkl')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[10]:
   :END:

   Load the fitted model.

   #+BEGIN_SRC ipython
     fit = anmf.modules.ANMF(input_dim=x.shape[1])
     fit.load_state_dict(torch.load('/scratch/midway2/aksarkar/ideas/zheng-10-way-anmf.pkl'))
     fit.cuda()
   #+END_SRC

   Plot the optimization trace, focusing on the tail.

   #+BEGIN_SRC ipython :ipyfile figure/time-course.org/zheng-10-way-trace.png
     plt.clf()
     plt.gcf().set_size_inches(4, 2)
     plt.plot(np.array(fit.trace).ravel()[-2000:] / (128 * x.shape[1]), lw=1, c='k')
     plt.xticks(np.arange(0, 2500, 500), np.arange(-2000, 500, 500))
     plt.xlabel('Minibatches before maximum')
     plt.ylabel('Neg log lik per obs')
     plt.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[11]:
   [[file:figure/time-course.org/zheng-10-way-trace.png]]
   :END:

   Get the loadings/factors.

   #+BEGIN_SRC ipython :async t
     l = np.vstack([fit.loadings(b) for b, s in data])
     f = fit.factors()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[20]:
   :END:

   Normalize into a topic model.

   #+BEGIN_SRC ipython
     w = l * f.sum(axis=1)
     w /= w.sum(axis=1, keepdims=True)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[31]:
   :END:

   Get the cell type identities.

   #+BEGIN_SRC ipython
     onehot = pd.get_dummies(x.obs['cell_type'])
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[33]:
   :END:

   Estimate the correlation between the topic weights and the cell types.

   #+BEGIN_SRC ipython :ipyfile figure/time-course.org/zheng-10-way-loadings.png
     r = np.corrcoef(w.T, onehot.T)
     plt.clf()
     plt.gcf().set_size_inches(4, 4)
     plt.imshow(r[:10,10:], cmap=colorcet.cm['coolwarm'], vmin=-1, vmax=1)
     plt.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[36]:
   [[file:figure/time-course.org/zheng-10-way-loadings.png]]
   :END:

** ANMF on iPSC data

   Read the data.

   #+BEGIN_SRC ipython
     x = anndata.read_h5ad('/project2/mstephens/aksarkar/projects/singlecell-ideas/data/ipsc/ipsc.h5ad')
     s = x.X.sum(axis=1)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[67]:
   :END:

   #+BEGIN_SRC ipython
     sparse_data = anmf.dataset.ExprDataset(x.X, s.A)
     data = td.DataLoader(sparse_data, batch_size=64, shuffle=False, collate_fn=sparse_data.collate_fn)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[69]:
   :END:

   Run ANMF on the data. A priori, we might expect the data to be rank 53
   (equal to the number of donor individuals). Indeed, this is the rank of the
   implicit factor model we originally fit to the data.

   #+BEGIN_SRC ipython :async t
     start = time.time()
     fit = (anmf.modules.ANMF(input_dim=x.shape[1], latent_dim=53)
            .fit(data, max_epochs=40, trace=True, verbose=True, lr=1e-2))
     elapsed = time.time() - start
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[24]:
   :END:

   Report how long the optimization took (minutes).

   #+BEGIN_SRC ipython
     elapsed / 60
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[28]:
   : 2.7217764496803283
   :END:

   Plot the end of the optimization trace.

   #+BEGIN_SRC ipython :ipyfile figure/time-course.org/ipsc-trace.png
     plt.clf()
     plt.gcf().set_size_inches(4, 2)
     plt.plot(np.array(fit.trace).ravel()[-100:] / (128 * x.shape[1]), lw=1, c='k')
     plt.xticks(np.arange(0, 125, 25), np.arange(-100, 25, 25))
     plt.xlabel('Minibatches before maximum')
     plt.ylabel('Neg log lik per obs')
     plt.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[29]:
   [[file:figure/time-course.org/ipsc-trace.png]]
   :END:

   Save the fitted model.

   #+BEGIN_SRC ipython
     torch.save(fit.state_dict(), '/scratch/midway2/aksarkar/ideas/ipsc-anmf.pkl')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[27]:
   :END:

   Load the fitted model.

   #+BEGIN_SRC ipython
     fit = anmf.modules.ANMF(input_dim=x.shape[1], latent_dim=53)     
     fit.load_state_dict(torch.load('/scratch/midway2/aksarkar/ideas/ipsc-anmf.pkl'))
     fit.cuda()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[74]:
   #+BEGIN_EXAMPLE
     ANMF(
     (encoder): Encoder(
     (net): Sequential(
     (0): Linear(in_features=9957, out_features=128, bias=True)
     (1): ReLU()
     (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
     (3): Linear(in_features=128, out_features=53, bias=True)
     (4): Softplus(beta=1, threshold=20)
     )
     )
     (decoder): Pois()
     )
   #+END_EXAMPLE
   :END:

   Look at the correlation between the loadings and the donor individuals.

   #+BEGIN_SRC ipython :async t
     l = np.vstack([fit.loadings(b) for b, s in data])
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[75]:
   :END:

   #+BEGIN_SRC ipython 
     keep_samples = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/quality-single-cells.txt', index_col=0, header=None)
     annotations = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-annotation.txt')
     annotations = annotations.loc[keep_samples.values.ravel()]
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[76]:
   :END:

   #+BEGIN_SRC ipython
     onehot = pd.get_dummies(annotations['chip_id'])
     r = np.corrcoef(l.T, onehot.T)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[84]:
   :END:

   #+BEGIN_SRC ipython :ipyfile figure/time-course.org/ipsc-loadings.png
     plt.clf()
     plt.gcf().set_size_inches(4, 4)
     plt.imshow(r, vmin=-1, vmax=1, cmap=colorcet.cm['coolwarm'])
     plt.xlabel('Estimated loadings')
     plt.ylabel('Donor individual')
     plt.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[89]:
   [[file:figure/time-course.org/ipsc-loadings.png]]
   :END:

