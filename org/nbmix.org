#+TITLE: Model-based clustering of scRNA-seq data
#+SETUPFILE: setup.org

* Introduction
  :PROPERTIES:
  :CUSTOM_ID: introdcution
  :END:

  Two major strategies for clustering scRNA-seq data are:

  1. Building a \(k\)-nearest neighbor graph on the data, and applying a
     community detection algorithm (e.g.,
     [[https://doi.org/10.1088/1742-5468/2008/10/P10008][Blondel et al. 2008]],
     [[https://arxiv.org/abs/1810.08473][Traag et al. 2018]])
  2. Fitting a topic model to the data
     (e.g., [[https://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1006599][Dey
     et al. 2017]],
     [[https://www.nature.com/articles/s41592-019-0367-1][Gonz√°les-Blas et
     al. 2019]])

  The main disadvantage of strategy (1) is that, as commonly applied to
  transformed counts, it does not separate measurement error and biological
  variation of interest. The main disadvantage of strategy (2) is that it does
  not account for transcriptional noise
  ([[https://doi.org/10.1016/j.cell.2008.09.050][Raj 2008]]). Here, we develop
  a simple model-based clustering algorithm which addresses both of these
  issues.

* Setup
  :PROPERTIES:
  :CUSTOM_ID: setup
  :END:

  #+BEGIN_SRC emacs-lisp :exports none
    (add-to-list 'python-shell-completion-native-disabled-interpreters "jupyter")
    (org-babel-lob-ingest "/home/aksarkar/.emacs.d/org-templates/library.org")
  #+END_SRC

  #+RESULTS:
  : 2

  #+CALL: ipython3(venv="singlecell",partition="gpu2",opts="--gres=gpu:1",memory="16G") :exports none :dir /scratch/midway2/aksarkar/singlecell/

  #+BEGIN_SRC ipython
    import anndata
    import mpebpm.gam_mix
    import mpebpm.sgd
    import numpy as np
    import pandas as pd
    import scanpy as sc
    import scipy.optimize as so
    import scipy.special as sp
    import scipy.stats as st
    import scmodes
    import torch
    import torch.utils.data as td
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[27]:
  :END:

  #+BEGIN_SRC ipython
    %matplotlib inline
    %config InlineBackend.figure_formats = set(['retina'])
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[2]:
  :END:

  #+BEGIN_SRC ipython
    import matplotlib
    import matplotlib.pyplot as plt
    plt.rcParams['figure.facecolor'] = 'w'
    plt.rcParams['font.family'] = 'Nimbus Sans'
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[3]:
  :END:

  #+CALL: tensorboard(venv="singlecell") :session tensorboard :dir /scratch/midway2/aksarkar/singlecell/

* Method
  :PROPERTIES:
  :CUSTOM_ID: method
  :END:

  We assume \(
  \DeclareMathOperator\Gam{Gamma}
  \DeclareMathOperator\Mult{Multinomial}
  \DeclareMathOperator\N{\mathcal{N}}
  \DeclareMathOperator\Pois{Poisson}
  \newcommand\mi{\mathbf{I}}
  \newcommand\vmu{\boldsymbol{\mu}}
  \newcommand\vphi{\boldsymbol{\phi}}
  \newcommand\vpi{\boldsymbol{\pi}}
  \)

  \begin{align}
    x_{ij} \mid x_{i+}, \lambda_{ij} &\sim \Pois(x_{i+} \lambda_{ij})\\
    \lambda_{ij} \mid \vpi, \vmu, \vphi &\sim \sum_{k=1}^{K} \pi_{ik} \Gam(\phi_{kj}^{-1}, \phi_{kj}^{-1}\mu_{kj}^{-1}),
  \end{align}

  where

  - \(x_{ij}\) denotes the number of molecules of gene \(j\) observed in cell \(i\)
  - \(x_{i+} \triangleq \sum_j x_{ij}\) denotes the total number of molecules
    observed in cell \(i\)
  - \(\vpi_i\) denotes cluster assignment probabilities for cell \(i\)
  - \(\vmu_k\) denotes the cluster "centroid" for cluster \(k\), and
    \(\vphi_k\) describes stochastic perturbations within each cluster

  The intuition behind this model is that each cluster \(k\) is defined by a
  collection of independent Gamma distributions, one per gene \(j\). These
  Gamma distributions describe the distribution of true gene expression for
  each gene in each cluster
  ([[https://dx.doi.org/10.1101/2020.04.07.030007][Sarkar and Stephens
  2020]]). (In this parameterization, each has mean \(\mu_{kj}\) and variance
  \(\mu_{kj}^2\phi_{kj}\).)

  Under this model, the marginal likelihood is a mixture of negative
  binomials. We can estimate \(\vpi, \vmu, \vphi\) by maximizing the likelihood
  using an EM algorithm. Letting \(z_{ik} \in \{0, 1\}\) indicate whether cell
  \(i\) is assigned to cluster \(k\), in the E step

  \begin{equation}
    E[z_{ik}] \propto \sum_j \int_0^{\infty} \Pois(x_{ij}; x_{i+}\lambda) \Gam(\lambda; \phi_{kj}^{-1}, \phi_{kj}^{-1}\mu_{kj}^{-1})\, d\lambda.
  \end{equation}

  In the M step, we improve the mixture of negative binomials likelihood by
  [[file:mepbpm.org][(batch) gradient descent]], implemented in the Python
  package ~mpebpm~. To make the model amenable to stochastic gradient descent,
  we can instead maximize the evidence lower bound and amortize inference
  ([[https://escholarship.org/content/qt34j1h7k5/qt34j1h7k5.pdf][Gershman and
  Goodman 2014]], [[https://arxiv.org/abs/1312.6114][Kingma and Welling 2014]],
  [[http://proceedings.mlr.press/v32/rezende14.html][Rezende et al. 2014]]),
  learning a neural network \(f_p\) mapping \(x_{i\cdot} \rightarrow
  z_{i\cdot}\)

  \begin{align}
    p(z_{i1}, \ldots, z_{iK}) &= \Mult(1, \vpi)\\
    q(z_{i1}, \ldots, z_{iK} \mid x_{i\cdot}) &= \Mult(1, f_p(x_{i\cdot})).
  \end{align}

* Results
  :PROPERTIES:
  :CUSTOM_ID: results
  :END:
** Example
   :PROPERTIES:
   :CUSTOM_ID: example
   :END:

   Read sorted immune cell scRNA-seq data
   ([[https://dx.doi.org/10.1038/ncomms14049][Zheng et al. 2017]]).

   #+BEGIN_SRC ipython :async t
     dat = anndata.read_h5ad('/scratch/midway2/aksarkar/ideas/zheng-10-way.h5ad')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[4]:
   :END:

   Get 256 B cells and 256 cytotoxic T cells.

   #+BEGIN_SRC ipython :async t
     b_cells = dat[dat.obs['cell_type'] == 'b_cells']
     sc.pp.subsample(b_cells, n_obs=256, random_state=0)
     t_cells = dat[dat.obs['cell_type'] == 'cytotoxic_t']
     sc.pp.subsample(t_cells, n_obs=256)
     temp = b_cells.concatenate(t_cells)
     sc.pp.filter_genes(temp, min_counts=1)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[5]:
   :END:

   Plot a UMAP embedding of the data, coloring points by the ground truth
   labels.

   #+BEGIN_SRC ipython :async t
     sc.pp.pca(temp)
     sc.pp.neighbors(temp)
     sc.tl.umap(temp)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[6]:
   :END:

   #+BEGIN_SRC ipython :ipyfile figure/nbmix.org/sim-ex.png
     cm = plt.get_cmap('Dark2')
     plt.clf()
     plt.gcf().set_size_inches(3, 3)
     for i, c in enumerate(temp.obs['cell_type'].unique()):
       plt.plot(*temp[temp.obs['cell_type'] == c].obsm["X_umap"].T, c=cm(i), marker='.', ms=2, lw=0, label=f'{c}')
     plt.legend(frameon=False, markerscale=4, handletextpad=0)
     plt.xlabel('UMAP 1')
     plt.ylabel('UMAP 2')
     plt.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[7]:
   [[file:figure/nbmix.org/sim-ex.png]]
   :END:

** Leiden algorithm
   :PROPERTIES:
   :CUSTOM_ID: leiden
   :END:

    Apply the Leiden algorithm ([[https://arxiv.org/abs/1810.08473][Traag et
    al. 2018]]) to the data (<1 s).

    #+BEGIN_SRC ipython :async t
      sc.tl.leiden(temp, random_state=0)
    #+END_SRC

    #+RESULTS:
    :RESULTS:
    # Out[8]:
    :END:

    #+BEGIN_SRC ipython :ipyfile figure/nbmix.org/leiden-ex.png
      cm = plt.get_cmap('Dark2')
      plt.clf()
      plt.gcf().set_size_inches(3, 3)
      for i, c in enumerate(temp.obs['leiden'].unique()):
        plt.plot(*temp[temp.obs['leiden'] == c].obsm["X_umap"].T, c=cm(i), marker='.', ms=2, lw=0, label=f'Cluster {i}')
      plt.legend(frameon=False, markerscale=4, handletextpad=0)
      plt.xlabel('UMAP 1')
      plt.ylabel('UMAP 2')
      plt.tight_layout()
    #+END_SRC

    #+RESULTS:
    :RESULTS:
    # Out[9]:
    [[file:figure/nbmix.org/leiden-ex.png]]
    :END:

** MPEBPM
   :PROPERTIES:
   :CUSTOM_ID: mpebpm
   :END:

   First, start from the ground truth \(z\) (labels), and estimate the Gamma
   expression models.

   #+BEGIN_SRC ipython :async t
     fit0 = mpebpm.sgd.ebpm_gamma(
       temp.X,
       onehot=pd.get_dummies(temp.obs['cell_type']).values,
       batch_size=32,
       num_epochs=320,
       shuffle=True)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[10]:
   :END:

   Look at the differences in the estimated mean parameter for each gene, to
   see how many genes are informative about the labels.

   #+BEGIN_SRC ipython :ipyfile figure/nbmix.org/mepbpm-log-mu.png
     query = np.sort(np.diff(fit0[0], axis=0).ravel())
     plt.clf()
     plt.gcf().set_size_inches(4, 2)
     plt.plot(query, lw=1, c='k')
     plt.axhline(y=0, lw=1, ls=':', c='k')
     plt.xlabel('Gene')
     plt.ylabel(r'Diff $\ln(\mu_j)$')
     plt.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[29]:
   [[file:figure/nbmix.org/mepbpm-log-mu.png]]
   :END:

   Estimate the cluster weights.

   #+BEGIN_SRC ipython
     L = mpebpm.gam_mix._nb_mix_llik(
       x=torch.tensor(temp.X.A, dtype=torch.float), 
       s=torch.tensor(temp.X.sum(axis=1), dtype=torch.float),
       log_mean=torch.tensor(fit0[0], dtype=torch.float),
       log_inv_disp=torch.tensor(fit0[1], dtype=torch.float))
     zhat = torch.nn.functional.softmax(L, dim=1)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[33]:
   :END:

   Plot the log likelihood difference between the two components for each data
   point.

   #+BEGIN_SRC ipython :ipyfile figure/nbmix.org/mpebpm-llik-diff.png
     plt.clf()
     plt.gcf().set_size_inches(4, 2)
     plt.plot(np.diff(L).ravel(), lw=0, marker='.', c='k', ms=2)
     plt.axhline(y=0, lw=1, ls=':', c='k')
     plt.xlabel('Cell')
     plt.ylabel('Diff log lik')
     plt.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[42]:
   [[file:figure/nbmix.org/mpebpm-llik-diff.png]]
   :END:

   Compute the cross entropy between the estimated \(\hat{z}\) and the ground
   truth.

   #+BEGIN_SRC ipython
     torch.nn.functional.binary_cross_entropy(
       zhat,
       torch.tensor(pd.get_dummies(temp.obs['cell_type']).values, dtype=torch.float))
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[108]:
   : tensor(0.)
   :END:

   Compute a weighted log likelihood.

   #+BEGIN_SRC ipython
     w = torch.rand([512, 2])
     w /= w.sum(dim=1).unsqueeze(-1)
     m, _ = L.max(dim=1, keepdim=True)
     (m + torch.log(w * torch.exp(L - m) + 1e-8)).mean()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[43]:
   : tensor(-1872.5645)
   :END:

   Try fitting the model from a random initialization (49 s).

   #+BEGIN_SRC ipython :async t
     import imp; imp.reload(mpebpm.gam_mix)
     torch.manual_seed(0)
     fit = mpebpm.gam_mix.ebpm_gam_mix_em(
       x=temp.X.A,
       s=temp.X.sum(axis=1),
       y=torch.tensor(pd.get_dummies(temp.obs['cell_type']).values, dtype=torch.float).cuda(),
       k=2,
       num_epochs=400,
       max_em_iters=10,
       log_dir='runs/nbmix/mpebpm-random-init0-iter10')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[53]:
   :END:

   Compute the cross entropy between the estimated \(\hat{z}\) and the ground
   truth.

   #+BEGIN_SRC ipython
     torch.nn.functional.binary_cross_entropy(
       torch.tensor(fit[-1], dtype=torch.float),
       torch.tensor(pd.get_dummies(temp.obs['cell_type']).values, dtype=torch.float))
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[55]:
   : tensor(0.)
   :END:

   Plot the UMAP, colored by the fitted clusters.

   #+BEGIN_SRC ipython :ipyfile figure/nbmix.org/sim-ex-fit.png
     cm = plt.get_cmap('Dark2')
     plt.clf()
     plt.gcf().set_size_inches(3, 3)
     for i in range(fit[-1].shape[1]):
       plt.plot(*temp[fit[-1][:,i].astype(bool)].obsm["X_umap"].T, c=cm(i), marker='.', ms=2, lw=0, label=f'Cluster {i}')
     plt.legend(frameon=False, markerscale=4, handletextpad=0)
     plt.xlabel('UMAP 1')
     plt.ylabel('UMAP 2')
     plt.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[56]:
   [[file:figure/nbmix.org/sim-ex-fit.png]]
   :END:

** EM
   :PROPERTIES:
   :CUSTOM_ID: em
   :END:

   #+BEGIN_SRC ipython
     def _nb_mix_llik(theta, x, s):
       """Return log likelihood matrix

       theta - array-like [2, k]
       x - array-like [n, 1]
       s - array-like [n, 1]

       """
       # Important: so.minimize flattens theta
       theta = theta.reshape(2, -1)
       mean = np.exp(theta[0]).reshape(1, -1)
       # Important: this can blow up
       inv_disp = np.exp(np.clip(theta[1], -15, 15)).reshape(1, -1)
       # [n, k]
       L = st.nbinom(n=inv_disp, p=1 / (1 + s * mean / inv_disp)).logpmf(x.reshape(-1, 1))
       assert L.shape == (x.shape[0], theta.shape[1])
       assert np.isfinite(L).all()
       return L

     def _nb_mix_obj(theta, x, z, s):
       """Return negative log likelihood

       theta - array-like [2, k]
       x - array-like [n, 1]
       z - array-like [n, k]
       s - array-like [n, 1]

       """
       L = _nb_mix_llik(theta, x, s)
       m = L.max(axis=1, keepdims=True)
       return -(np.log(z) + L).mean()

     def ebpm_gamma_mix(x, s, k, max_iters=100, tol=1e-3, verbose=False, seed=1):
       assert x.ndim == 2
       n, p = x.shape
       assert s.shape == (n, 1)
       assert k > 1
       rng = np.random.default_rng(seed)
       log_mean = rng.normal(size=(p, k))
       log_inv_disp = rng.normal(size=(p, k))
       z = np.ones((n, k)) / k
       loss = np.array([_nb_mix_obj(np.vstack([log_mean[j], log_inv_disp[j]]), x[:,j], z, s) for j in range(p)]).sum()
       if verbose:
         print(f'epoch 0: {loss:.4g}')
       for t in range(max_iters):
         for j in range(p):
           opt = so.minimize(_nb_mix_obj, x0=np.vstack([log_mean[j], log_inv_disp[j]]), args=(x[:,j], z, s), method='nelder-mead', options={'maxiter':1000})
           if not opt.success:
             raise RuntimeError(f'M step failed to converge: {opt.message}')
           log_mean[j] = opt.x[0]
           log_inv_disp[j] = opt.x[1]
         L = np.array([_nb_mix_llik(np.vstack(log_mean[j], log_inv_disp[j]), x[:,j], s) for j in range(p)]).sum(axis=0)
         z = sp.softmax(L, axis=1)
         update = np.array([_nb_mix_obj(np.vstack(log_mean[j], log_inv_disp[j]), x[:,j], z, s) for j in range(p)]).sum()
         if update > loss:
           raise RuntimeError('objective increased')
         elif loss - update < tol:
           return z, log_mean, log_inv_disp
         else:
           if verbose:
             print(f'epoch {t + 1}: {loss:.4g}')
           loss = update
       raise RuntimeError('failed to converge in max_iters')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[78]:
   :END:

** Amortized inference
   :PROPERTIES:
   :CUSTOM_ID: amortized
   :END:

   Fit the amortized inference model, initializing \(\vmu, \vphi\) from the MLE
   starting from the ground-truth labels.

   #+BEGIN_SRC ipython
     query = torch.tensor(temp.X.A)
     s = torch.tensor(temp.X.sum(axis=1))
     fit = mpebpm.gam_mix.EBPMGammaMix(
       p=temp.shape[1],
       k=2,
       log_mean=fit0[0],
       log_inv_disp=fit0[1])
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[57]:
   :END:

   Look at the initial loss.

   #+BEGIN_SRC ipython
     (
       mpebpm.gam_mix._nb_mix_loss(
         fit.encoder.forward(query),
         query,
         s,
         fit.log_mean,
         fit.log_inv_disp),
       mpebpm.gam_mix._nb_mix_loss(
         torch.tensor(pd.get_dummies(temp.obs['cell_type']).values),
         query,
         s,
         fit.log_mean,
         fit.log_inv_disp)
     )
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[94]:
   #+BEGIN_EXAMPLE
     (tensor(1927253.3750, grad_fn=<NegBackward>),
     tensor(1926631.7500, grad_fn=<NegBackward>))
   #+END_EXAMPLE
   :END:

   #+BEGIN_SRC ipython
     temp_loss = mpebpm.gam_mix._nb_mix_loss(
       fit.encoder.forward(query),
       query,
       torch.tensor(temp.X.sum(axis=1)),
       fit.log_mean,
       fit.log_inv_disp)
     temp_loss.retain_grad()
     temp_loss.backward()
     torch.norm(fit.encoder[0].weight.grad)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[95]:
   : tensor(31154.1895)
   :END:

   Perform amortized inference.

   #+BEGIN_SRC ipython :async t
     torch.manual_seed(1)
     fit1 = mpebpm.gam_mix.EBPMGammaMix(
       p=temp.shape[1],
       k=2,
       log_mean=fit0[0],
       log_inv_disp=fit0[1]).fit(
         x=temp.X.A,
         s=temp.X.sum(axis=1),
         y=pd.get_dummies(temp.obs['cell_type']).values,
         lr=1e-3,
         batch_size=64,
         shuffle=True,
         num_pretrain=80,
         num_epochs=80,
         log_dir='runs/nbmix/ai-inittruth-epoch80')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[62]:
   :END:

   #+BEGIN_SRC ipython
     zhat = fit1.forward(query.cuda()).detach().cpu().numpy()
     torch.nn.functional.binary_cross_entropy(
       torch.tensor(zhat, dtype=torch.float),
       torch.tensor(pd.get_dummies(temp.obs['cell_type']).values, dtype=torch.float))
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[63]:
   : tensor(0.2482)
   :END:

   Plot the approximate posterior over cluster assignments for each point.

   #+BEGIN_SRC ipython :ipyfile figure/nbmix.org/am-inf-ex.png
     cm = plt.get_cmap('Dark2')
     plt.clf()
     fig, ax = plt.subplots(1, 2, sharex=True, sharey=True)
     fig.set_size_inches(4.5, 2.5)
     for i, a in enumerate(ax):
       a.scatter(*temp.obsm["X_umap"].T, s=4, c=np.hstack((np.tile(np.array(cm(i)[:3]), zhat.shape[0]).reshape(-1, 3), zhat[:,i].reshape(-1, 1))))
       a.set_xlabel('UMAP 1')
     ax[0].set_ylabel('UMAP 2')
     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[82]:
   [[file:figure/nbmix.org/am-inf-ex.png]]
   :END:

   Threshold cluster assignments, and compute the cross entropy loss.

   #+BEGIN_SRC ipython
     zhat_thresh = pd.get_dummies(np.argmax(zhat, axis=1))
     torch.nn.functional.binary_cross_entropy(
       torch.tensor(zhat_thresh.values, dtype=torch.float),
       torch.tensor(pd.get_dummies(temp.obs['cell_type']).values, dtype=torch.float))
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[81]:
   : tensor(0.)
   :END:

   Now, fit the model starting from a random initialization.

   #+BEGIN_SRC ipython :async t
     import imp; imp.reload(mpebpm.gam_mix)
     torch.manual_seed(1)
     fit2 = mpebpm.gam_mix.EBPMGammaMix(
       p=temp.shape[1],
       k=2).fit(
         x=temp.X.A,
         s=temp.X.sum(axis=1),
         y=pd.get_dummies(temp.obs['cell_type']).values,
         lr=1e-2,
         batch_size=64,
         shuffle=True,
         num_pretrain=0,
         num_epochs=120,
         log_dir='runs/nbmix/test2')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[177]:
   :END:

   Compare the estimated \(\ln\mu\) from the clustering model to the estimates
   using the ground truth labels.

   #+BEGIN_SRC ipython :ipyfile figure/nbmix.org/am-inf-vs-mpebpm.png
     lim = [-10, 10]
     plt.clf()
     fig, ax = plt.subplots(1, 2, sharey=True)
     fig.set_size_inches(4.5, 2.5)
     for i, a in enumerate(ax):
       a.scatter(fit1.log_mean[i].detach().cpu().numpy(), fit2.log_mean[i].detach().cpu().numpy(), s=1, c='k', alpha=0.2)
       a.plot(lim, lim, c='r', lw=1, ls=':')
       a.set_title(f'Cluster {i}')
       a.set_ylim(lim)
       a.set_xlabel('Ground truth initialization')
     ax[0].set_ylabel('Random initialization')
     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[196]:
   [[file:figure/nbmix.org/am-inf-vs-mpebpm.png]]
   :END:

* Related work
  :PROPERTIES:
  :CUSTOM_ID: related-work
  :END:

  scVI ([[https://dx.doi.org/10.1038/s41592-018-0229-2][Lopez et al. 2018]],
  [[https://www.biorxiv.org/content/10.1101/532895v2][Xu et al. 2020]])
  implements a related deep unsupervised (more precisely, semi-supervised)
  clustering model ([[https://arxiv.org/abs/1406.5298][Kingma et al. 2014]],
  [[https://arxiv.org/abs/1611.02648][Dilokthanakul et al. 2016]]).

  \begin{align}
    x_{i\cdot} \mid s_i, \lambda_{ij} &\sim \Pois(s_i \lambda_{ij})\\
    \ln s_i &\sim \N(\cdot)\\
    \lambda_{ij} \mid z_{i\cdot} &\sim \Gam(\phi_j^{-1}, (\mu_{\lambda}^{-1}(z_{i\cdot}))_j \phi_j^{-1})\\
    z_{i\cdot} \mid y_i, u_i &\sim \N(\mu_i(u_i, y_i), \sigma^2(u_i, y_i))\\
    y_i &\sim \Mult(1, \vpi)\\
    u_i &\sim \N(0, \mi).
  \end{align}

  where

  - \(y_i\) denotes the cluster assignment for cell \(i\)
  - \(\mu_i(\cdot), \sigma^2(\cdot)\) are neural networks mapping the latent
    cluster variable \(y_i\) and Gaussian noise \(u_i\) to the latent variable
    \(z_i\)
  - \(\mu_{\lambda}(\cdot)\) is a neural network mapping latent variable \(z_i\) to
    latent gene expression \(\lambda_{ij}\)

  The intuition behind this model is that the prior \(p(z_i)\) (marginalizing
  over \(y_i\)) is a mixture of Gaussians, and therefore the model embeds
  examples in a space which makes clustering easy, and maps examples to those
  clusters simultaneously. To perform variational inference in this model,
  Lopez et al. introduce inference networks

  \begin{align}
    q(z_{i\cdot} \mid x_{i\cdot}) &= \N(\cdot)\\
    q(y_i \mid z_{i\cdot}) &= \Mult(1, \cdot)
  \end{align}

  The fundamental difference between scVI and our approach is that scVI
  clusters points in the low-dimensional latent space, and we cluster points in
  the space of latent gene expression (which has equal dimension to the
  observations).

  Rui Shu [[http://ruishu.io/2016/12/25/gmvae/][proposed an alternative
  generative model]], which has some practical benefits and can be adapted to
  this problem

  \begin{align}
    x_{i\cdot} \mid x_{i+}, \lambda_{ij} &\sim \Pois(x_{i+} \lambda_{ij})\\
    \lambda_{ij} \mid z_{i\cdot} &\sim \Gam(\phi_j^{-1}, (\mu_{\lambda}^{-1}(z_{i\cdot}))_j \phi_j^{-1})\\
    z_{i\cdot} \mid y_i &\sim \N(\mu_i(y_i), \sigma^2(y_i))\\
    y_i &\sim \Mult(1, \vpi)\\
    q(y_i, z_{i\cdot} \mid x_{i\cdot}) &= q(y_i \mid x_{i\cdot})\, q(z_i \mid y_i, x_{i\cdot})
  \end{align}

  There are additional deep unsupervised learning methods, which could
  potentially be adapted to this setting (reviewed in
  [[https://dx.doi.org/10.1109/ACCESS.2018.2855437][Min et al. 2018]]).
