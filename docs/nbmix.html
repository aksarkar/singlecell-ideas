<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2020-05-19 Tue 22:56 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Model-based clustering of scRNA-seq data</title>
<meta name="generator" content="Org mode">
<meta name="author" content="Abhishek Sarkar">
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link href="bootstrap.min.css" rel="stylesheet" />
<link rel="stylesheet" type="text/css" href="htmlize.css"/>
<link rel="stylesheet" type="text/css" href="main.css"/>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2017 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Model-based clustering of scRNA-seq data</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#introdcution">Introduction</a></li>
<li><a href="#setup">Setup</a></li>
<li><a href="#method">Method</a></li>
<li><a href="#results">Results</a>
<ul>
<li><a href="#em-nbmix">EM for Poisson-Gamma compound distribution</a></li>
<li><a href="#em-nbmix">EM for Poisson-Gamma mixture</a></li>
<li><a href="#org23cddea">EM for Poisson-Log compound distribution</a></li>
<li><a href="#example">Real data example</a></li>
<li><a href="#leiden">Leiden algorithm</a></li>
<li><a href="#mpebpm">MPEBPM</a></li>
<li><a href="#amortized">Amortized inference</a></li>
</ul>
</li>
<li><a href="#related-work">Related work</a></li>
</ul>
</div>
</div>

<div id="outline-container-org34e8d5e" class="outline-2">
<h2 id="introdcution"><a id="org34e8d5e"></a>Introduction</h2>
<div class="outline-text-2" id="text-introdcution">
<p>
Two major strategies for clustering scRNA-seq data are:
</p>

<ol class="org-ol">
<li>Building a \(k\)-nearest neighbor graph on the data, and applying a
community detection algorithm (e.g.,
<a href="https://doi.org/10.1088/1742-5468/2008/10/P10008">Blondel et al. 2008</a>,
<a href="https://arxiv.org/abs/1810.08473">Traag et al. 2018</a>)</li>
<li>Fitting a topic model to the data
(e.g., <a href="https://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1006599">Dey
et al. 2017</a>,
<a href="https://www.nature.com/articles/s41592-019-0367-1">Gonz√°les-Blas et
al. 2019</a>)</li>
</ol>

<p>
The main disadvantage of strategy (1) is that, as commonly applied to
transformed counts, it does not separate measurement error and biological
variation of interest. The main disadvantage of strategy (2) is that it does
not account for transcriptional noise
(<a href="https://doi.org/10.1016/j.cell.2008.09.050">Raj 2008</a>). Here, we develop
a simple model-based clustering algorithm which addresses both of these
issues.
</p>
</div>
</div>

<div id="outline-container-orgb4dd078" class="outline-2">
<h2 id="setup"><a id="orgb4dd078"></a>Setup</h2>
<div class="outline-text-2" id="text-setup">
<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> anndata
<span class="org-keyword">import</span> mpebpm.gam_mix
<span class="org-keyword">import</span> mpebpm.sgd
<span class="org-keyword">import</span> numpy <span class="org-keyword">as</span> np
<span class="org-keyword">import</span> pandas <span class="org-keyword">as</span> pd
<span class="org-keyword">import</span> scanpy <span class="org-keyword">as</span> sc
<span class="org-keyword">import</span> scipy.optimize <span class="org-keyword">as</span> so
<span class="org-keyword">import</span> scipy.special <span class="org-keyword">as</span> sp
<span class="org-keyword">import</span> scipy.stats <span class="org-keyword">as</span> st
<span class="org-keyword">import</span> scmodes
<span class="org-keyword">import</span> time
<span class="org-keyword">import</span> torch
<span class="org-keyword">import</span> torch.utils.data <span class="org-keyword">as</span> td
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">%matplotlib inline
%config <span class="org-variable-name">InlineBackend.figure_formats</span> = <span class="org-builtin">set</span>([<span class="org-string">'retina'</span>])
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> matplotlib
<span class="org-keyword">import</span> matplotlib.pyplot <span class="org-keyword">as</span> plt
<span class="org-variable-name">plt.rcParams</span>[<span class="org-string">'figure.facecolor'</span>] = <span class="org-string">'w'</span>
<span class="org-variable-name">plt.rcParams</span>[<span class="org-string">'font.family'</span>] = <span class="org-string">'Nimbus Sans'</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-org5ee032a" class="outline-2">
<h2 id="method"><a id="org5ee032a"></a>Method</h2>
<div class="outline-text-2" id="text-method">
<p>
We assume \(
  \DeclareMathOperator\Gam{Gamma}
  \DeclareMathOperator\Mult{Multinomial}
  \DeclareMathOperator\N{\mathcal{N}}
  \DeclareMathOperator\Pois{Poisson}
  \DeclareMathOperator\diag{diag}
  \newcommand\xiplus{x_{i+}}
  \newcommand\mi{\mathbf{I}}
  \newcommand\vu{\mathbf{u}}
  \newcommand\vx{\mathbf{x}}
  \newcommand\vz{\mathbf{z}}
  \newcommand\vlambda{\boldsymbol{\lambda}}
  \newcommand\vmu{\boldsymbol{\mu}}
  \newcommand\vphi{\boldsymbol{\phi}}
  \newcommand\vpi{\boldsymbol{\pi}}
  \)
</p>

\begin{align}
  x_{ij} \mid \xiplus, \lambda_{ij} &\sim \Pois(\xiplus \lambda_{ij})\\
  \lambda_{ij} \mid \vpi_i, \vmu_k, \vphi_k &\sim \sum_{k=1}^{K} \pi_{ik} \Gam(\phi_{kj}^{-1}, \phi_{kj}^{-1}\mu_{kj}^{-1}),
\end{align}

<p>
where
</p>

<ul class="org-ul">
<li>\(x_{ij}\) denotes the number of molecules of gene \(j\) observed in cell \(i\)</li>
<li>\(\xiplus \triangleq \sum_j x_{ij}\) denotes the total number of molecules
observed in cell \(i\)</li>
<li>\(\vpi_i\) denotes cluster assignment probabilities for cell \(i\)</li>
<li>\(\vmu_k\) denotes the cluster &ldquo;centroid&rdquo; for cluster \(k\), and
\(\vphi_k\) describes stochastic perturbations within each cluster</li>
</ul>

<p>
The intuition behind this model is that each cluster \(k\) is defined by a
collection of independent Gamma distributions (parameterized by shape and
rate), one per gene \(j\), which describe the distribution of true gene
expression for each gene in each cluster
(<a href="https://dx.doi.org/10.1101/2020.04.07.030007">Sarkar and Stephens
2020</a>). In this parameterization, each Gamma distribution has mean
\(\mu_{kj}\) and variance \(\mu_{kj}^2\phi_{kj}\). Under this model, the
marginal likelihood is a mixture of negative binomials
</p>

\begin{equation}
  p(x_{ij} \mid \xiplus, \vpi_i, \vmu_k, \vphi_k) = \sum_{k=1}^{K} \pi_{ik} \frac{\Gamma(x_{ij} + 1 / \phi_{kj})}{\Gamma(1 / \phi_{kj})\Gamma(x_{ij} + 1)}\left(\frac{\xiplus\mu_{kj}\phi_{kj}}{1 + \xiplus\mu_{kj}\phi_{kj}}\right)^{x_{ij}} \left(\frac{1}{1 + \xiplus\mu_{kj}\phi_{kj}}\right)^{1/\phi_{kj}}.
\end{equation}

<p>
We can estimate \(\vpi, \vmu, \vphi\) by maximizing the likelihood using an
EM algorithm. Letting \(z_{ik} \in \{0, 1\}\) indicate whether cell \(i\) is
assigned to cluster \(k\), the exact posterior
</p>

\begin{align}
  q(z_{i1}, \ldots, z_{iK}) &\triangleq p(z_{ik} \mid \cdot) = \Mult(1, \alpha_{i1}, \ldots, \alpha_{iK})\\
  \alpha_{ik} &\propto \sum_j \frac{\Gamma(x_{ij} + 1 / \phi_{kj})}{\Gamma(1 / \phi_{kj})\Gamma(x_{ij} + 1)}\left(\frac{\xiplus\mu_{kj}\phi_{kj}}{1 + \xiplus\mu_{kj}\phi_{kj}}\right)^{x_{ij}} \left(\frac{1}{1 + \xiplus\mu_{kj}\phi_{kj}}\right)^{1/\phi_{kj}}.
\end{align}

<p>
The expected log joint with respect to \(q\)
</p>

\begin{multline}
  E_q[\ln p(x_{ij}, z_{ik} \mid \xiplus, \vpi_i, \vmu_k, \vphi_k)] = \sum_k E_q[z_{ik}] \left[x_{ij} \ln\left(\frac{\xiplus\mu_{kj}\phi_{kj}}{1 + \xiplus\mu_{kj}\phi_{kj}}\right)\right.\\
    \left. - \phi_{kj}^{-1} \ln(1 + \xiplus\mu_{kj}\phi_{kj}) + \ln\Gamma(x_{ij} + 1 / \phi_{kj}) - \ln\Gamma(1 / \phi_{kj}) - \ln\Gamma(x_{ij} + 1)\right].
\end{multline}

<p>
In the E step, the necessary expectations are analytic. In the M step, we can
improve the likelihood by <a href="mepbpm.html">(batch) gradient descent</a>, or
by mixed analytic and Newton-Raphson updates (Karlis 2005). Alternatively, we
can write the NB distribution as a Poisson sum of
<a href="https://en.wikipedia.org/wiki/Logarithmic_distribution">log-series
distributed random variables</a> (Quenouille 1949), which admits an EM
algorithm with fully analytic M step
(<a href="https://doi.org/10.1111/1467-842X.00075">Adamidis 1999</a>, Huang et
al. 2019). Another alternative, which is amenable to stochastic gradient
descent and online learning, is to use the fact that EM can be viewed as
maximizing the evidence lower bound
(<a href="https://doi.org/10.1007/978-94-011-5014-9_12">Neal and Hinton 1998</a>)
</p>

\begin{equation}
  \max_{q, \theta} \ln p(x \mid \theta) - \mathcal{KL}(q(z) \Vert p(z \mid x, \theta)) = \max_{q, \theta} E_q[\ln p(x \mid z, \theta)] - \mathcal{KL}(q(z) \Vert p(z \mid \theta)).
\end{equation}

<p>
Exact EM corresponds to (fully) alternately optimizing \(q\) and \(\theta\);
however, we can instead amortize inference
(<a href="https://escholarship.org/content/qt34j1h7k5/qt34j1h7k5.pdf">Gershman and
Goodman 2014</a>, <a href="https://arxiv.org/abs/1312.6114">Kingma and Welling 2014</a>,
<a href="http://proceedings.mlr.press/v32/rezende14.html">Rezende et al. 2014</a>),
learning a neural network \(f_z\) mapping \(\vx_i \rightarrow \vz_i\)
</p>

\begin{align}
  p(z_{i1}, \ldots, z_{iK}) &= \Mult(1, \vpi)\\
  q(z_{i1}, \ldots, z_{iK} \mid \vx_i) &= \Mult(1, f_z(\vx_i)).
\end{align}
</div>
</div>

<div id="outline-container-orgdca968c" class="outline-2">
<h2 id="results"><a id="orgdca968c"></a>Results</h2>
<div class="outline-text-2" id="text-results">
</div>
<div id="outline-container-orga0d3d31" class="outline-3">
<h3 id="em-nbmix"><a id="orga0d3d31"></a>EM for Poisson-Gamma compound distribution</h3>
<div class="outline-text-3" id="text-em-nbmix">
<p>
<a href="https://doi.org/10.1017/S0515036100014033">Karlis 2005</a> gives an EM
algorithm for fitting a Gamma mixture of Poissons. The key idea is that, due
to Poisson-Gamma conjugacy, the exact posterior is available, as are the
necessary posterior moments. The main disadvantage of this approach is that
it requires (one-dimensional) numerical optimization in the M step.
</p>

\begin{align}
  x_i \mid \xiplus, \lambda_i &\sim \Pois(\xiplus \lambda_i)\\
  \lambda_i \mid \alpha, \beta &\sim \Gam(\alpha, \beta)\\
  \lambda_i \mid x_i, \xiplus, \alpha, \beta &\sim q \triangleq \Gam(x_i + \alpha, \xiplus + \beta)\\
  E_q[\lambda_i] &= \frac{x_i + \alpha}{\xiplus + \beta}\\
  E_q[\ln \lambda_i] &= \psi(x + \alpha) - \log(\xiplus + \beta)\\
  E_q[\ln p(x_i, \lambda_i \mid \xiplus, \alpha, \beta)] &= \ell_i \triangleq x_i E_q[\ln \lambda_i] - E_q[\lambda_i] - \ln\Gamma(x_i + 1) + \alpha \ln\beta - \ln\Gamma(\alpha) + (\alpha - 1) E_q[\lambda_i] - \beta E_q[\lambda_i]\\
  \ell &= \sum_i \ell_i\\
  \frac{\partial\ell}{\partial\beta} &= \sum_i \frac{\alpha}{\beta} - E_q[\lambda_i] = 0\\
  \beta &= \frac{\bar{\lambda}}{\alpha}\\
  \frac{\partial\ell}{\partial\alpha} &= \sum_i \ln \beta - \psi(\alpha) + E_q[\ln x_i]\\
  \frac{\partial^2\ell}{\partial\alpha^2} &= -n \psi^{(1)}(\alpha)
\end{align}

<p>
where \(\psi\) denotes the digamma function and \(\psi^{(1)}\) denotes the
trigamma function. The algorithm uses a partial M step (single
Newton-Raphson update) for \(\alpha\).
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">ebpm_gamma_em</span>(x, s, max_iters=100, tol=1e-3):
  <span class="org-doc">"""Return fitted parameters assuming g is a Gamma distribution</span>

<span class="org-doc">  Returns log mu and -log phi</span>

<span class="org-doc">  x - array-like [n,]</span>
<span class="org-doc">  s - array-like [n,]</span>

<span class="org-doc">  """</span>
  <span class="org-variable-name">n</span> = x.shape[0]
  <span class="org-keyword">assert</span> x.shape == (n,)
  <span class="org-keyword">assert</span> x.shape == s.shape
  <span class="org-comment-delimiter"># </span><span class="org-comment">a = 1 / phi; b = 1 / (mu phi)</span>
  <span class="org-variable-name">a</span> = 1
  <span class="org-comment-delimiter"># </span><span class="org-comment">Initialize at the Poisson MLE</span>
  <span class="org-variable-name">b</span> = s.<span class="org-builtin">sum</span>() / x.<span class="org-builtin">sum</span>()
  <span class="org-variable-name">obj</span> = []
  obj.append(st.nbinom(n=a, p=1 / (1 + s / b)).logpmf(x).<span class="org-builtin">sum</span>())
  <span class="org-keyword">for</span> i <span class="org-keyword">in</span> <span class="org-builtin">range</span>(max_iters):
    <span class="org-comment-delimiter"># </span><span class="org-comment">E[lam | x]</span>
    <span class="org-variable-name">pm</span> = (x + a) / (s + b)
    <span class="org-keyword">assert</span> (pm &gt; 0).<span class="org-builtin">all</span>()
    <span class="org-comment-delimiter"># </span><span class="org-comment">E[ln lam | x]</span>
    <span class="org-variable-name">plm</span> = sp.digamma(x + a) - np.log(s + b)
    <span class="org-variable-name">b</span> = a / pm.mean()
    <span class="org-comment-delimiter"># </span><span class="org-comment">Important: this appears to be incorrect in Karlis 2005</span>
    <span class="org-variable-name">a</span> += (np.log(b) - sp.digamma(a) + plm.mean()) / sp.polygamma(1, a)
    <span class="org-keyword">assert</span> a &gt; 0
    <span class="org-keyword">assert</span> b &gt; 0
    obj.append(st.nbinom(n=a, p=1 / (1 + s / b)).logpmf(x).<span class="org-builtin">sum</span>())
    <span class="org-keyword">if</span> obj[-1] &lt; obj[-2]:
      <span class="org-keyword">raise</span> <span class="org-type">RuntimeError</span>(<span class="org-string">'llik decreased'</span>)
    <span class="org-keyword">elif</span> obj[-1] - obj[-2] &lt; tol:
      <span class="org-variable-name">log_inv_disp</span> = np.log(a)
      <span class="org-variable-name">log_mean</span> = -np.log(b) + np.log(a)
      <span class="org-keyword">return</span> log_mean, log_inv_disp, obj
  <span class="org-keyword">else</span>:
    <span class="org-keyword">raise</span> <span class="org-type">RuntimeError</span>(<span class="org-string">'failed to converge in max_iters'</span>)
</pre>
</div>

<p>
Try EM for a simple example.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">rng</span> = np.random.default_rng(1)
<span class="org-variable-name">n</span> = 100
<span class="org-variable-name">log_mean</span> = -10
<span class="org-variable-name">log_inv_disp</span> = 0
<span class="org-variable-name">s</span> = np.repeat(1e5, n)
<span class="org-variable-name">lam</span> = rng.gamma(shape=np.exp(log_inv_disp), scale=np.exp(log_mean - log_inv_disp), size=n)
<span class="org-variable-name">x</span> = rng.poisson(s * lam)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">log_mu</span>, <span class="org-variable-name">neg_log_phi</span>, <span class="org-variable-name">trace</span> = ebpm_gamma_em(x, s)
</pre>
</div>

<p>
Plot the simulated data, the ground truth marginal distribution on counts,
and the NB MLE.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Dark2'</span>)
plt.clf()
plt.gcf().set_size_inches(4.5, 2.5)
<span class="org-variable-name">grid</span> = np.arange(x.<span class="org-builtin">max</span>() + 1)
plt.hist(x, bins=grid, color=<span class="org-string">'0.7'</span>, density=<span class="org-constant">True</span>)
plt.plot(grid + .5, st.nbinom(n=np.exp(-log_inv_disp), p=1 / (1 + s[0] * np.exp(log_mean - log_inv_disp))).pmf(grid), lw=1, color=cm(0), label=<span class="org-string">'Ground truth'</span>)
plt.plot(grid + .5, st.nbinom(n=np.exp(neg_log_phi), p=1 / (1 + s[0] * np.exp(log_mu - neg_log_phi))).pmf(grid), lw=1, color=cm(1), label=<span class="org-string">'NB MLE'</span>)
plt.legend(frameon=<span class="org-constant">False</span>)
plt.xlabel(<span class="org-string">'Number of molecules'</span>)
plt.ylabel(<span class="org-string">'Density'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/nbmix.org/nb-em.png" alt="nb-em.png">
</p>
</div>

<p>
Try a more extensive evaluation of the method.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">n</span> = 100
<span class="org-variable-name">s</span> = np.repeat(1e5, n)
<span class="org-variable-name">result</span> = <span class="org-builtin">dict</span>()
<span class="org-keyword">for</span> trial <span class="org-keyword">in</span> <span class="org-builtin">range</span>(5):
  <span class="org-keyword">for</span> log_mean <span class="org-keyword">in</span> np.linspace(-12, -6, 7):
    <span class="org-keyword">for</span> log_inv_disp <span class="org-keyword">in</span> np.linspace(0, 4, 5):
      <span class="org-variable-name">rng</span> = np.random.default_rng(trial)
      <span class="org-variable-name">lam</span> = rng.gamma(shape=np.exp(log_inv_disp), scale=np.exp(log_mean - log_inv_disp), size=n)
      <span class="org-variable-name">x</span> = rng.poisson(s * lam)
      <span class="org-variable-name">start</span> = time.time()
      <span class="org-variable-name">log_mean_hat</span>, <span class="org-variable-name">log_inv_disp_hat</span>, <span class="org-variable-name">trace</span> = ebpm_gamma_em(x, s, max_iters=1000)
      <span class="org-variable-name">elapsed</span> = time.time() - start
      <span class="org-variable-name">result</span>[(log_mean, log_inv_disp, trial)] = pd.Series([log_mean_hat, log_inv_disp_hat, <span class="org-builtin">len</span>(trace), elapsed])
<span class="org-variable-name">result</span> = (pd.DataFrame.from_dict(result, orient=<span class="org-string">'index'</span>)
          .reset_index()
          .rename({f<span class="org-string">'level_{i}'</span>: k <span class="org-keyword">for</span> i, k <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>([<span class="org-string">'log_mean'</span>, <span class="org-string">'log_inv_disp'</span>, <span class="org-string">'trial'</span>])}, axis=1)
          .rename({i: k <span class="org-keyword">for</span> i, k <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>([<span class="org-string">'log_mean_hat'</span>, <span class="org-string">'log_inv_disp_hat'</span>, <span class="org-string">'num_iters'</span>, <span class="org-string">'elapsed'</span>])}, axis=1))
</pre>
</div>

<p>
Plot the estimates against the ground truth values.
</p>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 2)
fig.set_size_inches(4.5, 2.5)
<span class="org-keyword">for</span> a <span class="org-keyword">in</span> ax:
  a.set_aspect(<span class="org-string">'equal'</span>, adjustable=<span class="org-string">'datalim'</span>)
ax[0].scatter(result[<span class="org-string">'log_mean'</span>], result[<span class="org-string">'log_mean_hat'</span>], c=<span class="org-string">'k'</span>, s=1)
ax[0].set_xlabel(<span class="org-string">'Ground truth $\ln(\mu)$'</span>)
ax[0].set_ylabel(<span class="org-string">'Estimated $\ln(\mu)$'</span>)
ax[1].scatter(-result[<span class="org-string">'log_inv_disp'</span>], -result[<span class="org-string">'log_inv_disp_hat'</span>], c=<span class="org-string">'k'</span>, s=1)
ax[1].set_xlabel(<span class="org-string">'Ground truth $\ln(\phi)$'</span>)
ax[1].set_ylabel(<span class="org-string">'Estimated $\ln(\phi)$'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/nbmix.org/nb-em-sim.png" alt="nb-em-sim.png">
</p>
</div>

<p>
Estimate the average time (seconds) taken to fit each trial.
</p>

<div class="org-src-container">
<pre class="src src-ipython">result[<span class="org-string">'elapsed'</span>].mean(), result[<span class="org-string">'elapsed'</span>].std()
</pre>
</div>

<pre class="example">
(0.18343861034938266, 0.18248113994496618)

</pre>
</div>
</div>

<div id="outline-container-orgecb1e60" class="outline-3">
<h3 id="em-nbmix"><a id="orgecb1e60"></a>EM for Poisson-Gamma mixture</h3>
<div class="outline-text-3" id="text-em-nbmix">
<p>
We can use numerical methods to perform the M step.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">_nb_mix_llik</span>(theta, x, s):
  <span class="org-doc">"""Return log likelihood matrix</span>

<span class="org-doc">  theta - array-like [2, k]</span>
<span class="org-doc">  x - array-like [n, 1]</span>
<span class="org-doc">  s - array-like [n, 1]</span>

<span class="org-doc">  """</span>
  <span class="org-comment-delimiter"># </span><span class="org-comment">Important: so.minimize flattens theta</span>
  <span class="org-variable-name">theta</span> = theta.reshape(2, -1)
  <span class="org-variable-name">mean</span> = np.exp(theta[0]).reshape(1, -1)
  <span class="org-comment-delimiter"># </span><span class="org-comment">Important: this can blow up</span>
  <span class="org-variable-name">inv_disp</span> = np.exp(np.clip(theta[1], -15, 15)).reshape(1, -1)
  <span class="org-comment-delimiter"># </span><span class="org-comment">[n, k]</span>
  <span class="org-variable-name">L</span> = st.nbinom(n=inv_disp, p=1 / (1 + s * mean / inv_disp)).logpmf(x.reshape(-1, 1))
  <span class="org-keyword">assert</span> L.shape == (x.shape[0], theta.shape[1])
  <span class="org-keyword">assert</span> np.isfinite(L).<span class="org-builtin">all</span>()
  <span class="org-keyword">return</span> L

<span class="org-keyword">def</span> <span class="org-function-name">_nb_mix_obj</span>(theta, x, z, s):
  <span class="org-doc">"""Return negative log likelihood</span>

<span class="org-doc">  theta - array-like [2, k]</span>
<span class="org-doc">  x - array-like [n, 1]</span>
<span class="org-doc">  z - array-like [n, k]</span>
<span class="org-doc">  s - array-like [n, 1]</span>

<span class="org-doc">  """</span>
  <span class="org-variable-name">L</span> = _nb_mix_llik(theta, x, s)
  <span class="org-variable-name">m</span> = L.<span class="org-builtin">max</span>(axis=1, keepdims=<span class="org-constant">True</span>)
  <span class="org-keyword">return</span> -(np.log(z) + L).mean()

<span class="org-keyword">def</span> <span class="org-function-name">ebpm_gamma_mix</span>(x, s, k, max_iters=100, tol=1e-3, verbose=<span class="org-constant">False</span>, seed=1):
  <span class="org-keyword">assert</span> x.ndim == 2
  <span class="org-variable-name">n</span>, <span class="org-variable-name">p</span> = x.shape
  <span class="org-keyword">assert</span> s.shape == (n, 1)
  <span class="org-keyword">assert</span> k &gt; 1
  <span class="org-variable-name">rng</span> = np.random.default_rng(seed)
  <span class="org-variable-name">log_mean</span> = rng.normal(size=(p, k))
  <span class="org-variable-name">log_inv_disp</span> = rng.normal(size=(p, k))
  <span class="org-variable-name">z</span> = np.ones((n, k)) / k
  <span class="org-variable-name">loss</span> = np.array([_nb_mix_obj(np.vstack([log_mean[j], log_inv_disp[j]]), x[:,j], z, s) <span class="org-keyword">for</span> j <span class="org-keyword">in</span> <span class="org-builtin">range</span>(p)]).<span class="org-builtin">sum</span>()
  <span class="org-keyword">if</span> verbose:
    <span class="org-keyword">print</span>(f<span class="org-string">'epoch 0: {loss:.4g}'</span>)
  <span class="org-keyword">for</span> t <span class="org-keyword">in</span> <span class="org-builtin">range</span>(max_iters):
    <span class="org-keyword">for</span> j <span class="org-keyword">in</span> <span class="org-builtin">range</span>(p):
      <span class="org-variable-name">opt</span> = so.minimize(_nb_mix_obj, x0=np.vstack([log_mean[j], log_inv_disp[j]]), args=(x[:,j], z, s), method=<span class="org-string">'nelder-mead'</span>, options={<span class="org-string">'maxiter'</span>:1000})
      <span class="org-keyword">if</span> <span class="org-keyword">not</span> opt.success:
        <span class="org-keyword">raise</span> <span class="org-type">RuntimeError</span>(f<span class="org-string">'M step failed to converge: {opt.message}'</span>)
      <span class="org-variable-name">log_mean</span>[j] = opt.x[0]
      <span class="org-variable-name">log_inv_disp</span>[j] = opt.x[1]
    <span class="org-variable-name">L</span> = np.array([_nb_mix_llik(np.vstack(log_mean[j], log_inv_disp[j]), x[:,j], s) <span class="org-keyword">for</span> j <span class="org-keyword">in</span> <span class="org-builtin">range</span>(p)]).<span class="org-builtin">sum</span>(axis=0)
    <span class="org-variable-name">z</span> = sp.softmax(L, axis=1)
    <span class="org-variable-name">update</span> = np.array([_nb_mix_obj(np.vstack(log_mean[j], log_inv_disp[j]), x[:,j], z, s) <span class="org-keyword">for</span> j <span class="org-keyword">in</span> <span class="org-builtin">range</span>(p)]).<span class="org-builtin">sum</span>()
    <span class="org-keyword">if</span> update &gt; loss:
      <span class="org-keyword">raise</span> <span class="org-type">RuntimeError</span>(<span class="org-string">'objective increased'</span>)
    <span class="org-keyword">elif</span> loss - update &lt; tol:
      <span class="org-keyword">return</span> z, log_mean, log_inv_disp
    <span class="org-keyword">else</span>:
      <span class="org-keyword">if</span> verbose:
        <span class="org-keyword">print</span>(f<span class="org-string">'epoch {t + 1}: {loss:.4g}'</span>)
      <span class="org-variable-name">loss</span> = update
  <span class="org-keyword">raise</span> <span class="org-type">RuntimeError</span>(<span class="org-string">'failed to converge in max_iters'</span>)
</pre>
</div>
</div>
</div>

<div id="outline-container-org23cddea" class="outline-3">
<h3 id="org23cddea">EM for Poisson-Log compound distribution</h3>
<div class="outline-text-3" id="text-org23cddea">
<p>
The NB distribution can be derived as a
<a href="https://en.wikipedia.org/wiki/Negative_binomial_distribution#Representation_as_compound_Poisson_distribution">Poisson-distributed
sum</a> of
<a href="https://en.wikipedia.org/wiki/Logarithmic_distribution">Log-distributed
random variables</a> (Quenouille 1949)
</p>

\begin{align}
  x_i \mid y_1, \ldots, y_{m_i}, m_i &= \sum_{t=1}^{m_i} y_t\\
  m_i &\sim \Pois(-n \ln(1 - p))\\
  p(y_t \mid p) &= -\frac{p^{y_t}}{y_t \ln(1 - p)}, \quad t = 1, \ldots\\
  p(x_i \mid n, p) &\propto p^n (1 - p)^{x_i}
\end{align}

<p>
To connect this parameterization to our parameterization, we have \(n =
   1/\phi\) and \(p = 1 / (1 + \xiplus\mu\phi)\). Adamidis 1999 uses this fact to
derive a new auxiliary variable representation of the NB distribution
</p>

\begin{equation}
  p(y_t, z_t \mid p) = \frac{(1 - p)^{z_t} p^{y_t - 1}}{y_t}, z_t \in (0, 1)
\end{equation}

<p>
Letting \(q \triangleq p(m_i, y_1, \ldots, z_1, \ldots \mid x_i, n, p)\),
</p>

\begin{align}
  E_q[\ln p(x_i \mid m_i, y_1, \ldots, z_1, \ldots, n, p)] &= E_q[m_i] \ln(-n \ln(1 - p)) - n \ln(1 - p) + E_q[\ln\Gamma(m_i + 1)]\\
  &\quad + E_q[\textstyle\sum_{t=1}^{m_i} z_t] \ln(1 - p) + (\textstyle\sum_{t=1}^{m_i} E_q[y_t] - E_q[m_i]) \ln p + \mathrm{const}\\
  &= E_q[m_i] \ln(-n \ln(1 - p)) - n \ln(1 - p) + E_q[\ln\Gamma(m_i + 1)]\\
  E_q[\textstyle\sum_t z_t] &= E_q[m_i] E_q[z_t]\\
  E_q[m_i] &= 
\end{align}

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">pois_log_series_em</span>(x, s, max_iters=100, tol=1e-3):
  <span class="org-variable-name">n</span> = x.shape[0]
  <span class="org-keyword">assert</span> x.shape == (n,)
  <span class="org-keyword">assert</span> x.shape == s.shape
  <span class="org-variable-name">z</span> = .5
  <span class="org-variable-name">m</span> = .5 * x
  <span class="org-variable-name">p</span> = (x - m).<span class="org-builtin">sum</span>() / (x + m * z - m)
  <span class="org-variable-name">n</span> = m.mean()
  <span class="org-variable-name">obj</span> = [st.nbinom(n=n, p=p).logpmf(x).<span class="org-builtin">sum</span>()]
  <span class="org-keyword">for</span> i <span class="org-keyword">in</span> <span class="org-builtin">range</span>(max_iters):
    <span class="org-variable-name">z</span> = -((1 - p) / p - 1 / np.log(1 - p))
    <span class="org-keyword">assert</span> 0 &lt; z &lt; 1
    <span class="org-variable-name">alpha</span> = n / np.log(1 - p)
    <span class="org-keyword">assert</span> alpha &gt; 0
    <span class="org-variable-name">m</span> = alpha * (sp.digamma(alpha + x) - sp.digamma(alpha))
    <span class="org-keyword">assert</span> m &gt; 0
    <span class="org-variable-name">p</span> = (x - m).<span class="org-builtin">sum</span>() / (x + m * z - m)
    <span class="org-keyword">assert</span> p &gt; 0
    <span class="org-variable-name">n</span> = m.mean()
    <span class="org-keyword">assert</span> n &gt; 0
    obj.append(st.nbinom(n=n, p=p).logpmf(x).<span class="org-builtin">sum</span>())
    <span class="org-keyword">if</span> obj[-1] &lt; obj[-2]:
      <span class="org-keyword">raise</span> <span class="org-type">RuntimeError</span>(<span class="org-string">'llik decreased'</span>)
    <span class="org-keyword">elif</span> obj[-1] - obj[-2] &lt; tol:
      <span class="org-variable-name">log_inv_disp</span> = -np.log(n)
      <span class="org-variable-name">log_mean</span> = np.log(1 / p - 1) - np.log(s) - np.log(n)
      <span class="org-keyword">return</span> log_mean, log_inv_disp, obj
  <span class="org-keyword">else</span>:
    <span class="org-keyword">raise</span> <span class="org-type">RuntimeError</span>(<span class="org-string">'failed to converge in max_iters'</span>)
</pre>
</div>

<p>
Try EM for a simple example.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">rng</span> = np.random.default_rng(1)
<span class="org-variable-name">n</span> = 100
<span class="org-variable-name">log_mean</span> = -10
<span class="org-variable-name">log_inv_disp</span> = 0
<span class="org-variable-name">s</span> = np.repeat(1e5, n)
<span class="org-variable-name">lam</span> = rng.gamma(shape=np.exp(log_inv_disp), scale=np.exp(log_mean - log_inv_disp), size=n)
<span class="org-variable-name">x</span> = rng.poisson(s * lam)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">log_mu</span>, <span class="org-variable-name">neg_log_phi</span>, <span class="org-variable-name">trace</span> = ebpm_gamma_em(x, s)
</pre>
</div>

<p>
Plot the simulated data, the ground truth marginal distribution on counts,
and the NB MLE.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Dark2'</span>)
plt.clf()
plt.gcf().set_size_inches(4.5, 2.5)
<span class="org-variable-name">grid</span> = np.arange(x.<span class="org-builtin">max</span>() + 1)
plt.hist(x, bins=grid, color=<span class="org-string">'0.7'</span>, density=<span class="org-constant">True</span>)
plt.plot(grid + .5, st.nbinom(n=np.exp(-log_inv_disp), p=1 / (1 + s[0] * np.exp(log_mean - log_inv_disp))).pmf(grid), lw=1, color=cm(0), label=<span class="org-string">'Ground truth'</span>)
plt.plot(grid + .5, st.nbinom(n=np.exp(neg_log_phi), p=1 / (1 + s[0] * np.exp(log_mu - neg_log_phi))).pmf(grid), lw=1, color=cm(1), label=<span class="org-string">'NB MLE'</span>)
plt.legend(frameon=<span class="org-constant">False</span>)
plt.xlabel(<span class="org-string">'Number of molecules'</span>)
plt.ylabel(<span class="org-string">'Density'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/nbmix.org/nb-em.png" alt="nb-em.png">
</p>
</div>
</div>
</div>

<div id="outline-container-org2ab3024" class="outline-3">
<h3 id="example"><a id="org2ab3024"></a>Real data example</h3>
<div class="outline-text-3" id="text-example">
<p>
Read sorted immune cell scRNA-seq data
(<a href="https://dx.doi.org/10.1038/ncomms14049">Zheng et al. 2017</a>).
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">dat</span> = anndata.read_h5ad(<span class="org-string">'/scratch/midway2/aksarkar/ideas/zheng-10-way.h5ad'</span>)
</pre>
</div>

<p>
Get 256 B cells and 256 cytotoxic T cells.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">b_cells</span> = dat[dat.obs[<span class="org-string">'cell_type'</span>] == <span class="org-string">'b_cells'</span>]
sc.pp.subsample(b_cells, n_obs=256, random_state=0)
<span class="org-variable-name">t_cells</span> = dat[dat.obs[<span class="org-string">'cell_type'</span>] == <span class="org-string">'cytotoxic_t'</span>]
sc.pp.subsample(t_cells, n_obs=256)
<span class="org-variable-name">temp</span> = b_cells.concatenate(t_cells)
sc.pp.filter_genes(temp, min_counts=1)
</pre>
</div>

<p>
Plot a UMAP embedding of the data, coloring points by the ground truth
labels.
</p>

<div class="org-src-container">
<pre class="src src-ipython">sc.pp.pca(temp)
sc.pp.neighbors(temp)
sc.tl.umap(temp)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Dark2'</span>)
plt.clf()
plt.gcf().set_size_inches(3, 3)
<span class="org-keyword">for</span> i, c <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(temp.obs[<span class="org-string">'cell_type'</span>].unique()):
  plt.plot(*temp[temp.obs[<span class="org-string">'cell_type'</span>] == c].obsm[<span class="org-string">"X_umap"</span>].T, c=cm(i), marker=<span class="org-string">'.'</span>, ms=2, lw=0, label=f<span class="org-string">'{c}'</span>)
plt.legend(frameon=<span class="org-constant">False</span>, markerscale=4, handletextpad=0)
plt.xlabel(<span class="org-string">'UMAP 1'</span>)
plt.ylabel(<span class="org-string">'UMAP 2'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/nbmix.org/sim-ex.png" alt="sim-ex.png">
</p>
</div>
</div>
</div>

<div id="outline-container-orgca14b7d" class="outline-3">
<h3 id="leiden"><a id="orgca14b7d"></a>Leiden algorithm</h3>
<div class="outline-text-3" id="text-leiden">
<p>
Apply the Leiden algorithm (<a href="https://arxiv.org/abs/1810.08473">Traag et
al. 2018</a>) to the data (&lt;1 s).
</p>

<div class="org-src-container">
<pre class="src src-ipython">sc.tl.leiden(temp, random_state=0)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Dark2'</span>)
plt.clf()
plt.gcf().set_size_inches(3, 3)
<span class="org-keyword">for</span> i, c <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(temp.obs[<span class="org-string">'leiden'</span>].unique()):
  plt.plot(*temp[temp.obs[<span class="org-string">'leiden'</span>] == c].obsm[<span class="org-string">"X_umap"</span>].T, c=cm(i), marker=<span class="org-string">'.'</span>, ms=2, lw=0, label=f<span class="org-string">'Cluster {i}'</span>)
plt.legend(frameon=<span class="org-constant">False</span>, markerscale=4, handletextpad=0)
plt.xlabel(<span class="org-string">'UMAP 1'</span>)
plt.ylabel(<span class="org-string">'UMAP 2'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/nbmix.org/leiden-ex.png" alt="leiden-ex.png">
</p>
</div>
</div>
</div>

<div id="outline-container-org97f5a8f" class="outline-3">
<h3 id="mpebpm"><a id="org97f5a8f"></a>MPEBPM</h3>
<div class="outline-text-3" id="text-mpebpm">
<p>
First, start from the ground truth \(z\) (labels), and estimate the Gamma
expression models.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">fit0</span> = mpebpm.sgd.ebpm_gamma(
  temp.X,
  onehot=pd.get_dummies(temp.obs[<span class="org-string">'cell_type'</span>]).values,
  batch_size=32,
  num_epochs=320,
  shuffle=<span class="org-constant">True</span>,
  log_dir=<span class="org-string">'runs/nbmix/pretrain/'</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">y</span> = pd.get_dummies(temp.obs[<span class="org-string">'cell_type'</span>]).values
<span class="org-variable-name">s</span> = temp.X.<span class="org-builtin">sum</span>(axis=1)
<span class="org-variable-name">nb_llik</span> = y.T @ st.nbinom(n=np.exp(y @ fit0[1]), p=1 / (1 + s.A * (y @ np.exp(fit0[0] - fit0[1])))).logpmf(temp.X.A)
</pre>
</div>

<p>
For comparison, estimate a point mass expression model for each gene, for
each cluster.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">y</span> = pd.get_dummies(temp.obs[<span class="org-string">'cell_type'</span>]).values
<span class="org-variable-name">s</span> = temp.X.<span class="org-builtin">sum</span>(axis=1)
<span class="org-variable-name">fit_pois</span> = (y.T @ temp.X) / (y.T @ s)
<span class="org-variable-name">pois_llik</span> = y.T @ st.poisson(mu=s.A * (y @ fit_pois).A).logpmf(temp.X.A)
</pre>
</div>

<p>
For each gene, for each cluster, plot the log likelihood under the point
mass and Gamma expression models.
</p>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 2, sharey=<span class="org-constant">True</span>)
fig.set_size_inches(4.5, 2.5)
<span class="org-variable-name">lim</span> = [-1500, 0]
<span class="org-keyword">for</span> i, (a, t) <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(<span class="org-builtin">zip</span>(ax, [<span class="org-string">'B cell'</span>, <span class="org-string">'Cytotoxic T'</span>])):
  a.scatter(pois_llik[i], nb_llik[i], c=<span class="org-string">'k'</span>, s=1, alpha=0.2)
  a.plot(lim, lim, c=<span class="org-string">'r'</span>, lw=1, ls=<span class="org-string">':'</span>)
  a.set_xlim(lim)
  a.set_ylim(lim)
  a.set_title(t)
  a.set_xlabel(<span class="org-string">'Poisson log lik'</span>)
ax[0].set_ylabel(<span class="org-string">'NB log lik'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/nbmix.org/ex-pois-nb-llik.png" alt="ex-pois-nb-llik.png">
</p>
</div>

<p>
Look at the differences in the estimated mean parameter for each gene, to
see how many genes are informative about the labels.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">query</span> = np.sort(np.diff(fit0[0], axis=0).ravel())
plt.clf()
plt.gcf().set_size_inches(4, 2)
plt.plot(query, lw=1, c=<span class="org-string">'k'</span>)
plt.axhline(y=0, lw=1, ls=<span class="org-string">':'</span>, c=<span class="org-string">'k'</span>)
plt.xlabel(<span class="org-string">'Gene'</span>)
plt.ylabel(r<span class="org-string">'Diff $\ln(\mu_j)$'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/nbmix.org/mepbpm-log-mu.png" alt="mepbpm-log-mu.png">
</p>
</div>

<p>
Estimate the cluster weights.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">L</span> = mpebpm.gam_mix._nb_mix_llik(
  x=torch.tensor(temp.X.A, dtype=torch.<span class="org-builtin">float</span>), 
  s=torch.tensor(temp.X.<span class="org-builtin">sum</span>(axis=1), dtype=torch.<span class="org-builtin">float</span>),
  log_mean=torch.tensor(fit0[0], dtype=torch.<span class="org-builtin">float</span>),
  log_inv_disp=torch.tensor(fit0[1], dtype=torch.<span class="org-builtin">float</span>))
<span class="org-variable-name">zhat</span> = torch.nn.functional.softmax(L, dim=1)
</pre>
</div>

<p>
Plot the log likelihood difference between the two components for each data
point.
</p>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
plt.gcf().set_size_inches(4, 2)
plt.plot(np.diff(L).ravel(), lw=0, marker=<span class="org-string">'.'</span>, c=<span class="org-string">'k'</span>, ms=2)
plt.axhline(y=0, lw=1, ls=<span class="org-string">':'</span>, c=<span class="org-string">'k'</span>)
plt.xlabel(<span class="org-string">'Cell'</span>)
plt.ylabel(<span class="org-string">'Diff log lik'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/nbmix.org/mpebpm-llik-diff.png" alt="mpebpm-llik-diff.png">
</p>
</div>

<p>
Compute the cross entropy between the estimated \(\hat{z}\) and the ground
truth.
</p>

<div class="org-src-container">
<pre class="src src-ipython">torch.nn.functional.binary_cross_entropy(
  zhat,
  torch.tensor(pd.get_dummies(temp.obs[<span class="org-string">'cell_type'</span>]).values, dtype=torch.<span class="org-builtin">float</span>))
</pre>
</div>

<pre class="example">
tensor(0.)

</pre>

<p>
Compute a weighted log likelihood.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">w</span> = torch.rand([512, 2])
<span class="org-variable-name">w</span> /= w.<span class="org-builtin">sum</span>(dim=1).unsqueeze(-1)
<span class="org-variable-name">m</span>, <span class="org-variable-name">_</span> = L.<span class="org-builtin">max</span>(dim=1, keepdim=<span class="org-constant">True</span>)
(m + torch.log(w * torch.exp(L - m) + 1e-8)).mean()
</pre>
</div>

<pre class="example">
tensor(-1872.5645)

</pre>

<p>
Try fitting the model from a random initialization (49 s).
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> imp; imp.<span class="org-builtin">reload</span>(mpebpm.gam_mix)
torch.manual_seed(0)
<span class="org-variable-name">fit</span> = mpebpm.gam_mix.ebpm_gam_mix_em(
  x=temp.X.A,
  s=temp.X.<span class="org-builtin">sum</span>(axis=1),
  y=torch.tensor(pd.get_dummies(temp.obs[<span class="org-string">'cell_type'</span>]).values, dtype=torch.<span class="org-builtin">float</span>).cuda(),
  k=2,
  num_epochs=400,
  max_em_iters=10,
  log_dir=<span class="org-string">'runs/nbmix/mpebpm-random-init0-iter10'</span>)
</pre>
</div>

<p>
Compute the cross entropy between the estimated \(\hat{z}\) and the ground
truth.
</p>

<div class="org-src-container">
<pre class="src src-ipython">torch.nn.functional.binary_cross_entropy(
  torch.tensor(fit[-1], dtype=torch.<span class="org-builtin">float</span>),
  torch.tensor(pd.get_dummies(temp.obs[<span class="org-string">'cell_type'</span>]).values, dtype=torch.<span class="org-builtin">float</span>))
</pre>
</div>

<pre class="example">
tensor(0.)

</pre>

<p>
Plot the UMAP, colored by the fitted clusters.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Dark2'</span>)
plt.clf()
plt.gcf().set_size_inches(3, 3)
<span class="org-keyword">for</span> i <span class="org-keyword">in</span> <span class="org-builtin">range</span>(fit[-1].shape[1]):
  plt.plot(*temp[fit[-1][:,i].astype(<span class="org-builtin">bool</span>)].obsm[<span class="org-string">"X_umap"</span>].T, c=cm(i), marker=<span class="org-string">'.'</span>, ms=2, lw=0, label=f<span class="org-string">'Cluster {i}'</span>)
plt.legend(frameon=<span class="org-constant">False</span>, markerscale=4, handletextpad=0)
plt.xlabel(<span class="org-string">'UMAP 1'</span>)
plt.ylabel(<span class="org-string">'UMAP 2'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/nbmix.org/sim-ex-fit.png" alt="sim-ex-fit.png">
</p>
</div>
</div>
</div>

<div id="outline-container-orge0613b0" class="outline-3">
<h3 id="amortized"><a id="orge0613b0"></a>Amortized inference</h3>
<div class="outline-text-3" id="text-amortized">
<p>
Fit the amortized inference model, initializing \(\vmu_k, \vphi_k\) from the
MLE starting from the ground-truth labels.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">query</span> = torch.tensor(temp.X.A)
<span class="org-variable-name">s</span> = torch.tensor(temp.X.<span class="org-builtin">sum</span>(axis=1))
<span class="org-variable-name">fit</span> = mpebpm.gam_mix.EBPMGammaMix(
  p=temp.shape[1],
  k=2,
  log_mean=fit0[0],
  log_inv_disp=fit0[1])
</pre>
</div>

<p>
Look at the initial loss.
</p>

<div class="org-src-container">
<pre class="src src-ipython">(
  mpebpm.gam_mix._nb_mix_loss(
    fit.encoder.forward(query),
    query,
    s,
    fit.log_mean,
    fit.log_inv_disp),
  mpebpm.gam_mix._nb_mix_loss(
    torch.tensor(pd.get_dummies(temp.obs[<span class="org-string">'cell_type'</span>]).values),
    query,
    s,
    fit.log_mean,
    fit.log_inv_disp)
)
</pre>
</div>

<pre class="example">
(tensor(1927253.3750, grad_fn=&lt;NegBackward&gt;),
tensor(1926631.7500, grad_fn=&lt;NegBackward&gt;))
</pre>

<p>
Look at the gradients with respect to the encoder network weights.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">temp_loss</span> = mpebpm.gam_mix._nb_mix_loss(
  fit.encoder.forward(query),
  query,
  torch.tensor(temp.X.<span class="org-builtin">sum</span>(axis=1)),
  fit.log_mean,
  fit.log_inv_disp)
temp_loss.retain_grad()
temp_loss.backward()
torch.norm(fit.encoder[0].weight.grad)
</pre>
</div>

<pre class="example">
tensor(31154.1895)

</pre>

<p>
Perform amortized inference, initializing \(\vmu_k, \vphi_k\) from the MLE
starting from the ground-truth labels.
</p>

<div class="org-src-container">
<pre class="src src-ipython">torch.manual_seed(1)
<span class="org-variable-name">fit1</span> = mpebpm.gam_mix.EBPMGammaMix(
  p=temp.shape[1],
  k=2,
  log_mean=fit0[0],
  log_inv_disp=fit0[1]).fit(
    x=temp.X.A,
    s=temp.X.<span class="org-builtin">sum</span>(axis=1),
    y=pd.get_dummies(temp.obs[<span class="org-string">'cell_type'</span>]).values,
    lr=1e-3,
    batch_size=64,
    shuffle=<span class="org-constant">True</span>,
    num_pretrain=80,
    num_epochs=80,
    log_dir=<span class="org-string">'runs/nbmix/ai-inittruth-epoch80'</span>)
</pre>
</div>

<p>
Compute the cross entropy loss over the posterior mean cluster assignments.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">zhat</span> = fit1.forward(query.cuda()).detach().cpu().numpy()
torch.nn.functional.binary_cross_entropy(
  torch.tensor(zhat, dtype=torch.<span class="org-builtin">float</span>),
  torch.tensor(pd.get_dummies(temp.obs[<span class="org-string">'cell_type'</span>]).values, dtype=torch.<span class="org-builtin">float</span>))
</pre>
</div>

<pre class="example">
tensor(0.2482)

</pre>

<p>
Plot the approximate posterior over cluster assignments for each point.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Dark2'</span>)
plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 2, sharex=<span class="org-constant">True</span>, sharey=<span class="org-constant">True</span>)
fig.set_size_inches(4.5, 2.5)
<span class="org-keyword">for</span> i, a <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(ax):
  a.scatter(*temp.obsm[<span class="org-string">"X_umap"</span>].T, s=4, c=np.hstack((np.tile(np.array(cm(i)[:3]), zhat.shape[0]).reshape(-1, 3), zhat[:,i].reshape(-1, 1))))
  a.set_xlabel(<span class="org-string">'UMAP 1'</span>)
ax[0].set_ylabel(<span class="org-string">'UMAP 2'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/nbmix.org/am-inf-ex.png" alt="am-inf-ex.png">
</p>
</div>

<p>
Threshold cluster assignments, and compute the cross entropy loss.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">zhat_thresh</span> = pd.get_dummies(np.argmax(zhat, axis=1))
torch.nn.functional.binary_cross_entropy(
  torch.tensor(zhat_thresh.values, dtype=torch.<span class="org-builtin">float</span>),
  torch.tensor(pd.get_dummies(temp.obs[<span class="org-string">'cell_type'</span>]).values, dtype=torch.<span class="org-builtin">float</span>))
</pre>
</div>

<pre class="example">
tensor(0.)

</pre>

<p>
Now, fit the model starting from a random initialization.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> imp; imp.<span class="org-builtin">reload</span>(mpebpm.gam_mix)
torch.manual_seed(1)
<span class="org-variable-name">fit2</span> = mpebpm.gam_mix.EBPMGammaMix(
  p=temp.shape[1],
  k=2).fit(
    x=temp.X.A,
    s=temp.X.<span class="org-builtin">sum</span>(axis=1),
    y=pd.get_dummies(temp.obs[<span class="org-string">'cell_type'</span>]).values,
    lr=1e-2,
    batch_size=64,
    shuffle=<span class="org-constant">True</span>,
    num_pretrain=1,
    num_epochs=120,
    log_dir=<span class="org-string">'runs/nbmix/ai-init1-pretrain1'</span>)
</pre>
</div>

<p>
Compare the estimated \(\ln\mu\) from the clustering model to the estimates
using the ground truth labels.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">lim</span> = [-10, 10]
plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 2, sharey=<span class="org-constant">True</span>)
fig.set_size_inches(4.5, 2.5)
<span class="org-keyword">for</span> i, a <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(ax):
  a.scatter(fit1.log_mean[i].detach().cpu().numpy(), fit2.log_mean[i].detach().cpu().numpy(), s=1, c=<span class="org-string">'k'</span>, alpha=0.2)
  a.plot(lim, lim, c=<span class="org-string">'r'</span>, lw=1, ls=<span class="org-string">':'</span>)
  a.set_title(f<span class="org-string">'Cluster {i}'</span>)
  a.set_ylim(lim)
  a.set_xlabel(<span class="org-string">'Ground truth initialization'</span>)
ax[0].set_ylabel(<span class="org-string">'Random initialization'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/nbmix.org/am-inf-vs-mpebpm.png" alt="am-inf-vs-mpebpm.png">
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org089f29e" class="outline-2">
<h2 id="related-work"><a id="org089f29e"></a>Related work</h2>
<div class="outline-text-2" id="text-related-work">
<p>
scVI (<a href="https://dx.doi.org/10.1038/s41592-018-0229-2">Lopez et al. 2018</a>,
<a href="https://www.biorxiv.org/content/10.1101/532895v2">Xu et al. 2020</a>)
implements a related deep unsupervised (more precisely, semi-supervised)
clustering model (<a href="https://arxiv.org/abs/1406.5298">Kingma et al. 2014</a>,
<a href="https://arxiv.org/abs/1611.02648">Dilokthanakul et al. 2016</a>).
</p>

\begin{align}
  x_{ij} \mid s_i, \lambda_{ij} &\sim \Pois(s_i \lambda_{ij})\\
  \ln s_i &\sim \N(\cdot)\\
  \lambda_{ij} \mid \vz_i &\sim \Gam(\phi_j^{-1}, (\mu_{\lambda}(\vz_i))_j^{-1} \phi_j^{-1})\\
  \vz_i \mid y_i, \vu_i &\sim \N(\mu_z(\vu_i, y_i), \diag(\sigma^2(\vu_i, y_i)))\\
  y_i \mid \vpi &\sim \Mult(1, \vpi)\\
  \vu_i &\sim \N(0, \mi).
\end{align}

<p>
where
</p>

<ul class="org-ul">
<li>\(y_i\) denotes the cluster assignment for cell \(i\)</li>
<li>\(\mu_z(\cdot), \sigma^2(\cdot)\) are neural networks mapping the latent
cluster variable \(y_i\) and Gaussian noise \(\vu_i\) to the latent variable
\(\vz_i\)</li>
<li>\(\mu_{\lambda}(\cdot)\) is a neural network mapping latent variable \(\vz_i\) to
latent gene expression \(\vlambda_{i}\)</li>
</ul>

<p>
The intuition behind this model is that, marginalizing over \(y_i\), the
prior \(p(z_i)\) is a mixture of Gaussians, and therefore the model embeds
examples in a space which makes clustering easy, and maps examples to those
clusters simultaneously. To perform variational inference in this model,
Lopez et al. introduce inference networks
</p>

\begin{align}
  q(\vz_i \mid \vx_i) &= \N(\cdot)\\
  q(y_i \mid \vz_i) &= \Mult(1, \cdot).
\end{align}

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> scvi.dataset
<span class="org-keyword">import</span> scvi.inference
<span class="org-keyword">import</span> scvi.models

<span class="org-variable-name">expr</span> = scvi.dataset.AnnDatasetFromAnnData(temp)
<span class="org-variable-name">m</span> = scvi.models.VAEC(expr.nb_genes, n_batch=0, n_labels=2)
<span class="org-variable-name">train</span> = scvi.inference.UnsupervisedTrainer(m, expr, train_size=1, batch_size=32, show_progbar=<span class="org-constant">False</span>, n_epochs_kl_warmup=100)
train.train(n_epochs=1000, lr=1e-2)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">post</span> = train.create_posterior(train.model, expr)
<span class="org-variable-name">_</span>, <span class="org-variable-name">_</span>, <span class="org-variable-name">label</span> = post.get_latent()
</pre>
</div>

<p>
The fundamental difference between scVI and our approach is that scVI
clusters points in the low-dimensional latent space, and we cluster points in
the space of latent gene expression (which has equal dimension to the
observations).
</p>

<p>
Rui Shu <a href="http://ruishu.io/2016/12/25/gmvae/">proposed an alternative
generative model</a>, which has some practical benefits and can be adapted to
this problem
</p>

\begin{align}
  x_{ij} \mid \xiplus, \lambda_{ij} &\sim \Pois(\xiplus \lambda_{ij})\\
  \lambda_{ij} \mid \vz_i &\sim \Gam(\phi_j^{-1}, (\mu_{\lambda}(\vz_i))_j^{-1} \phi_j^{-1})\\
  \vz_i \mid y_i &\sim \N(\mu_z(y_i), \sigma^2(y_i))\\
  y_i \mid \vpi &\sim \Mult(1, \vpi)\\
  q(y_i, \vz_i \mid \vx_i) &= q(y_i \mid \vx_i)\, q(\vz_i \mid y_i, \vx_i).
\end{align}

<p>
There are additional deep unsupervised learning methods, which could
potentially be adapted to this setting (reviewed in
<a href="https://dx.doi.org/10.1109/ACCESS.2018.2855437">Min et al. 2018</a>).
</p>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Abhishek Sarkar</p>
<p class="date">Created: 2020-05-19 Tue 22:56</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
