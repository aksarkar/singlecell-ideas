#+TITLE: Normalizing flows for EBPM
#+SETUPFILE: setup.org

* Introduction

  Fitting an expression model to observed scRNA-seq data at a single gene can
  be thought of as solving an empirical Bayes problem (Sarkar and Stephens
  2020). \(
  \DeclareMathOperator\Pois{Poisson}
  \DeclareMathOperator\Gam{Gamma}
  \DeclareMathOperator\E{E}
  \DeclareMathOperator\V{V}
  \DeclareMathOperator\N{\mathcal{N}}
  \newcommand\abs[1]{\left\vert #1 \right\vert}
  \newcommand\const{\mathrm{const}}
  \)

  \begin{align}
    x_i \mid s_i, \lambda_i &\sim \Pois(s_i \lambda_i)\\
    \lambda_i &\sim g(\cdot) \in \mathcal{G},
  \end{align}

  where \(i = 1, \ldots, n\) indexes samples. Assuming \(\mathcal{G}\) is the
  family of Gamma distributions yields analytic gradients and admits
  [[file:mpebpm.org][fast implementation on GPUs]]. However, the fitted model
  can fail to accurately describe expression variation at some genes.

  In contrast, the family of non-parametric unimodal distributions
  (Stephens 2016) could be sufficient for all but a minority of genes. In
  practice, this family is approximated as the family of mixture of uniform
  distributions with fixed endpoints, including the common mode. Then,
  inference in this model can be achieved by a combination of convex
  optimization (over mixture weights, given the mode) and line search (over the
  mode, as an outer loop). However, in practice this approach is expensive and
  cumbersome to parallelize for large data sets.

  One idea which could bridge the gap between these approaches (in both
  computational cost and flexibility) is /normalizing flows/ (reviewed in
  [[https://arxiv.org/abs/1912.02762][Papamakarios et al. 2019]]). The key idea
  of normalizing flows is to apply a series of invertible, differentiable
  transformations \(T_1, \ldots, T_K\) to a tractable density, in order to
  obtain a different density

  \begin{align}
    x &= (T_K \circ \cdots \circ T_1)(u)\\
    f_x(x) &= f_u(u) \prod_{k=1}^{K} \det \abs{J_k(\cdot)},
  \end{align}

  where \(J_k\) is the Jacobian of \(T_k^{-1}\). If the \(T_k\) have free
  parameters, gradients with respect to those parameters are available,
  allowing the transformations to be learned from the data. Here, we
  investigate using flows to define a flexible family of priors, and use that
  family to fit expression models to scRNA-seq data.

* Setup
  :PROPERTIES:
  :CUSTOM_ID: setup
  :END:

  #+BEGIN_SRC emacs-lisp :exports none
    (add-to-list 'python-shell-completion-native-disabled-interpreters "jupyter")
    (org-babel-lob-ingest "/home/aksarkar/.emacs.d/org-templates/library.org")
  #+END_SRC

  #+RESULTS:
  : 2

  #+CALL: ipython3(venv="singlecell",partition="gpu2",opts="--gres=gpu:1",memory="8G") :exports none :dir /scratch/midway2/aksarkar/singlecell

  #+CALL: tensorboard(venv="singlecell",partition="mstephens") :dir /scratch/midway2/aksarkar/singlecell

  #+BEGIN_SRC ipython
    import numpy as np
    import pandas as pd
    import scipy.special as sp
    import scipy.stats as st
    import torch
    import torch.utils.tensorboard as tb
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[15]:
  :END:

  #+BEGIN_SRC ipython
    %matplotlib inline
    %config InlineBackend.figure_formats = set(['retina'])
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[2]:
  :END:

  #+BEGIN_SRC ipython
    import colorcet
    import matplotlib.pyplot as plt
    plt.rcParams['figure.facecolor'] = 'w'
    plt.rcParams['font.family'] = 'Nimbus Sans'
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[3]:
  :END:

* Methods
** Planar flow

   The specific class of transformations we will consider are /planar flows/
   (Rezende and Mohamed 2015)

   \begin{equation}
     T(x) = x + u \operatorname{sigmoid}(w x + b),
   \end{equation}

   where \(u, w, b\) are free (scalar) parameters. 

   #+BEGIN_SRC ipython
     class PlanarFlow(torch.nn.Module):
       # Rezende and Mohamed 2015
       def __init__(self, n_features, random_init=True):
         super().__init__()
         self.weight = torch.nn.Parameter(torch.zeros([n_features, 1]))
         self.bias = torch.nn.Parameter(torch.zeros([1]))
         self.post_act = torch.nn.Parameter(torch.zeros([n_features, 1]))
         if random_init:
           torch.nn.init.xavier_normal_(self.weight)
           torch.nn.init.xavier_normal_(self.post_act)

       def forward(self, x, eps=1e-15):
         # x is [batch_size, n_features]
         pre_act = x @ self.weight + self.bias
         # This is required to invert the flow
         post_act = self.post_act + self.weight / (self.weight.T @ self.weight + eps) * (-1 + torch.nn.functional.softplus(self.weight.T @ self.post_act) - self.weight.T @ self.post_act)
         out = x + torch.sigmoid(pre_act) @ post_act.T
         log_det = torch.log(torch.abs(1 + torch.sigmoid(pre_act) * torch.sigmoid(-pre_act) @ self.weight.T @ post_act))
         assert not torch.isnan(log_det).any()
         return out, log_det

       def __repr__(self):
         return f'PlanarFlow(post_act={self.post_act.data}, weight={self.weight.data}, bias={self.bias.data})'

     # Important: these are needed to transform distributions with constrained
     # support to unconstrained support

     # y = softplus(x) = log1p(exp(x))

     # dy/dx = exp(x) / (1 + exp(x)) = sigmoid(x)

     class Softplus(torch.nn.Module):
       def __init__(self):
         super().__init__()

       def forward(self, x):
         return torch.nn.functional.softplus(x), torch.log(torch.sigmoid(x))

     # x = softplus^{-1}(y) = ln(expm1(y))

     # dx/dy = exp(y) / (exp(y) - 1) = 1 / (1 - exp(-y))

     class InverseSoftplus(torch.nn.Module):
       def __init__(self):
         super().__init__()

       def forward(self, x):
         # c.f. https://github.com/tensorflow/probability/blob/v0.12.1/tensorflow_probability/python/math/generic.py#L456-L507    
         return torch.log(torch.expm1(x)), x - torch.log(torch.expm1(x))

     # For completeness. In preliminary experiments, Exp tends to overflow

     class Exp(torch.nn.Module):
       def __init__(self):
         super().__init__()

       def forward(self, x):
         return torch.exp(x), x

     class Log(torch.nn.Module):
       def __init__(self):
         super().__init__()

       def forward(self, x):
         assert (x > 0).all()
         return torch.log(x), -torch.log(x)

     class NormalizingFlow(torch.nn.Module):
       # https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#Sequential
       def __init__(self, flows):
         super().__init__()
         self.flows = torch.nn.ModuleList(flows)

       def forward(self, x):
         log_det = torch.zeros(x.shape)
         for f in self.flows:
           x, l = f.forward(x)
           log_det += l
         return x, log_det
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[4]:
   :END:

   The intuition behind this transform is that the pre-activation \(w x + b\)
   defines a (hyper)plane, and the post-activation \(u\) dilates the density
   about that hyperplane.

   #+BEGIN_SRC ipython :ipyfile figure/flows.org/planar-ex0.png
     cm = colorcet.cm['bmy']
     T = PlanarFlow(1)
     T.weight.data = torch.ones([1, 1])
     T.bias.data = torch.ones([1, 1])
     grid = np.linspace(-3, 3, 1000)

     plt.clf()
     plt.gcf().set_size_inches(3.5, 2.5)
     for u in np.linspace(0, 2, 5):
       T.post_act.data = torch.tensor(np.array(u).reshape(-1, 1), dtype=torch.float)
       with torch.no_grad():
         log_det = T.forward(torch.tensor(grid.reshape(-1, 1), dtype=torch.float))[1].numpy().squeeze()
       plt.plot(grid, np.exp(st.norm().logpdf(grid) + log_det), lw=1, c=cm((2 - u) / 2), label=f'u={u:.1g}')
     plt.legend(frameon=False, loc='center left', bbox_to_anchor=(1, .5))
     plt.xlabel('Observation $x$')
     plt.ylabel('Density')
     plt.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[42]:
   [[file:figure/flows.org/planar-ex0.png]]
   :END:

** Normalizing flow for density estimation

   Suppose we have observations \(x_1, \ldots, x_n\) drawn from \(f^*\). One
   can estimate \(f_x\) by maximizing the likelihood of the data

   \begin{align}
     &\max_{f_x} \E_{f^*}[\ln f_x(x)]\\
     = &\max_{T_1, \ldots, T_K} \E_{f^*}\left[\ln f_u(T(x)) + \sum_{k=1}^K \ln\det J_k(\cdot)\right]\\
     = &\max_{T_1, \ldots, T_K} \frac{1}{n} \sum_i\left[\ln f_u(T(x_i)) + \sum_{k=1}^K \ln\det J_k(\cdot)\right],
   \end{align}

   where \(T = T_K \circ \cdots \circ T_1\) is the mapping from \(x \in
   \mathcal{X} \rightarrow u \in \mathcal{U}\), and \(f_u\) is the density of
   some simple distribution (e.g., standard Gaussian). This optimization
   problem can be readily solved using automatic differentiation and gradient
   descent.

   #+BEGIN_SRC ipython
     class DensityEstimator(torch.nn.Module):
       def __init__(self, n_features, K):
         super().__init__()
         # Important: here the flow maps x in ambient measure to u in base measure
         self.flow = NormalizingFlow([PlanarFlow(n_features) for _ in range(K)])

       def forward(self, x):
         loss = -self.log_prob(x).mean()
         assert loss > 0
         return loss

       def fit(self, x, n_epochs, log_dir=None, **kwargs):
         if log_dir is not None:
           writer = tb.SummaryWriter(log_dir)
         opt = torch.optim.RMSprop(self.parameters(), **kwargs)
         global_step = 0
         for _ in range(n_epochs):
           opt.zero_grad()
           loss = self.forward(x)
           if log_dir is not None:
             writer.add_scalar('loss', loss, global_step)
           if torch.isnan(loss):
             raise RuntimeError
           loss.backward()
           opt.step()
           global_step += 1
         return self

       def log_prob(self, x):
         u, log_det = self.flow.forward(x)
         l = torch.distributions.Normal(loc=0., scale=1.).log_prob(u) + log_det
         return l
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[6]:
   :END:

** Normalizing flow for empirical Bayes

   Now consider the EBPM problem

   \begin{align}
     x_i \mid s_i, \lambda_i &\sim \Pois(s_i \lambda_i)\\
     \lambda_i &\sim g(\cdot) = g_0(\cdot) \prod_k \det \abs{J^g_k}
   \end{align}

   where \(i = 1, \ldots, n\), and \(g_0(\cdot) = \N(\cdot; 0, 1)\) for
   simplicity. One can estimate \(g\) by maximizing the marginal likelihood

   \begin{align}
      &\max_g \sum_i \ln p(x_i \mid s_i, g)\\
      \geq &\max_{g, q} \E_{\lambda_i \sim q}\left[\sum_i \ln p(x_i \mid s_i, \lambda_i) + \ln g(\lambda_i) - \ln q(\lambda_i)\right]\\
      = &\max_{T_g, q} \E_{\lambda_i \sim q}\left[\sum_i \ln p(x_i \mid s_i, \lambda_i) + \ln g_0(T_g(\lambda_i)) + \sum_k \ln\det\abs{J^g_k} - \ln q(\lambda_i)\right]\\
   \end{align}

   where \(T_g = T^g_K \circ \cdots \circ T^g_1\) maps \(g\) to a base measure
   and \(J^g_k\) denotes the Jacobian of \(T^g_k\). It is straightforward to
   show that, holding \(g\) fixed, the optimal \(q\) is the true posterior
   \(p(\lambda_i \mid x_i, s_i, g)\) (e.g., Neal and Hinton 1998). In order to
   ensure \(q\) is flexible enough to capture the true posterior, suppose it
   too is represened by a normalizing flow

   \begin{equation}
     q(\cdot) = q_0(\cdot) \prod_k \det\abs{J^q_k},
   \end{equation}

   where \(J^q_k\) denotes the Jacobian of the transform \(T^q_k\). In order to
   make sampling easy, suppose \(T_q = T^q_K \circ \cdots \circ T^q_1\) maps
   the base measure \(q_0(u_i \mid x_i)\) to \(q\). Then, the optimization
   problem is

   \begin{equation}
     \max_{T_g, T_q} \E_{u_i \sim q_0}\left[\sum_i \ln p(x_i \mid s_i, T_q(u_i)) + \ln g_0(T_g(T_q(u_i))) + \sum_k \ln\det\abs{J^g_k} - \ln q_0(u_i) + \sum_k \ln\det\abs{J^q_k}\right].
   \end{equation}

   /Remark/ It is critical that \(u_i \sim q_0\) depends on \(x_i\) in the
   variational approximation. Rezende and Mohamed 2015 propose using amortized
   inference; however, in the context of this problem, a simpler alternative
   could be a log-Gamma posterior.

   /Remark/ Since the transformation \(T_q\) maps \(u_i \in \mathcal{U}\) to
   \(\lambda_i \in \Lambda\), the signs of the log determinant terms need to be
   inverted.

   Since \(T_g, T_q\) are differentiable, this problem can be solved by
   replacing the expectation with a Monte Carlo integral (e.g., Kingma and
   Welling 2014), and then using automatic differentiation and gradient descent
   to optimize the resulting stochastic objective.

   /Remark/ When reducing problems in scRNA-seq data analysis to EBPM, we are
   primarily interested in the estimated prior \(\hat{g}\). Depending on the
   choice of flow, obtaining expectations with respect to \(\hat{g}\) might be
   difficult. One possibility is to approximate these expectations by
   discretizing \(\hat{g}\) and taking weighted sums.

   #+BEGIN_SRC ipython
     class EBNM(torch.nn.Module):
       def __init__(self, K, scale, random_init=True):
         super().__init__()
         self.scale = scale
         self.p0 = torch.distributions.Normal(loc=0., scale=1.)
         self.pz = NormalizingFlow([PlanarFlow(n_features=1, random_init=random_init) for _ in range(K)])
         self.qz = NormalizingFlow([PlanarFlow(n_features=1, random_init=random_init) for _ in range(K)])

       def forward(self, x, n_samples):
         q0 = torch.distributions.Normal(
           loc=x / (1 + self.scale ** 2),
           scale=torch.sqrt(1 / (1 + 1 / self.scale ** 2)))
         u = q0.rsample(n_samples)
         z, log_det_q = self.qz.forward(u)
         w, log_det_p = self.pz.forward(z)
         # Important: qz is forward transforms, so we need to invert the sign of
         # log_det_q
         elbo = (torch.distributions.Normal(z, self.scale).log_prob(x)
                 + self.p0.log_prob(w) + log_det_p
                 - (q0.log_prob(u) - log_det_q)).mean(dim=0).sum()
         assert elbo <= 0
         return -elbo

       def fit(self, x, n_epochs, n_samples=1, log_dir=None, **kwargs):
         if log_dir is not None:
           writer = tb.SummaryWriter(log_dir)
         n_samples = torch.Size([n_samples])
         opt = torch.optim.RMSprop(self.parameters(), **kwargs)
         global_step = 0
         for _ in range(n_epochs):
           opt.zero_grad()
           loss = self.forward(x, n_samples)
           if log_dir is not None:
             writer.add_scalar('loss', loss, global_step)
           if torch.isnan(loss):
             raise RuntimeError
           loss.backward()
           opt.step()
           global_step += 1
         return self

       @torch.no_grad()
       def fitted_g(self, z, log=True):
         u, log_det = self.pz.forward(z)
         log_prob = self.p0.log_prob(u) + log_det
         if log:
           return log_prob.numpy()
         else:
           return torch.exp(log_prob).numpy()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[47]:
   :END:

   #+BEGIN_SRC ipython
     class EBPM(torch.nn.Module):
       def __init__(self, K):
         super().__init__()
         self.p0 = torch.distributions.Gamma(concentration=1., rate=1.)
         self.pz = NormalizingFlow([InverseSoftplus()] + [PlanarFlow(1) for _ in range(K)] + [Softplus()])
         self.qz = NormalizingFlow([InverseSoftplus()] + [PlanarFlow(1) for _ in range(K)] + [Softplus()])

       def forward(self, x, s, n_samples):
         q0 = torch.distributions.Gamma(concentration=1 + x, rate=1 + s)
         u = q0.rsample(n_samples)
         z, log_det_q = self.qz.forward(u)
         w, log_det_p = self.pz.forward(z)
         elbo = (torch.distributions.Poisson(s * z).log_prob(x)
                 + self.p0.log_prob(w) + log_det_p
                 - (q0.log_prob(u) - log_det_q)).mean(dim=0).sum()
         assert elbo <= 0
         return -elbo

       def fit(self, x, s, n_epochs, n_samples=1, log_dir=None, **kwargs):
         if log_dir is not None:
           writer = tb.SummaryWriter(log_dir)
         n_samples = torch.Size([n_samples])
         opt = torch.optim.RMSprop(self.parameters(), **kwargs)
         global_step = 0
         for _ in range(n_epochs):
           opt.zero_grad()
           loss = self.forward(x, s, n_samples)
           if log_dir is not None:
             writer.add_scalar('loss', loss, global_step)
           if torch.isnan(loss):
             raise RuntimeError
           loss.backward()
           opt.step()
           global_step += 1
         return self

       @torch.no_grad()
       def fitted_g(self, z, log=True):
         u, log_det = self.pz.forward(z)
         log_prob = self.p0.log_prob(u) + log_det
         if log:
           return log_prob.numpy()
         else:
           return torch.exp(log_prob).numpy()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[5]:
   :END:

* Results
** Example of density estimation

   Draw data from a scale mixture of Gaussians.

   #+BEGIN_SRC ipython
     rng = np.random.default_rng(1)
     n = 1000
     pi = np.array([0.3, 0.7])
     scale = np.array([0.1, 0.4])
     z = rng.uniform(size=(n, 1)) < pi[0]
     x = rng.normal(scale=scale @ np.hstack([z, ~z]).T)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[8]:
   :END:

   Fit normalizing flows for different choices of \(K\).

   #+begin_SRC ipython :async t
     run = 0
     lr = 1e-2
     n_epochs = 8000
     torch.manual_seed(run)
     models = [DensityEstimator(n_features=1, K=K)
               .fit(torch.tensor(x.reshape(-1, 1), dtype=torch.float),
                    n_epochs=n_epochs,
                    lr=lr) for K in range(1, 5)]
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[17]:
   :END:

   Plot the fit.

   #+BEGIN_SRC ipython :ipyfile figure/flows.org/ex0-fit.png
     cm = plt.get_cmap('Dark2')
     plt.clf()
     plt.gcf().set_size_inches(3.5, 2.5)
     plt.hist(x, bins=19, density=True, color='0.8')
     grid = np.linspace(x.min(), x.max(), 5000)
     mixpdf = st.norm(scale=scale).pdf(grid.reshape(-1, 1)) @ pi
     plt.plot(grid, mixpdf, lw=2, c='k', label='Simulated')
     for k, m in enumerate(models):
       with torch.no_grad():
         f = np.exp(m.log_prob(torch.tensor(grid.reshape(-1, 1), dtype=torch.float)).numpy())
       plt.plot(grid, f, lw=1, c=cm(k), label=f'K = {k + 1}')
     plt.legend(frameon=False, loc='center left', bbox_to_anchor=(1, .5))
     plt.xlabel('Observation $x$')
     plt.ylabel('Density')
     plt.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[20]:
   [[file:figure/flows.org/ex0-fit.png]]
   :END:

** Density estimation sanity check

   Make sure the method can learn the identity transform. Draw data from a
   standard Gaussian.

   #+BEGIN_SRC ipython
     rng = np.random.default_rng(1)
     n = 1000
     x = rng.normal(size=n)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[21]:
   :END:

   Fit the model.

   #+BEGIN_SRC ipython :async t
     run = 0
     lr = 1e-2
     n_epochs = 8000
     torch.manual_seed(run)
     models = [DensityEstimator(n_features=1, K=K)
               .fit(torch.tensor(x.reshape(-1, 1), dtype=torch.float),
                    n_epochs=n_epochs,
                    lr=lr) for K in range(1, 5)]
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[24]:
   :END:

   Plot the fitted models against the simulated data.

   #+BEGIN_SRC ipython :ipyfile figure/flows.org/ex2.png
     cm = plt.get_cmap('Dark2')
     plt.clf()
     plt.gcf().set_size_inches(3.5, 2.5)
     plt.hist(x, bins=23, density=True, color='0.7')
     grid = np.linspace(x.min(), x.max(), 1000)
     plt.plot(grid, st.norm().pdf(grid), lw=2, c='k', label='Simulated')
     for k, m in enumerate(models):
       with torch.no_grad():
         flow = np.exp(m.log_prob(torch.tensor(grid.reshape(-1, 1), dtype=torch.float)).numpy())
       plt.plot(grid, flow, lw=1, c=cm(k), label=f'K={k + 1}')
     plt.legend(frameon=False, loc='center left', bbox_to_anchor=(1, .5))
     plt.xlabel('Observation $x$')
     plt.ylabel('Density')
     plt.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[25]:
   [[file:figure/flows.org/ex2.png]]
   :END:

** EBNM via VBEM sanity check

   A critical assumption we make is that optimizing the ELBO will yield the
   correct \(\hat{g}\) if the space of approximations \(q \in \mathcal{Q}\)
   contains the true posterior. The intuition behind this assumption is that,
   in this case, VBEM equals EM (Neal and Hinton 1998). Check whether this is
   the case for a simple example

   \begin{align}
     x_i \mid z_i, s_i^2 &\sim \N(z_i, s_i^2), \qquad i = 1, \ldots, n\\
     z_i &\sim \N(0, \sigma_0^2).
   \end{align}

   The exact posterior

   \begin{align}
     q(z_i \mid x_i, s_i^2, \sigma_0^2) &= \N\left(\frac{\sigma_0^2}{\sigma_0^2 + s_i^2} x_i, \frac{1}{1 / \sigma_0^2 + 1 / s_i^2}\right)\\
     &\triangleq \N(\mu_i, \sigma_i^2)
   \end{align}

   and the ELBO is

   \begin{align}
     h &\triangleq \E\left[\sum_i \ln p(x_i, z_i \mid s_i^2, \sigma_0^2) - \ln q(z_i \mid x_i, s_i^2, \sigma_0^2)\right]\\
     &= \sum_i -\ln s_i^2 - \frac{(x_i - \E[z_i])^2 - \V[z_i]}{2 s_i^2} - \ln \sigma_0^2 - \frac{\E[z_i]^2 - \V[z_i]}{2 \sigma_0^2} - \ln \sigma_i^2 + \const,
   \end{align}

   where expectations are with respect to \(q\), yielding M step update

   \begin{align}
     \frac{\partial h}{\partial \sigma_0^2} &= \frac{n}{\sigma_0^2} - \frac{1}{(\sigma_0^2)^2} \sum_i \E[z_i]^2 - \V[z_i] = 0\\
     \sigma_0^2 &:= \frac{1}{n} \sum_i \E[z_i]^2 - \V[z_i]
   \end{align}
   
   #+BEGIN_SRC ipython
     def ebnm_em(x, s2, max_iters=100, tol=1e-3):
       init = np.array([1])
       sigma2hat, elbo = squarem(init, _ebnm_elbo, _ebnm_update, x=x, s2=s2,
                                  max_iters=max_iters, tol=tol)
       return sigma2hat, elbo

     def _ebnm_elbo(sigma2, x, s2):
       pm = sigma2 / (sigma2 + s2) * x
       pv = 1 / (1 / sigma2 + 1 / s2)
       return (-np.log(s2)
               - ((x - pm) ** 2 - pv) / (2 * s2)
               - np.log(sigma2)
               - (pm ** 2 - pv) / (2 * sigma2)).sum()

     def _ebnm_update(sigma2, x, s2):
       pm = sigma2 / (sigma2 + s2) * x
       pv = 1 / (1 / sigma2 + 1 / s2)
       sigma2 = (pm ** 2 - pv).mean()
       assert sigma2 >= 0
       return sigma2

     def squarem(init, objective_fn, update_fn, max_iters, tol, par_tol=1e-8, max_step_updates=10, *args, **kwargs):
       """Squared extrapolation scheme for accelerated EM

       Reference: 

         Varadhan, R. and Roland, C. (2008), Simple and Globally Convergent Methods
         for Accelerating the Convergence of Any EM Algorithm. Scandinavian Journal
         of Statistics, 35: 335-353. doi:10.1111/j.1467-9469.2007.00585.x

       """
       theta = init
       obj = objective_fn(theta, *args, **kwargs)
       for i in range(max_iters):
         x1 = update_fn(theta, *args, **kwargs)
         r = x1 - theta
         if i == 0 and objective_fn(x1, *args, **kwargs) < obj:
           # Hack: this is needed for numerical reasons, because in e.g.,
           # ebpm_gamma, a point mass is the limit as a = 1/φ → ∞
           return init, obj
         x2 = update_fn(x1, *args, **kwargs)
         v = (x2 - x1) - r
         if np.linalg.norm(v) < par_tol:
           return x2, objective_fn(x2, *args, **kwargs)
         step = -np.sqrt(r @ r) / np.sqrt(v @ v)
         if step > -1:
           step = -1
           theta += -2 * step * r + step * step * v
           update = objective_fn(theta, *args, **kwargs)
           diff = update - obj
         else:
           # Step length = -1 is EM; use as large a step length as is feasible to
           # maintain monotonicity
           for j in range(max_step_updates):
             candidate = theta - 2 * step * r + step * step * v
             update = objective_fn(candidate, *args, **kwargs)
             diff = update - obj
             if np.isfinite(update) and diff > 0:
               theta = candidate
               break
             else:
               step = (step - 1) / 2
           else:
             step = -1
             theta += -2 * step * r + step * step * v
             update = objective_fn(theta, *args, **kwargs)
             diff = update - obj
         if diff < tol:
           return theta, update
         else:
           obj = update
       else:
         raise RuntimeError(f'failed to converge in max_iters ({diff:.3g} > {tol:.3g})')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[29]:
   :END:

   Draw from the model.

   #+BEGIN_SRC ipython
     rng = np.random.default_rng(1)
     n = 1000
     s = 0.05
     sigma = 0.5
     mu = rng.normal(scale=sigma, size=n)
     x = rng.normal(loc=mu, scale=s)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[32]:
   :END:

   Fit VBEM.

   #+BEGIN_SRC ipython :async t
     sigma2hat, trace = ebnm_vbem(x, s ** 2, max_iters=1000)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[33]:
   :END:

   Plot the simulated data and model fit.

   #+BEGIN_SRC ipython :ipyfile figure/flows.org/ebnm-sanity-check.png
     plt.clf()
     fig, ax = plt.subplots(2, 1)
     fig.set_size_inches(4, 4)
     ax[0].hist(x, bins=25, density='True', color='0.7')
     grid = np.linspace(x.min(), x.max(), 1000)
     ax[0].plot(grid, st.norm(scale=np.sqrt(s ** 2 + sigma ** 2)).pdf(grid), lw=1, c='k', label='Simulated')
     ax[0].plot(grid, st.norm(scale=np.sqrt(s ** 2 + sigma2hat)).pdf(grid), lw=1, c='r', label='Fit')
     ax[0].legend(loc='upper right', frameon=True)
     ax[0].set_xlabel('Observation $x$')
     ax[0].set_ylabel('Density')

     ax[1].hist(mu, bins=25, density='True', color='0.7')
     grid = np.linspace(mu.min(), mu.max(), 1000)
     ax[1].plot(grid, st.norm(scale=sigma).pdf(grid), lw=1, c='k', label='Simulated')
     ax[1].plot(grid, st.norm(scale=np.sqrt(sigma2hat)).pdf(grid), lw=1, c='r', label='Fit')
     ax[1].set_xlabel('Latent variable $z$')
     ax[1].set_ylabel('Density')

     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[49]:
   [[file:figure/flows.org/ebnm-sanity-check.png]]
   :END:

** EBNM via flows sanity check

   For EBNM, a simple choice of \(q_0\) is 

   \begin{equation}
     u_i \mid x_i, s_i^2 \sim \N\left(\frac{1}{1 + s_i^2} x_i, \frac{1}{1 + 1 / s_i^2}\right),
   \end{equation}

   which is the exact posterior under the simple model

   \begin{align}
     x_i \mid u_i, s_i^2 &\sim \N(u_i, s_i^2)\\
     u_i &\sim \N(0, 1).
   \end{align}

   Draw observations from the simple model.

   #+BEGIN_SRC ipython
     rng = np.random.default_rng(1)
     n = 1000
     s = 0.05
     mu = rng.normal(size=n)
     x = rng.normal(loc=mu, scale=s)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[7]:
   :END:

   Fit the model for different choices of \(K\).

   #+BEGIN_SRC ipython :async t
     run = 21
     lr = 1e-2
     n_epochs = 10000
     torch.manual_seed(run)
     models = [EBNM(K=K, scale=torch.tensor(s))
               .fit(torch.tensor(x.reshape(-1, 1), dtype=torch.float),
                    n_epochs=n_epochs,
                    lr=lr)
               for K in range(1, 5)]
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[48]:
   :END:

   Plot the model fits against the simulated data.

   #+BEGIN_SRC ipython :ipyfile figure/flows.org/ebnm-ex.png
     cm = plt.get_cmap('Dark2')
     plt.clf()
     fig, ax = plt.subplots(2, 1)
     fig.set_size_inches(4, 4)
     ax[0].hist(x, bins=25, color='0.7', density=True)
     ax[0].set_xlabel('Observation $x$')
     ax[0].set_ylabel('Density')

     grid = np.linspace(mu.min(), mu.max(), 1000)
     ax[1].hist(mu, bins=25, color='0.7', density=True)
     ax[1].plot(grid, st.norm().pdf(grid), lw=2, c='k', label='Simulated')
     for k, m in enumerate(models):
       ax[1].plot(grid, m.fitted_g(torch.tensor(grid.reshape(-1, 1), dtype=torch.float), log=False), c=cm(k), label=f'K={k + 1}')
     ax[1].legend(frameon=False, loc='center left', bbox_to_anchor=(1, .5))
     ax[1].set_xlabel('Latent variable $\mu$')
     ax[1].set_ylabel('Density')
     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[50]:
   [[file:figure/flows.org/ebnm-ex.png]]
   :END:

** Example of EBNM

   Draw observations from a scale mixture of Gaussians prior.

   #+BEGIN_SRC ipython
     rng = np.random.default_rng(1)
     n = 1000
     s = 0.05
     pi = np.array([0.3, 0.7])
     scale = np.array([0.1, 0.4])
     z = rng.choice(a=scale.shape[0], p=pi, size=n)
     mu = rng.normal(scale=scale[z], size=n)
     x = rng.normal(loc=mu, scale=s)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[71]:
   :END:

   Fit the model for different choices of \(K\).

   #+BEGIN_SRC ipython :async t
     run = 1
     lr = 1e-2
     n_epochs = 5000
     torch.manual_seed(run)
     models = [EBNM(K=K, scale=torch.tensor(s))
               .fit(torch.tensor(x.reshape(-1, 1), dtype=torch.float),
                    n_epochs=n_epochs,
                    lr=lr,
                    log_dir=f'/scratch/midway2/aksarkar/singlecell/runs/ebnm-nf-normmix-{K}-{run}-{n_epochs}')
               for K in range(1, 5)]
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[74]:
   :END:

   Plot the model fits against the simulated data.

   #+BEGIN_SRC ipython :ipyfile figure/flows.org/ebnm-normmix-ex.png
     cm = plt.get_cmap('Dark2')
     plt.clf()
     fig, ax = plt.subplots(2, 1)
     fig.set_size_inches(4, 4)
     ax[0].hist(x, bins=25, color='0.7', density=True)
     ax[0].set_xlabel('Observation $x$')
     ax[0].set_ylabel('Density')

     grid = np.linspace(mu.min(), mu.max(), 1000)
     ax[1].hist(mu, bins=25, color='0.7', density=True)
     g = st.norm(scale=scale).pdf(grid.reshape(-1, 1)) @ pi
     ax[1].plot(grid, g, lw=2, c='k', label='Simulated')
     for k, m in enumerate(models):
       ax[1].plot(grid, m.fitted_g(torch.tensor(grid.reshape(-1, 1), dtype=torch.float), log=False), c=cm(k), label=f'K={k + 1}')
     ax[1].legend(frameon=False, loc='center left', bbox_to_anchor=(1, .5))
     ax[1].set_xlabel('Latent variable $\mu$')
     ax[1].set_ylabel('Density')
     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[75]:
   [[file:figure/flows.org/ebnm-normmix-ex.png]]
   :END:

** EBPM sanity check

   For EBPM, a simple choice of \(q_0\) is 

   \begin{equation}
     u_i \mid x_i, s_i \sim \Gam(1 + x_i, 1 + s_i),
   \end{equation}

   where the Gamma distribution is parameterized by shape and rate, which is
   the exact posterior under the simple model

   \begin{align}
     x_i \mid s_i, \lambda_i &\sim \Pois(s_i \lambda_i)\\
     u_i &\sim \Gam(1, 1).
   \end{align}

   Simulate data from the simple model.

   #+BEGIN_SRC ipython
     rng = np.random.default_rng(1)
     n = 1000
     s = np.full(n, 1)
     lam = rng.gamma(shape=1, scale=1, size=n)
     x = rng.poisson(s * lam)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[83]:
   :END:

   Fit EBPM for different choices of \(K\).

   #+BEGIN_SRC ipython :async t
     run = 3
     lr = 1e-2
     n_epochs = 2000
     torch.manual_seed(run)
     models = [EBPM(K=K)
               .fit(torch.tensor(x.reshape(-1, 1), dtype=torch.float),
                    torch.tensor(s.reshape(-1, 1), dtype=torch.float),
                    n_epochs=n_epochs,
                    lr=lr,
                    log_dir=f'/scratch/midway2/aksarkar/singlecell/runs/ebpm-sanity-{run}-{K}-{n_epochs}')
               for K in range(1, 5)]
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[84]:
   :END:

   Plot the fitted models against the simulated data.

   #+BEGIN_SRC ipython :ipyfile figure/flows.org/ebpm-sanity-check.png
     cm = plt.get_cmap('Dark2')
     plt.clf()
     fig, ax = plt.subplots(2, 1)
     fig.set_size_inches(4, 4)
     grid = np.arange(x.max() + 1)
     ax[0].hist(x, bins=grid, density='True', color='0.7')
     ax[0].set_xticks(grid)
     ax[0].set_xlabel('Observation $x$')
     ax[0].set_ylabel('Density')

     ax[1].hist(lam, bins=50, density='True', color='0.7')
     grid = np.linspace(lam.min(), lam.max(), 1000)
     ax[1].plot(grid, st.gamma(a=1, scale=1).pdf(grid), lw=2, c='k', label='Simulated')
     for k, m in enumerate(models):
       ax[1].plot(grid, m.fitted_g(torch.tensor(grid.reshape(-1, 1), dtype=torch.float), log=False), lw=1, c=cm(k), label=f'K = {k + 1}')
     ax[1].legend(frameon=False)
     ax[1].set_xlabel('Latent variable $\lambda$')
     ax[1].set_ylabel('Density')

     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[85]:
   [[file:figure/flows.org/ebpm-sanity-check.png]]
   :END:

** Example of EBPM

   Draw data from a Poisson convolved with a Gamma.

   #+BEGIN_SRC ipython
     rng = np.random.default_rng(1)
     n = 1000
     s = np.full(n, 1e4)
     log_mean = -8
     log_inv_disp = 1
     lam = rng.gamma(shape=np.exp(log_inv_disp), scale=np.exp(log_mean - log_inv_disp), size=n)
     x = rng.poisson(s * lam)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[6]:
   :END:

   Fit the model for different choices of \(K\).

   #+BEGIN_SRC ipython :async t
     run = 2
     lr = 1e-2
     n_epochs = 16000
     torch.manual_seed(run)
     models = {K: EBPM(K=K)
               .fit(torch.tensor(x.reshape(-1, 1), dtype=torch.float),
                    torch.tensor(s.reshape(-1, 1), dtype=torch.float),
                    n_epochs=n_epochs,
                    lr=lr,
                    log_dir=f'/scratch/midway2/aksarkar/singlecell/runs/ebpm-nf-{run}-{K}-{lr}-{n_epochs}')
               for K in (1, 4, 8)}
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[9]:
   :END:

   Plot the simulated data.

   #+BEGIN_SRC ipython :ipyfile figure/flows.org/ebpm-gamma-ex.png
     n_samples = 1000
     cm = plt.get_cmap('Dark2')
     plt.clf()
     fig, ax = plt.subplots(2, 1)
     fig.set_size_inches(4, 4)
     grid = np.arange(x.max() + 1)
     ax[0].hist(x, bins=grid, density='True', color='0.7')
     ax[0].plot(grid + 0.5, st.nbinom(n=np.exp(log_inv_disp), p=1 / (1 + s[0] * np.exp(log_mean - log_inv_disp))).pmf(grid), lw=2, c='k')
     ax[0].set_xticks(grid[::3])
     ax[0].set_xlabel('Observation $x$')
     ax[0].set_ylabel('Density')

     ax[1].hist(lam, bins=30, density='True', color='0.7')
     grid = np.linspace(lam.min(), lam.max(), 1000)
     ax[1].plot(grid, st.gamma(a=np.exp(log_inv_disp), scale=np.exp(log_mean - log_inv_disp)).pdf(grid), lw=2, c='k', label='Simulated')
     for i, k in enumerate(models):
       ax[1].plot(grid, models[k].fitted_g(torch.tensor(grid.reshape(-1, 1), dtype=torch.float), log=False), lw=1, c=cm(i), label=f'K = {k}')
     ax[1].legend(frameon=False)
     ax[1].set_xlabel('Latent variable $\lambda$')
     ax[1].set_ylabel('Density')

     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[55]:
   [[file:figure/flows.org/ebpm-gamma-ex.png]]
   :END:

   Under this model, the true posterior is

   \begin{equation}
     \lambda_i \mid x_i, s_i \sim \Gam(a + x_i, b + s_i),
   \end{equation}

   where the true prior is \(\lambda_i \sim \Gam(a, b)\). Compare the
   approximate posterior mean to the true posterior mean.

   #+BEGIN_SRC ipython :ipyfile figure/flows.org/ebpm-gamma-ex-pm.png
     cm = plt.get_cmap('Dark2')
     pm = (x + np.exp(log_inv_disp)) / (s + np.exp(log_mean - log_inv_disp))
     plt.clf()
     plt.gcf().set_size_inches(3, 2.5)
     plt.gca().set_aspect('equal')
     q0 = st.gamma(a=1 + x.reshape(-1, 1), scale=1 / (1 + s.reshape(-1, 1)))
     for i, k in enumerate(models):
       with torch.no_grad():
         samples = np.stack([models[k].qz.forward(torch.tensor(q0.rvs(), dtype=torch.float))[0].numpy() for _ in range(100)]).squeeze()
         muhat = samples.mean(axis=0)
       plt.scatter(pm, muhat, s=1, color=cm(i), label=f'K = {k}')
     lim = [0, 0.0025]
     plt.plot(lim, lim, lw=1, ls=':', c='r')
     plt.legend(frameon=False, handletextpad=0, markerscale=4)
     plt.xlabel('True posterior mean')
     plt.ylabel('Approximate posterior mean')
     plt.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[96]:
   [[file:figure/flows.org/ebpm-gamma-ex-pm.png]]
   :END:

   Compare the approximate posterior to the true posterior for a subset of
   observations.

   #+BEGIN_SRC ipython :ipyfile figure/flows.org/ebpm-gamma-ex-q.png
     plt.clf()
     fig, ax = plt.subplots(1, 4, sharey=True)
     fig.set_size_inches(7.5, 2.5)

     for i, a in enumerate(ax):
       grid = np.log(np.linspace(0, 3e-3, 1000)[1:])
       q = st.gamma(a=np.exp(log_inv_disp) + x[i], scale=1 / (np.exp(log_mean - log_inv_disp) + s[i]))
       ax[i].plot(grid, q.pdf(np.exp(grid)) * np.exp(grid), lw=2, c='k', label='$q$')
       q0 = st.gamma(a=1 + x[i], scale=1 / (1 + s[i]))
       ax[i].plot(grid, q0.pdf(np.exp(grid)) * np.exp(grid), lw=1, c='k', ls='--', label='$q_0$')
       with torch.no_grad():
         samples = models[8].qz.forward(torch.tensor(q0.rvs(size=(500, 1)), dtype=torch.float))[0].numpy()
       ax[i].hist(np.log(samples), bins=9, density=True, color='0.8')
       ax[i].set_xlabel('$\ln(\lambda)$')
       ax[i].set_title(f'$x$ = {x[i]}')
     ax[0].set_ylabel('Density')
     ax[0].legend(frameon=False, loc='upper left')
     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[62]:
   [[file:figure/flows.org/ebpm-gamma-ex-q.png]]
   :END:
