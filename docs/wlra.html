<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2018-09-27 Thu 20:11 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Weighted low rank approximation</title>
<meta name="generator" content="Org mode">
<meta name="author" content="Abhishek Sarkar">
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link href="https://maxcdn.bootstrapcdn.com/bootswatch/3.3.7/cosmo/bootstrap.min.css" rel="stylesheet" />
<link rel="stylesheet" type="text/css" href="https://www.pirilampo.org/styles/readtheorg/css/htmlize.css"/>
<style type="text/css">body {width: 60em; margin:auto} pre.src {overflow:auto}</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2017 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Weighted low rank approximation</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgb49048c">Introduction</a></li>
<li><a href="#org3822917">Methods</a>
<ul>
<li><a href="#orgc964f6b">EM algorithm</a></li>
<li><a href="#org206ac9d">Alternative derivation</a></li>
<li><a href="#org6090171">Maximizing non-Gaussian likelihoods</a></li>
<li><a href="#org9330ab3">Imputing missing values</a></li>
</ul>
</li>
<li><a href="#org23e1a28">Results</a>
<ul>
<li><a href="#org9f2be86">Recovering low rank structure</a></li>
<li><a href="#org3ddbdfa">Imputing missing values</a></li>
<li><a href="#orgd780580">Explaining held out data</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-orgb49048c" class="outline-2">
<h2 id="orgb49048c">Introduction</h2>
<div class="outline-text-2" id="text-orgb49048c">
<p>
We are interested in solving the <i>weighted low-rank approximation problem</i>:
</p>

<p>
\[ \min_{\mathbf{Z}} \sum_{i,j} w_{ij} \left(x_{ij} - z_{ij} \right)^2 \]
</p>

<p>
where \(n \times p\) target matrix \(\mathbf{X}\) and \(n \times p\) weight
matrix \(\mathbf{W}\) are given, and \(\mathbf{Z}\) is constrained to some
rank.
</p>

<p>
Solving WLRA allows us to solve two main problems. First, we can learn low
rank structure in non-Gaussian data. Using Taylor expansion of non-Gaussian
likelihoods, we can rewrite the MLE of factor models as the solution to
WLRA. The key idea is that the Taylor expansion is performed around a
different value for each observation, naturally leading to an iterative
approach.
</p>

<p>
Prior work has tried to learn low rank structure to solve the <i>collaborative
filtering</i> problem: given user \(i\) interacted with item \(j\), predict what
other items (<a href="http://yifanhu.net/PUB/cf.pdf">Hu et al. 2001</a>). Low rank structure in this context corresponds
to learning patterns of user preferences (factors), and the specific
preferences of individual users (loadings). 
</p>

<p>
One special feature of the problem is that only <i>implicit feedback</i> is
assumed: we only have observations \(x_{ij} = 1\) when an interaction is
recorded, and \(x_{ij} = 0\) could either reflect negative feedback or
missing data. However, this assumption does not make sense for scRNA-Seq
data, where observations of 0 are not missing.
</p>

<p>
The second problem we can solve usign LRA is true imputation. By setting
weights to zero, we can code missing data. This approach works even in
settings where observations can also take the value zero, such as single cell
RNA sequencing data.
</p>

<p>
The methods are implemented in the Python package <a href="https://www.github.com/aksarkar/wlra">wlra</a>.
</p>
</div>
</div>

<div id="outline-container-org3822917" class="outline-2">
<h2 id="org3822917">Methods</h2>
<div class="outline-text-2" id="text-org3822917">
</div>
<div id="outline-container-orgc964f6b" class="outline-3">
<h3 id="orgc964f6b">EM algorithm</h3>
<div class="outline-text-3" id="text-orgc964f6b">
<p>
<a href="https://www.aaai.org/Papers/ICML/2003/ICML03-094.pdf">Srebro and Jaakkola 2003</a> propose an EM algorithm to solve WLRA. The algorithm
is EM in the following sense: suppose the weights \(w_{ij} \in \{0, 1\}\),
corresponding to presence/absence, and suppose \(\mathbf{X} = \mathbf{Z} +
   \mathbf{E}\), where \(\mathbf{Z}\) is low-rank and elements of \(\mathbf{E}\)
are Gaussian.
</p>

<p>
Then, \(E[x_{ij} \mid w_{ij} = 0] = z_{ij}\), naturally giving an EM
algorithm. The E-step fills in \(x_{ij}\) with \(z_{ij}\), and the M-step
estimates \(\mathbf{Z}\) from the filled in \(\mathbf{X}\). The solution to
the M-step is given by the optimal unweighted rank \(k\) approximation,
i.e. truncated SVD, because all the non-zero weights are equal to 1.
</p>

<p>
Conceptually, the method for arbitrary weights is to suppose instead that we
have rational \(w_{ij} \in \{0, 1/N, \ldots, 1\}\). 
</p>

<p>
Then, we can reduce this problem to a problem in 0/1 weights by supposing we
have \(X^{(k)} = Z + E^{(k)}\), \(k \in 1, \ldots, N\), and each entry is
observed in only \(N w_{ij}\) of the \(X^{(k)}\).
</p>

<p>
Then, the M-step becomes:
</p>

<p>
\[ \mathbf{Z}^{(t + 1)} = \mathrm{LRA}_k(\mathbf{W} \circ \mathbf{X} +
   (\mathbf{1} - \mathbf{W}) \circ \mathbf{Z}^{(t)}) \]
</p>

<p>
where \(\mathrm{LRA}_k\) is the unweighted rank \(k\) approximation and
\(\circ\) denotes Hadamard product.
</p>

<p>
Intuitively, the implied E-step corresponds to taking the expectation of
\(z_{ij}^{(k)}\) over the targets \(k\). Clearly, the algorithm generalizes
to any weight matrix where \(0 \leq w_{ij} \leq 1\) are stored in finite
precision.
</p>

<p>
The weights must be constrained to be between zero and one in the EM
algorithm, so we scale them by the maximum weight.
</p>
</div>
</div>

<div id="outline-container-org206ac9d" class="outline-3">
<h3 id="org206ac9d">Alternative derivation</h3>
<div class="outline-text-3" id="text-org206ac9d">
<p>
Suppose \(\mathbf{X} = \mathbf{M} + \mathbf{Z} + \mathbf{E}\) where
\(\mathbf{M}\) is low rank, and:
</p>

<p>
\[ z_{ij} \sim N(0, s_{ij}^2) \]
</p>

<p>
\[ e_{ij} \sim N(0, \tau^{-1}) \]
</p>

<p>
Let \(\mathbf{R} = \mathbf{X} - \mathbf{M}\). Then:
</p>

<p>
\[ r_{ij} \mid z_{ij} \sim N(r_{ij}, \tau^{-1}) \]
</p>

<p>
\[ z_{ij} \mid \cdot \sim N(\mu_1, \tau_1^{-1}) \]
</p>

<p>
\[ \mu_1 = r_{ij} \tau / \tau_1 \]
</p>

<p>
\[ \tau_1 = \tau + 1 / s_{ij}^2 \]
</p>

<p>
Therefore,
</p>

<p>
\[ E[z_{ij} \mid x_{ij}, m_{ij}, s_{ij}] = \frac{\tau}{\tau + 1 /
   s_{ij}^2} (x_{ij} - m_{ij}) = w_{ij} (x_{ij} - m_{ij}) \]
</p>

<p>
and the solution to the E step is:
</p>

<p>
\[ \mathbf{Z}^{(t + 1)} = \mathbf{W} \circ (\mathbf{X} - \mathbf{M}^{(t)}) \]
</p>

<p>
Given \(\mathbf{Z}^{(t)}\), the solution to the M step is PCA with
homoscedastic errors, i.e. truncated SVD.
</p>

<p>
\[ \mathbf{M}^{(t + 1)} = \mathrm{LRA}_k(\mathbf{X} - \mathbf{Z}) \]
</p>

<p>
\[ = \mathrm{LRA}_k((1 - \mathbf{W}) \circ \mathbf{X} + \mathbf{W} \circ
   \mathbf{M^{(t)}}) \]
</p>

<p>
To recover the algorithm of Srebro and Jaakkola, we make several
observations:
</p>

<ol class="org-ol">
<li>Taking \(\tau \rightarrow 0\), we have \(w_{ij} = 1 / s_{ij}^2\).</li>
<li>To mark missing values, we set \(s_{ij}^2 = \infty\), in which case
\(w_{ij} = 1\), and the M-step takes the value of \(m_{ij}\) instead of
\(x_{ij}\).</li>
<li>Srebro and Jaakkola explicitly require weights to be between zero and
one; this derivation only suggests that they should be scaled (because
the M step involves the form of a linear interpolation).</li>
</ol>
</div>
</div>

<div id="outline-container-org6090171" class="outline-3">
<h3 id="org6090171">Maximizing non-Gaussian likelihoods</h3>
<div class="outline-text-3" id="text-org6090171">
<p>
Suppose \(l(\theta) = \ln p(x \mid \theta)\). Then, taking second-order
Taylor expansion about \(\theta_0\):
</p>

<p>
\[ l(\theta) \approx l(\theta_0) + (\theta - \theta_0)\,l'(\theta_0) +
   \frac{(\theta - \theta)^2}{2}\,l''(\theta_0) \]
</p>

<p>
\[ = \frac{l''(\theta_0)}{2} \left[ \theta - \left(\theta_0 -
   \frac{l'(\theta_0)}{l''(\theta_0)}\right)\right]^2 + \mathrm{const}\]
</p>

<p>
where the constant does not depend on \(\theta\). Now, maximizing the
objective is equivalent to maximizing a Gaussian likelihood:
</p>

<p>
\[= \mathcal{N}\left(\theta; \theta_0 -
   \frac{l'(\theta_0)}{l''(\theta_0)}, -\frac{1}{l''(\theta_0)}\right) +
   \mathrm{const} \]
</p>

<p>
Equivalently, minimizing the negative of the objective function is
WLRA. This result makes sense because for fixed \(\sigma^2\), maximizing the
Gaussian likelihood is the same as minimizing the Frobenius norm.
</p>

<p>
The result suggests we can improve \(l\) by optimizing this objective
instead, and suggests an iterative algorithm where we alternate updates
between \(\theta_0\) and \(\theta\).
</p>

<p>
Because this new objective is written in terms of derivatives of the log
likelihood, we can readily write down the required quantities for relevant
distributions:
</p>

<table class="table text-left">


<colgroup>
<col  class="org-left">

<col  class="org-left">

<col  class="org-left">

<col  class="org-left">
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Distribution</th>
<th scope="col" class="org-left">Parameter \(\theta\)</th>
<th scope="col" class="org-left">Target (mean)</th>
<th scope="col" class="org-left">Weight (precision)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Bernoulli</td>
<td class="org-left">\(\mathrm{logit}(p)\)</td>
<td class="org-left">\(\theta + \frac{x - S(\theta)}{S(\theta)(1 - S(\theta))}\)</td>
<td class="org-left">\(\frac{1}{S(\theta)(1 - S(\theta))}\)</td>
</tr>

<tr>
<td class="org-left">Poisson</td>
<td class="org-left">\(\ln\lambda\)</td>
<td class="org-left">\(1 + \theta + x\exp(-\theta)\)</td>
<td class="org-left">\(\exp(-\theta)\)</td>
</tr>
</tbody>
</table>

<p>
where \(S(\cdot)\) denotes the sigmoid function.
</p>
</div>
</div>

<div id="outline-container-org9330ab3" class="outline-3">
<h3 id="org9330ab3">Imputing missing values</h3>
<div class="outline-text-3" id="text-org9330ab3">
<p>
The EM algorithm for WLRA supports missing values by setting \(s_{ij}^2 =
   \infty\).
</p>

<p>
When maximizing non-Gaussian likelihoods using WLRA, we need to introduce
external weights \(\tilde{w}_{ij} \in \{0, 1\}\) to denote
presence/absence. We can do this by incorporating the weights
\(\tilde{\mathbf{W}} \circ \mathbf{W}\) in each outer iteration.
</p>
</div>
</div>
</div>

<div id="outline-container-org23e1a28" class="outline-2">
<h2 id="org23e1a28">Results</h2>
<div class="outline-text-2" id="text-org23e1a28">
</div>
<div id="outline-container-org9f2be86" class="outline-3">
<h3 id="org9f2be86">Recovering low rank structure</h3>
<div class="outline-text-3" id="text-org9f2be86">
<p>
We first consider the problem of recovering a planted low rank matrix after
convolving with Gaussian noise, assuming we know the rank.
</p>

<p>
\[ l_{ik} \sim \mathcal{N}(0, 1) \]
\[ f_{kj} \sim \mathcal{N}(0, 1) \]
\[ \mu_{ij} = (\mathbf{l}\mathbf{f})_{ij} \]
\[ \sigma^2_{ij} \sim \mathrm{Uniform}(1, \sigma^2_0) \]
\[ x_{ij} \sim \mathcal{N}(\mu_{ij}, \sigma^2_{ij}) \]
</p>

<p>
We assume the noise variances for each observation are known, and use the
inverse variances as the weights.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="org57c9be9"><span class="org-keyword">def</span> <span class="org-function-name">wnorm</span>(x, w):
  <span class="org-keyword">return</span> (w * np.square(x)).<span class="org-builtin">sum</span>()

<span class="org-keyword">def</span> <span class="org-function-name">simulate_gaussian</span>(n, p, rank, s0=10, seed=0):
  np.random.seed(seed)
  <span class="org-variable-name">l</span> = np.random.normal(size=(n, rank))
  <span class="org-variable-name">f</span> = np.random.normal(size=(rank, p))
  <span class="org-variable-name">eta</span> = l.dot(f)
  <span class="org-variable-name">noise</span> = np.random.uniform(1, s0, size=eta.shape)
  <span class="org-variable-name">w</span> = 1 / noise
  <span class="org-variable-name">x</span> = np.random.normal(loc=eta, scale=noise)
  <span class="org-keyword">return</span> x, w, eta

<span class="org-keyword">def</span> <span class="org-function-name">rrmse</span>(pred, true):
  <span class="org-keyword">return</span> np.sqrt(np.linalg.norm(pred - true) / np.linalg.norm(true))

<span class="org-keyword">def</span> <span class="org-function-name">score_wlra</span>(x, w, eta, rank):
  <span class="org-variable-name">res</span> = wlra.wlra(x, w, rank=rank, max_iters=1000)
  <span class="org-keyword">return</span> rrmse(res, eta)

<span class="org-keyword">def</span> <span class="org-function-name">score_lra</span>(x, eta, rank):
  <span class="org-variable-name">res</span> = wlra.lra(x, rank)
  <span class="org-keyword">return</span> rrmse(res, eta)

<span class="org-keyword">def</span> <span class="org-function-name">score_hsvd</span>(x, w, eta, rank):
  <span class="org-variable-name">res</span> = wlra.hsvd(x, w, rank=rank, max_iters=1000)
  <span class="org-keyword">return</span> rrmse(res, eta)

<span class="org-keyword">def</span> <span class="org-function-name">evaluate_gaussian_known_rank</span>(num_trials=10):
  <span class="org-variable-name">result</span> = []
  <span class="org-keyword">for</span> trial <span class="org-keyword">in</span> <span class="org-builtin">range</span>(num_trials):
    <span class="org-variable-name">x</span>, <span class="org-variable-name">w</span>, <span class="org-variable-name">eta</span> = simulate_gaussian(n=100, p=1000, rank=1, s0=10, seed=trial)
    result.append([trial,
                   score_lra(x, eta, rank=1),
                   score_wlra(x, w, eta, rank=1),
                   score_hsvd(x, 1 / w, eta, rank=1)])
  <span class="org-variable-name">result</span> = pd.DataFrame(result)
  <span class="org-variable-name">result.columns</span> = [<span class="org-string">'trial'</span>, <span class="org-string">'LRA'</span>, <span class="org-string">'WLRA'</span>, <span class="org-string">'HSVD'</span>]
  <span class="org-keyword">return</span> result
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">&lt;&lt;imports&gt;&gt;
&lt;&lt;gaussian-reconstruction&gt;&gt;
<span class="org-variable-name">res</span> = evaluate_gaussian_known_rank(num_trials=100)
res.to_csv(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-ideas/data/wlra/gaussian-known-rank.txt.gz'</span>, compression=<span class="org-string">'gzip'</span>, sep=<span class="org-string">'\t'</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-sh">sbatch --partition=broadwl --job-name=gaussian-wlra
<span class="org-comment-delimiter">#</span><span class="org-comment">!/bin/bash</span>
<span class="org-builtin">source</span> activate singlecell
python /project2/mstephens/aksarkar/projects/singlecell-ideas/code/gaussian-known-rank.py
</pre>
</div>

<pre class="example">
Submitted batch job 50149976

</pre>

<p>
Read the results.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">results_gaussian_known_rank</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-ideas/data/wlra/gaussian-known-rank.txt.gz'</span>, index_col=0)
</pre>
</div>

<p>
Plot the performance.
</p>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
plt.gcf().set_size_inches(3, 3)
plt.boxplot(x=results_gaussian_known_rank[[<span class="org-string">'LRA'</span>, <span class="org-string">'WLRA'</span>]].T, positions=<span class="org-builtin">range</span>(2), medianprops={<span class="org-string">'color'</span>: <span class="org-string">'k'</span>})
plt.xticks(<span class="org-builtin">range</span>(2), [<span class="org-string">'LRA'</span>, <span class="org-string">'WLRA'</span>])
plt.xlabel(<span class="org-string">'Method'</span>)
<span class="org-variable-name">_</span> = plt.ylabel(<span class="org-string">'RRMSE'</span>)
</pre>
</div>


<div class="figure">
<p><img src="figure/wlra.org/gaussian-known-rank.png" alt="gaussian-known-rank.png">
</p>
</div>
</div>
</div>

<div id="outline-container-org3ddbdfa" class="outline-3">
<h3 id="org3ddbdfa">Imputing missing values</h3>
<div class="outline-text-3" id="text-org3ddbdfa">
<p>
We generate a Poisson data matrix with planted row rank structure.
</p>

<p>
\[ \eta_{ij} = (\mathbf{l f})_{ij} \]
\[ x_{ij} \sim \mathrm{Poisson}(\exp(\eta_{ij})) \]
</p>

<p>
We then hold out entries using numpy masked arrays, and impute them using
the Poisson low rank approximation. We compare the imputation accuracy
against non-negative matrix factorization (<a href="https://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf">Lee and Seung 2001</a>,
<a href="https://arxiv.org/abs/1010.1763">Févotte and Idier 2011</a>). We consider several loss functions:
</p>

<ul class="org-ul">
<li>Frobenius norm (\(\beta=2\) divergence)</li>
<li>KL divergence (after normalizing the factors and loadings; \(\beta=1\)
divergence)</li>
<li>\(\beta=0.5\) divergence</li>
</ul>

<div class="org-src-container">
<pre class="src src-ipython" id="orgc702bc9"><span class="org-keyword">def</span> <span class="org-function-name">simulate_pois</span>(n, p, rank, seed=0):
  np.random.seed(seed)
  <span class="org-variable-name">l</span> = np.random.normal(size=(n, rank))
  <span class="org-variable-name">f</span> = np.random.normal(size=(rank, p))
  <span class="org-variable-name">eta</span> = l.dot(f)
  <span class="org-variable-name">x</span> = np.random.poisson(lam=np.exp(eta))
  <span class="org-keyword">return</span> x, eta

<span class="org-keyword">def</span> <span class="org-function-name">rmse</span>(pred, true):
  <span class="org-keyword">return</span> np.sqrt(np.square(pred - true).mean())

<span class="org-keyword">def</span> <span class="org-function-name">imputation_score_nmf</span>(x, rank, **kwargs):
  <span class="org-doc">"""Non-negative matrix factorization"""</span>
  <span class="org-keyword">if</span> <span class="org-string">'max_iter'</span> <span class="org-keyword">not</span> <span class="org-keyword">in</span> kwargs:
    <span class="org-variable-name">kwargs</span>[<span class="org-string">'max_iter'</span>] = 1000
  <span class="org-keyword">if</span> <span class="org-string">'tol'</span> <span class="org-keyword">not</span> <span class="org-keyword">in</span> kwargs:
    <span class="org-variable-name">kwargs</span>[<span class="org-string">'tol'</span>] = 1e-3
  <span class="org-keyword">if</span> <span class="org-string">'init'</span> <span class="org-keyword">not</span> <span class="org-keyword">in</span> kwargs:
    <span class="org-variable-name">kwargs</span>[<span class="org-string">'init'</span>] = <span class="org-string">'random'</span>
  <span class="org-variable-name">m</span> = skd.NMF(n_components=rank, **kwargs).fit(x)
  <span class="org-variable-name">res</span> = m.transform(x).dot(m.components_)
  <span class="org-keyword">return</span> rmse(res[x.mask], x.data[x.mask])

<span class="org-keyword">def</span> <span class="org-function-name">imputation_score_pois_lra</span>(x, rank):
  <span class="org-keyword">try</span>:
    <span class="org-variable-name">res</span> = wlra.pois_lra(x, rank=rank, max_outer_iters=50)
    <span class="org-keyword">return</span> rmse(res[x.mask], x.data[x.mask])
  <span class="org-keyword">except</span> <span class="org-type">RuntimeError</span>:
    <span class="org-keyword">return</span> np.nan

<span class="org-keyword">def</span> <span class="org-function-name">evaluate_pois_imputation</span>(rank=3, num_trials=10):
  <span class="org-variable-name">result</span> = []
  <span class="org-keyword">for</span> trial <span class="org-keyword">in</span> <span class="org-builtin">range</span>(num_trials):
    <span class="org-variable-name">x</span>, <span class="org-variable-name">eta</span> = simulate_pois(n=200, p=300, rank=rank, seed=trial)
    <span class="org-variable-name">mask</span> = np.random.uniform(size=(200, 300)) &lt; 0.25
    <span class="org-variable-name">x</span> = np.ma.masked_array(x, mask=mask)
    result.append([trial,
                   imputation_score_nmf(x, rank, beta_loss=<span class="org-string">'frobenius'</span>),
                   imputation_score_nmf(x, rank, solver=<span class="org-string">'mu'</span>, beta_loss=<span class="org-string">'kullback-leibler'</span>),
                   imputation_score_nmf(x, rank, solver=<span class="org-string">'mu'</span>, beta_loss=0.5),
                   imputation_score_pois_lra(x, rank)])
  <span class="org-variable-name">result</span> = pd.DataFrame(result)
  <span class="org-variable-name">result.columns</span> = [<span class="org-string">'trial'</span>, <span class="org-string">'Poisson LRA'</span>, <span class="org-string">'NMF-Frob'</span>, <span class="org-string">'NMF-KL'</span>, <span class="org-string">'NMF-&#946;=0.5'</span>]
  <span class="org-keyword">return</span> result
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">&lt;&lt;imports&gt;&gt;
&lt;&lt;poisson-imputation&gt;&gt;
<span class="org-variable-name">res</span> = evaluate_pois_imputation(num_trials=100)
res.to_csv(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-ideas/data/wlra/pois-imputation.txt.gz'</span>, compression=<span class="org-string">'gzip'</span>, sep=<span class="org-string">'\t'</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-sh">sbatch --partition=broadwl --job-name=pois-imputation
<span class="org-comment-delimiter">#</span><span class="org-comment">!/bin/bash</span>
<span class="org-builtin">source</span> activate singlecell
python /project2/mstephens/aksarkar/projects/singlecell-ideas/code/pois-imputation.py
</pre>
</div>

<pre class="example">
Submitted batch job 50209831

</pre>

<p>
Read the results.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">results_pois_imputation</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-ideas/data/wlra/pois-imputation.txt.gz'</span>, index_col=0)
</pre>
</div>

<p>
Plot the performance.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">T</span> = results_pois_imputation.dropna()

plt.clf()
plt.gcf().set_size_inches(3, 3)
plt.yscale(<span class="org-string">'symlog'</span>, linthreshy=10)
<span class="org-variable-name">keys</span> = [<span class="org-string">'NMF-Frob'</span>, <span class="org-string">'NMF-KL'</span>, <span class="org-string">'NMF-&#946;=0.5'</span>]
<span class="org-keyword">for</span> i, k <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(keys):
  <span class="org-variable-name">y</span> = T[k] - T[<span class="org-string">'Poisson LRA'</span>]
  <span class="org-variable-name">f</span> = st.gaussian_kde(y)
  <span class="org-variable-name">py</span> = f(y)
  <span class="org-variable-name">x</span> = i + .1 / py.<span class="org-builtin">max</span>() * np.random.uniform(-py, py)
  plt.scatter(x, y, c=<span class="org-string">'k'</span>, s=4)
plt.axhline(y=0, c=<span class="org-string">'k'</span>, ls=<span class="org-string">':'</span>, lw=1)
plt.xticks(<span class="org-builtin">range</span>(<span class="org-builtin">len</span>(keys)), keys)
plt.xlabel(<span class="org-string">'Method'</span>)
<span class="org-variable-name">_</span> = plt.ylabel(<span class="org-string">'Difference in RMSE (Poisson LRA)'</span>)
</pre>
</div>


<div class="figure">
<p><img src="figure/wlra.org/pois-imputation.png" alt="pois-imputation.png">
</p>
</div>
</div>
</div>

<div id="outline-container-orgd780580" class="outline-3">
<h3 id="orgd780580">Explaining held out data</h3>
<div class="outline-text-3" id="text-orgd780580">
<p>
A number of methods have been proposed to estimate low rank structure from
count data, including scRNA-Seq data:
</p>

<ul class="org-ul">
<li>Non-negative matrix factorization (<a href="https://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf">Lee and Seung 2001</a>, <a href="https://arxiv.org/abs/1010.1763">Févotte and Idier 2011</a>)</li>
<li>Hierarchical Bayesian Poisson Factorization (<a href="http://www.cs.columbia.edu/~blei/papers/GopalanHofmanBlei2015.pdf">Gopalan et al. 2015</a>)</li>
<li>ZINB-WAVE (<a href="https://www.nature.com/articles/s41467-017-02554-5">Risso et al. 2018</a>)</li>
<li>scVI (<a href="https://www.biorxiv.org/content/early/2018/03/30/292037">Lopez et al 2018</a>)</li>
</ul>

<p>
These methods typically make the <i>implicit feedback</i> assumption of
collaborative filtering, and treat zeros in the input data as missing
data. Clearly, this assumption does not hold for scRNA-Seq data, and
therefore we cannot evaluate all of the methods on the true imputation task
above.
</p>

<p>
Instead, we evaluate the methods on their ability to generalize to new
data. We assume:
</p>

<p>
\[ x_{ij} \sim \mathrm{Poisson}(\lambda_{ij}) \]
</p>

<p>
and hold out molecules by randomly thinning the observed counts:
</p>

<p>
\[ y_{ij} \sim \mathrm{Binomial}(x_{ij}, 0.5) \]
</p>

<p>
\[ \tilde{y}_{ij} = x_{ij} - y_{ij} \]
</p>

<p>
This approach leaves the relative abundance of the transcripts unchanged in
expectation, implying that the low rank structure learned in \(\mathbf{Y}\)
should explain the data in \(\tilde\mathbf{Y}\). Our metric is then the
likelihood of the data under the Poisson likelihood.
</p>

<div class="org-src-container">
<pre class="src src-ipython">
</pre>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Abhishek Sarkar</p>
<p class="date">Created: 2018-09-27 Thu 20:11</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
