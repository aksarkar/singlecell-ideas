#+TITLE: Model-based clustering of scRNA-seq data
#+SETUPFILE: setup.org

* Introduction
  :PROPERTIES:
  :CUSTOM_ID: introdcution
  :END:

  Two major strategies for clustering scRNA-seq data are:

  1. Building a \(k\)-nearest neighbor graph on the data, and applying a
     community detection algorithm (e.g.,
     [[https://doi.org/10.1088/1742-5468/2008/10/P10008][Blondel et al. 2008]],
     [[https://arxiv.org/abs/1810.08473][Traag et al. 2018]])
  2. Fitting a topic model to the data
     (e.g., [[https://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1006599][Dey
     et al. 2017]],
     [[https://www.nature.com/articles/s41592-019-0367-1][Gonzáles-Blas et
     al. 2019]])

  The main disadvantage of strategy (1) is that, as commonly applied to
  transformed counts, it does not separate measurement error and biological
  variation of interest. The main disadvantage of strategy (2) is that it does
  not account for transcriptional noise
  ([[https://doi.org/10.1016/j.cell.2008.09.050][Raj 2008]]). Here, we develop
  a simple model-based clustering algorithm which addresses both of these
  issues.

* Setup
  :PROPERTIES:
  :CUSTOM_ID: setup
  :END:

  #+BEGIN_SRC emacs-lisp :exports none
    (add-to-list 'python-shell-completion-native-disabled-interpreters "jupyter")
    (org-babel-lob-ingest "/home/aksarkar/.emacs.d/org-templates/library.org")
  #+END_SRC

  #+RESULTS:
  : 2

  #+CALL: ipython3(venv="singlecell",partition="mstephens",memory="16G") :exports none :dir /scratch/midway2/aksarkar/singlecell/

  #+CALL: ipython3(venv="singlecell",partition="gpu2",opts="--gres=gpu:1",memory="16G") :exports none :dir /scratch/midway2/aksarkar/singlecell/

  #+CALL: tensorboard(venv="singlecell") :session tensorboard :dir /scratch/midway2/aksarkar/singlecell/

  #+BEGIN_SRC ipython
    import anndata
    import mpebpm.gam_mix
    import mpebpm.sgd
    import numpy as np
    import pandas as pd
    import scanpy as sc
    import scipy.optimize as so
    import scipy.special as sp
    import scipy.stats as st
    import scmodes
    import time
    import torch
    import torch.utils.data as td
    import umap
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[1]:
  :END:

  #+BEGIN_SRC ipython
    %matplotlib inline
    %config InlineBackend.figure_formats = set(['retina'])
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[2]:
  :END:

  #+BEGIN_SRC ipython
    import colorcet
    import matplotlib
    import matplotlib.pyplot as plt
    plt.rcParams['figure.facecolor'] = 'w'
    plt.rcParams['font.family'] = 'Nimbus Sans'
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[3]:
  :END:

* Methods
  :PROPERTIES:
  :CUSTOM_ID: method
  :END:
** Model specification
   :PROPERTIES:
   :CUSTOM_ID: model
   :END:

   We assume \(
   \DeclareMathOperator\Gam{Gamma}
   \DeclareMathOperator\Mult{Multinomial}
   \DeclareMathOperator\N{\mathcal{N}}
   \DeclareMathOperator\Pois{Poisson}
   \DeclareMathOperator\diag{diag}
   \DeclareMathOperator\KL{\mathcal{KL}}
   \newcommand\kl[2]{\KL(#1\;\Vert\;#2)}
   \newcommand\xiplus{x_{i+}}
   \newcommand\mi{\mathbf{I}}
   \newcommand\vu{\mathbf{u}}
   \newcommand\vx{\mathbf{x}}
   \newcommand\vz{\mathbf{z}}
   \newcommand\vlambda{\boldsymbol{\lambda}}
   \newcommand\vmu{\boldsymbol{\mu}}
   \newcommand\vphi{\boldsymbol{\phi}}
   \newcommand\vpi{\boldsymbol{\pi}}
   \)

   \begin{align}
     x_{ij} \mid \xiplus, \lambda_{ij} &\sim \Pois(\xiplus \lambda_{ij})\\
     \lambda_{ij} \mid \vpi_i, \vmu_k, \vphi_k &\sim \sum_{k=1}^{K} \pi_{ik} \Gam(\phi_{kj}^{-1}, \phi_{kj}^{-1}\mu_{kj}^{-1}),
   \end{align}

   where

   - \(x_{ij}\) denotes the number of molecules of gene \(j = 1, \ldots, p\)
     observed in cell \(i = 1, \ldots, n\)
   - \(\xiplus \triangleq \sum_j x_{ij}\) denotes the total number of molecules
     observed in cell \(i\)
   - \(\vpi_i\) denotes cluster assignment probabilities for cell \(i\)
   - \(\vmu_k\) denotes the cluster "centroid" for cluster \(k\), and
     \(\vphi_k\) describes stochastic perturbations within each cluster

   The intuition behind this model is that each cluster \(k\) is defined by a
   collection of independent Gamma distributions (parameterized by shape and
   rate), one per gene \(j\), which describe the distribution of true gene
   expression for each gene in each cluster
   ([[https://dx.doi.org/10.1101/2020.04.07.030007][Sarkar and Stephens
   2020]]). In this parameterization, each Gamma distribution has mean
   \(\mu_{kj}\) and variance \(\mu_{kj}^2\phi_{kj}\). Under this model, the
   marginal likelihood is a mixture of negative binomials

   \begin{equation}
     p(x_{ij} \mid \xiplus, \vpi_i, \vmu_k, \vphi_k) = \sum_{k=1}^{K} \pi_{ik} \frac{\Gamma(x_{ij} + 1 / \phi_{kj})}{\Gamma(1 / \phi_{kj})\Gamma(x_{ij} + 1)}\left(\frac{\xiplus\mu_{kj}\phi_{kj}}{1 + \xiplus\mu_{kj}\phi_{kj}}\right)^{x_{ij}} \left(\frac{1}{1 + \xiplus\mu_{kj}\phi_{kj}}\right)^{1/\phi_{kj}}.
   \end{equation}

** EM for Poisson--Gamma-mixture
   :PROPERTIES:
   :CUSTOM_ID: em-nbmix
   :END:

   We can estimate \(\vpi, \vmu, \vphi\) by maximizing the likelihood using an
   EM algorithm. Letting \(z_{ik} \in \{0, 1\}\) indicate whether cell \(i\) is
   assigned to cluster \(k\), the exact posterior

   \begin{align}
     q(z_{i1}, \ldots, z_{iK}) &\triangleq p(z_{ik} \mid \cdot) = \Mult(1, \alpha_{i1}, \ldots, \alpha_{iK})\\
     \alpha_{ik} &\propto \sum_j \frac{\Gamma(x_{ij} + 1 / \phi_{kj})}{\Gamma(1 / \phi_{kj})\Gamma(x_{ij} + 1)}\left(\frac{\xiplus\mu_{kj}\phi_{kj}}{1 + \xiplus\mu_{kj}\phi_{kj}}\right)^{x_{ij}} \left(\frac{1}{1 + \xiplus\mu_{kj}\phi_{kj}}\right)^{1/\phi_{kj}}.
   \end{align}

   The expected log joint probability with respect to \(q\)

   \begin{multline}
     E_q[\ln p(x_{ij}, z_{ik} \mid \xiplus, \vpi_i, \vmu_k, \vphi_k)] = E_q[z_{ik}] \Biggl[\ln \pi_{ik} + x_{ij} \ln\left(\frac{\xiplus\mu_{kj}\phi_{kj}}{1 + \xiplus\mu_{kj}\phi_{kj}}\right)\\
       - \phi_{kj}^{-1} \ln(1 + \xiplus\mu_{kj}\phi_{kj}) + \ln\Gamma(x_{ij} + 1 / \phi_{kj}) - \ln\Gamma(1 / \phi_{kj}) - \ln\Gamma(x_{ij} + 1)\Biggr].
   \end{multline}

   In the E step, the necessary expectations are analytic. In the M step, we
   can improve the expected log joint probability, e.g. by
   [[file:mepbpm.org][(batch) gradient descent]].

** Amortized inference
   :PROPERTIES:
   :CUSTOM_ID: ai
   :END:

   An alternative algorithm, which is amenable to stochastic gradient descent
   and online learning, is to use the fact that EM can be viewed as maximizing
   the evidence lower bound
   ([[https://doi.org/10.1007/978-94-011-5014-9_12][Neal and Hinton 1998]])

   \begin{align}
     \max_{\theta} \ln p(x \mid \theta) &= \max_{q, \theta} \ln p(x \mid \theta) - \kl{q(z)}{p(z \mid x, \theta)}\\
     &= \max_{q, \theta} E_q[\ln p(x \mid z, \theta)] - \kl{q(z)}{p(z \mid \theta)}.
   \end{align}

   Exact EM corresponds to (fully) alternately optimizing \(q^* = p(z \mid x,
   \theta)\) and \(\theta\). However, we can instead amortize inference
   ([[https://escholarship.org/content/qt34j1h7k5/qt34j1h7k5.pdf][Gershman and
   Goodman 2014]], [[https://arxiv.org/abs/1312.6114][Kingma and Welling
   2014]], [[http://proceedings.mlr.press/v32/rezende14.html][Rezende et
   al. 2014]]), estimating a variational approximation parameterized by a
   neural network \(f_z\) mapping \(\vx_i \rightarrow \vz_i\)

   \begin{align}
     p(z_{i1}, \ldots, z_{iK} \mid \vpi) &= \Mult(1, \vpi)\\
     q(z_{i1}, \ldots, z_{iK} \mid \vx_i) &= \Mult(1, f_z(\vx_i)).
   \end{align}

   The evidence lower bound is analytic

   \begin{multline}
     \mathcal{L} = \sum_{i, k} (f_z(\vx_i))_k \Biggl[\ln\left(\frac{\pi_{ik}}{(f_z(\vx_i))_k}\right) + \sum_j \biggl[ x_{ij} \ln\left(\frac{\xiplus\mu_{kj}\phi_{kj}}{1 + \xiplus\mu_{kj}\phi_{kj}}\right) - \phi_{kj}^{-1} \ln(1 + \xiplus\mu_{kj}\phi_{kj})\\
         + \ln\Gamma(x_{ij} + 1 / \phi_{kj}) - \ln\Gamma(1 / \phi_{kj}) - \ln\Gamma(x_{ij} + 1)\biggr]\Biggr],
   \end{multline}

   and can be optimized using SGD.

* Results
  :PROPERTIES:
  :CUSTOM_ID: results
  :END:
** Real data example
   :PROPERTIES:
   :CUSTOM_ID: example
   :END:

   Read sorted immune cell scRNA-seq data
   ([[https://dx.doi.org/10.1038/ncomms14049][Zheng et al. 2017]]).

   #+BEGIN_SRC ipython :async t
     dat = anndata.read_h5ad('/scratch/midway2/aksarkar/ideas/zheng-10-way.h5ad')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[4]:
   :END:

   Get 256 B cells and 256 cytotoxic T cells.

   #+BEGIN_SRC ipython :async t
     b_cells = dat[dat.obs['cell_type'] == 'b_cells']
     sc.pp.subsample(b_cells, n_obs=256, random_state=0)
     t_cells = dat[dat.obs['cell_type'] == 'cytotoxic_t']
     sc.pp.subsample(t_cells, n_obs=256)
     temp = b_cells.concatenate(t_cells)
     sc.pp.filter_genes(temp, min_counts=1)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   :END:

   Plot a UMAP embedding of the data, coloring points by the ground truth
   labels.

   #+BEGIN_SRC ipython :async t
     sc.pp.pca(temp)
     sc.pp.neighbors(temp)
     sc.tl.umap(temp)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[6]:
   :END:

   Write out the estimated embedding.

   #+BEGIN_SRC ipython
     temp.write('/scratch/midway2/aksarkar/singlecell/nbmix-example.h5ad')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[7]:
   :END:

   Read the annotated data.

   #+BEGIN_SRC ipython
     temp = anndata.read_h5ad('/scratch/midway2/aksarkar/singlecell/nbmix-example.h5ad')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[5]:
   :END:

   #+BEGIN_SRC ipython :ipyfile figure/nbmix.org/sim-ex.png
     cm = plt.get_cmap('Dark2')
     plt.clf()
     plt.gcf().set_size_inches(3, 3)
     for i, c in enumerate(temp.obs['cell_type'].unique()):
       plt.plot(*temp[temp.obs['cell_type'] == c].obsm["X_umap"].T, c=cm(i), marker='.', ms=2, lw=0, label=f'{c}')
     plt.legend(frameon=False, markerscale=4, handletextpad=0)
     plt.xlabel('UMAP 1')
     plt.ylabel('UMAP 2')
     plt.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[9]:
   [[file:figure/nbmix.org/sim-ex.png]]
   :END:

** Leiden algorithm
   :PROPERTIES:
   :CUSTOM_ID: leiden
   :END:

    Apply the Leiden algorithm ([[https://arxiv.org/abs/1810.08473][Traag et
    al. 2018]]) to the data (<1 s).

    #+BEGIN_SRC ipython :async t
      sc.tl.leiden(temp, random_state=0)
    #+END_SRC

    #+RESULTS:
    :RESULTS:
    # Out[10]:
    :END:

    #+BEGIN_SRC ipython :ipyfile figure/nbmix.org/leiden-ex.png
      cm = plt.get_cmap('Dark2')
      plt.clf()
      plt.gcf().set_size_inches(3, 3)
      for i, c in enumerate(temp.obs['leiden'].unique()):
        plt.plot(*temp[temp.obs['leiden'] == c].obsm["X_umap"].T, c=cm(i), marker='.', ms=2, lw=0, label=f'Cluster {i}')
      plt.legend(frameon=False, markerscale=4, handletextpad=0)
      plt.xlabel('UMAP 1')
      plt.ylabel('UMAP 2')
      plt.tight_layout()
    #+END_SRC

    #+RESULTS:
    :RESULTS:
    # Out[11]:
    [[file:figure/nbmix.org/leiden-ex.png]]
    :END:

** MPEBPM
   :PROPERTIES:
   :CUSTOM_ID: mpebpm
   :END:

   First, start from the ground truth \(z\) (labels), and estimate the Gamma
   expression models.

   #+BEGIN_SRC ipython :async t
     fit0 = mpebpm.sgd.ebpm_gamma(
       temp.X,
       onehot=pd.get_dummies(temp.obs['cell_type']).values,
       batch_size=32,
       num_epochs=320,
       shuffle=True,
       log_dir='runs/nbmix/pretrain/')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[33]:
   :END:

   #+BEGIN_SRC ipython
     np.savez('/scratch/midway2/aksarkar/singlecell/nbmix-example-pretrain.npz', fit0)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[39]:
   :END:

   #+BEGIN_SRC ipython
     with np.load('/scratch/midway2/aksarkar/singlecell/nbmix-example-pretrain.npz') as f:
       fit0 = f['arr_0']
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[47]:
   :END:

   #+BEGIN_SRC ipython
     y = pd.get_dummies(temp.obs['cell_type']).values
     s = temp.X.sum(axis=1)
     nb_llik = y.T @ st.nbinom(n=np.exp(y @ fit0[1]), p=1 / (1 + s.A * (y @ np.exp(fit0[0] - fit0[1])))).logpmf(temp.X.A)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[49]:
   :END:

   For comparison, estimate a point mass expression model for each gene, for
   each cluster.

   #+BEGIN_SRC ipython
     y = pd.get_dummies(temp.obs['cell_type']).values
     s = temp.X.sum(axis=1)
     fit_pois = (y.T @ temp.X) / (y.T @ s)
     pois_llik = y.T @ st.poisson(mu=s.A * (y @ fit_pois).A).logpmf(temp.X.A)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[65]:
   :END:

   For each gene, for each cluster, plot the log likelihood under the point
   mass and Gamma expression models.

   #+BEGIN_SRC ipython :ipyfile figure/nbmix.org/ex-pois-nb-llik.png
     plt.clf()
     fig, ax = plt.subplots(1, 2, sharey=True)
     fig.set_size_inches(4.5, 2.5)
     lim = [-1500, 0]
     for i, (a, t) in enumerate(zip(ax, ['B cell', 'Cytotoxic T'])):
       a.scatter(pois_llik[i], nb_llik[i], c='k', s=1, alpha=0.2)
       a.plot(lim, lim, c='r', lw=1, ls=':')
       a.set_xlim(lim)
       a.set_ylim(lim)
       a.set_title(t)
       a.set_xlabel('Poisson log lik')
     ax[0].set_ylabel('NB log lik')
     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[71]:
   [[file:figure/nbmix.org/ex-pois-nb-llik.png]]
   :END:

   Look at the differences in the estimated mean parameter for each gene, to
   see how many genes are informative about the labels.

   #+BEGIN_SRC ipython :ipyfile figure/nbmix.org/mepbpm-log-mu.png
     query = np.sort(np.diff(fit0[0], axis=0).ravel())
     plt.clf()
     plt.gcf().set_size_inches(4, 2)
     plt.plot(query, lw=1, c='k')
     plt.axhline(y=0, lw=1, ls=':', c='k')
     plt.xlabel('Gene')
     plt.ylabel(r'Diff $\ln(\mu_j)$')
     plt.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[29]:
   [[file:figure/nbmix.org/mepbpm-log-mu.png]]
   :END:

   Estimate the cluster weights.

   #+BEGIN_SRC ipython
     L = mpebpm.gam_mix._nb_mix_llik(
       x=torch.tensor(temp.X.A, dtype=torch.float), 
       s=torch.tensor(temp.X.sum(axis=1), dtype=torch.float),
       log_mean=torch.tensor(fit0[0], dtype=torch.float),
       log_inv_disp=torch.tensor(fit0[1], dtype=torch.float))
     zhat = torch.nn.functional.softmax(L, dim=1)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[19]:
   :END:

   Plot the log likelihood difference between the two components for each data
   point.

   #+BEGIN_SRC ipython :ipyfile figure/nbmix.org/mpebpm-llik-diff.png
     plt.clf()
     plt.gcf().set_size_inches(4, 2)
     plt.plot(np.diff(L).ravel(), lw=0, marker='.', c='k', ms=2)
     plt.axhline(y=0, lw=1, ls=':', c='k')
     plt.xlabel('Cell')
     plt.ylabel('Diff log lik')
     plt.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[42]:
   [[file:figure/nbmix.org/mpebpm-llik-diff.png]]
   :END:

   Compute the cross entropy between the estimated \(\hat{z}\) and the ground
   truth.

   #+BEGIN_SRC ipython
     torch.nn.functional.binary_cross_entropy(
       zhat,
       torch.tensor(pd.get_dummies(temp.obs['cell_type']).values, dtype=torch.float))
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[21]:
   : tensor(0.)
   :END:

   Compute a weighted log likelihood.

   #+BEGIN_SRC ipython
     w = torch.rand([512, 2])
     w /= w.sum(dim=1).unsqueeze(-1)
     m, _ = L.max(dim=1, keepdim=True)
     (m + torch.log(w * torch.exp(L - m) + 1e-8)).mean()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[22]:
   : tensor(-1872.7723)
   :END:

   Try fitting the model from a random initialization (4 s).

   #+BEGIN_SRC ipython :async t
     import imp; imp.reload(mpebpm.gam_mix)
     torch.manual_seed(0)
     fit = mpebpm.gam_mix.ebpm_gam_mix_em(
       x=temp.X.A,
       s=temp.X.sum(axis=1),
       y=torch.tensor(pd.get_dummies(temp.obs['cell_type']).values, dtype=torch.float).cuda(),
       k=2,
       num_epochs=50,
       max_em_iters=8,
       log_dir='runs/nbmix/mpebpm-random-init0-pois-50-8')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[30]:
   :END:

   Compare the log likelihood using the ground truth labels to the log
   likelihood using the estimated cluster weights.

   #+BEGIN_SRC ipython
     pd.Series({'ground_truth': nb_llik.mean(),
                'est': (fit[2].T @ st.nbinom(n=np.exp(fit[2] @ fit[1]), p=1 / (1 + s.A * (fit[2] @ np.exp(fit[0] - fit[1])))).logpmf(temp.X.A)).mean()})
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[50]:
   #+BEGIN_EXAMPLE
     ground_truth   -41.151726
     est            -41.145436
     dtype: float64
   #+END_EXAMPLE
   :END:

   Compute the cross entropy between the estimated \(\hat{z}\) and the ground
   truth.

   #+BEGIN_SRC ipython
     torch.min(
       torch.nn.functional.binary_cross_entropy(
         torch.tensor(fit[-1], dtype=torch.float),
         torch.tensor(pd.get_dummies(temp.obs['cell_type']).values, dtype=torch.float)),
       torch.nn.functional.binary_cross_entropy(
         torch.tensor(1 - fit[-1], dtype=torch.float),
         torch.tensor(pd.get_dummies(temp.obs['cell_type']).values, dtype=torch.float)))
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[51]:
   : tensor(0.)
   :END:

   Plot the UMAP, colored by the fitted clusters.

   #+BEGIN_SRC ipython :ipyfile figure/nbmix.org/sim-ex-fit.png
     cm = plt.get_cmap('Dark2')
     plt.clf()
     plt.gcf().set_size_inches(3, 3)
     for i in range(fit[-1].shape[1]):
       plt.plot(*temp[fit[-1][:,i].astype(bool)].obsm["X_umap"].T, c=cm(i), marker='.', ms=2, lw=0, label=f'Cluster {i}')
     plt.legend(frameon=False, markerscale=4, handletextpad=0)
     plt.xlabel('UMAP 1')
     plt.ylabel('UMAP 2')
     plt.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[52]:
   [[file:figure/nbmix.org/sim-ex-fit.png]]
   :END:

** Amortized inference
   :PROPERTIES:
   :CUSTOM_ID: amortized
   :END:

   Construct the amortized inference model, initializing \(\vmu_k, \vphi_k\)
   from the MLE starting from the ground-truth labels.

   #+BEGIN_SRC ipython
     query = torch.tensor(temp.X.A)
     s = torch.tensor(temp.X.sum(axis=1))
     fit = mpebpm.gam_mix.EBPMGammaMix(
       p=temp.shape[1],
       k=2,
       log_mean=fit0[0],
       log_inv_disp=fit0[1])
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[71]:
   :END:

   Look at the initial loss.

   #+BEGIN_SRC ipython
     (
       mpebpm.gam_mix._nb_mix_loss(
         fit.encoder.forward(query),
         query,
         s,
         fit.log_mean,
         fit.log_inv_disp),
       mpebpm.gam_mix._nb_mix_loss(
         torch.tensor(pd.get_dummies(temp.obs['cell_type']).values),
         query,
         s,
         fit.log_mean,
         fit.log_inv_disp)
     )
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[94]:
   #+BEGIN_EXAMPLE
     (tensor(1927253.3750, grad_fn=<NegBackward>),
     tensor(1926631.7500, grad_fn=<NegBackward>))
   #+END_EXAMPLE
   :END:

   Look at the gradients with respect to the encoder network weights.

   #+BEGIN_SRC ipython
     temp_loss = mpebpm.gam_mix._nb_mix_loss(
       fit.encoder.forward(query),
       query,
       torch.tensor(temp.X.sum(axis=1)),
       fit.log_mean,
       fit.log_inv_disp)
     temp_loss.retain_grad()
     temp_loss.backward()
     torch.norm(fit.encoder[0].weight.grad)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[95]:
   : tensor(31154.1895)
   :END:

   Perform amortized inference, initializing \(\vmu_k, \vphi_k\)
   at the batch GD clustering solution.

   #+BEGIN_SRC ipython :async t
     import imp; imp.reload(mpebpm.gam_mix)
     torch.manual_seed(0)
     fit1 = mpebpm.gam_mix.EBPMGammaMix(
       p=temp.shape[1],
       k=2,
       log_mean=fit0[0],
       log_inv_disp=fit0[1])
     fit1.fit(
         x=temp.X.A,
         s=temp.X.sum(axis=1),
         y=pd.get_dummies(temp.obs['cell_type']).values,
         lr=1e-3,
         batch_size=64,
         shuffle=True,
         num_epochs=10,
         log_dir='runs/nbmix/ai-freeze-64-1e-3-10')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[73]:
   #+BEGIN_EXAMPLE
     EBPMGammaMix(
     (encoder): Sequential(
     (0): Linear(in_features=11590, out_features=128, bias=True)
     (1): ReLU()
     (2): Linear(in_features=128, out_features=2, bias=True)
     (3): Softmax(dim=1)
     )
     )
   #+END_EXAMPLE
   :END:

   Compute the cross entropy loss over the posterior mean cluster assignments.

   #+BEGIN_SRC ipython
     zhat = fit1.forward(query.cuda()).detach().cpu().numpy()
     torch.nn.functional.binary_cross_entropy(
       torch.tensor(zhat, dtype=torch.float),
       torch.tensor(pd.get_dummies(temp.obs['cell_type']).values, dtype=torch.float))
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[76]:
   : tensor(4.5151e-05)
   :END:

   Plot the approximate posterior over cluster assignments for each point.

   #+BEGIN_SRC ipython :ipyfile figure/nbmix.org/am-inf-ex.png
     cm = plt.get_cmap('Dark2')
     plt.clf()
     fig, ax = plt.subplots(1, 2, sharex=True, sharey=True)
     fig.set_size_inches(4.5, 2.5)
     for i, a in enumerate(ax):
       a.scatter(*temp.obsm["X_umap"].T, s=4, c=np.hstack((np.tile(np.array(cm(i)[:3]), zhat.shape[0]).reshape(-1, 3), zhat[:,i].reshape(-1, 1))))
       a.set_xlabel('UMAP 1')
     ax[0].set_ylabel('UMAP 2')
     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[77]:
   [[file:figure/nbmix.org/am-inf-ex.png]]
   :END:

   Try amortized inference, using the first minibatch to initialize.

   #+BEGIN_SRC ipython :async t
     import imp; imp.reload(mpebpm.gam_mix)
     torch.manual_seed(0)
     np.histogram(np.argmax(mpebpm.gam_mix.EBPMGammaMix(p=temp.shape[1], k=2).forward(query).detach().cpu().numpy(), axis=1), np.arange(3))
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[40]:
   : (array([393, 119]), array([0, 1, 2]))
   :END:

   #+BEGIN_SRC ipython :async t
     import imp; imp.reload(mpebpm.gam_mix)
     torch.manual_seed(1)
     fit1 = mpebpm.gam_mix.EBPMGammaMix(
       p=temp.shape[1],
       k=2)
     fit1.fit(
         x=temp.X.A,
         s=temp.X.sum(axis=1),
         y=pd.get_dummies(temp.obs['cell_type']).values,
         lr=1e-2,
         batch_size=64,
         shuffle=True,
         num_epochs=100,
         log_dir='runs/nbmix/ai-hack1-64-1e-2-100')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   :END:

   #+BEGIN_SRC ipython :ipyfile figure/nbmix.org/ai-fit-log-mean.png
     plt.clf()
     plt.gcf().set_size_inches(3, 3)
     plt.plot(*fit1.log_mean.detach().cpu().numpy(), marker='.', lw=0, ms=1, c='k')
     plt.xlabel('Component 1 $\ln\mu$')
     plt.ylabel('Component 2 $\ln\mu$')
     plt.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[75]:
   [[file:figure/nbmix.org/ai-fit-log-mean.png]]
   :END:

** (Full) 2-way example

   Apply the standard methodology (~1 min).

   #+BEGIN_SRC ipython :async t
     mix2 = dat[dat.obs['cell_type'].isin(['cytotoxic_t', 'b_cells'])]
     sc.pp.filter_genes(mix2, min_counts=1)
     sc.pp.pca(mix2, zero_center=False)
     sc.pp.neighbors(mix2)
     sc.tl.leiden(mix2)
     sc.tl.umap(mix2)
     mix2
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[5]:
   #+BEGIN_EXAMPLE
     AnnData object with n_obs × n_vars = 20294 × 17808
     obs: 'barcode', 'cell_type', 'leiden'
     var: 'ensg', 'name', 'n_cells', 'n_counts'
     uns: 'pca', 'neighbors', 'leiden', 'umap'
     obsm: 'X_pca', 'X_umap'
     varm: 'PCs'
   #+END_EXAMPLE
   :END:

   #+BEGIN_SRC ipython :ipyfile figure/nbmix.org/mix2.png
     cm = plt.get_cmap('Paired')
     plt.clf()
     fig, ax = plt.subplots(1, 2, sharex=True, sharey=True)
     fig.set_size_inches(4.5, 4)
     for a, k, t in zip(ax, ['cell_type', 'leiden'], ['Ground truth', 'Leiden']):
       for i, c in enumerate(mix2.obs[k].unique()):
         a.plot(*mix2[mix2.obs[k] == c].obsm["X_umap"].T, c=cm(i), marker='.', ms=1, lw=0, alpha=0.1, label=f'{k}_{i}')
       leg = a.legend(frameon=False, markerscale=8, handletextpad=0, loc='upper center', bbox_to_anchor=(.5, -.25), ncol=2)
       for h in leg.legendHandles:
         h._legmarker.set_alpha(1)
       a.set_xlabel('UMAP 1')
       a.set_title(t)
     ax[0].set_ylabel('UMAP 2')
     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[9]:
   [[file:figure/nbmix.org/mix2.png]]
   :END:

   Take a subsample to run batch EM.

   #+BEGIN_SRC ipython :async t
     mix2sub = sc.pp.subsample(mix2, n_obs=1000, random_state=1, copy=True)
     mix2sub.obs['cell_type'].value_counts()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[11]:
   #+BEGIN_EXAMPLE
     cytotoxic_t    513
     b_cells        487
     Name: cell_type, dtype: int64
   #+END_EXAMPLE
   :END:

   Run batch EM (45 s).

   #+BEGIN_SRC ipython :async t
     import imp; imp.reload(mpebpm.gam_mix)
     k = 2
     seed = 0
     lr = 1e-2
     num_epochs = 50
     max_em_iters = 10
     torch.manual_seed(seed)
     fit = mpebpm.gam_mix.ebpm_gam_mix_em(
       x=mix2sub.X.A,
       s=mix2sub.X.sum(axis=1),
       k=k,
       lr=lr,
       num_epochs=num_epochs,
       max_em_iters=max_em_iters,
       log_dir=f'runs/nbmix/mix2-init-{k}-{seed}-{lr:.1g}-{num_epochs}-{max_em_iters}')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[13]:
   :END:

   #+BEGIN_SRC ipython :ipyfile figure/nbmix.org/mix2-init.png
     plt.clf()
     fig, ax = plt.subplots(1, 2, sharex=True, sharey=True)
     plt.gcf().set_size_inches(5, 3.5)

     cm = plt.get_cmap('Paired')
     for i, c in enumerate(mix2sub.obs['cell_type'].unique()):
       ax[0].plot(*mix2sub[mix2sub.obs['cell_type'] == c].obsm["X_umap"].T, c=cm(i), marker='.', ms=2, lw=0, label=f'{c}')
     ax[0].set_title('Ground truth')
     leg = ax[0].legend(frameon=False, markerscale=8, handletextpad=0, loc='upper center', bbox_to_anchor=(.5, -.25), ncol=2)
     for h in leg.legendHandles:
       h._legmarker.set_alpha(1)

     z = pd.get_dummies(np.argmax(fit[-1], axis=1)).values.astype(bool)
     cm = plt.get_cmap('Dark2')
     for i in range(z.shape[1]):
       ax[1].plot(*mix2sub[z[:,i]].obsm["X_umap"].T, c=cm(i), marker='.', ms=2, lw=0, label=f'Cluster {i}')
     ax[1].set_title('Batch EM')
     leg = ax[1].legend(frameon=False, markerscale=8, handletextpad=0, loc='upper center', bbox_to_anchor=(.5, -.25), ncol=2)
     for h in leg.legendHandles:
       h._legmarker.set_alpha(1)

     for a in ax:
       a.set_xlabel('UMAP 1')
     ax[0].set_ylabel('UMAP 2')
     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[14]:
   [[file:figure/nbmix.org/mix2-init.png]]
   :END:

   Run amortized inference on the full data set, initializing components from
   the batch EM solution. (Perfect accuracy in ~10 s, 49 s total)

   #+BEGIN_SRC ipython :async t
     import imp; imp.reload(mpebpm.gam_mix)
     seed = 1
     lr = 1e-3
     num_epochs = 10
     torch.manual_seed(0)
     fit1 = mpebpm.gam_mix.EBPMGammaMix(
       p=mix2.shape[1],
       k=2,
       log_mean=fit[0],
       log_inv_disp=fit[1])
     fit1.fit(
         x=mix2.X.A,
         s=mix2.X.sum(axis=1),
         y=pd.get_dummies(mix2.obs['cell_type']).values,
         lr=1e-3,
         batch_size=64,
         shuffle=True,
         num_epochs=10,
         log_dir=f'runs/nbmix/mix2-full-{seed}-{lr:.1g}-{num_epochs}')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[35]:
   #+BEGIN_EXAMPLE
     EBPMGammaMix(
     (encoder): Sequential(
     (0): Log1p()
     (1): Linear(in_features=17808, out_features=128, bias=True)
     (2): ReLU()
     (3): Linear(in_features=128, out_features=2, bias=True)
     (4): Softmax(dim=1)
     )
     )
   #+END_EXAMPLE
   :END:

   #+BEGIN_SRC ipython :async t
     x = mix2.X
     data = mpebpm.sparse.SparseDataset(
       mpebpm.sparse.CSRTensor(x.data, x.indices, x.indptr, x.shape, dtype=torch.float).cuda(),
       torch.tensor(mix2.X.sum(axis=1), dtype=torch.float).cuda())
     collate_fn = getattr(data, 'collate_fn', td.dataloader.default_collate)
     data = td.DataLoader(data, batch_size=64, shuffle=False, collate_fn=data.collate_fn)
     zhat = []
     with torch.no_grad():
       for x, s in data:
         zhat.append(fit1.forward(x).cpu().numpy())
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[36]:
   :END:

   #+BEGIN_SRC ipython
     mix2.obs['comp'] = np.argmax(np.vstack(zhat), axis=1)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[40]:
   :END:

   #+BEGIN_SRC ipython :ipyfile figure/nbmix.org/mix2-full.png
     plt.clf()
     fig, ax = plt.subplots(1, 2, sharex=True, sharey=True)
     fig.set_size_inches(5, 3)
     for a, k, t, cm in zip(ax, ['cell_type', 'comp'], ['Ground truth', 'Online'], ['Paired', 'Dark2']):
       for i, c in enumerate(mix2.obs[k].unique()):
         a.plot(*mix2[mix2.obs[k] == c].obsm["X_umap"].T, c=plt.get_cmap(cm)(i), marker='.', ms=1, lw=0, alpha=0.1, label=f'{k}_{i}')
       leg = a.legend(frameon=False, markerscale=8, handletextpad=0, loc='upper center', bbox_to_anchor=(.5, -.25), ncol=2)
       for h in leg.legendHandles:
         h._legmarker.set_alpha(1)
       a.set_xlabel('UMAP 1')
       a.set_title(t)
     ax[0].set_ylabel('UMAP 2')
     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[44]:
   [[file:figure/nbmix.org/mix2-full.png]]
   :END:

** 4-way example

   Pick 4 cell types which /a priori/ should be easy to distinguish.

   #+BEGIN_SRC ipython :async t
     mix4 = dat[dat.obs['cell_type'].isin(['cytotoxic_t', 'regulatory_t', 'b_cells', 'cd14_monocytes'])]
     sc.pp.filter_genes(mix4, min_counts=1)
     sc.pp.pca(mix4, zero_center=False)
     sc.pp.neighbors(mix4)
     sc.tl.leiden(mix4)
     sc.tl.umap(mix4)
     mix4
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[154]:
   #+BEGIN_EXAMPLE
     AnnData object with n_obs × n_vars = 33169 × 19241
     obs: 'barcode', 'cell_type', 'leiden'
     var: 'ensg', 'name', 'n_cells', 'n_counts'
     uns: 'pca', 'neighbors', 'leiden', 'umap'
     obsm: 'X_pca', 'X_umap'
     varm: 'PCs'
   #+END_EXAMPLE
   :END:

   #+BEGIN_SRC ipython
     mix4.write('/scratch/midway2/aksarkar/singlecell/mix4.h5ad')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[155]:
   :END:

   Plot the data, colored by ground truth label.

   #+BEGIN_SRC ipython :ipyfile figure/nbmix.org/mix4.png
     cm = plt.get_cmap('Paired')
     plt.clf()
     fig, ax = plt.subplots(1, 2, sharex=True, sharey=True)
     fig.set_size_inches(4.5, 2.75)
     for a, k, t in zip(ax, ['cell_type', 'leiden'], ['Ground truth', 'Leiden']):
       for i, c in enumerate(mix4.obs[k].unique()):
         a.plot(*mix4[mix4.obs[k] == c].obsm["X_umap"].T, c=cm(i), marker='.', ms=1, lw=0, alpha=0.1)
       a.set_title(t)
     for a in ax:
       a.set_xlabel('UMAP 1')
     ax[0].set_ylabel('UMAP 2')
     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[170]:
   [[file:figure/nbmix.org/mix4.png]]
   :END:

   Fit batch EM for a subset of the data.

   #+BEGIN_SRC ipython
     mix4sub = sc.pp.subsample(mix4, n_obs=1000, random_state=1, copy=True)
     mix4sub.obs['cell_type'].value_counts()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[166]:
   #+BEGIN_EXAMPLE
     b_cells           320
     cytotoxic_t       315
     regulatory_t      290
     cd14_monocytes     75
     Name: cell_type, dtype: int64
   #+END_EXAMPLE
   :END:

   #+BEGIN_SRC ipython :async t
     import imp; imp.reload(mpebpm.gam_mix)
     k = 4
     seed = 4
     lr = 1e-2
     num_epochs = 50
     max_em_iters = 10
     torch.manual_seed(seed)
     fit = mpebpm.gam_mix.ebpm_gam_mix_em(
       x=mix4sub.X.A,
       s=mix4sub.X.sum(axis=1),
       k=k,
       lr=lr,
       num_epochs=num_epochs,
       max_em_iters=max_em_iters,
       log_dir=f'runs/nbmix/mix4-init-{k}-{seed}-{lr:.1g}-{num_epochs}-{max_em_iters}')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[173]:
   :END:

   #+BEGIN_SRC ipython :ipyfile figure/nbmix.org/mix4-init.png
     plt.clf()
     fig, ax = plt.subplots(1, 2, sharex=True, sharey=True)
     plt.gcf().set_size_inches(5, 3.5)

     cm = plt.get_cmap('Paired')
     for i, c in enumerate(mix4sub.obs['cell_type'].unique()):
       ax[0].plot(*mix4sub[mix4sub.obs['cell_type'] == c].obsm["X_umap"].T, c=cm(i), marker='.', ms=2, lw=0, label=f'{c}')
     ax[0].set_title('Ground truth')
     leg = ax[0].legend(frameon=False, markerscale=8, handletextpad=0, loc='upper center', bbox_to_anchor=(.5, -.25), ncol=2)
     for h in leg.legendHandles:
       h._legmarker.set_alpha(1)

     z = pd.get_dummies(np.argmax(fit[-1], axis=1)).values.astype(bool)
     cm = plt.get_cmap('Dark2')
     for i in range(z.shape[1]):
       ax[1].plot(*mix4sub[z[:,i]].obsm["X_umap"].T, c=cm(i), marker='.', ms=2, lw=0, label=f'Cluster {i}')
     ax[1].set_title('Batch EM')
     leg = ax[1].legend(frameon=False, markerscale=8, handletextpad=0, loc='upper center', bbox_to_anchor=(.5, -.25), ncol=2)
     for h in leg.legendHandles:
       h._legmarker.set_alpha(1)

     for a in ax:
       a.set_xlabel('UMAP 1')
     ax[0].set_ylabel('UMAP 2')
     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[174]:
   [[file:figure/nbmix.org/mix4-init.png]]
   :END:

*** Placeholder                                                    :noexport:

   See whether the UMAP embedding is misleading, by constructing an alternative
   embedding using latent gene expression.

   #+BEGIN_SRC ipython :async t
     import rpy2.robjects.packages
     import rpy2.robjects.pandas2ri
     rpy2.robjects.pandas2ri.activate()
     matrix = rpy2.robjects.packages.importr('Matrix')
     fasttopics = rpy2.robjects.packages.importr('fastTopics')

     temp = mix4sub.X.tocoo()
     y = matrix.sparseMatrix(i=pd.Series(temp.row + 1), j=pd.Series(temp.col + 1), x=pd.Series(temp.data), dims=pd.Series(temp.shape))
     res = fasttopics.fit_poisson_nmf(y, k=10, numiter=40, method='scd', control=rpy2.robjects.ListVector({'extrapolate': True}), verbose=True)
     lam = np.array(res.rx2('L')) @ np.array(res.rx2('F')).T
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[105]:
   :END:

   #+BEGIN_SRC ipython :async t
     nmf_umap = umap.UMAP(metric='cosine', random_state=1, n_neighbors=10).fit_transform(lam)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[145]:
   :END:

   #+BEGIN_SRC ipython :ipyfile figure/nbmix.org/mix4-init-cos-umap.png
     plt.clf()
     fig, ax = plt.subplots(1, 2, sharex=True, sharey=True)
     plt.gcf().set_size_inches(5, 3.5)
     cm = plt.get_cmap('Paired')
     for i, c in enumerate(mix4sub.obs['cell_type'].unique()):
       ax[0].plot(*nmf_umap[mix4sub.obs['cell_type'] == c].T, c=cm(i), marker='.', ms=2, lw=0, label=f'{c}')
     ax[0].set_title('Ground truth')
     leg = ax[0].legend(frameon=False, markerscale=8, handletextpad=0, loc='upper center', bbox_to_anchor=(.5, -.25), ncol=2)
     for h in leg.legendHandles:
       h._legmarker.set_alpha(1)

     z = pd.get_dummies(np.argmax(fit[-1], axis=1)).values.astype(bool)
     cm = plt.get_cmap('Dark2')
     for i in range(k):
       ax[1].plot(*nmf_umap[z[:,i]].T, c=cm(i), marker='.', ms=2, lw=0, label=f'Cluster {i}')
     ax[1].set_title('Batch EM')
     leg = ax[1].legend(frameon=False, markerscale=8, handletextpad=0, loc='upper center', bbox_to_anchor=(.5, -.25), ncol=2)
     for h in leg.legendHandles:
       h._legmarker.set_alpha(1)

     for a in ax:
       a.set_xlabel('UMAP 1')
     ax[0].set_ylabel('UMAP 2')
     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[146]:
   [[file:figure/nbmix.org/mix4-init-cos-umap.png]]
   :END:

   Set the prior mixture weights to the true proportions (based on the ground
   truth labels).

   #+BEGIN_SRC ipython :async t
     import imp; imp.reload(mpebpm.gam_mix)
     k = 4
     seed = 5
     lr = 1e-2
     num_epochs = 50
     max_em_iters = 10
     torch.manual_seed(seed)
     fit = mpebpm.gam_mix.ebpm_gam_mix_em(
       x=mix4sub.X.A,
       s=mix4sub.X.sum(axis=1),
       k=k,
       pi=pd.get_dummies(mix4sub.obs['cell_type']).values.mean(axis=0),
       lr=lr,
       num_epochs=num_epochs,
       max_em_iters=max_em_iters,
       log_dir=f'runs/nbmix/mix4-init-truepi-{k}-{seed}-{lr:.1g}-{num_epochs}-{max_em_iters}')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[151]:
   :END:

   #+BEGIN_SRC ipython :ipyfile figure/nbmix.org/mix4-init-pi-umap.png
     plt.clf()
     fig, ax = plt.subplots(1, 2, sharex=True, sharey=True)
     plt.gcf().set_size_inches(5, 3.5)
     cm = plt.get_cmap('Paired')
     for i, c in enumerate(mix4sub.obs['cell_type'].unique()):
       ax[0].plot(*nmf_umap[mix4sub.obs['cell_type'] == c].T, c=cm(i), marker='.', ms=2, lw=0, label=f'{c}')
     ax[0].set_title('Ground truth')
     leg = ax[0].legend(frameon=False, markerscale=8, handletextpad=0, loc='upper center', bbox_to_anchor=(.5, -.25), ncol=2)
     for h in leg.legendHandles:
       h._legmarker.set_alpha(1)

     z = pd.get_dummies(np.argmax(fit[-1], axis=1)).values.astype(bool)
     cm = plt.get_cmap('Dark2')
     for i in range(k):
       ax[1].plot(*nmf_umap[z[:,i]].T, c=cm(i), marker='.', ms=2, lw=0, label=f'Cluster {i}', alpha=0.5)
     ax[1].set_title('Batch EM')
     leg = ax[1].legend(frameon=False, markerscale=8, handletextpad=0, loc='upper center', bbox_to_anchor=(.5, -.25), ncol=2)
     for h in leg.legendHandles:
       h._legmarker.set_alpha(1)

     for a in ax:
       a.set_xlabel('UMAP 1')
     ax[0].set_ylabel('UMAP 2')
     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[152]:
   [[file:figure/nbmix.org/mix4-init-pi-umap.png]]
   :END:

** 10-way example

   Apply the standard methodology to the 10-way mixture.

   #+BEGIN_SRC ipython :async t
     dat = anndata.read_h5ad('/scratch/midway2/aksarkar/ideas/zheng-10-way.h5ad')
     # Important: this is required for sparse data; otherwise, scanpy makes a dense
     # copy and runs out of memory
     sc.pp.pca(dat, zero_center=False)
     sc.pp.neighbors(dat)
     sc.tl.umap(dat)
     sc.tl.leiden(dat)
   #+END_SRC

   #+BEGIN_SRC ipython :async t
     dat.write('/scratch/midway2/aksarkar/ideas/mix10.h5ad')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[12]:
   :END:

   #+BEGIN_SRC ipython :async t
     dat = anndata.read_h5ad('/scratch/midway2/aksarkar/ideas/mix10.h5ad')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[34]:
   :END:

   #+BEGIN_SRC ipython :ipyfile figure/nbmix.org/mix10-full-std.png
     cm = plt.get_cmap('Paired')
     plt.clf()
     fig, ax = plt.subplots(1, 2, sharex=True, sharey=True)
     fig.set_size_inches(6, 3)
     for i, c in enumerate(dat.obs['cell_type'].unique()):
       ax[0].plot(*dat[dat.obs['cell_type'] == c].obsm["X_umap"].T, c=cm(i), marker='.', ms=1, lw=0, label=f'{c}', alpha=0.1)
     for i, c in enumerate(dat.obs['leiden'].unique()):
       ax[1].plot(*dat[dat.obs['leiden'] == c].obsm["X_umap"].T, c=colorcet.cm['fire']((int(c) + .5) / 22), marker='.', ms=1, lw=0, label=f'Cluster {i}', alpha=0.1)
     for a, t in zip(ax, ['Ground truth', 'Leiden']):
       a.set_title(t)
     for a in ax:
       # a.legend(markerscale=4, handletextpad=0)
       a.set_xlabel('UMAP 1')
     ax[0].set_ylabel('UMAP 2')
     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[41]:
   [[file:figure/nbmix.org/mix10-full-std.png]]
   :END:

   Subsample the 10-way mixture of sorted cells to get an initialization.

   #+BEGIN_SRC ipython :async t
     mix10 = sc.pp.subsample(dat, fraction=0.01, copy=True)
     sc.pp.filter_genes(mix10, min_counts=1)
     sc.pp.pca(mix10, zero_center=False)
     sc.pp.neighbors(mix10)
     sc.tl.umap(mix10)
     sc.tl.leiden(mix10)
     mix10.write('/scratch/midway2/aksarkar/ideas/mix10-init.h5ad')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[6]:
   :END:

   #+BEGIN_SRC ipython :async t
     mix10 = anndata.read_h5ad('/scratch/midway2/aksarkar/ideas/mix10-init.h5ad')
     y = pd.get_dummies(mix10.obs['cell_type']).values
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[4]:
   :END:

   Try running UMAP directly on the sparse data.

   #+BEGIN_SRC ipython :async t :results output example
     %%time
     embedding = umap.UMAP(metric='cosine', random_state=0).fit_transform(mix10.X)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   CPU times: user 3.75 s, sys: 41 ms, total: 3.79 s
   Wall time: 3.8 s
   :END:

   Run NMF on the sparse data, and estimate latent gene expression. Then,
   estimate a UMAP embedding on latent gene expression.

   #+BEGIN_SRC ipython :async t
     import rpy2.robjects.packages
     import rpy2.robjects.pandas2ri
     rpy2.robjects.pandas2ri.activate()
     matrix = rpy2.robjects.packages.importr('Matrix')
     fasttopics = rpy2.robjects.packages.importr('fastTopics')

     temp = mix10.X.tocoo()
     y = matrix.sparseMatrix(i=pd.Series(temp.row + 1), j=pd.Series(temp.col + 1), x=pd.Series(temp.data), dims=pd.Series(temp.shape))
     res = fasttopics.fit_poisson_nmf(y, k=10, numiter=40, method='scd', control=rpy2.robjects.ListVector({'extrapolate': True}), verbose=True)
     lam = np.array(res.rx2('L')) @ np.array(res.rx2('F')).T
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[21]:
   :END:

   #+BEGIN_SRC ipython :async t
     nmf_umap = umap.UMAP(metric='cosine', random_state=0, n_neighbors=10).fit_transform(lam)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[36]:
   :END:

   Compare the UMAP embeddings.

   #+BEGIN_SRC ipython :ipyfile figure/nbmix.org/mix10-umap.png
     cm = plt.get_cmap('Paired')
     plt.clf()
     fig, ax = plt.subplots(1, 3)
     fig.set_size_inches(7.5, 2.5)
     for i, c in enumerate(mix10.obs['cell_type'].unique()):
       ax[0].plot(*mix10[mix10.obs['cell_type'] == c].obsm["X_umap"].T, c=cm(i), marker='.', ms=1, lw=0, label=f'{c}')
       ax[1].plot(*embedding[mix10.obs['cell_type'] == c].T, c=cm(i), marker='.', ms=1, lw=0, label=f'{c}')
       ax[2].plot(*nmf_umap[mix10.obs['cell_type'] == c].T, c=cm(i), marker='.', ms=1, lw=0, label=f'{c}')
     for a, t in zip(ax, ['Euclidean/PCA', 'Cosine/counts', 'Cosine/latent']):
       a.set_title(t)
       a.set_xlabel('UMAP 1')
     ax[0].set_ylabel('UMAP 2')
     ax[-1].legend(frameon=False, loc='center left', bbox_to_anchor=(1, .5), handletextpad=0, markerscale=8)
     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[38]:
   [[file:figure/nbmix.org/mix10-umap.png]]
   :END:

   Fit point mass expression models, given the ground truth labels.

   #+BEGIN_SRC ipython :async t
     mean = (y.T @ mix10.X) / (y.T @ mix10.X.sum(axis=1))
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[11]:
   :END:

   Estimate the posterior cluster weights, given the components. Then, estimate
   the log loss against the ground truth labels.

   #+BEGIN_SRC ipython :async t
     L = st.poisson(mu=np.expand_dims(mean, 0)).logpmf(np.expand_dims(mix10.X.A, 1)).sum(axis=2)
     zhat = sp.softmax(L, axis=1)
     -(y * np.log(zhat + 1e-16)).sum()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[33]:
   : -0.0
   :END:

   Run EM, starting from a random initialization.

   #+BEGIN_SRC ipython :async t
     import imp; imp.reload(mpebpm.gam_mix)
     num_epochs = 50
     max_em_iters = 8
     seed = 4
     torch.manual_seed(seed)
     init = mpebpm.gam_mix.ebpm_gam_mix_em(
       x=mix10.X.A,
       s=mix10.X.sum(axis=1),
       k=10,
       num_epochs=num_epochs,
       max_em_iters=max_em_iters,
       log_dir=f'runs/nbmix/mix10-init{seed}-{num_epochs}-{max_em_iters}')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[57]:
   :END:

   Plot the correlation between the posterior cluster weights and the ground
   truth labels.

   #+BEGIN_SRC ipython :ipyfile figure/nbmix.org/mix10-corr.png
     r = np.corrcoef(y.T, init[-1].T)
     plt.clf()
     plt.gcf().set_size_inches(3, 3)
     plt.imshow(r[:10,10:], cmap=colorcet.cm['coolwarm'], vmin=-1, vmax=1)
     c = plt.colorbar(shrink=0.5)
     c.set_label('Correlation')
     plt.xticks(np.arange(10))
     plt.yticks(np.arange(10))
     plt.xlabel('Ground truth label')
     plt.ylabel('Component')
     plt.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[58]:
   [[file:figure/nbmix.org/mix10-corr.png]]
   :END:

   Plot the embedding of the data, colored by the ground truth labels, the
   Leiden cluster assignments, and the model-based cluster assignments.

   #+BEGIN_SRC ipython :ipyfile figure/nbmix.org/mix10-init-std.png
     zhat = np.argmax(init[-1], axis=1)
     cm = plt.get_cmap('Paired')
     plt.clf()
     fig, ax = plt.subplots(1, 3, sharex=True, sharey=True)
     fig.set_size_inches(7.5, 3)
     for i, c in enumerate(mix10.obs['cell_type'].unique()):
       ax[0].plot(*mix10[mix10.obs['cell_type'] == c].obsm["X_umap"].T, c=cm(i), marker='.', ms=1, lw=0, label=f'{c}')
     for i, c in enumerate(mix10.obs['leiden'].unique()):
       ax[1].plot(*mix10[mix10.obs['leiden'] == c].obsm["X_umap"].T, c=cm(i), marker='.', ms=1, lw=0, label=f'Cluster {i}')
     for i, c in enumerate(pd.Series(zhat).unique()):
       ax[2].plot(*mix10[zhat == c].obsm["X_umap"].T, c=cm(i), marker='.', ms=1, lw=0, label=f'Component {i}')
     for a, t in zip(ax, ['Ground truth', 'Leiden', 'NB mix']):
       a.set_title(t)
     for a in ax:
       # a.legend(markerscale=4, handletextpad=0)
       a.set_xlabel('UMAP 1')
     ax[0].set_ylabel('UMAP 2')
     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[56]:
   [[file:figure/nbmix.org/mix10-init-std.png]]
   :END:

* Related work
  :PROPERTIES:
  :CUSTOM_ID: related-work
  :END:
** EM for Poisson-Gamma
   :PROPERTIES:
   :CUSTOM_ID: em-nb
   :END:

   Considering just a simple Gamma prior,
   [[https://doi.org/10.1017/S0515036100014033][Karlis 2005]] gives an EM
   algorithm for maximizing the marginal likelihood. The key idea is that, due
   to Poisson-Gamma conjugacy, the exact posterior is analytic, as are the
   necessary posterior moments. The main disadvantage of this approach is that
   it requires (one-dimensional) numerical optimization in the M step.

   \begin{align}
     x_i \mid \xiplus, \lambda_i &\sim \Pois(\xiplus \lambda_i)\\
     \lambda_i \mid \alpha, \beta &\sim \Gam(\alpha, \beta)\\
     \lambda_i \mid x_i, \xiplus, \alpha, \beta &\sim q \triangleq \Gam(x_i + \alpha, \xiplus + \beta)\\
     E_q[\lambda_i] &= \frac{x_i + \alpha}{\xiplus + \beta}\\
     E_q[\ln \lambda_i] &= \psi(x + \alpha) - \log(\xiplus + \beta)\\
     E_q[\ln p(x_i, \lambda_i \mid \xiplus, \alpha, \beta)] &= \ell_i \triangleq x_i E_q[\ln \lambda_i] - E_q[\lambda_i] - \ln\Gamma(x_i + 1) + \alpha \ln\beta - \ln\Gamma(\alpha) + (\alpha - 1) E_q[\lambda_i] - \beta E_q[\lambda_i]\\
     \ell &= \sum_i \ell_i\\
     \frac{\partial\ell}{\partial\beta} &= \sum_i \frac{\alpha}{\beta} - E_q[\lambda_i] = 0\\
     \beta &= \frac{\bar{\lambda}}{\alpha}\\
     \frac{\partial\ell}{\partial\alpha} &= \sum_i \ln \beta - \psi(\alpha) + E_q[\ln x_i]\\
     \frac{\partial^2\ell}{\partial\alpha^2} &= -n \psi^{(1)}(\alpha)
   \end{align}

   where \(\psi\) denotes the digamma function and \(\psi^{(1)}\) denotes the
   trigamma function. The algorithm uses a partial M step (single
   Newton-Raphson update) for \(\alpha\).

   Try EM for a simple example.

   #+BEGIN_SRC ipython
     rng = np.random.default_rng(1)
     n = 100
     log_mean = -10
     log_inv_disp = 0
     s = np.repeat(1e5, n)
     lam = rng.gamma(shape=np.exp(log_inv_disp), scale=np.exp(log_mean - log_inv_disp), size=n)
     x = rng.poisson(s * lam)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[4]:
   :END:

   #+BEGIN_SRC ipython :async t
     import nbmix.em
     log_mu, neg_log_phi, trace = nbmix.em.fit_pois_gam(x, s)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[5]:
   :END:

   Plot the simulated data, the ground truth marginal distribution on counts,
   and the NB MLE.

   #+BEGIN_SRC ipython :ipyfile figure/nbmix.org/nb-em.png
     cm = plt.get_cmap('Dark2')
     plt.clf()
     plt.gcf().set_size_inches(4.5, 2.5)
     grid = np.arange(x.max() + 1)
     plt.hist(x, bins=grid, color='0.7', density=True)
     plt.plot(grid + .5, st.nbinom(n=np.exp(-log_inv_disp), p=1 / (1 + s[0] * np.exp(log_mean - log_inv_disp))).pmf(grid), lw=1, color=cm(0), label='Ground truth')
     plt.plot(grid + .5, st.nbinom(n=np.exp(neg_log_phi), p=1 / (1 + s[0] * np.exp(log_mu - neg_log_phi))).pmf(grid), lw=1, color=cm(1), label='NB MLE')
     plt.legend(frameon=False)
     plt.xlabel('Number of molecules')
     plt.ylabel('Density')
     plt.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[6]:
   [[file:figure/nbmix.org/nb-em.png]]
   :END:

   Try a more extensive evaluation of the method.

   #+BEGIN_SRC ipython :async t
     n = 100
     s = np.repeat(1e5, n)
     result = dict()
     for trial in range(5):
       for log_mean in np.linspace(-12, -6, 7):
         for log_inv_disp in np.linspace(0, 4, 5):
           rng = np.random.default_rng(trial)
           lam = rng.gamma(shape=np.exp(log_inv_disp), scale=np.exp(log_mean - log_inv_disp), size=n)
           x = rng.poisson(s * lam)
           start = time.time()
           log_mean_hat, log_inv_disp_hat, trace = nbmix.em.fit_pois_gam(x, s, max_iters=1000)
           elapsed = time.time() - start
           result[(log_mean, log_inv_disp, trial)] = pd.Series([log_mean_hat, log_inv_disp_hat, len(trace), elapsed])
     result = (pd.DataFrame.from_dict(result, orient='index')
               .reset_index()
               .rename({f'level_{i}': k for i, k in enumerate(['log_mean', 'log_inv_disp', 'trial'])}, axis=1)
               .rename({i: k for i, k in enumerate(['log_mean_hat', 'log_inv_disp_hat', 'num_iters', 'elapsed'])}, axis=1))
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[7]:
   :END:

   Plot the estimates against the ground truth values.

   #+BEGIN_SRC ipython :ipyfile figure/nbmix.org/nb-em-sim.png
     plt.clf()
     fig, ax = plt.subplots(1, 2)
     fig.set_size_inches(4.5, 2.5)
     for a in ax:
       a.set_aspect('equal', adjustable='datalim')
     ax[0].scatter(result['log_mean'], result['log_mean_hat'], c='k', s=1)
     ax[0].set_xlabel('Ground truth $\ln(\mu)$')
     ax[0].set_ylabel('Estimated $\ln(\mu)$')
     ax[1].scatter(-result['log_inv_disp'], -result['log_inv_disp_hat'], c='k', s=1)
     ax[1].set_xlabel('Ground truth $\ln(\phi)$')
     ax[1].set_ylabel('Estimated $\ln(\phi)$')
     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[8]:
   [[file:figure/nbmix.org/nb-em-sim.png]]
   :END:

   Estimate the average time (seconds) taken to fit each trial.

   #+BEGIN_SRC ipython
     result['elapsed'].mean(), result['elapsed'].std()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[9]:
   : (0.16765535082135882, 0.1686101890016258)
   :END:

** EM for Poisson-Log compound distribution

   The NB distribution can be derived as a
   [[https://en.wikipedia.org/wiki/Negative_binomial_distribution#Representation_as_compound_Poisson_distribution][Poisson-distributed
   sum]] of
   [[https://en.wikipedia.org/wiki/Logarithmic_distribution][Log-distributed
   random variables]] (Quenouille 1949)

   \begin{align}
     x_i \mid y_1, \ldots, y_{m_i}, m_i &= \sum_{t=1}^{m_i} y_t\\
     m_i \mid \lambda &\sim \Pois(\lambda)\\
     p(y_t \mid \theta) &= -\frac{\theta^{y_t}}{y_t \ln(1 - \theta)}, \quad t = 1, 2, \ldots\\
     p(x_i \mid \lambda, \theta) &\propto p^n (1 - p)^{x_i}, \quad n = -\lambda / \log(1 - \theta), p = 1 - \theta
   \end{align}

   To connect this parameterization to our parameterization, we have \(n =
   1/\phi\) and \(p = 1 / (1 + \xiplus\mu\phi)\). Adamidis 1999 uses this fact to
   derive a new auxiliary variable representation of the NB distribution

   \begin{equation}
     p(y_t, z_t \mid \theta) = \frac{(1 - \theta)^{z_t} \theta^{y_t - 1}}{y_t}, z_t \in (0, 1), y_t \in \mathbb{N}
   \end{equation}

   which they claim admits an EM algorithm with analytic M step. Letting \(q
   \triangleq p(m_i, y_1, \ldots, z_1, \ldots \mid x_i, n, p)\),

   \begin{multline}
     E_q[\ln p(x_i \mid m_i, y_1, \ldots, z_1, \ldots, \lambda, \theta)] = E_q[m_i] \ln\lambda - \lambda - E_q[\ln\Gamma(m_i + 1)]\\
     + E_q[\textstyle\sum_{t=1}^{m_i} z_t] \ln(1 - \theta) + (\textstyle\sum_{t=1}^{m_i} E_q[y_t] - E_q[m_i]) \ln \theta + \mathrm{const}
   \end{multline}

   However, it appears they neglect the intractable term \(E_q[\ln\Gamma(m_i +
   1)]\) in their derivation, and the algorithm as given does not appear to
   improve the expected log joint probability.

   Try EM for a simple example.

   #+BEGIN_SRC ipython
     rng = np.random.default_rng(1)
     n = 100
     log_mean = -10
     log_inv_disp = 0
     s = np.repeat(1e5, n)
     lam = rng.gamma(shape=np.exp(log_inv_disp), scale=np.exp(log_mean - log_inv_disp), size=n)
     x = rng.poisson(s * lam)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[10]:
   :END:

   #+BEGIN_SRC ipython :async t
     import nbmix.em
     log_mu, neg_log_phi, trace = nbmix.em.fit_pois_log(x, s)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[11]:
   :END:

** Deep unsupervised learning

   scVI ([[https://dx.doi.org/10.1038/s41592-018-0229-2][Lopez et al. 2018]],
   [[https://www.biorxiv.org/content/10.1101/532895v2][Xu et al. 2020]])
   implements a related deep unsupervised (more precisely, semi-supervised)
   clustering model ([[https://arxiv.org/abs/1406.5298][Kingma et al. 2014]],
   [[https://arxiv.org/abs/1611.02648][Dilokthanakul et al. 2016]]).

   \begin{align}
     x_{ij} \mid s_i, \lambda_{ij} &\sim \Pois(s_i \lambda_{ij})\\
     \ln s_i &\sim \N(\cdot)\\
     \lambda_{ij} \mid \vz_i &\sim \Gam(\phi_j^{-1}, (\mu_{\lambda}(\vz_i))_j^{-1} \phi_j^{-1})\\
     \vz_i \mid y_i, \vu_i &\sim \N(\mu_z(\vu_i, y_i), \diag(\sigma^2(\vu_i, y_i)))\\
     y_i \mid \vpi &\sim \Mult(1, \vpi)\\
     \vu_i &\sim \N(0, \mi).
   \end{align}

   where

   - \(y_i\) denotes the cluster assignment for cell \(i\)
   - \(\mu_z(\cdot), \sigma^2(\cdot)\) are neural networks mapping the latent
     cluster variable \(y_i\) and Gaussian noise \(\vu_i\) to the latent variable
     \(\vz_i\)
   - \(\mu_{\lambda}(\cdot)\) is a neural network mapping latent variable \(\vz_i\) to
     latent gene expression \(\vlambda_{i}\)

   The intuition behind this model is that, marginalizing over \(y_i\), the
   prior \(p(z_i)\) is a mixture of Gaussians, and therefore the model embeds
   examples in a space which makes clustering easy, and maps examples to those
   clusters simultaneously. To perform variational inference in this model,
   Lopez et al. introduce inference networks

   \begin{align}
     q(\vz_i \mid \vx_i) &= \N(\cdot)\\
     q(y_i \mid \vz_i) &= \Mult(1, \cdot).
   \end{align}

   #+BEGIN_SRC ipython :async t
     import scvi.dataset
     import scvi.inference
     import scvi.models

     expr = scvi.dataset.AnnDatasetFromAnnData(temp)
     m = scvi.models.VAEC(expr.nb_genes, n_batch=0, n_labels=2)
     train = scvi.inference.UnsupervisedTrainer(m, expr, train_size=1, batch_size=32, show_progbar=False, n_epochs_kl_warmup=100)
     train.train(n_epochs=1000, lr=1e-2)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[125]:
   :END:

   # https://scvi.readthedocs.io/en/stable/scvi.models.html#scvi.models.SCANVI
   # https://scvi.readthedocs.io/en/stable/tutorials/scanvi.html?highlight=scanvi#EVF-/-not-EVF

   #+BEGIN_SRC ipython
     post = train.create_posterior(train.model, expr)
     _, _, label = post.get_latent()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[118]:
   :END:

   The fundamental difference between scVI and our approach is that scVI
   clusters points in the low-dimensional latent space, and we cluster points in
   the space of latent gene expression (which has equal dimension to the
   observations).

   Rui Shu [[http://ruishu.io/2016/12/25/gmvae/][proposed an alternative
   generative model]], which has some practical benefits and can be adapted to
   this problem

   \begin{align}
     x_{ij} \mid \xiplus, \lambda_{ij} &\sim \Pois(\xiplus \lambda_{ij})\\
     \lambda_{ij} \mid \vz_i &\sim \Gam(\phi_j^{-1}, (\mu_{\lambda}(\vz_i))_j^{-1} \phi_j^{-1})\\
     \vz_i \mid y_i &\sim \N(\mu_z(y_i), \sigma^2(y_i))\\
     y_i \mid \vpi &\sim \Mult(1, \vpi)\\
     q(y_i, \vz_i \mid \vx_i) &= q(y_i \mid \vx_i)\, q(\vz_i \mid y_i, \vx_i).
   \end{align}

   There are additional deep unsupervised learning methods, which could
   potentially be adapted to this setting (reviewed in
   [[https://dx.doi.org/10.1109/ACCESS.2018.2855437][Min et al. 2018]]).
* Technical stuff                                                  :noexport:

   #+BEGIN_SRC ipython
     import gc
     for o in gc.get_objects():
       try:
         if torch.is_tensor(o) or torch.is_tensor(getattr(o, 'data', None)):
           print(type(o), o.size(), o.device)
       except:
         pass
     torch.cuda.empty_cache()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[32]:
   :END:

