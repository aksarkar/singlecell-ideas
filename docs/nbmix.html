<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2020-07-27 Mon 14:09 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Model-based clustering of scRNA-seq data</title>
<meta name="generator" content="Org mode">
<meta name="author" content="Abhishek Sarkar">
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link href="bootstrap.min.css" rel="stylesheet" />
<link rel="stylesheet" type="text/css" href="htmlize.css"/>
<link rel="stylesheet" type="text/css" href="main.css"/>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2017 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Model-based clustering of scRNA-seq data</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#introdcution">Introduction</a></li>
<li><a href="#setup">Setup</a></li>
<li><a href="#method">Methods</a>
<ul>
<li><a href="#model">Model specification</a></li>
<li><a href="#em-nbmix">EM for Poisson&#x2013;Gamma-mixture</a></li>
<li><a href="#ai">Amortized inference</a></li>
<li><a href="#orge1585a2">Simulation</a></li>
</ul>
</li>
<li><a href="#results">Results</a>
<ul>
<li><a href="#orgc1db78e">Simulated example</a></li>
<li><a href="#example">Real data example</a></li>
<li><a href="#leiden">Leiden algorithm</a></li>
<li><a href="#mpebpm">MPEBPM</a></li>
<li><a href="#amortized">Amortized inference</a></li>
<li><a href="#org59c95a8">(Full) 2-way example</a></li>
<li><a href="#org60dbe51">4-way example</a></li>
<li><a href="#org2b76619">10-way example</a></li>
</ul>
</li>
<li><a href="#related-work">Related work</a>
<ul>
<li><a href="#em-nb">EM for Poisson-Gamma</a></li>
<li><a href="#orgdb50eed">EM for Poisson-Log compound distribution</a></li>
<li><a href="#orgd1763fc">Deep unsupervised learning</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-org953207f" class="outline-2">
<h2 id="introdcution"><a id="org953207f"></a>Introduction</h2>
<div class="outline-text-2" id="text-introdcution">
<p>
Two major strategies for clustering scRNA-seq data are:
</p>

<ol class="org-ol">
<li>Building a \(k\)-nearest neighbor graph on the data, and applying a
community detection algorithm (e.g.,
<a href="https://doi.org/10.1088/1742-5468/2008/10/P10008">Blondel et al. 2008</a>,
<a href="https://arxiv.org/abs/1810.08473">Traag et al. 2018</a>)</li>
<li>Fitting a topic model to the data
(e.g., <a href="https://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1006599">Dey
et al. 2017</a>,
<a href="https://www.nature.com/articles/s41592-019-0367-1">Gonzáles-Blas et
al. 2019</a>)</li>
</ol>

<p>
The main disadvantage of strategy (1) is that, as commonly applied to
transformed counts, it does not separate measurement error and biological
variation of interest. The main disadvantage of strategy (2) is that it does
not account for transcriptional noise
(<a href="https://doi.org/10.1016/j.cell.2008.09.050">Raj 2008</a>). Here, we develop
a simple model-based clustering algorithm which addresses both of these
issues.
</p>
</div>
</div>

<div id="outline-container-org7cb0925" class="outline-2">
<h2 id="setup"><a id="org7cb0925"></a>Setup</h2>
<div class="outline-text-2" id="text-setup">
<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> anndata
<span class="org-keyword">import</span> mpebpm.gam_mix
<span class="org-keyword">import</span> mpebpm.sgd
<span class="org-keyword">import</span> numpy <span class="org-keyword">as</span> np
<span class="org-keyword">import</span> pandas <span class="org-keyword">as</span> pd
<span class="org-keyword">import</span> pickle
<span class="org-keyword">import</span> scanpy <span class="org-keyword">as</span> sc
<span class="org-keyword">import</span> scipy.optimize <span class="org-keyword">as</span> so
<span class="org-keyword">import</span> scipy.special <span class="org-keyword">as</span> sp
<span class="org-keyword">import</span> scipy.stats <span class="org-keyword">as</span> st
<span class="org-keyword">import</span> scmodes
<span class="org-keyword">import</span> sklearn.linear_model <span class="org-keyword">as</span> sklm
<span class="org-keyword">import</span> sklearn.metrics <span class="org-keyword">as</span> skm
<span class="org-keyword">import</span> sklearn.model_selection <span class="org-keyword">as</span> skms
<span class="org-keyword">import</span> time
<span class="org-keyword">import</span> torch
<span class="org-keyword">import</span> torch.utils.data <span class="org-keyword">as</span> td
<span class="org-keyword">import</span> umap
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> rpy2.robjects.packages
<span class="org-keyword">import</span> rpy2.robjects.pandas2ri
rpy2.robjects.pandas2ri.activate()

<span class="org-variable-name">ft</span> = rpy2.robjects.packages.importr(<span class="org-string">'fastTopics'</span>)
<span class="org-variable-name">matrix</span> = rpy2.robjects.packages.importr(<span class="org-string">'Matrix'</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">%matplotlib inline
%config <span class="org-variable-name">InlineBackend.figure_formats</span> = <span class="org-builtin">set</span>([<span class="org-string">'retina'</span>])
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> colorcet
<span class="org-keyword">import</span> matplotlib
<span class="org-keyword">import</span> matplotlib.pyplot <span class="org-keyword">as</span> plt
<span class="org-variable-name">plt.rcParams</span>[<span class="org-string">'figure.facecolor'</span>] = <span class="org-string">'w'</span>
<span class="org-variable-name">plt.rcParams</span>[<span class="org-string">'font.family'</span>] = <span class="org-string">'Nimbus Sans'</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-orgc2ae14c" class="outline-2">
<h2 id="method"><a id="orgc2ae14c"></a>Methods</h2>
<div class="outline-text-2" id="text-method">
</div>
<div id="outline-container-org07585ee" class="outline-3">
<h3 id="model"><a id="org07585ee"></a>Model specification</h3>
<div class="outline-text-3" id="text-model">
<p>
We assume \(
   \DeclareMathOperator\Gam{Gamma}
   \DeclareMathOperator\Mult{Multinomial}
   \DeclareMathOperator\N{\mathcal{N}}
   \DeclareMathOperator\Pois{Poisson}
   \DeclareMathOperator\diag{diag}
   \DeclareMathOperator\KL{\mathcal{KL}}
   \newcommand\kl[2]{\KL(#1\;\Vert\;#2)}
   \newcommand\xiplus{x_{i+}}
   \newcommand\mi{\mathbf{I}}
   \newcommand\vb{\mathbf{b}}
   \newcommand\vu{\mathbf{u}}
   \newcommand\vx{\mathbf{x}}
   \newcommand\vz{\mathbf{z}}
   \newcommand\vlambda{\boldsymbol{\lambda}}
   \newcommand\vmu{\boldsymbol{\mu}}
   \newcommand\vphi{\boldsymbol{\phi}}
   \newcommand\vpi{\boldsymbol{\pi}}
   \)
</p>

\begin{align}
  x_{ij} \mid \xiplus, \lambda_{ij} &\sim \Pois(\xiplus \lambda_{ij})\\
  \lambda_{ij} \mid \vpi_i, \vmu_k, \vphi_k &\sim \sum_{k=1}^{K} \pi_{ik} \Gam(\phi_{kj}^{-1}, \phi_{kj}^{-1}\mu_{kj}^{-1}),
\end{align}

<p>
where
</p>

<ul class="org-ul">
<li>\(x_{ij}\) denotes the number of molecules of gene \(j = 1, \ldots, p\)
observed in cell \(i = 1, \ldots, n\)</li>
<li>\(\xiplus \triangleq \sum_j x_{ij}\) denotes the total number of molecules
observed in cell \(i\)</li>
<li>\(\vpi_i\) denotes cluster assignment probabilities for cell \(i\)</li>
<li>\(\vmu_k\) denotes the cluster &ldquo;centroid&rdquo; for cluster \(k\), and
\(\vphi_k\) describes stochastic perturbations within each cluster</li>
</ul>

<p>
The intuition behind this model is that each cluster \(k\) is defined by a
collection of independent Gamma distributions (parameterized by shape and
rate), one per gene \(j\), which describe the distribution of true gene
expression for each gene in each cluster
(<a href="https://dx.doi.org/10.1101/2020.04.07.030007">Sarkar and Stephens
2020</a>). In this parameterization, each Gamma distribution has mean
\(\mu_{kj}\) and variance \(\mu_{kj}^2\phi_{kj}\). Under this model, the
marginal likelihood is a mixture of negative binomials
</p>

\begin{equation}
  p(x_{ij} \mid \xiplus, \vpi_i, \vmu_k, \vphi_k) = \sum_{k=1}^{K} \pi_{ik} \frac{\Gamma(x_{ij} + 1 / \phi_{kj})}{\Gamma(1 / \phi_{kj})\Gamma(x_{ij} + 1)}\left(\frac{\xiplus\mu_{kj}\phi_{kj}}{1 + \xiplus\mu_{kj}\phi_{kj}}\right)^{x_{ij}} \left(\frac{1}{1 + \xiplus\mu_{kj}\phi_{kj}}\right)^{1/\phi_{kj}}.
\end{equation}
</div>
</div>

<div id="outline-container-org085bb21" class="outline-3">
<h3 id="em-nbmix"><a id="org085bb21"></a>EM for Poisson&#x2013;Gamma-mixture</h3>
<div class="outline-text-3" id="text-em-nbmix">
<p>
We can estimate \(\vpi, \vmu, \vphi\) by maximizing the likelihood using an
EM algorithm. Letting \(z_{ik} \in \{0, 1\}\) indicate whether cell \(i\) is
assigned to cluster \(k\), the exact posterior
</p>

\begin{align}
  q(z_{i1}, \ldots, z_{iK}) &\triangleq p(z_{ik} \mid x_{ij}, \xiplus, \vmu_k, \vphi_k) = \Mult(1, \alpha_{i1}, \ldots, \alpha_{iK})\\
  \alpha_{ik} &\propto \sum_j \frac{\Gamma(x_{ij} + 1 / \phi_{kj})}{\Gamma(1 / \phi_{kj})\Gamma(x_{ij} + 1)}\left(\frac{\xiplus\mu_{kj}\phi_{kj}}{1 + \xiplus\mu_{kj}\phi_{kj}}\right)^{x_{ij}} \left(\frac{1}{1 + \xiplus\mu_{kj}\phi_{kj}}\right)^{1/\phi_{kj}}.
\end{align}

<p>
The expected log joint probability with respect to \(q\)
</p>

\begin{multline}
  E_q[\ln p(x_{ij}, z_{ik} \mid \xiplus, \vpi_i, \vmu_k, \vphi_k)] = E_q[z_{ik}] \Biggl[\ln \pi_{ik} + x_{ij} \ln\left(\frac{\xiplus\mu_{kj}\phi_{kj}}{1 + \xiplus\mu_{kj}\phi_{kj}}\right)\\
    - \phi_{kj}^{-1} \ln(1 + \xiplus\mu_{kj}\phi_{kj}) + \ln\Gamma(x_{ij} + 1 / \phi_{kj}) - \ln\Gamma(1 / \phi_{kj}) - \ln\Gamma(x_{ij} + 1)\Biggr].
\end{multline}

<p>
In the E step, the necessary expectations are analytic. In the M step, we
can improve the expected log joint probability, e.g. by
<a href="mepbpm.html">(batch) gradient descent</a>.
</p>
</div>
</div>

<div id="outline-container-orgce984ca" class="outline-3">
<h3 id="ai"><a id="orgce984ca"></a>Amortized inference</h3>
<div class="outline-text-3" id="text-ai">
<p>
An alternative algorithm, which is amenable to stochastic gradient descent
and online learning, is to use the fact that EM can be viewed as maximizing
the evidence lower bound
(<a href="https://doi.org/10.1007/978-94-011-5014-9_12">Neal and Hinton 1998</a>)
</p>

\begin{align}
  \max_{\theta} \ln p(x \mid \theta) &= \max_{q, \theta} \ln p(x \mid \theta) - \kl{q(z)}{p(z \mid x, \theta)}\\
  &= \max_{q, \theta} E_q[\ln p(x \mid z, \theta)] - \kl{q(z)}{p(z \mid \theta)}.
\end{align}

<p>
Exact EM corresponds to (fully) alternately optimizing \(q^* = p(z \mid x,
   \theta)\) and \(\theta\). However, we can instead amortize inference
(<a href="https://escholarship.org/content/qt34j1h7k5/qt34j1h7k5.pdf">Gershman and
Goodman 2014</a>, <a href="https://arxiv.org/abs/1312.6114">Kingma and Welling
2014</a>, <a href="http://proceedings.mlr.press/v32/rezende14.html">Rezende et
al. 2014</a>), estimating a variational approximation parameterized by a
neural network \(f_z\) mapping \(\vx_i \rightarrow \vz_i\)
</p>

\begin{align}
  p(z_{i1}, \ldots, z_{iK} \mid \vpi) &= \Mult(1, \vpi)\\
  q(z_{i1}, \ldots, z_{iK} \mid \vx_i) &= \Mult(1, f_z(\vx_i)).
\end{align}

<p>
The evidence lower bound is analytic
</p>

\begin{multline}
  \mathcal{L} = \sum_{i, k} (f_z(\vx_i))_k \Biggl[\ln\left(\frac{\pi_{ik}}{(f_z(\vx_i))_k}\right) + \sum_j \biggl[ x_{ij} \ln\left(\frac{\xiplus\mu_{kj}\phi_{kj}}{1 + \xiplus\mu_{kj}\phi_{kj}}\right) - \phi_{kj}^{-1} \ln(1 + \xiplus\mu_{kj}\phi_{kj})\\
      + \ln\Gamma(x_{ij} + 1 / \phi_{kj}) - \ln\Gamma(1 / \phi_{kj}) - \ln\Gamma(x_{ij} + 1)\biggr]\Biggr],
\end{multline}

<p>
and can be optimized using SGD.
</p>
</div>
</div>

<div id="outline-container-orge1585a2" class="outline-3">
<h3 id="orge1585a2">Simulation</h3>
<div class="outline-text-3" id="text-orge1585a2">
<p>
Simulate from the model.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">simulate</span>(n, p, k, s=1e4, seed=0):
  <span class="org-variable-name">rng</span> = np.random.default_rng(seed)
  <span class="org-variable-name">z</span> = pd.get_dummies(np.argmax(rng.uniform(size=(n, k)), axis=1)).values
  <span class="org-comment-delimiter"># </span><span class="org-comment">Values from Sarkar et al. 2019</span>
  <span class="org-variable-name">log_mean</span> = rng.uniform(-12, -6, size=(p, k))
  <span class="org-variable-name">log_inv_disp</span> = rng.uniform(0, 4, size=(p, k))
  <span class="org-variable-name">lam</span> = rng.negative_binomial(n=np.exp(z @ log_inv_disp.T), p=1 / (1 + s * (z @ np.exp(log_mean - log_inv_disp).T)))
  <span class="org-variable-name">x</span> = rng.poisson(lam)
  <span class="org-keyword">return</span> x, z, log_mean, log_inv_disp
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-org3b8829f" class="outline-2">
<h2 id="results"><a id="org3b8829f"></a>Results</h2>
<div class="outline-text-2" id="text-results">
</div>
<div id="outline-container-orgc1db78e" class="outline-3">
<h3 id="orgc1db78e">Simulated example</h3>
<div class="outline-text-3" id="text-orgc1db78e">
<p>
Simulate from the model.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">x</span>, <span class="org-variable-name">z</span>, <span class="org-variable-name">log_mean</span>, <span class="org-variable-name">log_inv_disp</span> = simulate(n=100, p=500, k=4, seed=1)
</pre>
</div>

<p>
Fit EM, starting from a random initialization.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> imp; imp.<span class="org-builtin">reload</span>(mpebpm.gam_mix)
<span class="org-variable-name">k</span> = 4
<span class="org-variable-name">seed</span> = 0
<span class="org-variable-name">lr</span> = 1e-2
<span class="org-variable-name">num_epochs</span> = 50
<span class="org-variable-name">max_em_iters</span> = 10
torch.manual_seed(seed)
<span class="org-variable-name">fit</span> = mpebpm.gam_mix.ebpm_gam_mix_em(
  x=x,
  s=x.<span class="org-builtin">sum</span>(axis=1, keepdims=<span class="org-constant">True</span>),
  k=k,
  lr=lr,
  num_epochs=num_epochs,
  max_em_iters=max_em_iters,
  log_dir=f<span class="org-string">'runs/nbmix/sim-{k}-{seed}-{lr:.1g}-{num_epochs}-{max_em_iters}'</span>)
</pre>
</div>

<p>
Evaluate the clustering accuracy.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">zhat</span> = pd.get_dummies(np.argmax(fit[-1], axis=1)).values.astype(<span class="org-builtin">bool</span>)
<span class="org-variable-name">idx</span> = np.array([np.argmax(z[zhat[:,k]].<span class="org-builtin">sum</span>(axis=0)) <span class="org-keyword">for</span> k <span class="org-keyword">in</span> <span class="org-builtin">range</span>(4)])
pd.Series({
  <span class="org-string">'accuracy'</span>: (np.argmax(z, axis=1) == idx[np.argmax(zhat, axis=1)]).mean(),
  <span class="org-string">'log_loss'</span>: np.where(z[:,idx], -np.log(fit[-1]), 0).<span class="org-builtin">sum</span>(),
  <span class="org-string">'nmi'</span>: skm.normalized_mutual_info_score(np.argmax(z, axis=1), idx[np.argmax(zhat, axis=1)]),
  <span class="org-string">'ari'</span>: skm.adjusted_rand_score(np.argmax(z, axis=1), idx[np.argmax(zhat, axis=1)])
})
</pre>
</div>

<pre class="example">
accuracy    1.0
log_loss    0.0
nmi         1.0
ari         1.0
dtype: float64
</pre>
</div>
</div>

<div id="outline-container-org54ab750" class="outline-3">
<h3 id="example"><a id="org54ab750"></a>Real data example</h3>
<div class="outline-text-3" id="text-example">
<p>
Read sorted immune cell scRNA-seq data
(<a href="https://dx.doi.org/10.1038/ncomms14049">Zheng et al. 2017</a>).
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">dat</span> = anndata.read_h5ad(<span class="org-string">'/scratch/midway2/aksarkar/ideas/zheng-10-way.h5ad'</span>)
</pre>
</div>

<p>
Get 256 B cells and 256 cytotoxic T cells.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">b_cells</span> = dat[dat.obs[<span class="org-string">'cell_type'</span>] == <span class="org-string">'b_cells'</span>]
sc.pp.subsample(b_cells, n_obs=256, random_state=0)
<span class="org-variable-name">t_cells</span> = dat[dat.obs[<span class="org-string">'cell_type'</span>] == <span class="org-string">'cytotoxic_t'</span>]
sc.pp.subsample(t_cells, n_obs=256)
<span class="org-variable-name">temp</span> = b_cells.concatenate(t_cells)
sc.pp.filter_genes(temp, min_counts=1)
</pre>
</div>

<p>
Plot a UMAP embedding of the data, coloring points by the ground truth
labels.
</p>

<div class="org-src-container">
<pre class="src src-ipython">sc.pp.pca(temp)
sc.pp.neighbors(temp)
sc.tl.umap(temp)
</pre>
</div>

<p>
Write out the estimated embedding.
</p>

<div class="org-src-container">
<pre class="src src-ipython">temp.write(<span class="org-string">'/scratch/midway2/aksarkar/singlecell/nbmix-example.h5ad'</span>)
</pre>
</div>

<p>
Read the annotated data.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">temp</span> = anndata.read_h5ad(<span class="org-string">'/scratch/midway2/aksarkar/singlecell/nbmix-example.h5ad'</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Dark2'</span>)
plt.clf()
plt.gcf().set_size_inches(3, 3)
<span class="org-keyword">for</span> i, c <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(temp.obs[<span class="org-string">'cell_type'</span>].unique()):
  plt.plot(*temp[temp.obs[<span class="org-string">'cell_type'</span>] == c].obsm[<span class="org-string">"X_umap"</span>].T, c=cm(i), marker=<span class="org-string">'.'</span>, ms=2, lw=0, label=f<span class="org-string">'{c}'</span>)
plt.legend(frameon=<span class="org-constant">False</span>, markerscale=4, handletextpad=0)
plt.xlabel(<span class="org-string">'UMAP 1'</span>)
plt.ylabel(<span class="org-string">'UMAP 2'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/nbmix.org/sim-ex.png" alt="sim-ex.png">
</p>
</div>
</div>
</div>

<div id="outline-container-org8f81d07" class="outline-3">
<h3 id="leiden"><a id="org8f81d07"></a>Leiden algorithm</h3>
<div class="outline-text-3" id="text-leiden">
<p>
Apply the Leiden algorithm (<a href="https://arxiv.org/abs/1810.08473">Traag et
al. 2018</a>) to the data (&lt;1 s).
</p>

<div class="org-src-container">
<pre class="src src-ipython">sc.tl.leiden(temp, random_state=0)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Dark2'</span>)
plt.clf()
plt.gcf().set_size_inches(3, 3)
<span class="org-keyword">for</span> i, c <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(temp.obs[<span class="org-string">'leiden'</span>].unique()):
  plt.plot(*temp[temp.obs[<span class="org-string">'leiden'</span>] == c].obsm[<span class="org-string">"X_umap"</span>].T, c=cm(i), marker=<span class="org-string">'.'</span>, ms=2, lw=0, label=f<span class="org-string">'Cluster {i}'</span>)
plt.legend(frameon=<span class="org-constant">False</span>, markerscale=4, handletextpad=0)
plt.xlabel(<span class="org-string">'UMAP 1'</span>)
plt.ylabel(<span class="org-string">'UMAP 2'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/nbmix.org/leiden-ex.png" alt="leiden-ex.png">
</p>
</div>
</div>
</div>

<div id="outline-container-orgb0a214e" class="outline-3">
<h3 id="mpebpm"><a id="orgb0a214e"></a>MPEBPM</h3>
<div class="outline-text-3" id="text-mpebpm">
<p>
First, start from the ground truth \(z\) (labels), and estimate the Gamma
expression models.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">fit0</span> = mpebpm.sgd.ebpm_gamma(
  temp.X,
  onehot=pd.get_dummies(temp.obs[<span class="org-string">'cell_type'</span>]).values,
  batch_size=32,
  num_epochs=320,
  shuffle=<span class="org-constant">True</span>,
  log_dir=<span class="org-string">'runs/nbmix/pretrain/'</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">np.savez(<span class="org-string">'/scratch/midway2/aksarkar/singlecell/nbmix-example-pretrain.npz'</span>, fit0)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">with</span> np.load(<span class="org-string">'/scratch/midway2/aksarkar/singlecell/nbmix-example-pretrain.npz'</span>) <span class="org-keyword">as</span> f:
  <span class="org-variable-name">fit0</span> = f[<span class="org-string">'arr_0'</span>]
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">y</span> = pd.get_dummies(temp.obs[<span class="org-string">'cell_type'</span>]).values
<span class="org-variable-name">s</span> = temp.X.<span class="org-builtin">sum</span>(axis=1)
<span class="org-variable-name">nb_llik</span> = y.T @ st.nbinom(n=np.exp(y @ fit0[1]), p=1 / (1 + s.A * (y @ np.exp(fit0[0] - fit0[1])))).logpmf(temp.X.A)
</pre>
</div>

<p>
For comparison, estimate a point mass expression model for each gene, for
each cluster.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">y</span> = pd.get_dummies(temp.obs[<span class="org-string">'cell_type'</span>]).values
<span class="org-variable-name">s</span> = temp.X.<span class="org-builtin">sum</span>(axis=1)
<span class="org-variable-name">fit_pois</span> = (y.T @ temp.X) / (y.T @ s)
<span class="org-variable-name">pois_llik</span> = y.T @ st.poisson(mu=s.A * (y @ fit_pois).A).logpmf(temp.X.A)
</pre>
</div>

<p>
For each gene, for each cluster, plot the log likelihood under the point
mass and Gamma expression models.
</p>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 2, sharey=<span class="org-constant">True</span>)
fig.set_size_inches(4.5, 2.5)
<span class="org-variable-name">lim</span> = [-1500, 0]
<span class="org-keyword">for</span> i, (a, t) <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(<span class="org-builtin">zip</span>(ax, [<span class="org-string">'B cell'</span>, <span class="org-string">'Cytotoxic T'</span>])):
  a.scatter(pois_llik[i], nb_llik[i], c=<span class="org-string">'k'</span>, s=1, alpha=0.2)
  a.plot(lim, lim, c=<span class="org-string">'r'</span>, lw=1, ls=<span class="org-string">':'</span>)
  a.set_xlim(lim)
  a.set_ylim(lim)
  a.set_title(t)
  a.set_xlabel(<span class="org-string">'Poisson log lik'</span>)
ax[0].set_ylabel(<span class="org-string">'NB log lik'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/nbmix.org/ex-pois-nb-llik.png" alt="ex-pois-nb-llik.png">
</p>
</div>

<p>
Look at the differences in the estimated mean parameter for each gene, to
see how many genes are informative about the labels.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">query</span> = np.sort(np.diff(fit0[0], axis=0).ravel())
plt.clf()
plt.gcf().set_size_inches(4, 2)
plt.plot(query, lw=1, c=<span class="org-string">'k'</span>)
plt.axhline(y=0, lw=1, ls=<span class="org-string">':'</span>, c=<span class="org-string">'k'</span>)
plt.xlabel(<span class="org-string">'Gene'</span>)
plt.ylabel(r<span class="org-string">'Diff $\ln(\mu_j)$'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/nbmix.org/mepbpm-log-mu.png" alt="mepbpm-log-mu.png">
</p>
</div>

<p>
Estimate the cluster weights.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">L</span> = mpebpm.gam_mix._nb_mix_llik(
  x=torch.tensor(temp.X.A, dtype=torch.<span class="org-builtin">float</span>), 
  s=torch.tensor(temp.X.<span class="org-builtin">sum</span>(axis=1), dtype=torch.<span class="org-builtin">float</span>),
  log_mean=torch.tensor(fit0[0], dtype=torch.<span class="org-builtin">float</span>),
  log_inv_disp=torch.tensor(fit0[1], dtype=torch.<span class="org-builtin">float</span>))
<span class="org-variable-name">zhat</span> = torch.nn.functional.softmax(L, dim=1)
</pre>
</div>

<p>
Plot the log likelihood difference between the two components for each data
point.
</p>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
plt.gcf().set_size_inches(4, 2)
plt.plot(np.diff(L).ravel(), lw=0, marker=<span class="org-string">'.'</span>, c=<span class="org-string">'k'</span>, ms=2)
plt.axhline(y=0, lw=1, ls=<span class="org-string">':'</span>, c=<span class="org-string">'k'</span>)
plt.xlabel(<span class="org-string">'Cell'</span>)
plt.ylabel(<span class="org-string">'Diff log lik'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/nbmix.org/mpebpm-llik-diff.png" alt="mpebpm-llik-diff.png">
</p>
</div>

<p>
Compute the cross entropy between the estimated \(\hat{z}\) and the ground
truth.
</p>

<div class="org-src-container">
<pre class="src src-ipython">torch.nn.functional.binary_cross_entropy(
  zhat,
  torch.tensor(pd.get_dummies(temp.obs[<span class="org-string">'cell_type'</span>]).values, dtype=torch.<span class="org-builtin">float</span>))
</pre>
</div>

<pre class="example">
tensor(0.)

</pre>

<p>
Compute a weighted log likelihood.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">w</span> = torch.rand([512, 2])
<span class="org-variable-name">w</span> /= w.<span class="org-builtin">sum</span>(dim=1).unsqueeze(-1)
<span class="org-variable-name">m</span>, <span class="org-variable-name">_</span> = L.<span class="org-builtin">max</span>(dim=1, keepdim=<span class="org-constant">True</span>)
(m + torch.log(w * torch.exp(L - m) + 1e-8)).mean()
</pre>
</div>

<pre class="example">
tensor(-1872.7723)

</pre>

<p>
Try fitting the model from a random initialization (4 s).
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> imp; imp.<span class="org-builtin">reload</span>(mpebpm.gam_mix)
torch.manual_seed(0)
<span class="org-variable-name">fit</span> = mpebpm.gam_mix.ebpm_gam_mix_em(
  x=temp.X.A,
  s=temp.X.<span class="org-builtin">sum</span>(axis=1),
  y=torch.tensor(pd.get_dummies(temp.obs[<span class="org-string">'cell_type'</span>]).values, dtype=torch.<span class="org-builtin">float</span>).cuda(),
  k=2,
  num_epochs=50,
  max_em_iters=8,
  log_dir=<span class="org-string">'runs/nbmix/mpebpm-random-init0-pois-50-8'</span>)
</pre>
</div>

<p>
Compare the log likelihood using the ground truth labels to the log
likelihood using the estimated cluster weights.
</p>

<div class="org-src-container">
<pre class="src src-ipython">pd.Series({<span class="org-string">'ground_truth'</span>: nb_llik.mean(),
           <span class="org-string">'est'</span>: (fit[2].T @ st.nbinom(n=np.exp(fit[2] @ fit[1]), p=1 / (1 + s.A * (fit[2] @ np.exp(fit[0] - fit[1])))).logpmf(temp.X.A)).mean()})
</pre>
</div>

<pre class="example">
ground_truth   -41.151726
est            -41.145436
dtype: float64
</pre>

<p>
Compute the cross entropy between the estimated \(\hat{z}\) and the ground
truth.
</p>

<div class="org-src-container">
<pre class="src src-ipython">torch.<span class="org-builtin">min</span>(
  torch.nn.functional.binary_cross_entropy(
    torch.tensor(fit[-1], dtype=torch.<span class="org-builtin">float</span>),
    torch.tensor(pd.get_dummies(temp.obs[<span class="org-string">'cell_type'</span>]).values, dtype=torch.<span class="org-builtin">float</span>)),
  torch.nn.functional.binary_cross_entropy(
    torch.tensor(1 - fit[-1], dtype=torch.<span class="org-builtin">float</span>),
    torch.tensor(pd.get_dummies(temp.obs[<span class="org-string">'cell_type'</span>]).values, dtype=torch.<span class="org-builtin">float</span>)))
</pre>
</div>

<pre class="example">
tensor(0.)

</pre>

<p>
Plot the UMAP, colored by the fitted clusters.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Dark2'</span>)
plt.clf()
plt.gcf().set_size_inches(3, 3)
<span class="org-keyword">for</span> i <span class="org-keyword">in</span> <span class="org-builtin">range</span>(fit[-1].shape[1]):
  plt.plot(*temp[fit[-1][:,i].astype(<span class="org-builtin">bool</span>)].obsm[<span class="org-string">"X_umap"</span>].T, c=cm(i), marker=<span class="org-string">'.'</span>, ms=2, lw=0, label=f<span class="org-string">'Cluster {i}'</span>)
plt.legend(frameon=<span class="org-constant">False</span>, markerscale=4, handletextpad=0)
plt.xlabel(<span class="org-string">'UMAP 1'</span>)
plt.ylabel(<span class="org-string">'UMAP 2'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/nbmix.org/sim-ex-fit.png" alt="sim-ex-fit.png">
</p>
</div>
</div>
</div>

<div id="outline-container-org9503c28" class="outline-3">
<h3 id="amortized"><a id="org9503c28"></a>Amortized inference</h3>
<div class="outline-text-3" id="text-amortized">
<p>
Construct the amortized inference model, initializing \(\vmu_k, \vphi_k\)
from the MLE starting from the ground-truth labels.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">query</span> = torch.tensor(temp.X.A)
<span class="org-variable-name">s</span> = torch.tensor(temp.X.<span class="org-builtin">sum</span>(axis=1))
<span class="org-variable-name">fit</span> = mpebpm.gam_mix.EBPMGammaMix(
  p=temp.shape[1],
  k=2,
  log_mean=fit0[0],
  log_inv_disp=fit0[1])
</pre>
</div>

<p>
Look at the initial loss.
</p>

<div class="org-src-container">
<pre class="src src-ipython">(
  mpebpm.gam_mix._nb_mix_loss(
    fit.encoder.forward(query),
    query,
    s,
    fit.log_mean,
    fit.log_inv_disp),
  mpebpm.gam_mix._nb_mix_loss(
    torch.tensor(pd.get_dummies(temp.obs[<span class="org-string">'cell_type'</span>]).values),
    query,
    s,
    fit.log_mean,
    fit.log_inv_disp)
)
</pre>
</div>

<pre class="example">
(tensor(1927253.3750, grad_fn=&lt;NegBackward&gt;),
tensor(1926631.7500, grad_fn=&lt;NegBackward&gt;))
</pre>

<p>
Look at the gradients with respect to the encoder network weights.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">temp_loss</span> = mpebpm.gam_mix._nb_mix_loss(
  fit.encoder.forward(query),
  query,
  torch.tensor(temp.X.<span class="org-builtin">sum</span>(axis=1)),
  fit.log_mean,
  fit.log_inv_disp)
temp_loss.retain_grad()
temp_loss.backward()
torch.norm(fit.encoder[0].weight.grad)
</pre>
</div>

<pre class="example">
tensor(31154.1895)

</pre>

<p>
Perform amortized inference, initializing \(\vmu_k, \vphi_k\)
at the batch GD clustering solution.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> imp; imp.<span class="org-builtin">reload</span>(mpebpm.gam_mix)
torch.manual_seed(0)
<span class="org-variable-name">fit1</span> = mpebpm.gam_mix.EBPMGammaMix(
  p=temp.shape[1],
  k=2,
  log_mean=fit0[0],
  log_inv_disp=fit0[1])
fit1.fit(
    x=temp.X.A,
    s=temp.X.<span class="org-builtin">sum</span>(axis=1),
    y=pd.get_dummies(temp.obs[<span class="org-string">'cell_type'</span>]).values,
    lr=1e-3,
    batch_size=64,
    shuffle=<span class="org-constant">True</span>,
    num_epochs=10,
    log_dir=<span class="org-string">'runs/nbmix/ai-freeze-64-1e-3-10'</span>)
</pre>
</div>

<pre class="example">
EBPMGammaMix(
(encoder): Sequential(
(0): Linear(in_features=11590, out_features=128, bias=True)
(1): ReLU()
(2): Linear(in_features=128, out_features=2, bias=True)
(3): Softmax(dim=1)
)
)
</pre>

<p>
Compute the cross entropy loss over the posterior mean cluster assignments.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">zhat</span> = fit1.forward(query.cuda()).detach().cpu().numpy()
torch.nn.functional.binary_cross_entropy(
  torch.tensor(zhat, dtype=torch.<span class="org-builtin">float</span>),
  torch.tensor(pd.get_dummies(temp.obs[<span class="org-string">'cell_type'</span>]).values, dtype=torch.<span class="org-builtin">float</span>))
</pre>
</div>

<pre class="example">
tensor(4.5151e-05)

</pre>

<p>
Plot the approximate posterior over cluster assignments for each point.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Dark2'</span>)
plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 2, sharex=<span class="org-constant">True</span>, sharey=<span class="org-constant">True</span>)
fig.set_size_inches(4.5, 2.5)
<span class="org-keyword">for</span> i, a <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(ax):
  a.scatter(*temp.obsm[<span class="org-string">"X_umap"</span>].T, s=4, c=np.hstack((np.tile(np.array(cm(i)[:3]), zhat.shape[0]).reshape(-1, 3), zhat[:,i].reshape(-1, 1))))
  a.set_xlabel(<span class="org-string">'UMAP 1'</span>)
ax[0].set_ylabel(<span class="org-string">'UMAP 2'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/nbmix.org/am-inf-ex.png" alt="am-inf-ex.png">
</p>
</div>

<p>
Try amortized inference, using the first minibatch to initialize.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> imp; imp.<span class="org-builtin">reload</span>(mpebpm.gam_mix)
torch.manual_seed(0)
np.histogram(np.argmax(mpebpm.gam_mix.EBPMGammaMix(p=temp.shape[1], k=2).forward(query).detach().cpu().numpy(), axis=1), np.arange(3))
</pre>
</div>

<pre class="example">
(array([393, 119]), array([0, 1, 2]))

</pre>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> imp; imp.<span class="org-builtin">reload</span>(mpebpm.gam_mix)
torch.manual_seed(1)
<span class="org-variable-name">fit1</span> = mpebpm.gam_mix.EBPMGammaMix(
  p=temp.shape[1],
  k=2)
fit1.fit(
    x=temp.X.A,
    s=temp.X.<span class="org-builtin">sum</span>(axis=1),
    y=pd.get_dummies(temp.obs[<span class="org-string">'cell_type'</span>]).values,
    lr=1e-2,
    batch_size=64,
    shuffle=<span class="org-constant">True</span>,
    num_epochs=100,
    log_dir=<span class="org-string">'runs/nbmix/ai-hack1-64-1e-2-100'</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
plt.gcf().set_size_inches(3, 3)
plt.plot(*fit1.log_mean.detach().cpu().numpy(), marker=<span class="org-string">'.'</span>, lw=0, ms=1, c=<span class="org-string">'k'</span>)
plt.xlabel(<span class="org-string">'Component 1 $\ln\mu$'</span>)
plt.ylabel(<span class="org-string">'Component 2 $\ln\mu$'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/nbmix.org/ai-fit-log-mean.png" alt="ai-fit-log-mean.png">
</p>
</div>
</div>
</div>

<div id="outline-container-org59c95a8" class="outline-3">
<h3 id="org59c95a8">(Full) 2-way example</h3>
<div class="outline-text-3" id="text-org59c95a8">
<p>
Apply the standard methodology (~1 min).
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">mix2</span> = dat[dat.obs[<span class="org-string">'cell_type'</span>].isin([<span class="org-string">'cytotoxic_t'</span>, <span class="org-string">'b_cells'</span>])]
sc.pp.filter_genes(mix2, min_counts=1)
sc.pp.pca(mix2, zero_center=<span class="org-constant">False</span>)
sc.pp.neighbors(mix2)
sc.tl.leiden(mix2)
sc.tl.umap(mix2)
mix2
</pre>
</div>

<pre class="example">
AnnData object with n_obs × n_vars = 20294 × 17808
obs: 'barcode', 'cell_type', 'leiden'
var: 'ensg', 'name', 'n_cells', 'n_counts'
uns: 'pca', 'neighbors', 'leiden', 'umap'
obsm: 'X_pca', 'X_umap'
varm: 'PCs'
obsp: 'distances', 'connectivities'
</pre>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Paired'</span>)
plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 2, sharex=<span class="org-constant">True</span>, sharey=<span class="org-constant">True</span>)
fig.set_size_inches(4.5, 4)
<span class="org-keyword">for</span> a, k, t <span class="org-keyword">in</span> <span class="org-builtin">zip</span>(ax, [<span class="org-string">'cell_type'</span>, <span class="org-string">'leiden'</span>], [<span class="org-string">'Ground truth'</span>, <span class="org-string">'Leiden'</span>]):
  <span class="org-keyword">for</span> i, c <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(mix2.obs[k].unique()):
    a.plot(*mix2[mix2.obs[k] == c].obsm[<span class="org-string">"X_umap"</span>].T, c=cm(i), marker=<span class="org-string">'.'</span>, ms=1, lw=0, alpha=0.1, label=f<span class="org-string">'{k}_{i}'</span>)
  <span class="org-variable-name">leg</span> = a.legend(frameon=<span class="org-constant">False</span>, markerscale=8, handletextpad=0, loc=<span class="org-string">'upper center'</span>, bbox_to_anchor=(.5, -.25), ncol=2)
  <span class="org-keyword">for</span> h <span class="org-keyword">in</span> leg.legendHandles:
    h._legmarker.set_alpha(1)
  a.set_xlabel(<span class="org-string">'UMAP 1'</span>)
  a.set_title(t)
ax[0].set_ylabel(<span class="org-string">'UMAP 2'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/nbmix.org/mix2.png" alt="mix2.png">
</p>
</div>

<p>
Take a subsample to run batch EM.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">mix2sub</span> = sc.pp.subsample(mix2, n_obs=1000, random_state=1, copy=<span class="org-constant">True</span>)
mix2sub.obs[<span class="org-string">'cell_type'</span>].value_counts()
</pre>
</div>

<pre class="example">
cytotoxic_t    513
b_cells        487
Name: cell_type, dtype: int64
</pre>

<p>
Run batch EM (45 s).
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> imp; imp.<span class="org-builtin">reload</span>(mpebpm.gam_mix)
<span class="org-variable-name">k</span> = 2
<span class="org-variable-name">seed</span> = 0
<span class="org-variable-name">lr</span> = 1e-2
<span class="org-variable-name">num_epochs</span> = 50
<span class="org-variable-name">max_em_iters</span> = 10
torch.manual_seed(seed)
<span class="org-variable-name">fit</span> = mpebpm.gam_mix.ebpm_gam_mix_em(
  x=mix2sub.X.A,
  s=mix2sub.X.<span class="org-builtin">sum</span>(axis=1),
  k=k,
  lr=lr,
  num_epochs=num_epochs,
  max_em_iters=max_em_iters,
  log_dir=f<span class="org-string">'runs/nbmix/mix2-init-{k}-{seed}-{lr:.1g}-{num_epochs}-{max_em_iters}'</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 2, sharex=<span class="org-constant">True</span>, sharey=<span class="org-constant">True</span>)
plt.gcf().set_size_inches(5, 3.5)

<span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Paired'</span>)
<span class="org-keyword">for</span> i, c <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(mix2sub.obs[<span class="org-string">'cell_type'</span>].unique()):
  ax[0].plot(*mix2sub[mix2sub.obs[<span class="org-string">'cell_type'</span>] == c].obsm[<span class="org-string">"X_umap"</span>].T, c=cm(i), marker=<span class="org-string">'.'</span>, ms=2, lw=0, label=f<span class="org-string">'{c}'</span>)
ax[0].set_title(<span class="org-string">'Ground truth'</span>)
<span class="org-variable-name">leg</span> = ax[0].legend(frameon=<span class="org-constant">False</span>, markerscale=8, handletextpad=0, loc=<span class="org-string">'upper center'</span>, bbox_to_anchor=(.5, -.25), ncol=2)
<span class="org-keyword">for</span> h <span class="org-keyword">in</span> leg.legendHandles:
  h._legmarker.set_alpha(1)

<span class="org-variable-name">z</span> = pd.get_dummies(np.argmax(fit[-1], axis=1)).values.astype(<span class="org-builtin">bool</span>)
<span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Dark2'</span>)
<span class="org-keyword">for</span> i <span class="org-keyword">in</span> <span class="org-builtin">range</span>(z.shape[1]):
  ax[1].plot(*mix2sub[z[:,i]].obsm[<span class="org-string">"X_umap"</span>].T, c=cm(i), marker=<span class="org-string">'.'</span>, ms=2, lw=0, label=f<span class="org-string">'Cluster {i}'</span>)
ax[1].set_title(<span class="org-string">'Batch EM'</span>)
<span class="org-variable-name">leg</span> = ax[1].legend(frameon=<span class="org-constant">False</span>, markerscale=8, handletextpad=0, loc=<span class="org-string">'upper center'</span>, bbox_to_anchor=(.5, -.25), ncol=2)
<span class="org-keyword">for</span> h <span class="org-keyword">in</span> leg.legendHandles:
  h._legmarker.set_alpha(1)

<span class="org-keyword">for</span> a <span class="org-keyword">in</span> ax:
  a.set_xlabel(<span class="org-string">'UMAP 1'</span>)
ax[0].set_ylabel(<span class="org-string">'UMAP 2'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/nbmix.org/mix2-init.png" alt="mix2-init.png">
</p>
</div>

<p>
Run amortized inference on the full data set, initializing components from
the batch EM solution. (Perfect accuracy in ~10 s, 49 s total)
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> imp; imp.<span class="org-builtin">reload</span>(mpebpm.gam_mix)
<span class="org-variable-name">seed</span> = 1
<span class="org-variable-name">lr</span> = 1e-3
<span class="org-variable-name">num_epochs</span> = 10
torch.manual_seed(0)
<span class="org-variable-name">fit1</span> = mpebpm.gam_mix.EBPMGammaMix(
  p=mix2.shape[1],
  k=2,
  log_mean=fit[0],
  log_inv_disp=fit[1])
fit1.fit(
    x=mix2.X.A,
    s=mix2.X.<span class="org-builtin">sum</span>(axis=1),
    y=pd.get_dummies(mix2.obs[<span class="org-string">'cell_type'</span>]).values,
    lr=1e-3,
    batch_size=64,
    shuffle=<span class="org-constant">True</span>,
    num_epochs=10,
    log_dir=f<span class="org-string">'runs/nbmix/mix2-full-{seed}-{lr:.1g}-{num_epochs}'</span>)
</pre>
</div>

<pre class="example">
EBPMGammaMix(
(encoder): Sequential(
(0): Log1p()
(1): Linear(in_features=17808, out_features=128, bias=True)
(2): ReLU()
(3): Linear(in_features=128, out_features=2, bias=True)
(4): Softmax(dim=1)
)
)
</pre>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">x</span> = mix2.X
<span class="org-variable-name">data</span> = mpebpm.sparse.SparseDataset(
  mpebpm.sparse.CSRTensor(x.data, x.indices, x.indptr, x.shape, dtype=torch.<span class="org-builtin">float</span>).cuda(),
  torch.tensor(mix2.X.<span class="org-builtin">sum</span>(axis=1), dtype=torch.<span class="org-builtin">float</span>).cuda())
<span class="org-variable-name">collate_fn</span> = <span class="org-builtin">getattr</span>(data, <span class="org-string">'collate_fn'</span>, td.dataloader.default_collate)
<span class="org-variable-name">data</span> = td.DataLoader(data, batch_size=64, shuffle=<span class="org-constant">False</span>, collate_fn=data.collate_fn)
<span class="org-variable-name">zhat</span> = []
<span class="org-keyword">with</span> torch.no_grad():
  <span class="org-keyword">for</span> x, s <span class="org-keyword">in</span> data:
    zhat.append(fit1.forward(x).cpu().numpy())
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">mix2.obs</span>[<span class="org-string">'comp'</span>] = np.argmax(np.vstack(zhat), axis=1)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 2, sharex=<span class="org-constant">True</span>, sharey=<span class="org-constant">True</span>)
fig.set_size_inches(5, 3)
<span class="org-keyword">for</span> a, k, t, cm <span class="org-keyword">in</span> <span class="org-builtin">zip</span>(ax, [<span class="org-string">'cell_type'</span>, <span class="org-string">'comp'</span>], [<span class="org-string">'Ground truth'</span>, <span class="org-string">'Online'</span>], [<span class="org-string">'Paired'</span>, <span class="org-string">'Dark2'</span>]):
  <span class="org-keyword">for</span> i, c <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(mix2.obs[k].unique()):
    a.plot(*mix2[mix2.obs[k] == c].obsm[<span class="org-string">"X_umap"</span>].T, c=plt.get_cmap(cm)(i), marker=<span class="org-string">'.'</span>, ms=1, lw=0, alpha=0.1, label=f<span class="org-string">'{k}_{i}'</span>)
  <span class="org-variable-name">leg</span> = a.legend(frameon=<span class="org-constant">False</span>, markerscale=8, handletextpad=0, loc=<span class="org-string">'upper center'</span>, bbox_to_anchor=(.5, -.25), ncol=2)
  <span class="org-keyword">for</span> h <span class="org-keyword">in</span> leg.legendHandles:
    h._legmarker.set_alpha(1)
  a.set_xlabel(<span class="org-string">'UMAP 1'</span>)
  a.set_title(t)
ax[0].set_ylabel(<span class="org-string">'UMAP 2'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/nbmix.org/mix2-full.png" alt="mix2-full.png">
</p>
</div>
</div>
</div>

<div id="outline-container-org60dbe51" class="outline-3">
<h3 id="org60dbe51">4-way example</h3>
<div class="outline-text-3" id="text-org60dbe51">
<p>
Pick 4 cell types which <i>a priori</i> should be easy to distinguish. Apply the
standard methodology (1.4 minutes).
</p>

<div class="org-src-container">
<pre class="src src-ipython">%%time
<span class="org-variable-name">mix4</span> = dat[dat.obs[<span class="org-string">'cell_type'</span>].isin([<span class="org-string">'cytotoxic_t'</span>, <span class="org-string">'regulatory_t'</span>, <span class="org-string">'b_cells'</span>, <span class="org-string">'cd14_monocytes'</span>])]
sc.pp.filter_genes(mix4, min_counts=1)
sc.pp.pca(mix4, zero_center=<span class="org-constant">False</span>)
sc.pp.neighbors(mix4)
sc.tl.leiden(mix4)
sc.tl.umap(mix4)
mix4
</pre>
</div>

<pre class="example">
AnnData object with n_obs × n_vars = 33169 × 19241
obs: 'barcode', 'cell_type', 'leiden'
var: 'ensg', 'name', 'n_cells', 'n_counts'
uns: 'pca', 'neighbors', 'leiden', 'umap'
obsm: 'X_pca', 'X_umap'
varm: 'PCs'
obsp: 'distances', 'connectivities'
</pre>

<div class="org-src-container">
<pre class="src src-ipython">mix4.write(<span class="org-string">'/scratch/midway2/aksarkar/singlecell/mix4.h5ad'</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">mix4</span> = anndata.read_h5ad(<span class="org-string">'/scratch/midway2/aksarkar/singlecell/mix4.h5ad'</span>)
</pre>
</div>

<p>
Plot the data, colored by ground truth label.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Paired'</span>)
plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 2, sharex=<span class="org-constant">True</span>, sharey=<span class="org-constant">True</span>)
fig.set_size_inches(4.5, 2.75)
<span class="org-keyword">for</span> a, k, t <span class="org-keyword">in</span> <span class="org-builtin">zip</span>(ax, [<span class="org-string">'cell_type'</span>, <span class="org-string">'leiden'</span>], [<span class="org-string">'Ground truth'</span>, <span class="org-string">'Leiden'</span>]):
  <span class="org-keyword">for</span> i, c <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(mix4.obs[k].unique()):
    a.plot(*mix4[mix4.obs[k] == c].obsm[<span class="org-string">"X_umap"</span>].T, c=cm(i), marker=<span class="org-string">'.'</span>, ms=1, lw=0, alpha=0.1)
  a.set_title(t)
<span class="org-keyword">for</span> a <span class="org-keyword">in</span> ax:
  a.set_xlabel(<span class="org-string">'UMAP 1'</span>)
ax[0].set_ylabel(<span class="org-string">'UMAP 2'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/nbmix.org/mix4.png" alt="mix4.png">
</p>
</div>

<p>
Assign each community detected by the Leiden algorithm to a ground truth
label, based on the maximally represented label in each community, then
assess the accuracy of the clustering.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">z</span> = pd.get_dummies(mix4.obs[<span class="org-string">'cell_type'</span>]).values.astype(<span class="org-builtin">bool</span>)
<span class="org-variable-name">zhat</span> = pd.get_dummies(mix4.obs[<span class="org-string">'leiden'</span>]).values.astype(<span class="org-builtin">bool</span>)
<span class="org-variable-name">idx</span> = np.array([np.argmax(z[zhat[:,k]].<span class="org-builtin">sum</span>(axis=0)) <span class="org-keyword">for</span> k <span class="org-keyword">in</span> <span class="org-builtin">range</span>(zhat.shape[1])])
(np.argmax(z, axis=1) == idx[np.argmax(zhat, axis=1)]).mean()
</pre>
</div>

<pre class="example">
0.9918900177876934

</pre>

<p>
Use a subset of the data to run batch EM.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">mix4sub</span> = sc.pp.subsample(mix4, n_obs=1000, random_state=1, copy=<span class="org-constant">True</span>)
mix4sub.obs[<span class="org-string">'cell_type'</span>].value_counts()
</pre>
</div>

<pre class="example">
b_cells           320
cytotoxic_t       315
regulatory_t      290
cd14_monocytes     75
Name: cell_type, dtype: int64
</pre>

<p>
Evaluate the accuracy of Leiden clustering in this subset.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">z</span> = pd.get_dummies(mix4sub.obs[<span class="org-string">'cell_type'</span>]).values.astype(<span class="org-builtin">bool</span>)
<span class="org-variable-name">zhat</span> = pd.get_dummies(mix4sub.obs[<span class="org-string">'leiden'</span>]).values.astype(<span class="org-builtin">bool</span>)
<span class="org-variable-name">idx</span> = np.array([np.argmax(z[zhat[:,k]].<span class="org-builtin">sum</span>(axis=0)) <span class="org-keyword">for</span> k <span class="org-keyword">in</span> <span class="org-builtin">range</span>(zhat.shape[1])])
(np.argmax(z, axis=1) == idx[np.argmax(zhat, axis=1)]).mean()
</pre>
</div>

<pre class="example">
0.996

</pre>

<p>
Evaluate the normalized mutual information.
</p>

<div class="org-src-container">
<pre class="src src-ipython">skm.normalized_mutual_info_score(np.argmax(z, axis=1), idx[np.argmax(zhat, axis=1)])
</pre>
</div>

<pre class="example">
0.9613444754486798

</pre>

<p>
Evaluate the adjusted Rand index.
</p>

<div class="org-src-container">
<pre class="src src-ipython">skm.adjusted_rand_score(np.argmax(z, axis=1), idx[np.argmax(zhat, axis=1)])
</pre>
</div>

<pre class="example">
0.9779140374557265

</pre>

<p>
As a sanity check, estimate the components given the ground truth labels,
get the MAP estimate of the mixture weights given the components, and report
the log loss of the MAP mixture assignments.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">z</span> = pd.get_dummies(mix4sub.obs[<span class="org-string">'cell_type'</span>]).values
<span class="org-variable-name">num_epochs</span> = 100
<span class="org-variable-name">fit0</span> = mpebpm.sgd.ebpm_gamma(
  mix4sub.X,
  onehot=z,
  batch_size=32,
  num_epochs=num_epochs,
  shuffle=<span class="org-constant">True</span>,
  log_dir=f<span class="org-string">'runs/nbmix/mix4-init-gt-{num_epochs}'</span>)
np.savez(<span class="org-string">'mix4-oracle.npz'</span>, fit0)
<span class="org-keyword">with</span> torch.no_grad():
  <span class="org-variable-name">L</span> = mpebpm.gam_mix._nb_mix_llik(
    x=torch.tensor(mix4sub.X.A, dtype=torch.<span class="org-builtin">float</span>), 
    s=torch.tensor(mix4sub.X.<span class="org-builtin">sum</span>(axis=1), dtype=torch.<span class="org-builtin">float</span>),
    log_mean=torch.tensor(fit0[0], dtype=torch.<span class="org-builtin">float</span>),
    log_inv_disp=torch.tensor(fit0[1], dtype=torch.<span class="org-builtin">float</span>))
  <span class="org-variable-name">zhat</span> = torch.nn.functional.softmax(L, dim=1).cpu().numpy()
<span class="org-variable-name">loss</span> = (z * np.log(zhat + 1e-16)).<span class="org-builtin">sum</span>()
loss
</pre>
</div>

<pre class="example">
0.0

</pre>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">with</span> np.load(<span class="org-string">'mix4-oracle.npz'</span>) <span class="org-keyword">as</span> f:
  <span class="org-variable-name">fit0</span> = f[<span class="org-string">'arr_0'</span>]
</pre>
</div>

<p>
Run batch EM, starting from a random \(E[z]\).
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> imp; imp.<span class="org-builtin">reload</span>(mpebpm.gam_mix)
<span class="org-variable-name">k</span> = 4
<span class="org-variable-name">seed</span> = 0
<span class="org-variable-name">lr</span> = 1e-2
<span class="org-variable-name">num_epochs</span> = 50
<span class="org-variable-name">max_em_iters</span> = 10
torch.manual_seed(seed)
<span class="org-variable-name">fit</span> = mpebpm.gam_mix.ebpm_gam_mix_em(
  x=mix4sub.X.A,
  s=mix4sub.X.<span class="org-builtin">sum</span>(axis=1),
  k=k,
  lr=lr,
  num_epochs=num_epochs,
  max_em_iters=max_em_iters,
  log_dir=f<span class="org-string">'runs/nbmix/mix4-init-{k}-{seed}-{lr:.1g}-{num_epochs}-{max_em_iters}'</span>)
np.savez(<span class="org-string">'mix4-init.npz'</span>, *fit)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">with</span> np.load(<span class="org-string">'mix4-init.npz'</span>) <span class="org-keyword">as</span> f:
  <span class="org-variable-name">fit</span> = [f[<span class="org-string">'arr_0'</span>], f[<span class="org-string">'arr_1'</span>], f[<span class="org-string">'arr_2'</span>]]
</pre>
</div>

<p>
Plot the MAP cluster assignments.
</p>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 2, sharex=<span class="org-constant">True</span>, sharey=<span class="org-constant">True</span>)
plt.gcf().set_size_inches(5, 3.5)

<span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Paired'</span>)
<span class="org-keyword">for</span> i, c <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(mix4sub.obs[<span class="org-string">'cell_type'</span>].unique()):
  ax[0].plot(*mix4sub[mix4sub.obs[<span class="org-string">'cell_type'</span>] == c].obsm[<span class="org-string">"X_umap"</span>].T, c=cm(i), marker=<span class="org-string">'.'</span>, ms=2, lw=0, label=f<span class="org-string">'{c}'</span>)
ax[0].set_title(<span class="org-string">'Ground truth'</span>)
<span class="org-variable-name">leg</span> = ax[0].legend(frameon=<span class="org-constant">False</span>, markerscale=8, handletextpad=0, loc=<span class="org-string">'upper center'</span>, bbox_to_anchor=(.5, -.25), ncol=2)
<span class="org-keyword">for</span> h <span class="org-keyword">in</span> leg.legendHandles:
  h._legmarker.set_alpha(1)

<span class="org-variable-name">z</span> = pd.get_dummies(np.argmax(fit[-1], axis=1)).values.astype(<span class="org-builtin">bool</span>)
<span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Dark2'</span>)
<span class="org-keyword">for</span> i <span class="org-keyword">in</span> <span class="org-builtin">range</span>(z.shape[1]):
  ax[1].plot(*mix4sub[z[:,i]].obsm[<span class="org-string">"X_umap"</span>].T, c=cm(i), marker=<span class="org-string">'.'</span>, ms=2, lw=0, label=f<span class="org-string">'Cluster {i}'</span>)
ax[1].set_title(<span class="org-string">'Batch EM'</span>)
<span class="org-variable-name">leg</span> = ax[1].legend(frameon=<span class="org-constant">False</span>, markerscale=8, handletextpad=0, loc=<span class="org-string">'upper center'</span>, bbox_to_anchor=(.5, -.25), ncol=2)
<span class="org-keyword">for</span> h <span class="org-keyword">in</span> leg.legendHandles:
  h._legmarker.set_alpha(1)

<span class="org-keyword">for</span> a <span class="org-keyword">in</span> ax:
  a.set_xlabel(<span class="org-string">'UMAP 1'</span>)
ax[0].set_ylabel(<span class="org-string">'UMAP 2'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/nbmix.org/mix4-init.png" alt="mix4-init.png">
</p>
</div>

<p>
Compare the expected log joint of the fitted model to the oracle log joint.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">pi</span> = torch.ones(4) / 4.
<span class="org-keyword">with</span> torch.no_grad():
  <span class="org-variable-name">oracle_log_joint</span> = mpebpm.gam_mix._nb_mix_loss(
    z=torch.tensor(pd.get_dummies(mix4sub.obs[<span class="org-string">'cell_type'</span>]).values, dtype=torch.<span class="org-builtin">float</span>),
    x=torch.tensor(mix4sub.X.A, dtype=torch.<span class="org-builtin">float</span>), 
    s=torch.tensor(mix4sub.X.<span class="org-builtin">sum</span>(axis=1), dtype=torch.<span class="org-builtin">float</span>),
    log_mean=torch.tensor(fit0[0], dtype=torch.<span class="org-builtin">float</span>),
    log_inv_disp=torch.tensor(fit0[1], dtype=torch.<span class="org-builtin">float</span>),
    pi=pi).numpy()
  <span class="org-variable-name">em_log_joint</span> = mpebpm.gam_mix._nb_mix_loss(
    z=torch.tensor(fit[2], dtype=torch.<span class="org-builtin">float</span>),
    x=torch.tensor(mix4sub.X.A, dtype=torch.<span class="org-builtin">float</span>), 
    s=torch.tensor(mix4sub.X.<span class="org-builtin">sum</span>(axis=1), dtype=torch.<span class="org-builtin">float</span>),
    log_mean=torch.tensor(fit[0], dtype=torch.<span class="org-builtin">float</span>),
    log_inv_disp=torch.tensor(fit[1], dtype=torch.<span class="org-builtin">float</span>),
    pi=pi).numpy()
pd.Series({<span class="org-string">'oracle'</span>: oracle_log_joint,
           <span class="org-string">'em'</span>: em_log_joint})
</pre>
</div>

<pre class="example">
oracle    455.64493
em        467.37347
dtype: object
</pre>

<p>
Look at marker gene expression, stratified by the ground truth labels, and
by the MAP cluster assignments.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">idx</span> = np.where(mix4sub.var[<span class="org-string">'name'</span>] == <span class="org-string">'CD74'</span>)[0]
plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(3, 1)
fig.set_size_inches(6, 4.5)
<span class="org-variable-name">grid</span> = np.linspace(0, .05, 1000)
ax[0].hist(mix4sub.X[:,idx].A.ravel(), bins=np.arange(mix4sub.X[:,idx].<span class="org-builtin">max</span>() + 1), color=<span class="org-string">'0.7'</span>)
ax[0].set_title(<span class="org-string">'CD74'</span>)
ax[0].set_xlabel(<span class="org-string">'Number of molecules'</span>)
ax[0].set_ylabel(<span class="org-string">'Number of cells'</span>)
<span class="org-keyword">for</span> a, cm, f, t, ls <span class="org-keyword">in</span> <span class="org-builtin">zip</span>(ax[1:], [<span class="org-string">'Paired'</span>, <span class="org-string">'Dark2'</span>], [fit0, fit], [<span class="org-string">'Oracle'</span>, <span class="org-string">'Batch EM'</span>], [mix4sub.obs[<span class="org-string">'cell_type'</span>].unique(), [f<span class="org-string">'Cluster {k}'</span> <span class="org-keyword">for</span> k <span class="org-keyword">in</span> <span class="org-builtin">range</span>(4)]]):
  <span class="org-keyword">for</span> i, l <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(ls):
    <span class="org-variable-name">F</span> = st.gamma(a=np.exp(f[1][i,idx]), scale=np.exp(f[0][i,idx] - f[1][i,idx])).cdf(grid)
    a.plot(grid, F, lw=1, c=plt.get_cmap(cm)(i), label=l)
  a.legend(frameon=<span class="org-constant">False</span>, loc=<span class="org-string">'center left'</span>, bbox_to_anchor=(1, .5))
  a.set_ylabel(<span class="org-string">'CDF'</span>)
ax[-1].set_xlabel(<span class="org-string">'Latent gene expression'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/nbmix.org/mix4-cd74.png" alt="mix4-cd74.png">
</p>
</div>

<p>
Fit NMF with \(k=4\).
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">temp</span> = mix4sub.X.tocoo()
<span class="org-variable-name">y</span> = matrix.sparseMatrix(i=pd.Series(temp.row + 1), j=pd.Series(temp.col + 1), x=pd.Series(temp.data), dims=pd.Series(temp.shape))
<span class="org-variable-name">fit</span> = ft.fit_poisson_nmf(y, k=4, numiter=100, method=<span class="org-string">'scd'</span>, control=rpy2.robjects.ListVector({<span class="org-string">'extrapolate'</span>: <span class="org-constant">True</span>}), verbose=<span class="org-constant">True</span>)
<span class="org-variable-name">fit</span> = {k: v <span class="org-keyword">for</span> k, v <span class="org-keyword">in</span> <span class="org-builtin">zip</span>(fit.names, fit)}
</pre>
</div>

<p>
Normalize the loadings/factors to get a topic model.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">l</span> = fit[<span class="org-string">'L'</span>]
<span class="org-variable-name">f</span> = fit[<span class="org-string">'F'</span>]
<span class="org-variable-name">weights</span> = l * f.<span class="org-builtin">sum</span>(axis=0)
<span class="org-variable-name">scale</span> = weights.<span class="org-builtin">sum</span>(axis=1, keepdims=<span class="org-constant">True</span>)
<span class="org-variable-name">weights</span> /= scale
<span class="org-variable-name">topics</span> = f / f.<span class="org-builtin">sum</span>(axis=0, keepdims=<span class="org-constant">True</span>)
</pre>
</div>

<p>
Plot the correlation of the estimated topic weights against the ground truth
labels.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">r</span> = np.corrcoef(pd.get_dummies(mix4sub.obs[<span class="org-string">'cell_type'</span>]).T, weights.T)[:4,4:]
plt.clf()
plt.gcf().set_size_inches(3, 3)
plt.imshow(r, cmap=colorcet.cm[<span class="org-string">'coolwarm'</span>], vmin=-1, vmax=1)
plt.xticks(<span class="org-builtin">range</span>(4), mix4sub.obs[<span class="org-string">'cell_type'</span>].unique(), rotation=90)
plt.xlabel(<span class="org-string">'Cell type'</span>)
plt.yticks(<span class="org-builtin">range</span>(4), <span class="org-builtin">range</span>(4))
plt.ylabel(<span class="org-string">'Topic'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/nbmix.org/mix4-nmf.png" alt="mix4-nmf.png">
</p>
</div>

<p>
Estimate the log loss of topic weights against the ground truth
labels. Assign topics to labels based on the maximum weight.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">z</span> = pd.get_dummies(mix4sub.obs[<span class="org-string">'cell_type'</span>]).values.astype(<span class="org-builtin">bool</span>)
<span class="org-variable-name">zhat</span> = pd.get_dummies(np.argmax(weights, axis=1)).values.astype(<span class="org-builtin">bool</span>)
<span class="org-variable-name">idx</span> = np.array([np.argmax(z[zhat[:,k]].<span class="org-builtin">sum</span>(axis=0)) <span class="org-keyword">for</span> k <span class="org-keyword">in</span> <span class="org-builtin">range</span>(zhat.shape[1])])
-(z * np.log(weights[:,idx])).<span class="org-builtin">sum</span>()
</pre>
</div>

<pre class="example">
26991.17997391195

</pre>

<p>
Use the maximal topic weight as the cluster assignment for each sample, and
compute the accuracy against the ground truth labels.
</p>

<div class="org-src-container">
<pre class="src src-ipython">(np.argmax(z, axis=1) == idx[np.argmax(zhat, axis=1)]).mean()
</pre>
</div>

<pre class="example">
0.813

</pre>

<p>
Evaluate the normalized mutual information.
</p>

<div class="org-src-container">
<pre class="src src-ipython">skm.normalized_mutual_info_score(np.argmax(z, axis=1), idx[np.argmax(zhat, axis=1)])
</pre>
</div>

<pre class="example">
0.729630650183215

</pre>

<p>
Evaluate the adjusted Rand index.
</p>

<div class="org-src-container">
<pre class="src src-ipython">skm.adjusted_rand_score(np.argmax(z, axis=1), idx[np.argmax(zhat, axis=1)])
</pre>
</div>

<pre class="example">
0.6346534587723618

</pre>

<p>
Look at the entropy of the topic weights, for samples which were (not)
assigned to the correct cluster.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">query</span> = np.argmax(z, axis=1) != idx[np.argmax(zhat, axis=1)]
-(weights[query] * np.log(weights[query])).mean(), -(weights[~query] * np.log(weights[~query])).mean()
</pre>
</div>

<pre class="example">
(0.16659664113615175, 0.07020479818554375)

</pre>

<p>
Fit batch EM on the subset, initializing \(E[z]\) using the topic weights.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> imp; imp.<span class="org-builtin">reload</span>(mpebpm.gam_mix)
<span class="org-variable-name">k</span> = 4
<span class="org-variable-name">seed</span> = 0
<span class="org-variable-name">lr</span> = 1e-2
<span class="org-variable-name">num_epochs</span> = 50
<span class="org-variable-name">max_em_iters</span> = 10
torch.manual_seed(seed)
<span class="org-variable-name">fit_init_z</span> = mpebpm.gam_mix.ebpm_gam_mix_em(
  x=mix4sub.X.A,
  s=mix4sub.X.<span class="org-builtin">sum</span>(axis=1),
  z=weights,
  k=k,
  lr=lr,
  num_epochs=num_epochs,
  max_em_iters=max_em_iters,
  log_dir=f<span class="org-string">'runs/nbmix/mix4-init-z-{k}-{seed}-{lr:.1g}-{num_epochs}-{max_em_iters}'</span>)
</pre>
</div>

<p>
For reference, compute the initial E step, given the topic weight
initialization.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">with</span> torch.no_grad():
  <span class="org-variable-name">L</span> = mpebpm.gam_mix._nb_mix_llik(
    x=torch.tensor(mix4sub.X.A, dtype=torch.<span class="org-builtin">float</span>), 
    s=torch.tensor(mix4sub.X.<span class="org-builtin">sum</span>(axis=1), dtype=torch.<span class="org-builtin">float</span>),
    log_mean=torch.tensor(np.log(topics).T, dtype=torch.<span class="org-builtin">float</span>),
    log_inv_disp=torch.zeros(topics.T.shape, dtype=torch.<span class="org-builtin">float</span>)).numpy()
</pre>
</div>

<p>
Compute the average log likelihood difference between the maximum component
and the others.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">query</span> = np.<span class="org-builtin">max</span>(L, axis=1, keepdims=<span class="org-constant">True</span>) - L
query[np.where(~np.isclose(query, 0))].mean()
</pre>
</div>

<pre class="example">
2329.182

</pre>

<p>
Estimate the log loss of cluster weights against the ground truth
labels. Assign labels to clusters based on the maximum weight.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">z</span> = pd.get_dummies(mix4sub.obs[<span class="org-string">'cell_type'</span>]).values.astype(<span class="org-builtin">bool</span>)
<span class="org-variable-name">zhat</span> = pd.get_dummies(np.argmax(fit_init_z[-1], axis=1)).values.astype(<span class="org-builtin">bool</span>)
<span class="org-variable-name">idx</span> = np.array([np.argmax(z[zhat[:,k]].<span class="org-builtin">sum</span>(axis=0)) <span class="org-keyword">for</span> k <span class="org-keyword">in</span> <span class="org-builtin">range</span>(4)])
np.where(z, -np.log(fit_init_z[-1][:,idx]), 0).<span class="org-builtin">sum</span>()
</pre>
</div>

<pre class="example">
inf

</pre>

<p>
Use the maximal topic weight as the cluster assignment for each sample, and
compute the accuracy against the ground truth labels.
</p>

<div class="org-src-container">
<pre class="src src-ipython">(idx[np.argmax(zhat, axis=1)] == np.argmax(z, axis=1)).mean()
</pre>
</div>

<pre class="example">
0.854

</pre>

<p>
Evaluate the normalized mutual information.
</p>

<div class="org-src-container">
<pre class="src src-ipython">skm.normalized_mutual_info_score(np.argmax(z, axis=1), idx[np.argmax(zhat, axis=1)])
</pre>
</div>

<pre class="example">
0.7343759402985899

</pre>

<p>
Evaluate the adjusted Rand index.
</p>

<div class="org-src-container">
<pre class="src src-ipython">skm.adjusted_rand_score(np.argmax(z, axis=1), idx[np.argmax(zhat, axis=1)])
</pre>
</div>

<pre class="example">
0.6751367652638485

</pre>

<p>
Compare the cluster assignments from NMF to the cluster assignments from the
mixture of NBs.
</p>

<div class="org-src-container">
<pre class="src src-ipython">(np.argmax(fit_init_z[-1][:,idx], axis=1) == np.argmax(weights[:,idx], axis=1)).mean()
</pre>
</div>

<pre class="example">
0.913

</pre>

<p>
Plot the MAP cluster assignments.
</p>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 2, sharex=<span class="org-constant">True</span>, sharey=<span class="org-constant">True</span>)
plt.gcf().set_size_inches(5, 3.5)

<span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Paired'</span>)
<span class="org-keyword">for</span> i, c <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(mix4sub.obs[<span class="org-string">'cell_type'</span>].unique()):
  ax[0].plot(*mix4sub[mix4sub.obs[<span class="org-string">'cell_type'</span>] == c].obsm[<span class="org-string">"X_umap"</span>].T, c=cm(i), marker=<span class="org-string">'.'</span>, ms=2, lw=0, label=f<span class="org-string">'{c}'</span>)
ax[0].set_title(<span class="org-string">'Ground truth'</span>)
<span class="org-variable-name">leg</span> = ax[0].legend(frameon=<span class="org-constant">False</span>, markerscale=8, handletextpad=0, loc=<span class="org-string">'upper center'</span>, bbox_to_anchor=(.5, -.25), ncol=2)
<span class="org-keyword">for</span> h <span class="org-keyword">in</span> leg.legendHandles:
  h._legmarker.set_alpha(1)

<span class="org-variable-name">z</span> = pd.get_dummies(np.argmax(fit_init_z[-1], axis=1)).values.astype(<span class="org-builtin">bool</span>)
<span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Dark2'</span>)
<span class="org-keyword">for</span> i <span class="org-keyword">in</span> <span class="org-builtin">range</span>(z.shape[1]):
  ax[1].plot(*mix4sub[z[:,i]].obsm[<span class="org-string">"X_umap"</span>].T, c=cm(i), marker=<span class="org-string">'.'</span>, ms=2, lw=0, label=f<span class="org-string">'Cluster {i}'</span>)
ax[1].set_title(<span class="org-string">'Batch EM (initial $\mathrm{E}[z]$)'</span>)
<span class="org-variable-name">leg</span> = ax[1].legend(frameon=<span class="org-constant">False</span>, markerscale=8, handletextpad=0, loc=<span class="org-string">'upper center'</span>, bbox_to_anchor=(.5, -.25), ncol=2)
<span class="org-keyword">for</span> h <span class="org-keyword">in</span> leg.legendHandles:
  h._legmarker.set_alpha(1)

<span class="org-keyword">for</span> a <span class="org-keyword">in</span> ax:
  a.set_xlabel(<span class="org-string">'UMAP 1'</span>)
ax[0].set_ylabel(<span class="org-string">'UMAP 2'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/nbmix.org/mix4-init-z.png" alt="mix4-init-z.png">
</p>
</div>

<p>
Fit batch EM on the subset, initializing \(\mu\) using the topics.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> imp; imp.<span class="org-builtin">reload</span>(mpebpm.gam_mix)
<span class="org-variable-name">k</span> = 4
<span class="org-variable-name">seed</span> = 0
<span class="org-variable-name">lr</span> = 1e-2
<span class="org-variable-name">num_epochs</span> = 50
<span class="org-variable-name">max_em_iters</span> = 10
torch.manual_seed(seed)
<span class="org-variable-name">fit_init_mu</span> = mpebpm.gam_mix.ebpm_gam_mix_em(
  x=mix4sub.X.A,
  s=mix4sub.X.<span class="org-builtin">sum</span>(axis=1),
  log_mean=np.log(topics).T,
  k=k,
  lr=lr,
  num_epochs=num_epochs,
  max_em_iters=max_em_iters,
  log_dir=f<span class="org-string">'runs/nbmix/mix4-init-mu-{k}-{seed}-{lr:.1g}-{num_epochs}-{max_em_iters}'</span>)
</pre>
</div>

<p>
Plot the MAP cluster assignments.
</p>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 2, sharex=<span class="org-constant">True</span>, sharey=<span class="org-constant">True</span>)
plt.gcf().set_size_inches(5, 3.5)

<span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Paired'</span>)
<span class="org-keyword">for</span> i, c <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(mix4sub.obs[<span class="org-string">'cell_type'</span>].unique()):
  ax[0].plot(*mix4sub[mix4sub.obs[<span class="org-string">'cell_type'</span>] == c].obsm[<span class="org-string">"X_umap"</span>].T, c=cm(i), marker=<span class="org-string">'.'</span>, ms=2, lw=0, label=f<span class="org-string">'{c}'</span>)
ax[0].set_title(<span class="org-string">'Ground truth'</span>)
<span class="org-variable-name">leg</span> = ax[0].legend(frameon=<span class="org-constant">False</span>, markerscale=8, handletextpad=0, loc=<span class="org-string">'upper center'</span>, bbox_to_anchor=(.5, -.25), ncol=2)
<span class="org-keyword">for</span> h <span class="org-keyword">in</span> leg.legendHandles:
  h._legmarker.set_alpha(1)

<span class="org-variable-name">z</span> = pd.get_dummies(np.argmax(fit_init_mu[-1], axis=1)).values.astype(<span class="org-builtin">bool</span>)
<span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Dark2'</span>)
<span class="org-keyword">for</span> i <span class="org-keyword">in</span> <span class="org-builtin">range</span>(z.shape[1]):
  ax[1].plot(*mix4sub[z[:,i]].obsm[<span class="org-string">"X_umap"</span>].T, c=cm(i), marker=<span class="org-string">'.'</span>, ms=2, lw=0, label=f<span class="org-string">'Cluster {i}'</span>)
ax[1].set_title(<span class="org-string">'Batch EM (initial $\mu$)'</span>)
<span class="org-variable-name">leg</span> = ax[1].legend(frameon=<span class="org-constant">False</span>, markerscale=8, handletextpad=0, loc=<span class="org-string">'upper center'</span>, bbox_to_anchor=(.5, -.25), ncol=2)
<span class="org-keyword">for</span> h <span class="org-keyword">in</span> leg.legendHandles:
  h._legmarker.set_alpha(1)

<span class="org-keyword">for</span> a <span class="org-keyword">in</span> ax:
  a.set_xlabel(<span class="org-string">'UMAP 1'</span>)
ax[0].set_ylabel(<span class="org-string">'UMAP 2'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/nbmix.org/mix4-init-mu.png" alt="mix4-init-mu.png">
</p>
</div>

<p>
Fit a logistic regression predicting cytotoxic T vs. Treg from gene
expression. Report the validation set prediction accuracy.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">query</span> = dat[dat.obs[<span class="org-string">'cell_type'</span>].isin([<span class="org-string">'cytotoxic_t'</span>, <span class="org-string">'regulatory_t'</span>])]
<span class="org-variable-name">x_train</span>, <span class="org-variable-name">x_val</span>, <span class="org-variable-name">y_train</span>, <span class="org-variable-name">y_val</span> = skms.train_test_split(
  query.X,
  (query.obs[<span class="org-string">'cell_type'</span>] == <span class="org-string">'cytotoxic_t'</span>).astype(<span class="org-builtin">float</span>),
  test_size=.1)
<span class="org-variable-name">m</span> = sklm.SGDClassifier(loss=<span class="org-string">'log'</span>).fit(x_train, y_train)
m.score(x_val, y_val)
</pre>
</div>

<pre class="example">
0.9990234375

</pre>

<p>
For reference, fit a logistic regression using size factor as the predictor,
and report the validation set prediction accuracy.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">query</span> = dat[dat.obs[<span class="org-string">'cell_type'</span>].isin([<span class="org-string">'cytotoxic_t'</span>, <span class="org-string">'regulatory_t'</span>])]
<span class="org-variable-name">x_train</span>, <span class="org-variable-name">x_val</span>, <span class="org-variable-name">y_train</span>, <span class="org-variable-name">y_val</span> = skms.train_test_split(
  query.X.<span class="org-builtin">sum</span>(axis=1),
  (query.obs[<span class="org-string">'cell_type'</span>] == <span class="org-string">'cytotoxic_t'</span>).astype(<span class="org-builtin">float</span>),
  test_size=.1)
<span class="org-variable-name">m</span> = sklm.SGDClassifier(loss=<span class="org-string">'log'</span>).fit(x_train, y_train)
m.score(x_val, y_val)
</pre>
</div>

<pre class="example">
0.64501953125

</pre>

<p>
Try amortized inference on the full data set directly, initialized from the
NMF solution on the subset.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> imp; imp.<span class="org-builtin">reload</span>(mpebpm.gam_mix)
<span class="org-variable-name">seed</span> = 3
<span class="org-variable-name">lr</span> = 1e-3
<span class="org-variable-name">num_epochs</span> = 10
torch.manual_seed(0)
<span class="org-variable-name">fit1</span> = mpebpm.gam_mix.EBPMGammaMix(
  p=mix4.shape[1],
  k=4,
  log_mean=np.log(topics).T)
fit1.fit(
  x=mix4.X.A,
  s=mix4.X.<span class="org-builtin">sum</span>(axis=1),
  lr=1e-3,
  batch_size=64,
  shuffle=<span class="org-constant">True</span>,
  num_epochs=10,
  log_dir=f<span class="org-string">'runs/nbmix/mix4-full-{seed}-{lr:.1g}-{num_epochs}'</span>)
<span class="org-variable-name">x</span> = mix4.X
<span class="org-variable-name">data</span> = mpebpm.sparse.SparseDataset(
  mpebpm.sparse.CSRTensor(x.data, x.indices, x.indptr, x.shape, dtype=torch.<span class="org-builtin">float</span>).cuda(),
  torch.tensor(mix4.X.<span class="org-builtin">sum</span>(axis=1), dtype=torch.<span class="org-builtin">float</span>).cuda())
<span class="org-variable-name">collate_fn</span> = <span class="org-builtin">getattr</span>(data, <span class="org-string">'collate_fn'</span>, td.dataloader.default_collate)
<span class="org-variable-name">data</span> = td.DataLoader(data, batch_size=64, shuffle=<span class="org-constant">False</span>, collate_fn=data.collate_fn)
<span class="org-variable-name">zhat</span> = []
<span class="org-keyword">with</span> torch.no_grad():
  <span class="org-keyword">for</span> x, s <span class="org-keyword">in</span> data:
    zhat.append(fit1.forward(x).cpu().numpy())
<span class="org-variable-name">mix4.obs</span>[<span class="org-string">'comp'</span>] = np.argmax(np.vstack(zhat), axis=1)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 2, sharex=<span class="org-constant">True</span>, sharey=<span class="org-constant">True</span>)
fig.set_size_inches(4.5, 3)
<span class="org-keyword">for</span> a, k, t, cm <span class="org-keyword">in</span> <span class="org-builtin">zip</span>(ax, [<span class="org-string">'cell_type'</span>, <span class="org-string">'comp'</span>], [<span class="org-string">'Ground truth'</span>, <span class="org-string">'Online'</span>], [<span class="org-string">'Paired'</span>, <span class="org-string">'Dark2'</span>]):
  <span class="org-keyword">for</span> i, c <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(mix4.obs[k].unique()):
    a.plot(*mix4[mix4.obs[k] == c].obsm[<span class="org-string">"X_umap"</span>].T, c=plt.get_cmap(cm)(i), marker=<span class="org-string">'.'</span>, ms=1, lw=0, alpha=0.1, label=f<span class="org-string">'{k}_{i}'</span>)
  <span class="org-variable-name">leg</span> = a.legend(frameon=<span class="org-constant">False</span>, markerscale=8, handletextpad=0, loc=<span class="org-string">'upper center'</span>, bbox_to_anchor=(.5, -.25), ncol=2)
  <span class="org-keyword">for</span> h <span class="org-keyword">in</span> leg.legendHandles:
    h._legmarker.set_alpha(1)
  a.set_xlabel(<span class="org-string">'UMAP 1'</span>)
  a.set_title(t)
ax[0].set_ylabel(<span class="org-string">'UMAP 2'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/nbmix.org/mix4-full.png" alt="mix4-full.png">
</p>
</div>
</div>
</div>

<div id="outline-container-org2b76619" class="outline-3">
<h3 id="org2b76619">10-way example</h3>
<div class="outline-text-3" id="text-org2b76619">
<p>
Apply the standard methodology to the 10-way mixture.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">dat</span> = anndata.read_h5ad(<span class="org-string">'/scratch/midway2/aksarkar/ideas/zheng-10-way.h5ad'</span>)
<span class="org-comment-delimiter"># </span><span class="org-comment">Important: this is required for sparse data; otherwise, scanpy makes a dense</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">copy and runs out of memory</span>
sc.pp.pca(dat, zero_center=<span class="org-constant">False</span>)
sc.pp.neighbors(dat)
sc.tl.umap(dat)
sc.tl.leiden(dat)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">dat.write(<span class="org-string">'/scratch/midway2/aksarkar/ideas/mix10.h5ad'</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">dat</span> = anndata.read_h5ad(<span class="org-string">'/scratch/midway2/aksarkar/ideas/mix10.h5ad'</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Paired'</span>)
plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 2, sharex=<span class="org-constant">True</span>, sharey=<span class="org-constant">True</span>)
fig.set_size_inches(6, 3)
<span class="org-keyword">for</span> i, c <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(dat.obs[<span class="org-string">'cell_type'</span>].unique()):
  ax[0].plot(*dat[dat.obs[<span class="org-string">'cell_type'</span>] == c].obsm[<span class="org-string">"X_umap"</span>].T, c=cm(i), marker=<span class="org-string">'.'</span>, ms=1, lw=0, label=f<span class="org-string">'{c}'</span>, alpha=0.1)
<span class="org-keyword">for</span> i, c <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(dat.obs[<span class="org-string">'leiden'</span>].unique()):
  ax[1].plot(*dat[dat.obs[<span class="org-string">'leiden'</span>] == c].obsm[<span class="org-string">"X_umap"</span>].T, c=colorcet.cm[<span class="org-string">'fire'</span>]((<span class="org-builtin">int</span>(c) + .5) / 22), marker=<span class="org-string">'.'</span>, ms=1, lw=0, label=f<span class="org-string">'Cluster {i}'</span>, alpha=0.1)
<span class="org-keyword">for</span> a, t <span class="org-keyword">in</span> <span class="org-builtin">zip</span>(ax, [<span class="org-string">'Ground truth'</span>, <span class="org-string">'Leiden'</span>]):
  a.set_title(t)
<span class="org-keyword">for</span> a <span class="org-keyword">in</span> ax:
  <span class="org-comment-delimiter"># </span><span class="org-comment">a.legend(markerscale=4, handletextpad=0)</span>
  a.set_xlabel(<span class="org-string">'UMAP 1'</span>)
ax[0].set_ylabel(<span class="org-string">'UMAP 2'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/nbmix.org/mix10-full-std.png" alt="mix10-full-std.png">
</p>
</div>

<p>
Subsample the 10-way mixture of sorted cells to get an initialization.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">mix10</span> = sc.pp.subsample(dat, fraction=0.01, copy=<span class="org-constant">True</span>)
sc.pp.filter_genes(mix10, min_counts=1)
sc.pp.pca(mix10, zero_center=<span class="org-constant">False</span>)
sc.pp.neighbors(mix10)
sc.tl.umap(mix10)
sc.tl.leiden(mix10)
mix10.write(<span class="org-string">'/scratch/midway2/aksarkar/ideas/mix10-init.h5ad'</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">mix10</span> = anndata.read_h5ad(<span class="org-string">'/scratch/midway2/aksarkar/ideas/mix10-init.h5ad'</span>)
<span class="org-variable-name">y</span> = pd.get_dummies(mix10.obs[<span class="org-string">'cell_type'</span>]).values
</pre>
</div>

<p>
Try running UMAP directly on the sparse data.
</p>

<div class="org-src-container">
<pre class="src src-ipython">%%time
<span class="org-variable-name">embedding</span> = umap.UMAP(metric=<span class="org-string">'cosine'</span>, random_state=0).fit_transform(mix10.X)
</pre>
</div>

<p>
CPU times: user 3.75 s, sys: 41 ms, total: 3.79 s
Wall time: 3.8 s
</p>

<p>
Run NMF on the sparse data, and estimate latent gene expression. Then,
estimate a UMAP embedding on latent gene expression.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> rpy2.robjects.packages
<span class="org-keyword">import</span> rpy2.robjects.pandas2ri
rpy2.robjects.pandas2ri.activate()
<span class="org-variable-name">matrix</span> = rpy2.robjects.packages.importr(<span class="org-string">'Matrix'</span>)
<span class="org-variable-name">fasttopics</span> = rpy2.robjects.packages.importr(<span class="org-string">'fastTopics'</span>)

<span class="org-variable-name">temp</span> = mix10.X.tocoo()
<span class="org-variable-name">y</span> = matrix.sparseMatrix(i=pd.Series(temp.row + 1), j=pd.Series(temp.col + 1), x=pd.Series(temp.data), dims=pd.Series(temp.shape))
<span class="org-variable-name">res</span> = fasttopics.fit_poisson_nmf(y, k=10, numiter=40, method=<span class="org-string">'scd'</span>, control=rpy2.robjects.ListVector({<span class="org-string">'extrapolate'</span>: <span class="org-constant">True</span>}), verbose=<span class="org-constant">True</span>)
<span class="org-variable-name">lam</span> = np.array(res.rx2(<span class="org-string">'L'</span>)) @ np.array(res.rx2(<span class="org-string">'F'</span>)).T
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">nmf_umap</span> = umap.UMAP(metric=<span class="org-string">'cosine'</span>, random_state=0, n_neighbors=10).fit_transform(lam)
</pre>
</div>

<p>
Compare the UMAP embeddings.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Paired'</span>)
plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 3)
fig.set_size_inches(7.5, 2.5)
<span class="org-keyword">for</span> i, c <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(mix10.obs[<span class="org-string">'cell_type'</span>].unique()):
  ax[0].plot(*mix10[mix10.obs[<span class="org-string">'cell_type'</span>] == c].obsm[<span class="org-string">"X_umap"</span>].T, c=cm(i), marker=<span class="org-string">'.'</span>, ms=1, lw=0, label=f<span class="org-string">'{c}'</span>)
  ax[1].plot(*embedding[mix10.obs[<span class="org-string">'cell_type'</span>] == c].T, c=cm(i), marker=<span class="org-string">'.'</span>, ms=1, lw=0, label=f<span class="org-string">'{c}'</span>)
  ax[2].plot(*nmf_umap[mix10.obs[<span class="org-string">'cell_type'</span>] == c].T, c=cm(i), marker=<span class="org-string">'.'</span>, ms=1, lw=0, label=f<span class="org-string">'{c}'</span>)
<span class="org-keyword">for</span> a, t <span class="org-keyword">in</span> <span class="org-builtin">zip</span>(ax, [<span class="org-string">'Euclidean/PCA'</span>, <span class="org-string">'Cosine/counts'</span>, <span class="org-string">'Cosine/latent'</span>]):
  a.set_title(t)
  a.set_xlabel(<span class="org-string">'UMAP 1'</span>)
ax[0].set_ylabel(<span class="org-string">'UMAP 2'</span>)
ax[-1].legend(frameon=<span class="org-constant">False</span>, loc=<span class="org-string">'center left'</span>, bbox_to_anchor=(1, .5), handletextpad=0, markerscale=8)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/nbmix.org/mix10-umap.png" alt="mix10-umap.png">
</p>
</div>

<p>
Fit point mass expression models, given the ground truth labels.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">mean</span> = (y.T @ mix10.X) / (y.T @ mix10.X.<span class="org-builtin">sum</span>(axis=1))
</pre>
</div>

<p>
Estimate the posterior cluster weights, given the components. Then, estimate
the log loss against the ground truth labels.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">L</span> = st.poisson(mu=np.expand_dims(mean, 0)).logpmf(np.expand_dims(mix10.X.A, 1)).<span class="org-builtin">sum</span>(axis=2)
<span class="org-variable-name">zhat</span> = sp.softmax(L, axis=1)
-(y * np.log(zhat + 1e-16)).<span class="org-builtin">sum</span>()
</pre>
</div>

<pre class="example">
-0.0

</pre>

<p>
Run EM, starting from a random initialization.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> imp; imp.<span class="org-builtin">reload</span>(mpebpm.gam_mix)
<span class="org-variable-name">num_epochs</span> = 50
<span class="org-variable-name">max_em_iters</span> = 8
<span class="org-variable-name">seed</span> = 4
torch.manual_seed(seed)
<span class="org-variable-name">init</span> = mpebpm.gam_mix.ebpm_gam_mix_em(
  x=mix10.X.A,
  s=mix10.X.<span class="org-builtin">sum</span>(axis=1),
  k=10,
  num_epochs=num_epochs,
  max_em_iters=max_em_iters,
  log_dir=f<span class="org-string">'runs/nbmix/mix10-init{seed}-{num_epochs}-{max_em_iters}'</span>)
</pre>
</div>

<p>
Plot the correlation between the posterior cluster weights and the ground
truth labels.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">r</span> = np.corrcoef(y.T, init[-1].T)
plt.clf()
plt.gcf().set_size_inches(3, 3)
plt.imshow(r[:10,10:], cmap=colorcet.cm[<span class="org-string">'coolwarm'</span>], vmin=-1, vmax=1)
<span class="org-variable-name">c</span> = plt.colorbar(shrink=0.5)
c.set_label(<span class="org-string">'Correlation'</span>)
plt.xticks(np.arange(10))
plt.yticks(np.arange(10))
plt.xlabel(<span class="org-string">'Ground truth label'</span>)
plt.ylabel(<span class="org-string">'Component'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/nbmix.org/mix10-corr.png" alt="mix10-corr.png">
</p>
</div>

<p>
Plot the embedding of the data, colored by the ground truth labels, the
Leiden cluster assignments, and the model-based cluster assignments.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">zhat</span> = np.argmax(init[-1], axis=1)
<span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Paired'</span>)
plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 3, sharex=<span class="org-constant">True</span>, sharey=<span class="org-constant">True</span>)
fig.set_size_inches(7.5, 3)
<span class="org-keyword">for</span> i, c <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(mix10.obs[<span class="org-string">'cell_type'</span>].unique()):
  ax[0].plot(*mix10[mix10.obs[<span class="org-string">'cell_type'</span>] == c].obsm[<span class="org-string">"X_umap"</span>].T, c=cm(i), marker=<span class="org-string">'.'</span>, ms=1, lw=0, label=f<span class="org-string">'{c}'</span>)
<span class="org-keyword">for</span> i, c <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(mix10.obs[<span class="org-string">'leiden'</span>].unique()):
  ax[1].plot(*mix10[mix10.obs[<span class="org-string">'leiden'</span>] == c].obsm[<span class="org-string">"X_umap"</span>].T, c=cm(i), marker=<span class="org-string">'.'</span>, ms=1, lw=0, label=f<span class="org-string">'Cluster {i}'</span>)
<span class="org-keyword">for</span> i, c <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(pd.Series(zhat).unique()):
  ax[2].plot(*mix10[zhat == c].obsm[<span class="org-string">"X_umap"</span>].T, c=cm(i), marker=<span class="org-string">'.'</span>, ms=1, lw=0, label=f<span class="org-string">'Component {i}'</span>)
<span class="org-keyword">for</span> a, t <span class="org-keyword">in</span> <span class="org-builtin">zip</span>(ax, [<span class="org-string">'Ground truth'</span>, <span class="org-string">'Leiden'</span>, <span class="org-string">'NB mix'</span>]):
  a.set_title(t)
<span class="org-keyword">for</span> a <span class="org-keyword">in</span> ax:
  <span class="org-comment-delimiter"># </span><span class="org-comment">a.legend(markerscale=4, handletextpad=0)</span>
  a.set_xlabel(<span class="org-string">'UMAP 1'</span>)
ax[0].set_ylabel(<span class="org-string">'UMAP 2'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/nbmix.org/mix10-init-std.png" alt="mix10-init-std.png">
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org07ac419" class="outline-2">
<h2 id="related-work"><a id="org07ac419"></a>Related work</h2>
<div class="outline-text-2" id="text-related-work">
</div>
<div id="outline-container-org6a2975a" class="outline-3">
<h3 id="em-nb"><a id="org6a2975a"></a>EM for Poisson-Gamma</h3>
<div class="outline-text-3" id="text-em-nb">
<p>
Considering just a simple Gamma prior,
<a href="https://doi.org/10.1017/S0515036100014033">Karlis 2005</a> gives an EM
algorithm for maximizing the marginal likelihood. The key idea is that, due
to Poisson-Gamma conjugacy, the exact posterior is analytic, as are the
necessary posterior moments. The main disadvantage of this approach is that
it requires (one-dimensional) numerical optimization in the M step.
</p>

\begin{align}
  x_i \mid \xiplus, \lambda_i &\sim \Pois(\xiplus \lambda_i)\\
  \lambda_i \mid \alpha, \beta &\sim \Gam(\alpha, \beta)\\
  \lambda_i \mid x_i, \xiplus, \alpha, \beta &\sim q \triangleq \Gam(x_i + \alpha, \xiplus + \beta)\\
  E_q[\lambda_i] &= \frac{x_i + \alpha}{\xiplus + \beta}\\
  E_q[\ln \lambda_i] &= \psi(x + \alpha) - \log(\xiplus + \beta)\\
  E_q[\ln p(x_i, \lambda_i \mid \xiplus, \alpha, \beta)] &= \ell_i \triangleq x_i E_q[\ln \lambda_i] - E_q[\lambda_i] - \ln\Gamma(x_i + 1) + \alpha \ln\beta - \ln\Gamma(\alpha) + (\alpha - 1) E_q[\lambda_i] - \beta E_q[\lambda_i]\\
  \ell &= \sum_i \ell_i\\
  \frac{\partial\ell}{\partial\beta} &= \sum_i \frac{\alpha}{\beta} - E_q[\lambda_i] = 0\\
  \beta &= \frac{\bar{\lambda}}{\alpha}\\
  \frac{\partial\ell}{\partial\alpha} &= \sum_i \ln \beta - \psi(\alpha) + E_q[\ln x_i]\\
  \frac{\partial^2\ell}{\partial\alpha^2} &= -n \psi^{(1)}(\alpha)
\end{align}

<p>
where \(\psi\) denotes the digamma function and \(\psi^{(1)}\) denotes the
trigamma function. The algorithm uses a partial M step (single
Newton-Raphson update) for \(\alpha\).
</p>

<p>
Try EM for a simple example.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">rng</span> = np.random.default_rng(1)
<span class="org-variable-name">n</span> = 100
<span class="org-variable-name">log_mean</span> = -10
<span class="org-variable-name">log_inv_disp</span> = 0
<span class="org-variable-name">s</span> = np.repeat(1e5, n)
<span class="org-variable-name">lam</span> = rng.gamma(shape=np.exp(log_inv_disp), scale=np.exp(log_mean - log_inv_disp), size=n)
<span class="org-variable-name">x</span> = rng.poisson(s * lam)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> nbmix.em
<span class="org-variable-name">log_mu</span>, <span class="org-variable-name">neg_log_phi</span>, <span class="org-variable-name">trace</span> = nbmix.em.fit_pois_gam(x, s)
</pre>
</div>

<p>
Plot the simulated data, the ground truth marginal distribution on counts,
and the NB MLE.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Dark2'</span>)
plt.clf()
plt.gcf().set_size_inches(4.5, 2.5)
<span class="org-variable-name">grid</span> = np.arange(x.<span class="org-builtin">max</span>() + 1)
plt.hist(x, bins=grid, color=<span class="org-string">'0.7'</span>, density=<span class="org-constant">True</span>)
plt.plot(grid + .5, st.nbinom(n=np.exp(-log_inv_disp), p=1 / (1 + s[0] * np.exp(log_mean - log_inv_disp))).pmf(grid), lw=1, color=cm(0), label=<span class="org-string">'Ground truth'</span>)
plt.plot(grid + .5, st.nbinom(n=np.exp(neg_log_phi), p=1 / (1 + s[0] * np.exp(log_mu - neg_log_phi))).pmf(grid), lw=1, color=cm(1), label=<span class="org-string">'NB MLE'</span>)
plt.legend(frameon=<span class="org-constant">False</span>)
plt.xlabel(<span class="org-string">'Number of molecules'</span>)
plt.ylabel(<span class="org-string">'Density'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/nbmix.org/nb-em.png" alt="nb-em.png">
</p>
</div>

<p>
Try a more extensive evaluation of the method.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">n</span> = 100
<span class="org-variable-name">s</span> = np.repeat(1e5, n)
<span class="org-variable-name">result</span> = <span class="org-builtin">dict</span>()
<span class="org-keyword">for</span> trial <span class="org-keyword">in</span> <span class="org-builtin">range</span>(5):
  <span class="org-keyword">for</span> log_mean <span class="org-keyword">in</span> np.linspace(-12, -6, 7):
    <span class="org-keyword">for</span> log_inv_disp <span class="org-keyword">in</span> np.linspace(0, 4, 5):
      <span class="org-variable-name">rng</span> = np.random.default_rng(trial)
      <span class="org-variable-name">lam</span> = rng.gamma(shape=np.exp(log_inv_disp), scale=np.exp(log_mean - log_inv_disp), size=n)
      <span class="org-variable-name">x</span> = rng.poisson(s * lam)
      <span class="org-variable-name">start</span> = time.time()
      <span class="org-variable-name">log_mean_hat</span>, <span class="org-variable-name">log_inv_disp_hat</span>, <span class="org-variable-name">trace</span> = nbmix.em.fit_pois_gam(x, s, max_iters=1000)
      <span class="org-variable-name">elapsed</span> = time.time() - start
      <span class="org-variable-name">result</span>[(log_mean, log_inv_disp, trial)] = pd.Series([log_mean_hat, log_inv_disp_hat, <span class="org-builtin">len</span>(trace), elapsed])
<span class="org-variable-name">result</span> = (pd.DataFrame.from_dict(result, orient=<span class="org-string">'index'</span>)
          .reset_index()
          .rename({f<span class="org-string">'level_{i}'</span>: k <span class="org-keyword">for</span> i, k <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>([<span class="org-string">'log_mean'</span>, <span class="org-string">'log_inv_disp'</span>, <span class="org-string">'trial'</span>])}, axis=1)
          .rename({i: k <span class="org-keyword">for</span> i, k <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>([<span class="org-string">'log_mean_hat'</span>, <span class="org-string">'log_inv_disp_hat'</span>, <span class="org-string">'num_iters'</span>, <span class="org-string">'elapsed'</span>])}, axis=1))
</pre>
</div>

<p>
Plot the estimates against the ground truth values.
</p>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 2)
fig.set_size_inches(4.5, 2.5)
<span class="org-keyword">for</span> a <span class="org-keyword">in</span> ax:
  a.set_aspect(<span class="org-string">'equal'</span>, adjustable=<span class="org-string">'datalim'</span>)
ax[0].scatter(result[<span class="org-string">'log_mean'</span>], result[<span class="org-string">'log_mean_hat'</span>], c=<span class="org-string">'k'</span>, s=1)
ax[0].set_xlabel(<span class="org-string">'Ground truth $\ln(\mu)$'</span>)
ax[0].set_ylabel(<span class="org-string">'Estimated $\ln(\mu)$'</span>)
ax[1].scatter(-result[<span class="org-string">'log_inv_disp'</span>], -result[<span class="org-string">'log_inv_disp_hat'</span>], c=<span class="org-string">'k'</span>, s=1)
ax[1].set_xlabel(<span class="org-string">'Ground truth $\ln(\phi)$'</span>)
ax[1].set_ylabel(<span class="org-string">'Estimated $\ln(\phi)$'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/nbmix.org/nb-em-sim.png" alt="nb-em-sim.png">
</p>
</div>

<p>
Estimate the average time (seconds) taken to fit each trial.
</p>

<div class="org-src-container">
<pre class="src src-ipython">result[<span class="org-string">'elapsed'</span>].mean(), result[<span class="org-string">'elapsed'</span>].std()
</pre>
</div>

<pre class="example">
(0.16765535082135882, 0.1686101890016258)

</pre>
</div>
</div>

<div id="outline-container-orgdb50eed" class="outline-3">
<h3 id="orgdb50eed">EM for Poisson-Log compound distribution</h3>
<div class="outline-text-3" id="text-orgdb50eed">
<p>
The NB distribution can be derived as a
<a href="https://en.wikipedia.org/wiki/Negative_binomial_distribution#Representation_as_compound_Poisson_distribution">Poisson-distributed
sum</a> of
<a href="https://en.wikipedia.org/wiki/Logarithmic_distribution">Log-distributed
random variables</a> (Quenouille 1949)
</p>

\begin{align}
  x_i \mid y_1, \ldots, y_{m_i}, m_i &= \sum_{t=1}^{m_i} y_t\\
  m_i \mid \lambda &\sim \Pois(\lambda)\\
  p(y_t \mid \theta) &= -\frac{\theta^{y_t}}{y_t \ln(1 - \theta)}, \quad t = 1, 2, \ldots\\
  p(x_i \mid \lambda, \theta) &\propto p^n (1 - p)^{x_i}, \quad n = -\lambda / \log(1 - \theta), p = 1 - \theta
\end{align}

<p>
To connect this parameterization to our parameterization, we have \(n =
   1/\phi\) and \(p = 1 / (1 + \xiplus\mu\phi)\). Adamidis 1999 uses this fact to
derive a new auxiliary variable representation of the NB distribution
</p>

\begin{equation}
  p(y_t, z_t \mid \theta) = \frac{(1 - \theta)^{z_t} \theta^{y_t - 1}}{y_t}, z_t \in (0, 1), y_t \in \mathbb{N}
\end{equation}

<p>
which they claim admits an EM algorithm with analytic M step. Letting \(q
   \triangleq p(m_i, y_1, \ldots, z_1, \ldots \mid x_i, n, p)\),
</p>

\begin{multline}
  E_q[\ln p(x_i \mid m_i, y_1, \ldots, z_1, \ldots, \lambda, \theta)] = E_q[m_i] \ln\lambda - \lambda - E_q[\ln\Gamma(m_i + 1)]\\
  + E_q[\textstyle\sum_{t=1}^{m_i} z_t] \ln(1 - \theta) + (\textstyle\sum_{t=1}^{m_i} E_q[y_t] - E_q[m_i]) \ln \theta + \mathrm{const}
\end{multline}

<p>
However, it appears they neglect the intractable term \(E_q[\ln\Gamma(m_i +
   1)]\) in their derivation, and the algorithm as given does not appear to
improve the expected log joint probability.
</p>

<p>
Try EM for a simple example.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">rng</span> = np.random.default_rng(1)
<span class="org-variable-name">n</span> = 100
<span class="org-variable-name">log_mean</span> = -10
<span class="org-variable-name">log_inv_disp</span> = 0
<span class="org-variable-name">s</span> = np.repeat(1e5, n)
<span class="org-variable-name">lam</span> = rng.gamma(shape=np.exp(log_inv_disp), scale=np.exp(log_mean - log_inv_disp), size=n)
<span class="org-variable-name">x</span> = rng.poisson(s * lam)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> nbmix.em
<span class="org-variable-name">log_mu</span>, <span class="org-variable-name">neg_log_phi</span>, <span class="org-variable-name">trace</span> = nbmix.em.fit_pois_log(x, s)
</pre>
</div>
</div>
</div>

<div id="outline-container-orgd1763fc" class="outline-3">
<h3 id="orgd1763fc">Deep unsupervised learning</h3>
<div class="outline-text-3" id="text-orgd1763fc">
<p>
scVI (<a href="https://dx.doi.org/10.1038/s41592-018-0229-2">Lopez et al. 2018</a>,
<a href="https://www.biorxiv.org/content/10.1101/532895v2">Xu et al. 2020</a>)
implements a related deep unsupervised (more precisely, semi-supervised)
clustering model (<a href="https://arxiv.org/abs/1406.5298">Kingma et al. 2014</a>,
<a href="https://arxiv.org/abs/1611.02648">Dilokthanakul et al. 2016</a>).
</p>

\begin{align}
  x_{ij} \mid s_i, \lambda_{ij} &\sim \Pois(s_i \lambda_{ij})\\
  \ln s_i &\sim \N(\cdot)\\
  \lambda_{ij} \mid \vz_i &\sim \Gam(\phi_j^{-1}, (\mu_{\lambda}(\vz_i))_j^{-1} \phi_j^{-1})\\
  \vz_i \mid y_i, \vu_i &\sim \N(\mu_z(\vu_i, y_i), \diag(\sigma^2(\vu_i, y_i)))\\
  y_i \mid \vpi &\sim \Mult(1, \vpi)\\
  \vu_i &\sim \N(0, \mi).
\end{align}

<p>
where
</p>

<ul class="org-ul">
<li>\(y_i\) denotes the cluster assignment for cell \(i\)</li>
<li>\(\mu_z(\cdot), \sigma^2(\cdot)\) are neural networks mapping the latent
cluster variable \(y_i\) and Gaussian noise \(\vu_i\) to the latent variable
\(\vz_i\)</li>
<li>\(\mu_{\lambda}(\cdot)\) is a neural network mapping latent variable \(\vz_i\) to
latent gene expression \(\vlambda_{i}\)</li>
</ul>

<p>
The intuition behind this model is that, marginalizing over \(y_i\), the
prior \(p(z_i)\) is a mixture of Gaussians, and therefore the model embeds
examples in a space which makes clustering easy, and maps examples to those
clusters simultaneously. To perform variational inference in this model,
Lopez et al. introduce inference networks
</p>

\begin{align}
  q(\vz_i \mid \vx_i) &= \N(\cdot)\\
  q(y_i \mid \vz_i) &= \Mult(1, \cdot).
\end{align}

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> scvi.dataset
<span class="org-keyword">import</span> scvi.inference
<span class="org-keyword">import</span> scvi.models

<span class="org-variable-name">expr</span> = scvi.dataset.AnnDatasetFromAnnData(temp)
<span class="org-variable-name">m</span> = scvi.models.VAEC(expr.nb_genes, n_batch=0, n_labels=2)
<span class="org-variable-name">train</span> = scvi.inference.UnsupervisedTrainer(m, expr, train_size=1, batch_size=32, show_progbar=<span class="org-constant">False</span>, n_epochs_kl_warmup=100)
train.train(n_epochs=1000, lr=1e-2)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">post</span> = train.create_posterior(train.model, expr)
<span class="org-variable-name">_</span>, <span class="org-variable-name">_</span>, <span class="org-variable-name">label</span> = post.get_latent()
</pre>
</div>

<p>
The fundamental difference between scVI and our approach is that scVI
clusters points in the low-dimensional latent space, and we cluster points in
the space of latent gene expression (which has equal dimension to the
observations).
</p>

<p>
Rui Shu <a href="http://ruishu.io/2016/12/25/gmvae/">proposed an alternative
generative model</a>, which has some practical benefits and can be adapted to
this problem
</p>

\begin{align}
  x_{ij} \mid \xiplus, \lambda_{ij} &\sim \Pois(\xiplus \lambda_{ij})\\
  \lambda_{ij} \mid \vz_i &\sim \Gam(\phi_j^{-1}, (\mu_{\lambda}(\vz_i))_j^{-1} \phi_j^{-1})\\
  \vz_i \mid y_i &\sim \N(\mu_z(y_i), \sigma^2(y_i))\\
  y_i \mid \vpi &\sim \Mult(1, \vpi)\\
  q(y_i, \vz_i \mid \vx_i) &= q(y_i \mid \vx_i)\, q(\vz_i \mid y_i, \vx_i).
\end{align}

<p>
There are additional deep unsupervised learning methods, which could
potentially be adapted to this setting (reviewed in
<a href="https://dx.doi.org/10.1109/ACCESS.2018.2855437">Min et al. 2018</a>).
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Abhishek Sarkar</p>
<p class="date">Created: 2020-07-27 Mon 14:09</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
