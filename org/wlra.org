#+TITLE: Weighted low rank approximation
#+SETUPFILE: setup.org
#+OPTIONS: toc:2

* Setup                                                            :noexport:

  #+BEGIN_SRC emacs-lisp
    (org-babel-lob-ingest "/home/aksarkar/.emacs.d/org-templates/library.org")
  #+END_SRC

  #+RESULTS:
  : 1

  #+CALL: ipython3(memory="4G",venv="singlecell") :dir /scratch/midway2/aksarkar/ideas

  #+RESULTS:
  : Submitted batch job 50409883

  #+BEGIN_SRC ipython
    %matplotlib inline
    %config InlineBackend.figure_formats = set(['retina'])
    import colorcet
    import matplotlib.pyplot as plt
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[1]:
  :END:

  #+NAME: imports
  #+BEGIN_SRC ipython
    import numpy as np
    import pandas as pd
    import scipy.special as sp
    import scipy.stats as st
    import sklearn.decomposition as skd
    import wlra

    from wlra.nmf import nmf
  #+END_SRC

  #+RESULTS: imports
  :RESULTS:
  # Out[2]:
  :END:

* Introduction

  We are interested in solving the /weighted low-rank approximation problem/:

  \[ \min_{\mathbf{Z}} \sum_{i,j} w_{ij} \left(x_{ij} - z_{ij} \right)^2 \]

  where \(n \times p\) target matrix \(\mathbf{X}\) and \(n \times p\) weight
  matrix \(\mathbf{W}\) are given, and \(\mathbf{Z}\) is constrained to some
  rank.

  Our primary interest is to use WLRA to learn low rank structure in
  non-Gaussian data (especially Poisson data, as is generated by
  RNA-Seq). Using Taylor expansion of non-Gaussian likelihoods, we can rewrite
  the MLE of factor models as the solution to WLRA. The key idea is that the
  Taylor expansion is performed around a different value for each observation,
  naturally leading to an iterative approach.

  The methods are implemented in the Python package [[https://www.github.com/aksarkar/wlra][wlra]].

* Methods
** EM algorithm 

   [[https://www.aaai.org/Papers/ICML/2003/ICML03-094.pdf][Srebro and Jaakkola 2003]] propose an EM algorithm to solve WLRA. The algorithm
   is EM in the following sense: suppose the weights \(w_{ij} \in \{0, 1\}\),
   corresponding to presence/absence, and suppose \(\mathbf{X} = \mathbf{Z} +
   \mathbf{E}\), where \(\mathbf{Z}\) is low-rank and elements of \(\mathbf{E}\)
   are Gaussian.

   Then, \(E[x_{ij} \mid w_{ij} = 0] = z_{ij}\), naturally giving an EM
   algorithm. The E-step fills in \(x_{ij}\) with \(z_{ij}\), and the M-step
   estimates \(\mathbf{Z}\) from the filled in \(\mathbf{X}\). The solution to
   the M-step is given by the optimal unweighted rank \(k\) approximation,
   i.e. truncated SVD, because all the non-zero weights are equal to 1.

   Conceptually, the method for arbitrary weights is to suppose instead that we
   have rational \(w_{ij} \in \{0, 1/N, \ldots, 1\}\). 

   Then, we can reduce this problem to a problem in 0/1 weights by supposing we
   have \(X^{(k)} = Z + E^{(k)}\), \(k \in 1, \ldots, N\), and each entry is
   observed in only \(N w_{ij}\) of the \(X^{(k)}\).

   Then, the M-step becomes:

   \[ \mathbf{Z}^{(t + 1)} = \mathrm{LRA}_k(\mathbf{W} \circ \mathbf{X} +
   (\mathbf{1} - \mathbf{W}) \circ \mathbf{Z}^{(t)}) \]

   where \(\mathrm{LRA}_k\) is the unweighted rank \(k\) approximation and
   \(\circ\) denotes Hadamard product.

   Intuitively, the implied E-step corresponds to taking the expectation of
   \(z_{ij}^{(k)}\) over the targets \(k\). Clearly, the algorithm generalizes
   to any weight matrix where \(0 \leq w_{ij} \leq 1\) are stored in finite
   precision. We can support arbitrary weights by scaling by the maximum weight
   (this simply scales the objective).

** Alternative derivation

   [[https://stephens999.github.io/misc/wSVD.html][Stephens]] proposes an alternative EM algorithm. Suppose \(\mathbf{X} =
   \mathbf{M} + \mathbf{Z} + \mathbf{E}\) where \(\mathbf{M}\) is low rank,
   and:

   \[ z_{ij} \sim \mathcal{N}(0, \sigma_{ij}^2) \]

   \[ e_{ij} \sim \mathcal{N}(0, \sigma^{2}) \]

   Let \(\mathbf{R} = \mathbf{X} - \mathbf{M}\). Then:

   \[ r_{ij} \mid z_{ij} \sim \mathcal{N}(z_{ij}, \sigma^{2}) \]

   \[ \mathbb{E}[z_{ij} \mid x_{ij}, m_{ij}, s_{ij}] = \frac{\sigma_{ij}^2}{\sigma^2 +
   \sigma_{ij}^2} (x_{ij} - m_{ij}) = (1 - w_{ij}) (x_{ij} - m_{ij}) \]

   and the solution to the E step is:

   \[ \mathbf{Z}^{(t + 1)} = (1 - \mathbf{W}) \circ (\mathbf{X} - \mathbf{M}^{(t)}) \]

   Given \(\mathbf{Z}^{(t)}\), the solution to the M step is PCA with
   homoscedastic errors, i.e. truncated SVD.

   \[ \mathbf{M}^{(t + 1)} = \mathrm{LRA}_k(\mathbf{X} - \mathbf{Z}) \]

   \[ = \mathrm{LRA}_k(\mathbf{W} \circ \mathbf{X} + (1 - \mathbf{W}) \circ
   \mathbf{M}^{(t)}) \]

   In comparison to the algorithm of Srebro and Jaakkola, this algorithm has an
   extra parameter \(\sigma^2\). However, we need:

   \[ \mathbb{V}[x] = s_{ij}^2 = \sigma^{2} + \sigma_{ij}^{2} \]

   where \(s_{ij}^2\) is a known variance for each observation \(x_{ij}\). If
   we let \(\sigma^2 = \min(s_{ij}^2)\), then:

   \[ w_{ij} = \frac{\min(s_{ij}^2)}{s_{ij}^2} = \frac{1 / s_{ij}^2}{\max(1 / s_{ij}^2)} \]

** Maximizing non-Gaussian likelihoods

   Suppose \(l(\theta) = \ln p(x \mid \theta)\). Then, taking second-order
   Taylor expansion about \(\theta_0\):

   \[ l(\theta) \approx l(\theta_0) + (\theta - \theta_0)\,l'(\theta_0) +
   \frac{(\theta - \theta)^2}{2}\,l''(\theta_0) \]

   \[ = \frac{l''(\theta_0)}{2} \left[ \theta - \left(\theta_0 -
   \frac{l'(\theta_0)}{l''(\theta_0)}\right)\right]^2 + \mathrm{const}\]

   where the constant does not depend on \(\theta\). Now, maximizing the
   objective is equivalent to maximizing a Gaussian likelihood:

   \[= \mathcal{N}\left(\theta; \theta_0 -
   \frac{l'(\theta_0)}{l''(\theta_0)}, -\frac{1}{l''(\theta_0)}\right) +
   \mathrm{const} \]

   Equivalently, minimizing the negative of the objective function is
   WLRA. This result makes sense because for fixed \(\sigma^2\), maximizing the
   Gaussian likelihood is the same as minimizing the Frobenius norm.

   The result suggests we can improve \(l\) by optimizing this objective
   instead, and suggests an iterative algorithm where we alternate updates
   between \(\theta_0\) and \(\theta\).

   Because this new objective is written in terms of derivatives of the log
   likelihood, we can readily write down the required quantities for relevant
   distributions:

   #+ATTR_HTML: :class table text-left
   | Distribution | Parameter \(\theta\)  | Target (mean)                                               | Weight (precision)                     |
   |--------------+-----------------------+-------------------------------------------------------------+----------------------------------------|
   | Bernoulli    | \(\mathrm{logit}(p)\) | \(\theta + \frac{x - S(\theta)}{S(\theta)(1 - S(\theta))}\) | \(\frac{1}{S(\theta)(1 - S(\theta))}\) |
   | Poisson      | \(\ln\lambda\)        | \(1 + \theta + x\exp(-\theta)\)                             | \(\exp(-\theta)\)                      |

   where \(S(\cdot)\) denotes the sigmoid function.

** Imputing missing values

   The EM algorithm for WLRA supports missing values by setting \(w_{ij} =
   0\).

   When maximizing non-Gaussian likelihoods using WLRA, we need to introduce
   external weights \(\tilde{w}_{ij} \in \{0, 1\}\) to denote
   presence/absence. We can do this by using weights \(\tilde{\mathbf{W}} \circ
   \mathbf{W}\) in each outer iteration.

** Choosing the rank of the approximation

   The most obvious way to pick an optimal rank is to use cross-validation:
   hold out some samples, and evaluate the likelihood of the held-out samples
   using the estimated low rank structure.

   Because WLRA supports missing value imputation, we can instead hold out
   individual entries of the matrix (increasing the effective size of the
   training data) and evaluate the imputation RMSE ([[https://cran.r-project.org/web/packages/NNLM/vignettes/Fast-And-Versatile-NMF.html#determine-rank-k-via-missing-value-imputation][Lin 2018]]). Requiring the
   model to be able to reconstruct corrupted inputs is the key idea of
   denoising autoencoders ([[http://www.iro.umontreal.ca/~lisa/publications2/index.php/publications/show/217][Vincent et al. 2008]]). Intuitively, for low rank
   approximation the true rank should have the best imputation RMSE: lower rank
   should underfit, and higher rank should overfit.

* Results
** Recovering low rank structure

   We first consider the problem of recovering a planted low rank matrix after
   convolving with Gaussian noise, assuming we know the rank.

   \[ l_{ik} \sim \mathcal{N}(0, 1) \]
   \[ f_{kj} \sim \mathcal{N}(0, 1) \]
   \[ \mu_{ij} = (\mathbf{L F})_{ij} \]
   \[ \sigma^2_{ij} \sim \mathrm{Uniform}(1, \sigma^2_0) \]
   \[ x_{ij} \sim \mathcal{N}(\mu_{ij}, \sigma^2_{ij}) \]

   We assume the noise variances for each observation are known, and use the
   inverse variances as the weights.

   #+NAME: gaussian-reconstruction
   #+BEGIN_SRC ipython
     def wnorm(x, w):
       return (w * np.square(x)).sum()

     def simulate_gaussian(n, p, rank, s0=10, seed=0):
       np.random.seed(seed)
       l = np.random.normal(size=(n, rank))
       f = np.random.normal(size=(rank, p))
       eta = l.dot(f)
       noise = np.random.uniform(1, s0, size=eta.shape)
       w = 1 / noise
       x = np.random.normal(loc=eta, scale=noise)
       return x, w, eta

     def rrmse(pred, true):
       return np.sqrt(np.linalg.norm(pred - true) / np.linalg.norm(true))

     def score_wlra(x, w, eta, rank):
       try:
         res = wlra.wlra(x, w, rank=rank, max_iters=1000)
         return rrmse(res, eta)
       except RuntimeError:
         return np.nan

     def score_lra(x, eta, rank):
       res = wlra.lra(x, rank)
       return rrmse(res, eta)

     def evaluate_gaussian_known_rank(num_trials=10):
       result = []
       for s0 in np.logspace(0, 1, 4):
         for rank in range(1, 4):
           for trial in range(num_trials):
             x, w, eta = simulate_gaussian(n=100, p=1000, rank=rank, s0=s0, seed=trial)
             result.append([rank,
                            s0,
                            trial,
                            score_lra(x, eta, rank=rank),
                            score_wlra(x, w, eta, rank=rank)])
       result = pd.DataFrame(result)
       result.columns = ['rank', 's0', 'trial', 'LRA', 'WLRA']
       return result
   #+END_SRC

   #+RESULTS: gaussian-reconstruction
   :RESULTS:
   # Out[3]:
   :END:

   #+BEGIN_SRC ipython :eval never :noweb tangle :tangle /project2/mstephens/aksarkar/projects/singlecell-ideas/code/gaussian-known-rank.py
     <<imports>>
     <<gaussian-reconstruction>>
     res = evaluate_gaussian_known_rank(num_trials=25)
     res.to_csv('/project2/mstephens/aksarkar/projects/singlecell-ideas/data/wlra/gaussian-known-rank.txt.gz', compression='gzip', sep='\t')
   #+END_SRC

   #+BEGIN_SRC sh :dir /scratch/midway2/aksarkar/ideas/
     sbatch --partition=broadwl --job-name=gaussian-wlra -n1 -c8
     #!/bin/bash
     source activate singlecell
     python /project2/mstephens/aksarkar/projects/singlecell-ideas/code/gaussian-known-rank.py
   #+END_SRC

   #+RESULTS:
   : Submitted batch job 50413961

   Read the results.

   #+BEGIN_SRC ipython
     results_gaussian_known_rank = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-ideas/data/wlra/gaussian-known-rank.txt.gz', index_col=0)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[8]:
   :END:

   Plot the performance.

   #+BEGIN_SRC ipython :ipyfile figure/wlra.org/gaussian-known-rank.png
     T = results_gaussian_known_rank.dropna()

     plt.clf()
     plt.gcf().set_size_inches(3, 3)
     G = T.groupby(['rank', 's0'])
     for i, (k, g) in enumerate(G):
       y = g['WLRA'] - g['LRA']
       f = st.gaussian_kde(y)
       py = f(y)
       x = i + .1 / py.max() * np.random.uniform(-py, py)
       plt.scatter(x, y, c='k', s=4)
     plt.axhline(y=0, c='k', ls=':', lw=1)
     plt.xticks(range(len(G)), ['({}, {:.2f})'.format(*x) for x in G.groups.keys()], rotation=90)
     plt.xlabel('(Rank, $\sigma_0$)')
     _ = plt.ylabel('Difference in RMSE (LRA)')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[22]:
   [[file:figure/wlra.org/gaussian-known-rank.png]]
   :END:

** Imputing missing values

   We generate a Poisson data matrix with planted row rank structure.

   \[ \eta_{ij} = (\mathbf{L F})_{ij} \]
   \[ x_{ij} \sim \mathrm{Poisson}(\exp(\eta_{ij})) \]

   We then hold out entries using numpy masked arrays, and impute them using
   the Poisson low rank approximation. We compare the imputation accuracy
   against non-negative matrix factorization ([[https://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf][Lee and Seung 2001]],
   [[https://arxiv.org/abs/1010.1763][Févotte and Idier 2011]]).

   #+NAME: poisson-imputation
   #+BEGIN_SRC ipython
     def simulate_pois(n, p, rank, eta_max=None, holdout=None, seed=0):
       np.random.seed(seed)
       l = np.random.normal(size=(n, rank))
       f = np.random.normal(size=(rank, p))
       eta = l.dot(f)
       if eta_max is not None:
         # Scale the maximum value
         eta *= eta_max / eta.max()
       x = np.random.poisson(lam=np.exp(eta))
       if holdout is not None:
         mask = np.random.uniform(size=(n, p)) < holdout
         x = np.ma.masked_array(x, mask=mask)
       return x, eta

     def rmse(pred, true):
       return np.sqrt(np.square(pred - true).mean())

     def imputation_score_mean(x):
       """Mean-impute the data"""
       return rmse(x.mean(), x.data[x.mask])

     def imputation_score_nmf(x, rank):
       try:
         res = nmf(x, rank, atol=1e-3)
         return rmse(res[x.mask], x.data[x.mask])
       except RuntimeError:
         return np.nan

     def imputation_score_pois_lra(x, rank):
       try:
         res = np.exp(wlra.pois_lra(x, rank=rank))
         return rmse(res[x.mask], x.data[x.mask])
       except RuntimeError:
         return np.nan

     def evaluate_pois_imputation(rank=3, holdout=0.25, eta_max=None, num_trials=10):
       result = []
       for trial in range(num_trials):
         x, eta = simulate_pois(n=200, p=300, rank=rank, eta_max=eta_max, holdout=holdout, seed=trial)
         result.append([trial,
                        imputation_score_mean(x),
                        imputation_score_nmf(x, rank),
                        imputation_score_pois_lra(x, rank)])
       result = pd.DataFrame(result)
       result.columns = ['trial', 'Mean', 'NMF', 'PLRA']
       return result
   #+END_SRC

   #+RESULTS: poisson-imputation
   :RESULTS:
   # Out[79]:
   :END:

   #+BEGIN_SRC ipython :eval never :noweb tangle :tangle /project2/mstephens/aksarkar/projects/singlecell-ideas/code/pois-imputation.py
     <<imports>>
     <<poisson-imputation>>
     res = evaluate_pois_imputation(eta_max=3, num_trials=100)
     res.to_csv('/project2/mstephens/aksarkar/projects/singlecell-ideas/data/wlra/pois-imputation.txt.gz', compression='gzip', sep='\t')
   #+END_SRC

   #+BEGIN_SRC sh :dir /scratch/midway2/aksarkar/ideas/
     sbatch --partition=broadwl --job-name=pois-imputation -n1 -c8
     #!/bin/bash
     source activate singlecell
     python /project2/mstephens/aksarkar/projects/singlecell-ideas/code/pois-imputation.py
   #+END_SRC

   #+RESULTS:
   : Submitted batch job 50421191

   Read the results.

   #+BEGIN_SRC ipython
     results_pois_imputation = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-ideas/data/wlra/pois-imputation.txt.gz', index_col=0)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[86]:
   :END:

   Plot the performance.

   #+BEGIN_SRC ipython :ipyfile figure/wlra.org/pois-imputation.png
     T = results_pois_imputation.dropna()

     plt.clf()
     plt.gcf().set_size_inches(3, 3)
     keys = ['NMF', 'PLRA']
     for i, k in enumerate(keys):
       y = T[k] - T['Mean']
       f = st.gaussian_kde(y)
       py = f(y)
       x = i + .1 / py.max() * np.random.uniform(-py, py)
       plt.scatter(x, y, c='k', s=4)
     plt.axhline(y=0, c='k', ls=':', lw=1)
     plt.xticks(range(len(keys)), keys)
     plt.xlabel('Method')
     _ = plt.ylabel('Difference in RMSE (mean)')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[88]:
   [[file:figure/wlra.org/pois-imputation.png]]
   :END:

** Choosing the rank of the approximation

   We generate a Poisson data matrix with planted row rank structure.

   \[ \eta_{ij} = (\mathbf{L F})_{ij} \]
   \[ x_{ij} \sim \mathrm{Poisson}(\exp(\eta_{ij})) \]

   We then hold out \(p\) fraction of entries, and take the rank which
   minimizes the imputation RMSE.

   As a proof of principle, look at the imputation RMSE as a function of rank
   for one simulated data set.

   #+BEGIN_SRC ipython :ipyfile figure/wlra.org/select-rank-example.png
     x, eta = simulate_pois(n=200, p=300, rank=3, eta_max=3, seed=0)
     mask = np.random.uniform(size=x.shape) < 0.25
     x = np.ma.masked_array(x, mask=mask)

     query = range(1, 10)
     lra_scores = np.array([imputation_score_pois_lra(x, rank) for rank in query])
     nmf_scores = np.array([imputation_score_nmf(x, rank) for rank in query])

     plt.clf()
     plt.gcf().set_size_inches(3, 3)
     plt.plot(query, lra_scores, c='r', lw=1, label='PLRA')
     plt.plot(query, nmf_scores, c='k', lw=1, label='NMF')

     plt.axhline(y=imputation_score_mean(x), c='b', lw=1, ls=':', label='Mean')
     plt.axvline(x=1 + np.argmin(lra_scores), c='r', lw=1, ls=':')
     plt.axhline(y=lra_scores.min(), c='r', lw=1, ls=':')
     plt.axvline(x=1 + np.argmin(nmf_scores), c='k', lw=1, ls=':')
     plt.axhline(y=nmf_scores.min(), c='k', lw=1, ls=':')

     plt.legend(frameon=False, loc='center left', bbox_to_anchor=(1, 0.5))
     plt.xlabel('Assumed rank')
     _ = plt.ylabel('Imputation RMSE')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[99]:
   [[file:figure/wlra.org/select-rank-example.png]]
   :END:

   Now evaluate the method more systematically.

   #+NAME: poisson-rank
   #+BEGIN_SRC ipython
     def select_rank(x, holdout, seed=0):
       np.random.seed(seed)
       mask = np.random.uniform(size=x.shape) < holdout
       y = np.ma.masked_array(x, mask=mask)
       scores = np.array([imputation_score_pois_lra(y, rank) for rank in range(1, 10)])
       return np.argmin(scores) + 1

     def evaluate_select_rank(num_trials):
       result = []
       for true_rank in range(1, 5):
         for trial in range(num_trials):
           x, eta = simulate_pois(n=200, p=300, rank=true_rank, eta_max=2, seed=trial)
           result.append([true_rank,
                          trial,
                          select_rank(x, holdout=0.1, seed=trial)
           ])
       result = pd.DataFrame(result)
       result.columns = ['rank', 'trial', 'est_rank']
       return result
   #+END_SRC

   #+RESULTS: poisson-rank
   :RESULTS:
   # Out[100]:
   :END:

   #+BEGIN_SRC ipython :eval never :noweb tangle :tangle /project2/mstephens/aksarkar/projects/singlecell-ideas/code/pois-select-rank.py
     <<imports>>
     <<poisson-imputation>>
     <<poisson-rank>>
     res = evaluate_select_rank(num_trials=1)
     res.to_csv('/project2/mstephens/aksarkar/projects/singlecell-ideas/data/wlra/pois-select-rank.txt.gz', compression='gzip', sep='\t')
   #+END_SRC

   #+BEGIN_SRC sh :dir /scratch/midway2/aksarkar/ideas/
     sbatch --partition=broadwl --job-name=pois-imputation -n1 -c8
     #!/bin/bash
     source activate singlecell
     python /project2/mstephens/aksarkar/projects/singlecell-ideas/code/pois-select-rank.py
   #+END_SRC

   #+RESULTS:
   : Submitted batch job 50422249

** Explaining held out data

   A number of methods have been proposed to estimate low rank structure from
   count data. 

   - Non-negative matrix factorization ([[https://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf][Lee and Seung 2001]], [[https://arxiv.org/abs/1010.1763][Févotte and Idier 2011]])
   - Hierarchical Bayesian Poisson Factorization ([[http://www.cs.columbia.edu/~blei/papers/GopalanHofmanBlei2015.pdf][Gopalan et al. 2015]])
   - ZINB-WAVE ([[https://www.nature.com/articles/s41467-017-02554-5][Risso et al. 2018]])
   - scVI ([[https://www.biorxiv.org/content/early/2018/03/30/292037][Lopez et al 2018]])

   These methods typically try to learn low rank structure to solve the
   /collaborative filtering/ problem: given user \(i\) interacted with item
   \(j\), predict what other items they will interact with ([[http://yifanhu.net/PUB/cf.pdf][Hu et
   al. 2001]]). Low rank structure in this context corresponds to learning
   patterns of user preferences (factors), and the specific preferences of
   individual users (loadings).

   One special feature of the problem is that only /implicit feedback/ is
   assumed: we only have observations \(x_{ij} = 1\) when an interaction is
   recorded, and \(x_{ij} = 0\) could either reflect negative feedback or
   missing data. However, even methods developed specifically for scRNA-Seq
   make this assumption even though 0 does not represent a missing value in
   that context. It also means that many methods cannot deal with missing data
   which is not coded as 0.

   Here, we instead evaluate methods on their ability to generalize to new
   data. We use real data, assuming:

   \[ x_{ij} \sim \mathrm{Poisson}(R_i \lambda_{ij}) \]

   \[ \lambda_{ij} = (\mathbf{L F'})_{ij} \]

   and hold out molecules by randomly thinning the observed counts:

   \[ y_{ij} \sim \mathrm{Binomial}(x_{ij}, 0.5) \]

   \[ \tilde{y}_{ij} = x_{ij} - y_{ij} \]

   This approach leaves the relative abundance of the transcripts unchanged in
   expectation, implying that the low rank structure learned in \(\mathbf{Y}\)
   should explain the data in \(\tilde{\mathbf{Y}}\).

   Our metric is then the likelihood of the held-out data. We simply need to
   re-scale to account for different size factors \(R_i\).

   #+NAME: generalization
   #+BEGIN_SRC ipython
     def pois_llik(lam, train, test):
       lam *= test.sum(axis=1, keepdims=True) / train.sum(axis=1, keepdims=True)
       return st.poisson(mu=lam).logpmf(test).mean()

     def generalization_score_mean(train, test, rank):
       lam = np.ones(train.shape) * train.mean(axis=0, keepdims=True)
       return pois_llik(lam, train, test)

     def generalization_score_nmf(train, test, rank):
       lam = nmf(train, rank)
       return pois_llik(lam, train, test)

     def generalization_score_hpf(train, test, rank):
       x = pd.DataFrame(train).reset_index().melt(id_vars=['index'])
       x.columns = ['UserId', 'ItemId', 'Count']
       m = hpfrec.HPF(k=rank, produce_dicts=False, users_per_batch=10).fit(x)
       lam = m.Theta.dot(m.Beta.T)
       return pois_llik(lam, train, test)

     def generalization_score_lra(train, test, rank):
       eta = wlra.pois_lra(train, rank)
       lam = np.where(eta < 60, np.exp(eta), eta)
       return pois_llik(lam, train, test)

     def train_test_split(x, p=0.5):
       train = np.random.binomial(n=x, p=p, size=x.shape)
       test = x - train
       return train, test

     def read_ipsc():
       keep_samples = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/quality-single-cells.txt', index_col=0, header=None)
       keep_genes = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/genes-pass-filter.txt', index_col=0, header=None)
       x = (pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-counts.txt.gz', index_col=0)
            .loc[keep_genes.values.ravel(),keep_samples.values.ravel()]
            .values)
       return x

     def evaluate_generalization(data, num_trials):
       result = []
       x = data()
       for rank in (1,):
         for trial in range(num_trials):
           train, test = train_test_split(x)
           result.append([trial,
                          generalization_score_mean(train, test, rank),
                          generalization_score_nmf(train, test, rank),
                          generalization_score_hpf(train, test, rank),
                          generalization_score_lra(train, test, rank)])
       result = pd.DataFrame(result)
       result.columns = ['trial', 'NMF', 'HPF', 'PLRA']
       return result
   #+END_SRC

   #+RESULTS: generalization
   :RESULTS:
   # Out[83]:
   :END:

   #+BEGIN_SRC ipython :eval never :noweb tangle :tangle /project2/mstephens/aksarkar/projects/singlecell-ideas/code/pois-generalization.py
     <<imports>>
     import hpfrec
     <<generalization>>
     res = evaluate_generalization(read_ipsc, 1)
     res.to_csv('/project2/mstephens/aksarkar/projects/singlecell-ideas/data/wlra/pois-generalization.txt.gz', sep='\t')
   #+END_SRC

   #+BEGIN_SRC sh :dir /scratch/midway2/aksarkar/ideas/
     sbatch --partition=broadwl --job-name=pois-generalization --mem=16G --time=60:00 -n1 -c28 --exclusive
     #!/bin/bash
     source activate singlecell
     python /project2/mstephens/aksarkar/projects/singlecell-ideas/code/pois-generalization.py     
   #+END_SRC

   #+RESULTS:
   : Submitted batch job 50422053
