#+TITLE: An improved voom transform for scRNA-seq data
#+SETUPFILE: setup.org

* Introduction

  The key idea of limma-voom (Law et al. 2014) is to transform a count matrix
  generated by bulk RNA-seq into two matrices, representing the mean and
  variance of true (log) gene expression. These matrices can then be analyzed
  using (heteroscedastic) Gaussian methods. However, limma-voom was developed
  before the development of scRNA-seq, and therefore before it was possible to
  measure the variance of gene expression between cells from a single donor. To
  address this limitation, Law et al. instead proposed to pool information
  across both donors and genes, estimating a LOESS trend between the mean and
  variance of true gene expression values across donors. \(
  \DeclareMathOperator\E{E}
  \DeclareMathOperator\Gam{Gamma}
  \DeclareMathOperator\Poi{Poisson}
  \DeclareMathOperator\V{V}
  \DeclareMathOperator\digamma{\psi}
  \DeclareMathOperator\trigamma{\psi^{(1)}}
  \newcommand\vb{\mathbf{b}}
  \newcommand\vc{\mathbf{c}}
  \newcommand\xiplus{x_{i+}}
  \)

  Now suppose we have observed scRNA-seq data \(x_{ij}\), where \(x_{ij}\)
  denotes the number of molecules from gene \(j\) observed in cell \(i\). Then,
  there are two possible DE analysis we might be interested in. First, we might
  divide (some subset of) cells into two groups, and ask whether genes are
  differentially expressed between groups. Second, we might divide donors into
  groups, and test whether genes (in some subset of cells, per donor) are
  differentially expressed between groups. The key distinction is that single
  cells are the units in the first case, and donors are the units in the second
  case.

  We can apply limma-voom without modification to the first case, because the
  typical log transformation corresponds to an MLE

  \begin{align}
    x_{ij} \mid \xiplus, \theta_{ij} &\sim \Poi(\xiplus \exp(\theta_{ij}))\\
    \ell \triangleq \ln p(x_{ij} \mid \xiplus, \theta_{ij}) &= x_{ij} (\ln \xiplus + \theta_{ij}) - \xiplus \exp(\theta_{ij}) + \mathrm{const}\\
    \frac{\partial \ell}{\partial \theta_{ij}} &= x_{ij} - \xiplus \exp(\theta_{ij})\\
    \hat\theta_{ij} &= \ln\left(\frac{x_{ij}}{\xiplus}\right).
  \end{align}

  This simple theoretical argument and empirical studies have demonstrated that
  applying limma-voom to scRNA-seq data can work (Soneson and Robinson 2018,
  Hsiao 2019). However, unlike the bulk RNA-seq case, now the notion of
  "variance of gene expression" within a single unit no longer makes
  sense. Therefore, it is unclear what precisely limma-voom is fitting in this
  case.

  Applying limma-voom to the second case also works, because we can estimate a
  point mass expression model for the cells from each donor \(k\)

  \begin{align}
    x_{ij} \mid \xiplus, \theta_j &\sim \Poi(\xiplus \exp(\theta_j))\\
    \ell \triangleq \sum_i \ln p(x_{ij} \mid \xiplus, \theta_j) &= \sum_i x_{ij} (\ln \xiplus + \theta_j) - \xiplus \exp(\theta_j) + \mathrm{const}\\
    \frac{\partial \ell}{\partial \theta_j} &= \sum_i x_{ij} - \xiplus \exp(\theta_j)\\
    \theta_j &= \ln\left(\frac{\sum_i x_{ij}}{\sum_i \xiplus}\right)\\
  \end{align}

  where \(\xiplus \triangleq \sum_j x_{ij}\) (Sarkar and Stephens 2020). This
  approach is equivalent to constructing pseudobulk data \(y_{kj} \triangleq
  \sum_i x_{ij} z_{ik}\), where \(z_{ik}\) indicates whether cell \(i\) came
  from donor \(k\), and using \(\ln(y_{kj} / y_{k+})\) as the estimated mean of
  true log gene expression, where \(y_{k+} \triangleq \sum_j y_{kj}\). However,
  the relationship between the voom-estimated variance and the true gene
  expression variance is unclear, because the variance used by voom is between
  individuals, not within an individual. There is some evidence that the two
  are highly, but not perfectly, correlated. Further, it is unlikely that a
  point mass expression model will be supported by the data.

  We previously developed a method to efficiently estimate more complex
  expression models in large-scale scRNA-seq data sets (Sarkar et
  al. 2019). Here, we use that method to investigate two new possibilities for
  a precision weight derived from fitted expression models: (1) the inverse
  squared standard error of a point mass model, or (2) the inverse variance of
  the log true expression under a Gamma model. Specifically, we ask whether
  these alterantive approaches improve the power or robustness of DE analysis
  in scRNA-seq data.

* Setup
  :PROPERTIES:
  :CUSTOM_ID: setup
  :END:

  #+BEGIN_SRC emacs-lisp :exports none
    (add-to-list 'python-shell-completion-native-disabled-interpreters "jupyter")
    (org-babel-lob-ingest "/home/aksarkar/.emacs.d/org-templates/library.org")
  #+END_SRC

  #+RESULTS:
  : 2

  #+CALL: ipython3(venv="singlecell",partition="gpu2",opts="--gres=gpu:1",memory="8G") :dir /scratch/midway2/aksarkar/singlecell/

  #+CALL: ipython3(venv="singlecell",partition="mstephens",memory="8G") :dir /scratch/midway2/aksarkar/singlecell/

  #+CALL: tensorboard(venv="singlecell") :dir /scratch/midway2/aksarkar/singlecell/

  #+BEGIN_SRC ipython
    import anndata
    import numpy as np
    import mpebpm
    import pandas as pd
    import scanpy as sc
    import rpy2.robjects.packages
    import rpy2.robjects.pandas2ri
    import scipy.special as sp
    import scipy.sparse as ss
    import scipy.stats as st
    import sqlite3

    limma = rpy2.robjects.packages.importr('limma')
    rpy2.robjects.pandas2ri.activate()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[1]:
  :END:

  #+BEGIN_SRC ipython
    %matplotlib inline
    %config InlineBackend.figure_formats = set(['retina'])
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[2]:
  :END:

  #+BEGIN_SRC ipython
    import matplotlib.pyplot as plt
    plt.rcParams['figure.facecolor'] = 'w'
    plt.rcParams['font.family'] = 'Nimbus Sans'
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[3]:
  :END:

* Methods
** Standard error of point mass expression model

   The standard error of \(\hat\theta_j\) is analytic

   \begin{align}
     \frac{\partial^2 \ell}{\partial \theta_j^2} &= -\sum_i \xiplus \exp(\theta_j)\\
     \mathcal{I}(\mu_j) &= -\E\left[\frac{\partial^2 \ell}{\partial \mu_j^2}\right] = \sum_i \xiplus \exp(\theta_j)\\
     s_j^2 &= \frac{1}{\sum_i \xiplus \exp(\theta_j)},
   \end{align}

   where we have treated \(\xiplus\) as fixed. This treatment is justified by
   the fact that the Poisson measurement model for each gene arises from a
   Multinomial measurement model for all genes jointly, in which the total
   number of molecules observed is fixed rather than a sum of random
   variables. As an illustrative example, plot the bootstrap distribution of the
   \(\hat\theta_j\) against a normal density with mean \(\theta_j\) and variance
   \(s_j^2\) for a simple simulation.

   #+BEGIN_SRC ipython :async t
     rng = np.random.default_rng(1)
     n_trials = 1000
     n = 100
     s = 1e4
     theta = -10
     thetahat = []
     for i in range(n_trials):
       x = rng.poisson(s * np.exp(theta), size=n)
       thetahat.append(np.log(x.sum()) - np.log(n) - np.log(s))
     thetahat = np.array(thetahat)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[121]:
   :END:

   #+BEGIN_SRC ipython :ipyfile figure/voom.org/analytic-se-log-link.png
     plt.clf()
     plt.gcf().set_size_inches(2.5, 2.5)
     plt.hist(thetahat, bins=16, density=True, color='0.7')
     grid = np.linspace(thetahat.min(), thetahat.max(), 1000)
     plt.plot(grid, st.norm(loc=theta, scale=np.sqrt(1 / (np.exp(theta) * n * s))).pdf(grid), lw=1, c='k')
     plt.xlabel('Est ln mean gene expression')
     plt.ylabel('Density')
     plt.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[12]:
   [[file:figure/voom.org/analytic-se-log-link.png]]
   :END:

   After introducing multiplicative effects \(\vb_j\) for observed technical
   covariates \(\vc_i\) into the measurement model

   \begin{equation}
     x_{ij} \mid \xiplus, \vc_i, \vb_j, \theta_j \sim \Poi(\xiplus \exp(\vc_i' \vb_j + \theta_j)),
   \end{equation}

   the standard error of \(\hat\theta_j\) also depends on \(\vc_i'\vb_j\). In
   contrast, if we assume the identity link

   \begin{align}
     x_{ij} \mid \xiplus, \mu_j &\sim \Poi(\xiplus \mu_j)\\
     \ell \triangleq \sum_i \ln p(x_{ij} \mid \xiplus, \mu_j) &= \sum_i x_{ij} \ln(\xiplus \mu_j) - \xiplus \mu_j + \mathrm{const}\\
     \frac{\partial \ell}{\partial \mu_j} &= \sum_i \frac{x_{ij}}{\mu_j} - \xiplus\\
     \hat\mu_j &= \frac{\sum_i x_{ij}}{\sum_i \xiplus}\\
     \frac{\partial^2 \ell}{\partial \mu_j^2} &= -\sum_i \frac{x_{ij}}{\mu_j^2}\\
     \mathcal{I}(\mu_j) &= -\E\left[\frac{\partial^2 \ell}{\partial \mu_j^2}\right] = \frac{\E[\sum_i x_{ij}]}{\mu_j^2} = \frac{\sum_i \xiplus}{\mu_j}\\
     s_j^2 &= \frac{\mu_j}{\sum_i \xiplus},
   \end{align}

   where we have used the fact that \(\sum_i x_{ij} \sim \Poi(\mu_j \sum_i
   \xiplus)\). Surprisingly, \(\ln \hat\mu_j = \hat\theta_j\), the standard
   error of \(\hat\mu_j\) increases as \(\mu_j\) increases, and the standard
   error does not depend on technical covariates or their effects. As a sanity
   check, plot the bootstrap distribution of \(\hat\mu_j\) against a normal
   density with mean \(\theta_j\) and variance \(s_j^2\) for a simple
   simulation.

   #+BEGIN_SRC ipython :async t
     rng = np.random.default_rng(2)
     n_trials = 500
     n = 100
     s = 1e4
     log_mu = -10
     muhat = []
     for i in range(n_trials):
       x = rng.poisson(s * np.exp(log_mu), size=n)
       muhat.append(x.sum() / (n * s))
     muhat = np.array(muhat)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[16]:
   :END:

   #+BEGIN_SRC ipython :ipyfile figure/voom.org/analytic-se-identity-link.png
     plt.clf()
     plt.gcf().set_size_inches(2.5, 2.5)
     plt.hist(muhat, bins=14, density=True, color='0.7')
     grid = np.linspace(muhat.min(), muhat.max(), 1000)
     plt.plot(grid, st.norm(loc=muhat.mean(), scale=np.sqrt(muhat[0] / (n * s))).pdf(grid), lw=1, c='k')
     plt.xlabel('Est mean gene expression')
     plt.ylabel('Density')
     plt.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[17]:
   [[file:figure/voom.org/analytic-se-identity-link.png]]
   :END:

** Variance of Gamma expression model

   Assuming a Gamma expression model

   \begin{align}
     \lambda_{ij} &\sim \Gam(\phi_j^{-1}, \mu_j^{-1} \phi_j^{-1})\\
     \E[\ln \lambda_{ij}] &= \digamma(\phi_j^{-1}) + \ln(\mu_j \phi_j)\\
     \V[\ln \lambda_{ij}] &= \trigamma(\phi_j^{-1}),
   \end{align}

   where the Gamma distribution is parameterized by shape and rate,
   \(\digamma(\cdot)\) denotes the digamma function, and \(\trigamma(\cdot)\)
   denotes the trigamma function. We previously noted that robustly estimating
   \(\phi_j\) is difficult, even from hundreds of cells per condition; despite
   this difficulty, our method can still accurately estimate the variance of
   true gene expression.

** Improved limma

   Given transformed data and standard errors, DE analysis is performed in two
   steps:
   
   1. Estimate the effect of the covariate of interest by GLS
   2. Estimate moderated test statistics and \(p\)-values by EB treatment of
      the standard errors from (1)

   [[https://arxiv.org/abs/1901.10679][Lu and Stephens 2019]] describe a more
   powerful approach to solve (2).

** Simulation

   Implement a simplified
   [[https://stephenslab.github.io/dsc-wiki/overview.html][DSC]].

   #+BEGIN_SRC ipython
     def simulate_null(dat, n_donors=2, n_cells=100, to_dense=False, min_counts=1, seed=0):
       """Return counts and labels

       counts - matrix [n_donors * n_cells, n_genes]
       labels - CSR matrix [n_donors * n_cells, n_donors]

       """
       query = sc.pp.subsample(dat, n_obs=n_donors * n_cells, random_state=seed, copy=True)
       sc.pp.filter_genes(query, min_counts=min_counts)
       onehot = ss.coo_matrix((np.ones(query.shape[0]), (np.arange(query.shape[0]), np.repeat(np.arange(n_donors), n_cells)))).tocsr()
       if to_dense:
         return query.X.A, onehot.A
       else:
         return query.X, onehot

     def estimate_limma_voom(x, onehot):
       """Return DataFrame of bhat, se"""
       # Important: limma expects genes x samples
       y = limma.voom(x.T)
       fit = limma.lmFit(y, np.vstack([onehot[:,0], np.ones(x.shape[0])]).T)
       return fit

     def estimate_wls_point(x, onehot):
       """Return DataFrame of bhat, se

       Instead of voom, estimate θ_j = log μ_j under a point mass expression model
       and its sampling variance, and use those as input to WLS.

       """
       s = x.sum(axis=1, keepdims=True)
       log_mean = np.log(x + 1) - np.log(s)
       # [n_donors, n_genes]
       w = np.exp(log_mean) * s
       # Reuse limma
       fit = limma.lm_series(log_mean.T, design=np.vstack([onehot[:,0], np.ones(x.shape[0])]).T, weights=w.T)
       return fit

     def estimate_wls_gamma(x, onehot, lr=1e-2, num_epochs=40, batch_size=64, shuffle=True):
       """Return DataFrame of bhat, se

       Instead of voom, estimate E[log λ_{ij}] and V[log λ_{ij}] under a Gamma
       model, and use those as input to WLS.

       """
       s = x.sum(axis=1, keepdims=True)
       log_mean, log_inv_disp = mpebpm.ebpm_gamma(
         x,
         s=s,
         onehot=onehot,
         lr=lr,
         num_epochs=num_epochs,
         batch_size=batch_size,
         shuffle=shuffle)
       # [n_donors, n_genes]
       m = sp.digamma(np.exp(log_inv_disp)) + log_mean - log_inv_disp
       w = 1 / sp.polygamma(1, np.exp(log_inv_disp))
       return fit_wls(np.hstack([np.arange(2), np.ones(2)]).reshape(2, -1), m, w)

     def estimate_moderated_t(fit):
       fit = limma.eBayes(fit)
       return fit.rx2('p.value')[:,0]

     def estimate_z(fit):
       sf = st.chi2(1).sf
       # Important: this has estimates for the intercept also
       stat = fit.rx2('coefficients') / fit.rx2('stdev.unscaled')
       pval = sf(np.square(stat))
       return pval[:,0]

     def evaluate_type1(dat, alpha=0.01, n_trials=1, min_counts=10):
       result = []
       for i in range(n_trials):
         for n_cells in (200,):
           x, onehot = simulate_null(dat, n_cells=n_cells, to_dense=True, min_counts=min_counts, seed=i)
           for method in ('limma_voom', 'wls_point'):
             fit = globals()[f'estimate_{method}'](x, onehot)
             for test in ('moderated_t', 'z'):
               pval = globals()[f'estimate_{test}'](fit)
               result.append((n_cells, method, test, i, (pval < alpha).mean()))
       result = pd.DataFrame(result, columns=['n_cells', 'method', 'test', 'trial', 'fpr'])
       return result
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[70]:
   :END:

* Results
** Type 1 error rate

   To generate null data in the first scenario, randomly sample cells from a
   homogeneous population, and randomly assign labels.

   #+BEGIN_SRC ipython :async t
     dat = anndata.read_h5ad('/scratch/midway2/aksarkar/ideas/zheng-10-way.h5ad')
     b_cells = dat[dat.obs['cell_type'] == 'b_cells']
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[4]:
   :END:

   #+BEGIN_SRC ipython :async t
     result = evaluate_type1(b_cells, min_counts=10, n_trials=50)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[74]:
   :END:

   #+BEGIN_SRC ipython :ipyfile figure/voom.org/fpr.png
     plt.clf()
     fig, ax = plt.subplots(1, 2)
     fig.set_size_inches(4.5, 2.5)
     labels = ['LV-EB', 'LV-Z', 'PM-EB', 'PM-Z']
     for i, (k, g) in enumerate(result.groupby(['method', 'test'])):
       ax[0].boxplot(g['fpr'], positions=[i], widths=0.35, medianprops={'color': 'k'}, flierprops={'marker': '.', 'markersize': 2})
       ax[1].boxplot(g['fpr'], positions=[i], widths=0.35, medianprops={'color': 'k'}, flierprops={'marker': '.', 'markersize': 2})
     ax[1].set_ylim(0, 0.02)
     for a in ax:
       a.axhline(y=0.01, c='r', lw=1, ls=':')
       a.set_xticks(range(len(labels)))
       a.set_xticklabels(labels, rotation=90)
       a.set_xlabel('Method')
     ax[0].set_ylabel('Type 1 error rate')
     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[75]:
   [[file:figure/voom.org/fpr.png]]
   :END:

   Extract the case where limma-voom does worst.

   #+BEGIN_SRC ipython :async t
     query = result.loc[(result['method'] == 'limma_voom') & (result['test'] == 'moderated_t') & (result['fpr'] > 0.2)]
     x, onehot = simulate_null(b_cells, n_cells=200, min_counts=10, seed=query['trial'], to_dense=True)
     fits = {method: globals()[f'estimate_{method}'](x, onehot) for method in ('limma_voom', 'wls_point')}
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[143]:
   :END:

   Look at the distributions of estimated effect sizes.

   #+BEGIN_SRC ipython :ipyfile figure/voom.org/lv-ex.png
     plt.clf()
     fig, ax = plt.subplots(1, 2)
     fig.set_size_inches(4.5, 2.5)
     grid = np.linspace(-.3, .3, 1000)
     for a, k in zip(ax, fits):
       a.hist(fits[k].rx2('coefficients')[:,0], bins=15, color='0.7')
       a.plot(grid, st.norm(scale=fits[k].rx2('stdev.unscaled')[:,0].mean()).pdf(grid), c='k', lw=1)
       a.set_title(k)
       a.set_xlabel('Effect size')
     ax[0].set_ylabel('Density')
     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[99]:
   [[file:figure/voom.org/lv-ex.png]]
   :END:

   Look at the relationship between estimated effect size and estimated
   standard error.

   #+BEGIN_SRC ipython :ipyfile figure/voom.org/lv-bhat-se.png
     cm = plt.get_cmap('Dark2')
     plt.clf()
     fig, ax = plt.subplots(1, 2)
     fig.set_size_inches(5, 2.5)
     for a, k in zip(ax, ['coefficients', 'stdev.unscaled']):
       a.scatter(fits['limma_voom'].rx2(k)[:,0], fits['wls_point'].rx2(k)[:,0], s=1, c='k', alpha=0.1)
       a.set_xlabel(f'limma-voom {k}')
       a.set_ylabel(f'wls-SE {k}')
     lim = [-.5, .5]
     ax[0].plot(lim, lim, lw=1, ls=':', c='r')
     lim = [0, .1]
     ax[1].plot(lim, lim, lw=1, ls=':', c='r')
     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[117]:
   [[file:figure/voom.org/lv-bhat-se.png]]
   :END:

   Plot the voom-estimated weights against the point mass--estimated weights.

   #+BEGIN_SRC ipython :async t
     y = limma.voom(x.T)
     w0 = y.rx2('weights').T
     w1 = x + 1
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[144]:
   :END:

   #+BEGIN_SRC ipython :ipyfile figure/voom.org/sim-ex-voom.png
     plt.clf()
     plt.gcf().set_size_inches(2.5, 2.5)
     plt.scatter(y.rx2('E').ravel(), y.rx2('weights'), s=1, c='k', alpha=0.1)
     plt.xlabel('$\ln(10^6 (y + .5) / (s + 1))$')
     plt.ylabel('Weight')
     plt.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[146]:
   [[file:figure/voom.org/sim-ex-voom.png]]
   :END:

   #+BEGIN_SRC ipython :ipyfile figure/voom.org/voom-vs-point-mass.png
     plt.clf()
     plt.gcf().set_size_inches(2.5, 2.5)
     plt.scatter(w0.ravel(), w1.ravel(), s=1, c='k', alpha=0.1)
     plt.xlabel('voom weight')
     plt.ylabel('Point mass inverse var')
     plt.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[119]:
   [[file:figure/voom.org/voom-vs-point-mass.png]]
   :END:

** Power

   To generate true positives, randomly sample cells from a homogeneous
   population, randomly assign labels, and then use binomial thinning
   (Gerard 2019) to introduce effects of a given magnitude.
