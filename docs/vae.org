#+TITLE: ZIP VAE for single-cell expression
#+AUTHOR: Abhishek Sarkar
#+EMAIL: aksarkar@uchicago.edu
#+EXCLUDE_TAGS: noexport
#+HTML_CONTAINER: div
#+HTML_DOCTYPE: html-strict
#+LANGUAGE: en
#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline author:t
#+OPTIONS: broken-links:nil c:nil creator:nil d:(not "LOGBOOK") date:t e:t
#+OPTIONS: email:nil f:t inline:t num:t p:nil pri:nil prop:nil stat:t tags:t
#+OPTIONS: html-link-use-abs-url:nil html-postamble:auto html-preamble:t
#+OPTIONS: html-scripts:t html-style:t html5-fancy:nil tex:t
#+OPTIONS: tasks:t tex:t timestamp:t title:t toc:t todo:t |:t

#+PROPERTY: header-args:ipython+ :session kernel-aksarkar.json :results raw drawer :async t

* Setup :noexport:

  #+BEGIN_SRC emacs-lisp
    (setq python-shell-prompt-detect-failure-warning nil)
  #+END_SRC

  #+RESULTS:

  #+NAME: ipython3-kernel
  #+BEGIN_SRC shell :dir (concat (file-name-as-directory (getenv "SCRATCH"))) :var RESOURCES="--mem=36G --partition=gpu2 --gres=gpu:1"
    sbatch $RESOURCES --job-name=ipython3 --output=ipython3.out
    #!/bin/bash
    source activate singlecell
    rm -f $HOME/.local/share/jupyter/runtime/kernel-aksarkar.json
    ipython3 kernel --ip=$(hostname -i) -f kernel-aksarkar.json
  #+END_SRC

  #+RESULTS: ipython3-kernel
  : Submitted batch job 38512951

  #+NAME: imports
  #+BEGIN_SRC ipython
    %matplotlib inline

    import matplotlib.pyplot as plt
    import numpy as np
    import os
    import pandas as pd
    import scipy.linalg as spla
    import scipy.stats as sps
    import tensorflow as tf
    import tensorflow.contrib.bayesflow as bf
    import tensorflow.contrib.distributions as ds
    import tensorflow.contrib.slim as slim

    st = bf.stochastic_tensor
    vi = bf.variational_inference
  #+END_SRC

  #+RESULTS: imports
  :RESULTS:
  382c333e-1ff1-45db-bef8-d0edd0fecd91
  :END:

  #+BEGIN_SRC ipython
    from tensorflow.python.client import device_lib as dl
    dl.list_local_devices()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  #+BEGIN_EXAMPLE
  [name: "/cpu:0"
     device_type: "CPU"
     memory_limit: 268435456
     locality {
     }
     incarnation: 10620457322698722014, name: "/gpu:0"
     device_type: "GPU"
     memory_limit: 11324823962
     locality {
       bus_id: 2
     }
     incarnation: 13794022081997974724
     physical_device_desc: "device: 0, name: Tesla K80, pci bus id: 0000:89:00.0"]
  #+END_EXAMPLE
  :END:

* Get the data

  #+BEGIN_SRC shell :dir /home/aksarkar/projects/singlecell/data :async t
    test -f oligodendroma.txt.gz || curl --ftp-pasv 'ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE70nnn/GSE70630/suppl/GSE70630%5FOG%5Fprocessed%5Fdata%5Fv2%2Etxt%2Egz' -o oligodendroma.txt.gz
  #+END_SRC

  #+RESULTS:

  #+NAME: sample-processing
  #+BEGIN_SRC shell :results raw drawer
    curl --ftp-pasv 'ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE70nnn/GSE70630/soft/GSE70630_family.soft.gz' | zgrep -m1 'Supplementary_files_format_and_content'
  #+END_SRC

  #+RESULTS: sample-processing
  :RESULTS:
  !Sample_data_processing = Supplementary_files_format_and_content: tab-delimited text file containing the normalized expression levels (E) for 8008 analyzed genes across 2,594 oligodendroglioma cells that passed QC.
  :END:

  #+NAME: oligodendroma
  #+BEGIN_SRC ipython
    dge = pd.read_table('/home/aksarkar/projects/singlecell/data/oligodendroma.txt.gz', index_col=0)
    dge = dge.fillna(value=0)
    dge.columns = pd.Series(dge.columns).apply(lambda x: 'MGH{}'.format(x.split('_')[0]) if 'MGH' not in x else x.split('_')[0])
    dge.shape, dge.columns.value_counts()
  #+END_SRC

  #+RESULTS: oligodendroma
  :RESULTS:
  #+BEGIN_EXAMPLE
  ((23686, 4347), MGH54    1225
     MGH53     861
     MGH36     788
     MGH97     598
     MGH93     445
     MGH60     430
     dtype: int64)
  #+END_EXAMPLE
  :END:

* Replicate Tirosh et al

  Expression levels were quantified as \(E_{i,j} = \log_2 (TPM_{i,j} /10 +
  1)\), where \(TPM_{i,j}\) refers to transcript-per-million for gene \(i\) in
  sample \(j\), as calculated by RSEM.

  For each cell, we quantified two quality measures: the number of genes for which
  at least one read was mapped, and the average expression level of a curated list of
  housekeeping genes. We then conservatively excluded all cells with either fewer
  than 3,000 detected genes or an average housekeeping expression (E, as defined
  above) below 2.5.

  Get housekeeping genes from [[http://www.sciencedirect.com/science/article/pii/S0168952513000899~][Eisenberg and Levanon 2013]]:

  #+BEGIN_SRC ipython
    housekeeping_genes = pd.read_table('https://www.tau.ac.il/~elieis/HKG/HK_genes.txt', header=None, sep=r'\s+')
    housekeeping_genes.head()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  #+BEGIN_EXAMPLE
         0          1
    0   AAAS  NM_015665
    1  AAGAB  NM_024666
    2   AAMP  NM_001087
    3   AAR2  NM_015511
    4   AARS  NM_001605
  #+END_EXAMPLE
  :END:

  #+BEGIN_SRC ipython
    pass_detected_genes = np.count_nonzero(dge.values, axis=0) > 3000
    pass_housekeeping_expression = dge.loc[housekeeping_genes[0]].agg(np.mean, axis=0) > 2.5
    pass_housekeeping_expression.sum()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  : 0
  :END:

  For the remaining cells we calculated the aggregate expression of each gene
  as (\log 2 (\mathrm{average}(TPM_{i,1...n})+1)\), and excluded genes with an
  aggregate expression below 4, leaving a set of 8,008 analysed genes.

  #+BEGIN_SRC ipython
    pass_aggregate_expression = np.log(np.mean(np.exp((10 * dge.values - 1) / np.log(2)), axis=1)) / np.log(2) > 4
    pass_aggregate_expression.sum()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  : 22330
  :END:

  For the remaining cells and genes, we defined relative expression by
  centering the expression levels, \(Er_{i,j} = E_{i,j} - \mathrm{average}[E
  i,1...n ]\). Centring was performed within each tumour separately in order to
  decrease the impact of inter-tumoural variability on the combined analysis
  across tumours.

* Model

  \[ q(z \mid x) = N(\mu(x), \sigma(x)) \]

  \[ p(x \mid z) = \pi_0(z) \delta_0(x) + (1 - \pi_0(z)) \mathrm{Pois}(x; \lambda(z)) \]

  The idea is that \(\mu, \sigma, \pi_0, \lambda\) are outputs of neural
  networks.

  To achieve dimensionality reduction, we want the dimension of \(\mu(x)\) less
  than the dimension of \(x\).

  To visualize the result, we can take \(\mu(x)\).

  #+BEGIN_SRC ipython :session kernel-aksarkar.json :results raw drawer :async t
    num_epochs = 20
    minibatch_size = 100
    n, p = dge.shape
    num_minibatch = n // 100
    # Last layer is the target latent dimension
    layer_dim = [2048, 1024, 512]

    graph = tf.Graph()
    gpu_dev = '/gpu:0'
    with graph.as_default(), graph.device(gpu_dev):
      with tf.name_scope('input'):
        x = tf.placeholder(shape=[minibatch_size, p], dtype=tf.float32)
      with tf.variable_scope('encoder'):
        # Default activation is relu
        qz = slim.stack(x, slim.fully_connected, layer_dim[:-1], scope='fc')
        loc = slim.linear(qz, num_outputs=layer_dim[-1], activation_fn=None,
                          scope='loc')
        scale = slim.fully_connected(qz, num_outputs=layer_dim[-1],
                                     activation_fn=tf.nn.softplus, scope='scale')
        with st.value_type(st.MeanValue()):
          # Put minimum scale here because it has to be outside the softplus (bias
          # on sigma is inside the softplus)
          qz = st.StochasticTensor(ds.Normal(loc=loc, scale=(1e-6 + scale)))
      with tf.variable_scope('decoder'):
        pz = ds.Normal(loc=tf.zeros(layer_dim[-1]), scale=tf.ones(layer_dim[-1]))
        px = slim.stack(qz, slim.fully_connected, list(reversed(layer_dim[:-1])),
                        scope='fc')
        pi_0 = slim.fully_connected(px, num_outputs=1,
                                    activation_fn=tf.nn.sigmoid, scope='pi_0')
        rate = slim.linear(px, num_outputs=p, activation_fn=tf.nn.softplus,
                           scope='lambda')
        # pi_0 + (1 - pi_0) Pois(0; rate), x = 0
        #        (1 - pi_0) Pois(x; rate), x > 0
        #
        # We need minimum rate because otherwise log_prob gives nan
        px = ds.Poisson(1e-6 + rate)
        num_0 = tf.reduce_sum(tf.cast(x == 0, tf.float32))
        llik = tf.reduce_sum(pi_0 * num_0 + (1 - pi_0) * px.log_prob(x))
        reconstructed_x = (1 - pi_0) * rate
        frob = tf.norm(reconstructed_x - x)

      vi.register_prior(qz, pz)
      elbo = tf.reduce_sum(vi.elbo(llik))
      opt = tf.train.AdamOptimizer(learning_rate=1e-5)
      train = opt.minimize(-elbo)

    # This needs to be outside graph.device
    sv = tf.train.Supervisor(
      graph=graph,
      logdir=os.path.join(os.getenv('SCRATCH'), 'zip-vae-model'))
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

  #+BEGIN_SRC ipython :session kernel-aksarkar.json :results raw drawer :async t
    with sv.managed_session() as sess:
      for i in range(num_epochs * dge.shape[0] // minibatch_size):
        if sv.should_stop():
          break
        start = (i % num_minibatch) * minibatch_size
        _, *loss = sess.run([train, elbo, llik, frob], feed_dict={x: dge.iloc[start:start + minibatch_size]})
        if np.isnan(loss[0]):
          raise tf.train.NanLossDuringTrainingError
        if not i % 100:
          print(i, *loss)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

  #+BEGIN_SRC ipython :session kernel-aksarkar.json :results raw drawer :async t
    with sv.managed_session() as sess:
      frob = np.zeros(num_minibatch)
      for i in range(num_minibatch):
        frob[i] = sess.run(frob,
          feed_dict={x: dge.iloc[start:start + minibatch_size]})
      np.square(frob).sum()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  7d14022a-fcef-40ae-a6a9-65bd4a395171
  :END:

  #+BEGIN_SRC ipython :session kernel-aksarkar.json :results raw drawer :ipyfile test.png
  plt.clf()
  plt.scatter(dge.iloc[:,0], x_hat[:,0])
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  : <matplotlib.collections.PathCollection at 0x7ff8b4764080>
  [[file:test.png]]
  :END:

  #+BEGIN_SRC ipython :session kernel-aksarkar.json :results raw drawer :async t
    with sv.managed_session() as sess:
      z = np.zeros((n, layer_dim[-1]))
      for i in range(num_minibatch):
        start = i * minibatch_size
        z[start:start + minibatch_size] = sess.run(qz.value(), feed_dict={x: dge.iloc[start:start + minibatch_size]})
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

  #+BEGIN_SRC ipython :ipyfile pca.png :session kernel-aksarkar.json :results raw drawer :async t
    u, d, v = spla.svd(z, full_matrices=False)
    z_proj = z.dot(v)
    plt.clf()
    plt.scatter(z_proj[:,0], z_proj[:,1])
    plt.xlabel('PC1 $q(z \mid x)$')
    plt.ylabel('PC2 $q(z \mid x)')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  : <matplotlib.text.Text at 0x7fdd150733c8>
  [[file:pca.png]]
  :END:

  #+BEGIN_SRC ipython :ipyfile pi_0.png :session kernel-aksarkar.json :results raw drawer :async t
    pi_0_emp = (dge == 0).sum(axis=1) / dge.shape[1]
    with sv.managed_session() as sess:
      pi_0_hat = np.zeros((n, 1))
      for i in range(num_minibatch):
        start = i * minibatch_size
        pi_0_hat[start:start + minibatch_size] = sess.run(pi_0, feed_dict={x: dge.iloc[start:start + minibatch_size]})

    plt.clf()
    plt.scatter(pi_0_emp, pi_0_hat)
    plt.xlabel('Empirical $\pi_0$')
    plt.ylabel('Posterior mean $\pi_0$')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  : <matplotlib.text.Text at 0x7ff8b1787518>
  [[file:pi_0.png]]
  :END:
