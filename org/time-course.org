#+TITLE: Fully unsupervised topic models of scRNA-seq time course data
#+SETUPFILE: setup.org

* Introduction

  In our prior work ([[https://dx.doi.org/10.1371/journal.pgen.1008045][Sarkar
  et al. 2019]]), we introduced a factor model to capture between donor
  individual variation in the mean and variance of gene expression in a single
  cell type \( 
  \DeclareMathOperator\Gam{Gamma}
  \DeclareMathOperator\Poi{Poisson}
  \newcommand\mf{\mathbf{F}} 
  \newcommand\ml{\mathbf{L}}
  \newcommand\mx{\mathbf{X}}
  \newcommand\vl{\mathbf{l}}
  \newcommand\vx{\mathbf{x}}
  \)

  \begin{align*}
    x_{ij} &\sim \Poi(x_i^+ \lambda_{ij})\\
    \lambda_{ij} &\sim \pi_{ij} \delta_0(\cdot) + (1 - \pi_{ij}) \Gam(\phi_{ij}^{-1}, \mu_{ij}^{-1} \phi_{ij}^{-1})\\
    \ln \mu_{ij} &= (\ml \mf_\mu')_{ij}\\
    \ln \phi_{ij} &= (\ml \mf_\phi')_{ij}\\
    \operatorname{logit} \pi_{ij} &= (\ml \mf_\pi')_{ij}\\
  \end{align*}

  where 

  - \(x_{ij}\) is the number of molecules of gene \(j = 1, \ldots, p\) observed
    in cell \(i = 1, \ldots, n\)
  - cells are taken from \(m\) donor individuals, \(\ml\) is \(n \times m\),
    each \(\mf_{\cdot}\) is \(p \times m\)
  - assignments of cells to donors (loadings) \(l_{ik} \in \{0, 1\}\) are
    known and fixed.

  We are now interested in several lines of questions:

  1. If we analyze this kind of data in a fully unsupervised manner, can we
     recover the assignments of cells to donors? This approach has been
     previously proposed in our specific factor model
     ([[doi:10.1038/s41467-017-02554-5][Risso et al. 2018]]). If not, what do
     we recover?
  2. Can we generalize this analysis approach to data which additionally has
     multiple cell types, and then multiple time points?
  3. Can we implement this approach without forming entire products
     \(\ml\mf'\)? Can we implement each update without looking at the entire
     data \(\mx\)? How much faster can we get than existing methods? Can we
     analyze datasets which are currently impossible to analyze (due to size),
     e.g. Human Cell Atlas? As references, compare against
     [[doi:10.1038/s41592-018-0229-2][scVI]],
     [[https://rawcdn.githack.com/aertslab/cisTopic/8d15fa2813312aa0b20c1042604079558829e947/vignettes/10X_workflow.html#building_the_models][cisTopic]],
     [[https://github.com/stephenslab/fastTopics][fastTopics]]

* Setup
  :PROPERTIES:
  :CUSTOM_ID: setup
  :END:

  #+BEGIN_SRC emacs-lisp :exports none
    (add-to-list 'python-shell-completion-native-disabled-interpreters "jupyter")
    (org-babel-lob-ingest "/home/aksarkar/.emacs.d/org-templates/library.org")
  #+END_SRC

  #+RESULTS:
  : 1

  #+CALL: ipython3(venv="singlecell",partition="mstephens",memory="16G") :exports none :dir /scratch/midway2/aksarkar/singlecell

  #+RESULTS:
  : Submitted batch job 65478911

  #+BEGIN_SRC ipython
    import anndata
    import numpy as np
    import pandas as pd
    import scipy.sparse as ss
    import sklearn.decomposition as skd
    import torch
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[1]:
  :END:

  #+BEGIN_SRC ipython
    %matplotlib inline
    %config InlineBackend.figure_formats = set(['retina'])
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[2]:
  :END:

  #+BEGIN_SRC ipython
    import matplotlib.pyplot as plt
    plt.rcParams['figure.facecolor'] = 'w'
    plt.rcParams['font.family'] = 'Nimbus Sans'
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[3]:
  :END:

* Methods
** Amortized NMF/LDA

   Amortized inference
   ([[http://ruishu.io/2017/11/07/amortized-optimization/][Shu 2017]],
   [[https://papers.nips.cc/paper/7692-amortized-inference-regularization.pdf][Shu
   et al. 2018]]) refers to a general solution to coupled optimization
   problems:

   \[ \theta^* = \arg\min_\theta f(\theta, \phi) \]

   where \(\phi \in \Phi\) is some context variable. Rather than solving one
   problem for each \(\phi\), we learn a function \(h_\alpha: \phi \rightarrow
   \theta^*\) parameterized by \(\alpha\)

   \[ \alpha^* = \arg\min_\alpha E_{\phi \sim \hat{p}(\phi)} f(h_\alpha(\phi), \phi) \]

   where \(\hat{p}(\phi)\) denotes the empirical distribution of \(\phi\) in
   the training data. As a concrete example, NMF (Lee and Seung 2001q) can be
   written

   \[ \vl_i^* = \arg\min_{\vl_i} \sum_{j, k} \operatorname{Poisson}(x_{ij}; l_{ik} f_{jk})\]

   where \(\vx_i\) is the context variable, and the goal is to learn
   \(h_\alpha\) which maps observations \(\mathbf{x}_{i}\) to loadings
   \(\mathbf{l}_{i}\). This can be achieved by introducing an /encoder
   network/, which has been previously proposed
   ([[doi:10.1038/s41592-018-0229-2][Lopez et al. 2018]],
   [[doi:10.1038/s41467-018-07931-2][Eraslan et al. 2018]]).

   NMF is intimately connected to LDA via the Multinomial-Poisson transform
   ([[http://www.jstor.org/stable/2348134][Baker 1994]]). Briefly, if we have
   an MLE for NMF and scale \(\ml\) and \(\mf\) to satisfy the constraints
   \(\sum_k l_{ik} = 1\), \(\sum_k f_{jk} = 1\), we recover an MLE for the
   Multinomial likelihood underlying LDA. This fact suggests an amortized
   inference scheme for maximum likelihood estimation of topic models:

   \begin{align*}
     x_{ij} \mid s_i, \mathbf{l}_i, \mathbf{F} &\sim \operatorname{Poisson}(s_i \sum_k l_{ik} f_{jk})\\
     \mathbf{l}_i &= h(\mathbf{x}_i)
   \end{align*}

   where \(h\) is a neural network with softmax output. Unlike previous
   approaches for amortized inference in topic models
   ([[https://arxiv.org/pdf/1703.01488][Srivastava et al. 2017]]), we are not
   concerned with recovering an approximate posterior over \(\mathbf{L},
   \mathbf{F}\), simplifying the problem. Our scheme suggests a fast GPU
   implementation based on minibatch gradient descent.

** iPSC data

   Read the data.

   #+BEGIN_SRC ipython :async t
     keep_samples = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/quality-single-cells.txt', index_col=0, header=None)
     keep_genes = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/genes-pass-filter.txt', index_col=0, header=None)
     annotations = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-annotation.txt')
     annotations = annotations.loc[keep_samples.values.ravel()]
     header = sorted(set(annotations['chip_id']))
     umi = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-counts.txt.gz', index_col=0).loc[keep_genes.values.ravel(),keep_samples.values.ravel()]
     gene_info = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-genes.txt.gz', index_col=0)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[7]:
   :END:

   Convert to sparse ~h5ad~.

   #+BEGIN_SRC ipython :async t
     del annotations["index"]
     x = anndata.AnnData(ss.csr_matrix(umi.values.T), obs=annotations, var=gene_info.loc[umi.index])
     x.write('/project2/mstephens/aksarkar/projects/singlecell-ideas/data/ipsc/ipsc.h5ad', compression=None, force_dense=False)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[33]:
   :END:

* Results
** Unsupervised NMF on iPSCs

   #+BEGIN_SRC ipython
     x = anndata.read_h5ad('/project2/mstephens/aksarkar/projects/singlecell-ideas/data/ipsc/ipsc.h5ad')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[4]:
   :END:

   #+BEGIN_SRC ipython :async t
     m = skd.NMF(n_components=53, beta_loss=1, solver='mu', verbose=True).fit(x.X)
     f = m.components_.T
     l = m.transform(x.X)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   0 - f38355c7-4293-4104-babc-10a2759ce749
   :END:
