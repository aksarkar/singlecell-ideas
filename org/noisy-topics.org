#+TITLE: Noisy topic models
#+SETUPFILE: setup.org

* Introduction

  Matthew Stephens and Zihao Wang suggest a variation on hierarchical Poisson
  matrix factorization (Cemgil 2009)\(
  \DeclareMathOperator\Pois{Poisson}
  \DeclareMathOperator\Gam{Gamma}
  \DeclareMathOperator\Mult{Multinomial}
  \newcommand\E[1]{\left\langle #1 \right\rangle}
  \newcommand\const{\mathrm{const}}
  \newcommand\ml{\mathbf{L}}
  \newcommand\mf{\mathbf{F}}
  \newcommand\vmu{\boldsymbol{\mu}}
  \newcommand\mphi{\boldsymbol{\Phi}}
  \)

  \begin{align}
    x_{ij} &= \sum_{k=1}^K z_{ijk}\\
    z_{ijk} &\sim \Pois(l_{ik} \mu_j u_{jk})\\
    u_{jk} &\sim \Gam(\theta_{jk}, \theta_{jk}),
  \end{align}

  where the Gamma distribution is parameterized by shape and rate, \(\E{u_{jk}}
  = 1\), and \(V[u_{jk}] = 1 / \theta_{jk}\). The intuition is to rewrite
  factors \(f_{jk} = \mu_j u_{jk}\). After a suitable scaling, \(\ml\) and
  \(\mf\) are then a valid topic model in which most topics reflect the average
  gene expression at most genes, and \(\theta_{jk}\) can be used to find genes
  which depart from the mean, which could be of biological interest.

* Setup
  :PROPERTIES:
  :CUSTOM_ID: setup
  :END:

  #+BEGIN_SRC emacs-lisp :exports none
    (add-to-list 'python-shell-completion-native-disabled-interpreters "jupyter")
    (org-babel-lob-ingest "/home/aksarkar/.emacs.d/org-templates/library.org")
  #+END_SRC

  #+RESULTS:
  : 1

  #+CALL: ipython3(venv="singlecell",partition="mstephens",memory="16G") :exports none :dir /scratch/midway2/aksarkar

  #+BEGIN_SRC ipython
    %matplotlib inline
    %config InlineBackend.figure_formats = set(['retina'])
  #+END_SRC

  #+BEGIN_SRC ipython
    import matplotlib.pyplot as plt
    plt.rcParams['figure.facecolor'] = 'w'
    plt.rcParams['font.family'] = 'Nimbus Sans'
  #+END_SRC

* Methods
** Inference

  The log joint is

  \begin{multline}
    \ln p = \sum_{i,j,k} \left[ z_{ijk} \ln (l_{ik} \mu_j u_{jk}) - l_{ik} \mu_j u_{jk} - \ln\Gamma(z_{ijk} + 1) \right]\\
    + \sum_{j,k} \left[ \theta_{jk}\ln \theta_{jk} + (\theta_{jk} - 1) \ln u_{jk} - \theta_{jk} u_{jk} - \ln\Gamma(\theta_{jk})\right],
  \end{multline}

  if \(x_{ij} = \sum_k z_{ijk}\), and \(-\infty\) otherwise. By a
  variational argument

  \begin{align}
    q^*(z_{ij1}, \ldots, z_{ijK}) &\propto \exp(z_{ijk}(\ln(l_{ik} \mu_j) + \E{\ln u_{jk}}))\\
    &= \Mult(x_{ij}, \pi_{ij1}, \ldots, \pi_{ijK}), \qquad \pi_{ijk} \propto l_{ik}\mu_j\exp(\E{\ln u_{jk}})\\
    q^*(u_{jk}) &\propto \exp(\textstyle\sum_i (\E{z_{ijk}} + \theta_{jk} - 1) \ln u_{jk} - (l_{ik} \mu_j + \theta_{jk}) u_{jk})\\
    &= \Gam(\textstyle\sum_i \E{z_{ijk}} + \theta_{jk}, \textstyle\sum_i l_{ik}\mu_j + \theta_{jk})\\
    &\triangleq \Gam(\alpha_{jk}, \beta_{jk}).
  \end{align}

  where

  \begin{align}
    \E{z_{ijk}} &= x_{ij} \pi_{ijk}\\
    \E{u_{jk}} &= \alpha_{jk} / \beta_{jk}\\
    \E{\ln u_{jk}} &= \psi(\alpha_{jk}) - \ln \beta_{jk}
  \end{align}

  and \(\psi\) denotes the digamma function. The evidence lower bound (ELBO) is

  \begin{multline}
    \ell = \sum_{i,j,k} \left[ \E{z_{ijk}} (\ln (l_{ik} \mu_j) + \E{\ln u_{jk}} - \ln\pi_{ijk}) - l_{ik} \mu_j \E{u_{jk}} \right] - \sum_{i,j} \ln\Gamma(x_{ij} + 1)\\
    + \sum_{j,k} \left[ (\theta_{jk} - \alpha_{jk}) \E{\ln u_{jk}} - (\theta_{jk} - \beta_{jk}) \E{u_{jk}} - \theta_{jk}\ln \theta_{jk} + \beta_{jk} \ln\alpha_{jk} - \ln\Gamma(\theta_{jk}) + \ln\Gamma(\alpha_{jk})\right],
  \end{multline}

  To maximize the ELBO,

  \begin{align}
    \frac{\partial\ell}{\partial l_{ik}} &= \sum_j \frac{\E{z_{ijk}}}{l_{ik}} - \mu_j \E{u_{jk}} = 0\\
    l_{ik} &= \frac{\sum_j \E{z_{ijk}}}{\sum_j \mu_j \E{u_{jk}}}\\
    \frac{\partial\ell}{\partial \mu_j} &= \sum_{i, k} \frac{\E{z_{ijk}}}{\mu_j} - l_{ik} \E{u_{jk}} = 0\\
    \mu_j &= \frac{\sum_{i, k} \E{z_{ijk}}}{\sum_{i, k} l_{ik} \E{u_{jk}}}\\
    \frac{\partial\ell}{\partial \theta_{jk}} &= 1 + \ln \theta_{jk} + \E{\ln u_{jk}} - \psi(\theta_{jk})
  \end{align}

  where \(\theta_{jk}\) can be updated via gradient ascent with line search.

** Implementation

  #+BEGIN_SRC ipython
    def hpmf(x, rank, atol=1e-4, max_iters=1000, verbose=False):
      if not ss.isspmatrix_coo(x):
        x = ss.coo_matrix(x)
      n, p = x.shape
      l = np.random.uniform(size=(n, rank))
      mu = np.ones((p, 1))
      # Sparse representation of variational parameters for non-zero x
      z_idx = np.vstack((x.row, x.col))
      pi = np.ones((x.nnz, rank)) / rank
      alpha = np.ones((p, rank))
      beta = np.ones((p, rank))
      theta = np.ones((p, rank))

      obj = elbo(x, z_idx, pi, l, mu, alpha, beta)
      for t in range(max_iters):
        # Expectations wrt variational distribution
        u = alpha / beta
        log_u = sp.digamma(alpha) - np.log(beta)
        # Coordinate updates
        # Hyperparameter updates
        for j in range(p):
          for k in range(rank):
            theta[j,k] = update_theta(u[j,k], log_u[j,k], alpha[j,k], beta[j,k])
        update = elbo(x, z_idx, pi, l, mu, alpha, beta)
        if update < obj:
          raise RuntimeError('objective increased')
        elif abs(update - obj) < atol:
          return l, mu, alpha, beta
        else:
          obj = update
          print(f'[{t}] elbo={elbo:.2g}')
      raise RuntimeError('max_iters exceeded')

    def elbo(x, z_idx, pi, l, mu, alpha, beta):
      pass

    def theta_loss(theta, u, log_u, alpha, beta):
      pass

    def update_theta(theta, u, log_u, alpha, beta, step=1, c=0.5, tau=0.5, max_iters=32):
      # Important: take steps wrt log_inv_disp to avoid non-negativity constraint
      log_inv_disp = np.log(inv_disp)
      d = _D_loss_theta(inv_disp, u, log_u, w) * inv_disp
      loss = _nbmf_loss(x, lam, inv_disp=inv_disp, w=w)
      update = _nbmf_loss(x, lam, inv_disp=np.exp(log_inv_disp + step * d), w=w)
      while (not np.isfinite(update) or update > loss + c * step * d) and max_iters > 0:
        step *= tau
        update = _nbmf_loss(x, lam, inv_disp=np.exp(log_inv_disp + step * d), w=w)
        max_iters -= 1
      if max_iters == 0:
        # Step size is small enough that update can be skipped
        return inv_disp
      else:
        return np.exp(log_inv_disp + step * d) + 1e-15
  #+END_SRC
