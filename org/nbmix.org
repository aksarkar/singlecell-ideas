#+TITLE: Model-based clustering of scRNA-seq data
#+SETUPFILE: setup.org

* Introduction

  Two major strategies for clustering scRNA-seq data are:

  1. Building a \(k\)-nearest neighbor graph on the data, and applying a
     community detection algorithm (e.g.,
     [[https://doi.org/10.1088/1742-5468/2008/10/P10008][Blondel et al. 2008]],
     [[https://arxiv.org/abs/1810.08473][Traag et al. 2018]])
  2. Fitting a topic model to the data
     (e.g., [[https://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1006599][Dey
     et al. 2017]],
     [[https://www.nature.com/articles/s41592-019-0367-1][Gonz√°les-Blas et
     al. 2019]])

  The main disadvantage of strategy (1) is that, as commonly applied to
  transformed counts, it does not separate measurement error and biological
  variation of interest. The main disadvantage of strategy (2) is that it does
  not account for transcriptional noise
  ([[https://doi.org/10.1016/j.cell.2008.09.050][Raj 2008]]). Here, we develop
  a simple model-based clustering algorithm which addresses both of these
  issues.

* Setup
  :PROPERTIES:
  :CUSTOM_ID: setup
  :END:

  #+BEGIN_SRC emacs-lisp :exports none
    (add-to-list 'python-shell-completion-native-disabled-interpreters "jupyter")
    (org-babel-lob-ingest "/home/aksarkar/.emacs.d/org-templates/library.org")
  #+END_SRC

  #+RESULTS:
  : 2

  #+CALL: ipython3(venv="singlecell",partition="gpu2",opts="--gres=gpu:1",memory="16G") :exports none :dir /scratch/midway2/aksarkar/singlecell

  #+RESULTS:
  : Submitted batch job 1359667

  #+BEGIN_SRC ipython
    import anndata
    import mpebpm.gam_mix
    import mpebpm.sgd
    import numpy as np
    import pandas as pd
    import scanpy as sc
    import scipy.stats as st
    import torch
    import torch.utils.data as td
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[7]:
  :END:

  #+BEGIN_SRC ipython
    %matplotlib inline
    %config InlineBackend.figure_formats = set(['retina'])
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[2]:
  :END:

  #+BEGIN_SRC ipython
    import matplotlib.pyplot as plt
    plt.rcParams['figure.facecolor'] = 'w'
    plt.rcParams['font.family'] = 'Nimbus Sans'
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[3]:
  :END:

  #+CALL: tensorboard(venv="singlecell") :session tensorboard :dir /scratch/midway2/aksarkar/singlecell/

* Method

  We assume \(
  \DeclareMathOperator\Pois{Poisson}
  \DeclareMathOperator\Gam{Gamma}
  \DeclareMathOperator\N{\mathcal{N}}
  \newcommand\vmu{\boldsymbol{\mu}}
  \newcommand\vphi{\boldsymbol{\phi}}
  \newcommand\vpi{\boldsymbol{\pi}}
  \)

  \begin{align}
    x_{ij} \mid x_{i+}, \lambda_{ij} &\sim \Pois(x_{i+} \lambda_{ij})\\
    \lambda_{ij} \mid \vpi_i, \vmu_k, \vphi_k &\sim \sum_{k=1}^{K} \pi_{ik} \Gam(\phi_{kj}^{-1}, \phi_{kj}^{-1}\mu_{kj}^{-1}),
  \end{align}

  where

  - \(x_{ij}\) denotes the number of molecules of gene \(j\) observed in cell \(i\)
  - \(x_{i+} \triangleq \sum_j x_{ij}\) denotes the total number of molecules
    observed in cell \(i\)
  - \(\vpi_i\) denotes cluster assignment probabilities for cell \(i\)
  - \(\vmu_k, \vphi_k\) denote the cluster "centroid" for cluster \(k\)

  The intuition behind this model is that each cluster \(k\) is defined by a
  collection of independent Gamma distributions, one per gene \(j\). These
  Gamma distributions describe the distribution of true gene expression for
  each gene in each cluster
  ([[https://dx.doi.org/10.1101/2020.04.07.030007][Sarkar and Stephens 2020]]).
  This model admits a simple EM algorithm. Letting \(z_{ik} \in \{0, 1\}\)
  indicate whether cell \(i\) is assigned to cluster \(k\), in the E step

  \begin{equation}
    E[z_{ik}] \propto \sum_j \int_0^{\infty} \Pois(x_{ij}; x_{i+}\lambda) \Gam(\lambda; \phi_{kj}^{-1}, \phi_{kj}^{-1}\mu_{kj}^{-1})\, d\lambda.
  \end{equation}

  In the M step, we improve the weighted mixture of negative binomials
  likelihood by [[file:mepbpm.org][(batch) gradient descent]], implemented in
  the Python package ~mpebpm~. To make the model more amenable to stochastic
  gradient descent, we can amortize inference by learning a neural network
  mapping \(x_{i\cdot} \rightarrow z_{i\cdot}\)
  ([[https://arxiv.org/abs/1805.08913][Shu et al. 2018]]).

* Results
** Example

   Read sorted immune cell scRNA-seq data
   ([[https://dx.doi.org/10.1038/ncomms14049][Zheng et al. 2017]]).

   #+BEGIN_SRC ipython :async t
     dat = anndata.read_h5ad('/scratch/midway2/aksarkar/ideas/zheng-10-way.h5ad')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[4]:
   :END:

   Get 256 B cells and 256 cytotoxic T cells.

   #+BEGIN_SRC ipython :async t
     b_cells = dat[dat.obs['cell_type'] == 'b_cells']
     # Important: this has a set seed
     sc.pp.subsample(b_cells, n_obs=256)
     t_cells = dat[dat.obs['cell_type'] == 'cytotoxic_t']
     sc.pp.subsample(t_cells, n_obs=256)
     temp = b_cells.concatenate(t_cells)
     sc.pp.filter_genes(temp, min_counts=1)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[5]:
   :END:

   Plot a UMAP embedding of the data.

   #+BEGIN_SRC ipython
     sc.pp.pca(temp)
     sc.pp.neighbors(temp)
     sc.tl.umap(temp)
   #+END_SRC

   #+BEGIN_SRC ipython :ipyfile figure/nbmix.org/sim-ex.png
     cm = plt.get_cmap('Dark2')
     plt.clf()
     plt.gcf().set_size_inches(3, 3)
     for i, c in enumerate(temp.obs['cell_type'].unique()):
       plt.plot(*temp[temp.obs['cell_type'] == c].obsm["X_umap"].T, c=cm(i), marker='.', ms=2, lw=0, label=f'{c}')
     plt.legend(frameon=False, markerscale=4, handletextpad=0)
     plt.xlabel('UMAP 1')
     plt.ylabel('UMAP 2')
     plt.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[7]:
   [[file:figure/nbmix.org/sim-ex.png]]
   :END:

   First, start from the ground truth \(z\) (labels), and estimate the Gamma
   expression models.

   #+BEGIN_SRC ipython :async t
     fit = mpebpm.sgd.ebpm_gamma(
       temp.X,
       onehot=pd.get_dummies(temp.obs['cell_type']).values,
       batch_size=32,
       num_epochs=320,
       shuffle=True,
       log_dir='runs/nbmix/ex4')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[13]:
   :END:

   Estimate the cluster weights.

   #+BEGIN_SRC ipython
     L = mpebpm.gam_mix._nb_mix_llik(
       x=torch.tensor(temp.X.A, dtype=torch.float), 
       s=torch.tensor(temp.X.sum(axis=1), dtype=torch.float),
       log_mean=torch.tensor(fit[0], dtype=torch.float),
       log_inv_disp=torch.tensor(fit[1], dtype=torch.float)).sum(dim=-1)
     zhat = torch.nn.functional.softmax(L, dim=1)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[114]:
   :END:

   Compute the cross entropy between the estimated \(\hat{z}\) and the ground
   truth.

   #+BEGIN_SRC ipython
     torch.nn.functional.binary_cross_entropy(
       zhat,
       torch.tensor(pd.get_dummies(temp.obs['cell_type']).values, dtype=torch.float))
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[108]:
   : tensor(0.)
   :END:

   Compute a weighted log likelihood.

   #+BEGIN_SRC ipython
     w = torch.rand([512, 2])
     w /= w.sum(dim=1).unsqueeze(-1)
     m, _ = L.max(dim=1, keepdim=True)
     (m + torch.log(w * torch.exp(L - m) + 1e-8)).mean()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[159]:
   : tensor(-1872.6364)
   :END:

   Try fitting the model from a random initialization.

   #+BEGIN_SRC ipython :async t
     import imp; imp.reload(mpebpm.gam_mix)
     torch.manual_seed(1)
     fit = mpebpm.gam_mix.ebpm_gam_mix(
       x=temp.X.A,
       s=temp.X.sum(axis=1),
       y=torch.tensor(pd.get_dummies(temp.obs['cell_type']).values, dtype=torch.float).cuda(),
       k=2,
       batch_size=512,
       num_epochs=400,
       max_em_iters=100,
       tol=0.1,
       log_dir='runs/nbmix/test11')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[21]:
   :END:

   Compute the cross entropy between the estimated \(\hat{z}\) and the ground
   truth.

   #+BEGIN_SRC ipython
     torch.nn.functional.binary_cross_entropy(
       torch.tensor(1 - fit[-1], dtype=torch.float),
       torch.tensor(pd.get_dummies(temp.obs['cell_type']).values, dtype=torch.float))
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[24]:
   : tensor(0.)
   :END:

   Plot the UMAP, colored by the fitted clusters.

   #+BEGIN_SRC ipython :ipyfile figure/nbmix.org/sim-ex-fit.png
     cm = plt.get_cmap('Dark2')
     plt.clf()
     plt.gcf().set_size_inches(3, 3)
     for i in range(fit[-1].shape[1]):
       plt.plot(*temp[fit[-1][:,i].astype(bool)].obsm["X_umap"].T, c=cm(i), marker='.', ms=2, lw=0, label=f'Cluster {i}')
     plt.legend(frameon=False, markerscale=4, handletextpad=0)
     plt.xlabel('UMAP 1')
     plt.ylabel('UMAP 2')
     plt.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[26]:
   [[file:figure/nbmix.org/sim-ex-fit.png]]
   :END:

* Related work

  scVI (Lopez et al. 2018) implements a deep unsupervised (more precisely,
  semi-supervised) clustering algorithm (Kingma and Welling 2014)

  \begin{align}
    x_{i\cdot} \mid s_i, \lambda_{ij} &\sim \Pois(s_i \lambda_{ij})\\
    \lambda_{i\cdot} \mid z_{i\cdot} &\sim \Gam(f(z_{i\cdot}))\\
    z_{i\cdot} \mid y_i &\sim \N(\cdot).
  \end{align}
