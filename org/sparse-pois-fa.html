<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2020-06-30 Tue 14:23 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Sparse Poisson Factor Analysis</title>
<meta name="generator" content="Org mode">
<meta name="author" content="Abhishek Sarkar">
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link href="bootstrap.min.css" rel="stylesheet" />
<link rel="stylesheet" type="text/css" href="htmlize.css"/>
<link rel="stylesheet" type="text/css" href="main.css"/>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2017 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Sparse Poisson Factor Analysis</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org6828f39">Introduction</a></li>
<li><a href="#setup">Setup</a></li>
<li><a href="#org7af8d2e">Method</a></li>
<li><a href="#orgedbceb4">Results</a>
<ul>
<li><a href="#orge8b8de3">Sanity check</a></li>
<li><a href="#org899aad6">Simulated example</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-org6828f39" class="outline-2">
<h2 id="org6828f39">Introduction</h2>
<div class="outline-text-2" id="text-org6828f39">
<p>
<code>flash</code> (Wang and Stephens 2018) is a low rank model which estimates a
flexible prior on loadings and factors \(
  \DeclareMathOperator\E{E}
  \DeclareMathOperator\KL(\mathcal{KL})
  \DeclareMathOperator\N{\mathcal{N}}
  \DeclareMathOperator\Poi{Poisson}
  \DeclareMathOperator\V{V}
  \newcommand\Gfam{\mathcal{G}}
  \newcommand\mf{\mathbf{F}}
  \newcommand\ml{\mathbf{L}}
  \newcommand\vf{\mathbf{f}}
  \newcommand\vl{\mathbf{l}}
  \)
</p>

\begin{align}
  x_{ij} &\sim \N((\ml\mf')_{ij}, \sigma_{ij}^2)\\
  l_{1k}, \ldots, l_{nk} &\sim g_{\vl_k} \in \Gfam\\
  f_{1k}, \ldots, f_{pk} &\sim g_{\vf_k} \in \Gfam,
\end{align}

<p>
where \(\Gfam\) denotes a family of distributions. The posterior \(p(\ml, \mf
  \mid \mx)\) can be approximately estimated using variational inference. In
particular, coordinate ascent updates which maximize the ELBO correspond to
solutions to the Empirical Bayes Normal Means problem. They study two main
cases: (1) \(\Gfam\) is the family of unimodal, zero-mode scale mixtures of
Normals over a fixed grid of scales, and (2) \(\Gfam\) is the family of
point-Normal mixtures. They found that in some practical examples, the
differences in inference between these two families was minimal.
</p>

<p>
We are now interested in developing similarly flexible methods to learn
low-rank, (possibly) sparse representations of scRNA-seq data, which follow a
different generative process (Sarkar and Stephens 2020)
</p>

\begin{align}
  x_{ij} \mid s_i &\sim \Poi(s_i \lambda_{ij})\\
  \lambda_{ij} &= h^{-1}((\ml\mf')_{ij})
  l_{1k}, \ldots, l_{nk} &\sim g_{\vl_k} \in \Gfam\\
  f_{1k}, \ldots, f_{pk} &\sim g_{\vf_k} \in \Gfam,
\end{align}

<p>
where \(h\) is a link function. Although it is natural to choose \(h = \ln\),
for practical purposes it can be preferable to choose the softplus link
\(h(x) = \ln(1 + \exp(x))\) (Argelauget 2018). Variational inference in this
model requires either Taylor approximation (Seeger 2012) or a Monte Carlo
approach (Kingma and Welling 2014, Rezende et al. 2014). Here, we investigate
the latter.
</p>
</div>
</div>

<div id="outline-container-orgccb4a6a" class="outline-2">
<h2 id="setup"><a id="orgccb4a6a"></a>Setup</h2>
<div class="outline-text-2" id="text-setup">
<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> numpy <span class="org-keyword">as</span> np
<span class="org-keyword">import</span> torch
<span class="org-keyword">import</span> torch.utils.tensorboard <span class="org-keyword">as</span> tb
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">%matplotlib inline
%config <span class="org-variable-name">InlineBackend.figure_formats</span> = <span class="org-builtin">set</span>([<span class="org-string">'retina'</span>])
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> matplotlib.pyplot <span class="org-keyword">as</span> plt
<span class="org-variable-name">plt.rcParams</span>[<span class="org-string">'figure.facecolor'</span>] = <span class="org-string">'w'</span>
<span class="org-variable-name">plt.rcParams</span>[<span class="org-string">'font.family'</span>] = <span class="org-string">'Nimbus Sans'</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-org7af8d2e" class="outline-2">
<h2 id="org7af8d2e">Method</h2>
<div class="outline-text-2" id="text-org7af8d2e">
<p>
The evidence lower bound (ELBO) can be written
</p>

\begin{equation}
  \ell = \sum_{i,j} \E_q[\ln p(x_{ij} \mid s_i, \lambda_{ij})] - \sum_k \left[\KL{q(l_{ik})}{p(l_{ik})} + \KL{q(f_{jk})}{p(f_{jk})}\right].
\end{equation}

<p>
The KL divergence terms in the ELBO are analytic (e.g., Carbonetto and
Stephens 2012). The remaining terms involve intractable expectations, so we
take a Monte Carlo approach to estimate them, resulting in a stochastic
objective function. The critical calculation in this approach is
</p>

\begin{equation}
  \V[\ml\mf'] = \V[\ml]\V[\mf'] + \E[\ml]^2 \V[\mf]' + \V[\ml] \E[\mf]^2,
\end{equation}

<p>
where expectations (variances) are taken with respect to \(q\). We optimize
the stochastic objective function by gradient descent.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">class</span> <span class="org-type">SpikeSlab</span>(torch.nn.Module):
  <span class="org-doc">"""Independent Point-Normal priors on columns of matrix M"""</span>
  <span class="org-keyword">def</span> <span class="org-function-name">__init__</span>(<span class="org-keyword">self</span>, p, k):
    <span class="org-builtin">super</span>().__init__()
    <span class="org-keyword">self</span>.logits = torch.nn.Parameter(torch.randn([p, k]))
    <span class="org-keyword">self</span>.mean = torch.nn.Parameter(torch.randn([p, k]))
    <span class="org-keyword">self</span>.log_prec = torch.nn.Parameter(torch.zeros([p, k]))
    <span class="org-keyword">self</span>.prior_logodds = torch.nn.Parameter(torch.randn([1, k]))
    <span class="org-keyword">self</span>.log_prior_prec = torch.nn.Parameter(torch.full([1, k], 1.))

  <span class="org-keyword">def</span> <span class="org-function-name">forward</span>(<span class="org-keyword">self</span>):
    <span class="org-doc">"""Return E[M], V[M], and KL(q(M) || p_prior(M))"""</span>
    <span class="org-variable-name">finfo</span> = torch.finfo(<span class="org-keyword">self</span>.logits.dtype)
    <span class="org-variable-name">pip</span> = torch.clamp(torch.sigmoid(<span class="org-keyword">self</span>.logits), finfo.tiny, 1 - finfo.eps)
    <span class="org-variable-name">prec</span> = torch.exp(<span class="org-keyword">self</span>.log_prec)
    <span class="org-variable-name">kl</span> = (pip * torch.log(pip / torch.sigmoid(<span class="org-keyword">self</span>.prior_logodds)) +
          (1 - pip) * torch.log((1 - pip) / torch.sigmoid(-<span class="org-keyword">self</span>.prior_logodds)) +
          .5 * pip * (1 + <span class="org-keyword">self</span>.log_prec - <span class="org-keyword">self</span>.log_prior_prec + torch.exp(<span class="org-keyword">self</span>.log_prior_prec) * (<span class="org-keyword">self</span>.mean ** 2 + 1 / prec))).<span class="org-builtin">sum</span>()
    <span class="org-keyword">assert</span> <span class="org-keyword">not</span> torch.isnan(kl).<span class="org-builtin">any</span>()
    <span class="org-variable-name">pm</span> = pip * <span class="org-keyword">self</span>.mean
    <span class="org-variable-name">pv</span> = pip / prec + pip * (1 - pip) * <span class="org-keyword">self</span>.mean ** 2
    <span class="org-keyword">return</span> pm, pv, kl

<span class="org-keyword">class</span> <span class="org-type">SFA</span>(torch.nn.Module):
  <span class="org-keyword">def</span> <span class="org-function-name">__init__</span>(<span class="org-keyword">self</span>, n, p, k):
    <span class="org-builtin">super</span>().__init__()
    <span class="org-keyword">self</span>.l = SpikeSlab(n, k)
    <span class="org-keyword">self</span>.f = SpikeSlab(p, k)

  <span class="org-keyword">def</span> <span class="org-function-name">forward</span>(<span class="org-keyword">self</span>, x, writer=<span class="org-constant">None</span>, global_step=<span class="org-constant">None</span>):
    <span class="org-doc">"""Return ELBO(x, q)"""</span>
    <span class="org-variable-name">pm_l</span>, <span class="org-variable-name">pv_l</span>, <span class="org-variable-name">kl_l</span> = <span class="org-keyword">self</span>.l.forward()
    <span class="org-variable-name">pm_f</span>, <span class="org-variable-name">pv_f</span>, <span class="org-variable-name">kl_f</span> = <span class="org-keyword">self</span>.f.forward()
    <span class="org-variable-name">eta_mean</span> = pm_l @ pm_f.T
    <span class="org-variable-name">eta_scale</span> = torch.sqrt(pv_l @ pv_f.T + (pm_l ** 2) @ pv_f.T + pv_l @ (pm_f.T ** 2))
    <span class="org-comment-delimiter"># </span><span class="org-comment">TODO: more than one stochastic sample</span>
    <span class="org-variable-name">eta</span> = torch.distributions.Normal(eta_mean, eta_scale).rsample()
    <span class="org-variable-name">err</span> = torch.distributions.Normal(eta, 1).log_prob(x).<span class="org-builtin">sum</span>()
    <span class="org-variable-name">elbo</span> = err - kl_l - kl_f
    <span class="org-keyword">if</span> writer <span class="org-keyword">is</span> <span class="org-keyword">not</span> <span class="org-constant">None</span>:
      writer.add_scalar(<span class="org-string">'loss/kl_l'</span>, kl_l, global_step)
      writer.add_scalar(<span class="org-string">'loss/kl_f'</span>, kl_f, global_step)
      writer.add_scalar(<span class="org-string">'loss/err'</span>, err, global_step)
      writer.add_scalar(<span class="org-string">'loss/elbo'</span>, elbo, global_step)
    <span class="org-keyword">return</span> -elbo

  <span class="org-keyword">def</span> <span class="org-function-name">fit</span>(<span class="org-keyword">self</span>, x, init=<span class="org-constant">None</span>, n_epochs=1000, log_dir=<span class="org-constant">None</span>, **kwargs):
    <span class="org-keyword">assert</span> torch.is_tensor(x)
    <span class="org-keyword">if</span> init <span class="org-keyword">is</span> <span class="org-keyword">not</span> <span class="org-constant">None</span>:
      <span class="org-keyword">self</span>.l.mean.data = init[0]
      <span class="org-keyword">self</span>.f.mean.data = init[1]
    <span class="org-variable-name">opt</span> = torch.optim.RMSprop(<span class="org-keyword">self</span>.parameters(), **kwargs)
    <span class="org-variable-name">global_step</span> = 0
    <span class="org-keyword">if</span> log_dir <span class="org-keyword">is</span> <span class="org-keyword">not</span> <span class="org-constant">None</span>:
      <span class="org-variable-name">writer</span> = tb.SummaryWriter(log_dir)
    <span class="org-keyword">else</span>:
      <span class="org-variable-name">writer</span> = <span class="org-constant">None</span>
    <span class="org-keyword">for</span> i <span class="org-keyword">in</span> <span class="org-builtin">range</span>(n_epochs):
      opt.zero_grad()
      <span class="org-variable-name">loss</span> = <span class="org-keyword">self</span>.forward(x, writer=writer, global_step=global_step)
      <span class="org-keyword">if</span> torch.isnan(loss):
        <span class="org-keyword">raise</span> <span class="org-type">RuntimeError</span>(<span class="org-string">'nan loss'</span>)
      loss.backward()
      opt.step()
      <span class="org-variable-name">global_step</span> += 1
    <span class="org-keyword">return</span> <span class="org-keyword">self</span>

<span class="org-keyword">class</span> <span class="org-type">SPFA</span>(torch.nn.Module):
  <span class="org-keyword">def</span> <span class="org-function-name">__init__</span>(<span class="org-keyword">self</span>, n, p, k):
    <span class="org-builtin">super</span>().__init__()
    <span class="org-keyword">self</span>.l = SpikeSlab(n, k)
    <span class="org-keyword">self</span>.f = SpikeSlab(p, k)

  <span class="org-keyword">def</span> <span class="org-function-name">forward</span>(<span class="org-keyword">self</span>, x, s, writer=<span class="org-constant">None</span>, global_step=<span class="org-constant">None</span>):
    <span class="org-doc">"""Return ELBO(x, q)"""</span>
    <span class="org-variable-name">pm_l</span>, <span class="org-variable-name">pv_l</span>, <span class="org-variable-name">kl_l</span> = <span class="org-keyword">self</span>.l.forward()
    <span class="org-variable-name">pm_f</span>, <span class="org-variable-name">pv_f</span>, <span class="org-variable-name">kl_f</span> = <span class="org-keyword">self</span>.f.forward()
    <span class="org-variable-name">eta_mean</span> = pm_l @ pm_f.T
    <span class="org-variable-name">eta_scale</span> = torch.sqrt(pv_l @ pv_f.T + (pm_l ** 2) @ pv_f.T + pv_l @ (pm_f.T ** 2))
    <span class="org-comment-delimiter"># </span><span class="org-comment">TODO: more than one stochastic sample</span>
    <span class="org-variable-name">eta</span> = torch.distributions.Normal(eta_mean, eta_scale).rsample()
    <span class="org-variable-name">err</span> = torch.distributions.Poisson(s * torch.nn.functional.softplus(eta)).log_prob(x).<span class="org-builtin">sum</span>()
    <span class="org-variable-name">elbo</span> = err - kl_l - kl_f
    <span class="org-keyword">if</span> writer <span class="org-keyword">is</span> <span class="org-keyword">not</span> <span class="org-constant">None</span>:
      writer.add_scalar(<span class="org-string">'loss/kl_l'</span>, kl_l, global_step)
      writer.add_scalar(<span class="org-string">'loss/kl_f'</span>, kl_f, global_step)
      writer.add_scalar(<span class="org-string">'loss/err'</span>, err, global_step)
      writer.add_scalar(<span class="org-string">'loss/elbo'</span>, elbo, global_step)
    <span class="org-keyword">return</span> -elbo

  <span class="org-keyword">def</span> <span class="org-function-name">fit</span>(<span class="org-keyword">self</span>, x, s, init=<span class="org-constant">None</span>, n_epochs=1000, log_dir=<span class="org-constant">None</span>, **kwargs):
    <span class="org-keyword">assert</span> torch.is_tensor(x)
    <span class="org-keyword">assert</span> torch.is_tensor(s)
    <span class="org-keyword">assert</span> s.shape == torch.Size([x.shape[0], 1])
    <span class="org-keyword">if</span> init <span class="org-keyword">is</span> <span class="org-keyword">not</span> <span class="org-constant">None</span>:
      <span class="org-keyword">self</span>.l.mean.data = init[0]
      <span class="org-keyword">self</span>.f.mean.data = init[1]
    <span class="org-variable-name">opt</span> = torch.optim.RMSprop(<span class="org-keyword">self</span>.parameters(), **kwargs)
    <span class="org-variable-name">global_step</span> = 0
    <span class="org-keyword">if</span> log_dir <span class="org-keyword">is</span> <span class="org-keyword">not</span> <span class="org-constant">None</span>:
      <span class="org-variable-name">writer</span> = tb.SummaryWriter(log_dir)
    <span class="org-keyword">else</span>:
      <span class="org-variable-name">writer</span> = <span class="org-constant">None</span>
    <span class="org-keyword">for</span> i <span class="org-keyword">in</span> <span class="org-builtin">range</span>(n_epochs):
      opt.zero_grad()
      <span class="org-variable-name">loss</span> = <span class="org-keyword">self</span>.forward(x, s, writer=writer, global_step=global_step)
      <span class="org-keyword">if</span> torch.isnan(loss):
        <span class="org-keyword">raise</span> <span class="org-type">RuntimeError</span>(<span class="org-string">'nan loss'</span>)
      loss.backward()
      opt.step()
      <span class="org-variable-name">global_step</span> += 1
    <span class="org-keyword">return</span> <span class="org-keyword">self</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-orgedbceb4" class="outline-2">
<h2 id="orgedbceb4">Results</h2>
<div class="outline-text-2" id="text-orgedbceb4">
</div>
<div id="outline-container-orge8b8de3" class="outline-3">
<h3 id="orge8b8de3">Sanity check</h3>
<div class="outline-text-3" id="text-orge8b8de3">
<p>
Make sure the algorithm works by simulating from the Gaussian model, and
then fitting that model.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">rng</span> = np.random.default_rng(1)
<span class="org-variable-name">n</span> = 100
<span class="org-variable-name">p</span> = 500
<span class="org-variable-name">k</span> = 3
<span class="org-variable-name">pi0</span> = [0.1, 0.9, 0.99]
<span class="org-variable-name">sa</span> = np.array([[.5, 1, 2]])

<span class="org-variable-name">l</span> = np.zeros((n, k))
<span class="org-variable-name">zl</span> = rng.uniform(size=(n, k)) &gt; pi0
<span class="org-variable-name">l</span>[zl] = rng.normal(size=(n, k), scale=sa)[zl]
<span class="org-variable-name">f</span> = np.zeros((p, k))
<span class="org-variable-name">zf</span> = rng.uniform(size=(p, k)) &gt; pi0
<span class="org-variable-name">f</span>[zf] = rng.normal(size=(p, k), scale=sa)[zf]

<span class="org-variable-name">x</span> = rng.normal(loc=l @ f.T)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">fit</span> = (SFA(n=n, p=p, k=k)
       .fit(torch.tensor(x, dtype=torch.<span class="org-builtin">float</span>),
            init=(torch.tensor(l), torch.tensor(f)),
            n_epochs=2000,
            log_dir=<span class="org-string">'runs/spfa/norm-1/'</span>))
</pre>
</div>
</div>
</div>

<div id="outline-container-org899aad6" class="outline-3">
<h3 id="org899aad6">Simulated example</h3>
<div class="outline-text-3" id="text-org899aad6">
<p>
Simulate from the model. This simulation itself reveals some tricky aspects
of the problem: (1) it is typically estimated that \(\lambda_{ij}\) is close
to zero for almost all entries, but this means that e.g.,
\(\ln\lambda_{ij}\) is typically large and negative, rather than zero; (2)
the maximum value of \(\lambda_{ij}\) is typically not observed to be very
large, putting strong constraints on the sparsity and scale of the prior.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">rng</span> = np.random.default_rng(1)
<span class="org-variable-name">n</span> = 100
<span class="org-variable-name">p</span> = 500
<span class="org-variable-name">k</span> = 3
<span class="org-variable-name">pi0</span> = [0.1, 0.9, 0.99]
<span class="org-variable-name">sa</span> = np.array([[.5, 1, 2]])

<span class="org-variable-name">l</span> = np.zeros((n, k))
<span class="org-variable-name">zl</span> = rng.uniform(size=(n, k)) &gt; pi0
<span class="org-variable-name">l</span>[zl] = rng.normal(size=(n, k), scale=sa)[zl]
<span class="org-variable-name">f</span> = np.zeros((p, k))
<span class="org-variable-name">zf</span> = rng.uniform(size=(p, k)) &gt; pi0
<span class="org-variable-name">f</span>[zf] = rng.normal(size=(p, k), scale=sa)[zf]

<span class="org-variable-name">lam</span> = np.log1p(np.exp(l @ f.T))
<span class="org-variable-name">x</span> = rng.poisson(lam)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">fit</span> = (SPFA(n=n, p=p, k=k)
       .fit(torch.tensor(x, dtype=torch.<span class="org-builtin">float</span>),
            s=torch.ones([x.shape[0], 1]),
            init=(torch.tensor(l), torch.tensor(f)),
            n_epochs=4000,
            log_dir=<span class="org-string">'runs/spfa/sim-ex-7/'</span>))
</pre>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Abhishek Sarkar</p>
<p class="date">Created: 2020-06-30 Tue 14:23</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
