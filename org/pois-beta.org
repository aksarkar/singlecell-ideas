#+TITLE: Poisson-Beta model
#+SETUPFILE: setup.org

* Introduction

  One idealized model for transcriptional regulation is the /telegraph model/
  (Peccoud and Ycart 1995, Raj et al. 2006, Kim and Marioni 2013, Munsky et
  al. 2013), whose steady state is described by \(
  \newcommand\Bin{\operatorname{Binomial}}
  \newcommand\B{\operatorname{Beta}}
  \newcommand\E[1]{\left\langle #1 \right\rangle}
  \newcommand\Gam{\operatorname{Gamma}}
  \newcommand\Pois{\operatorname{Poisson}}
  \newcommand\aoff{a_{\text{off}}}
  \newcommand\aon{a_{\text{on}}}
  \newcommand\ar{a_r}
  \newcommand\betafun{\operatorname{B}}
  \newcommand\boff{b_{\text{off}}}
  \newcommand\bon{b_{\text{on}}}
  \newcommand\br{b_r}
  \newcommand\const{\mathrm{const}}
  \newcommand\koff{k_{\text{off}}}
  \newcommand\kon{k_{\text{on}}}
  \newcommand\kr{k_r}
  \)

  \begin{align*}
    x_i \mid p_i, \kr &\sim \Pois(p_i \kr)\\
    p_i \mid \kon, \koff &\sim \B(\kon, \koff)
  \end{align*}

  where \(x_i\) is the number of mRNA molecules in cell \(i=1, \ldots, n\)
  (considering only one gene), \(\kon\) is the rate of off\(\rightarrow\)on
  promoter switching, \(\koff\) is the rate of on\(\rightarrow\)off promoter
  switching, and \(\kr\) is the rate of mRNA synthesis. (All rates are scaled
  relative to the mRNA decay rate.)

  The inference goal is to estimate \(\kr, \kon, \koff\) given data \(x\)
  assumed to be at steady state. The marginal likelihood \(p(x_i \mid \kr,
  \kon, \koff)\) does have a closed form; however, it involves the
  [[http://mathworld.wolfram.com/HypergeometricFunction.html][confluent
  hypergeometric function of the first kind]] (Raj et al 2006), which is
  difficult to evaluate. To avoid this challenge, Kim and Marioni 2013 develop
  an MCMC scheme to sample from the posterior \(p(\kr, \kon, \koff \mid x_i)\),
  and Larsson et al. 2019 use numerical integration to evaluate the marginal
  likelihood. Here, we investigate how the running time of the MLE procedure
  (using numerical integration) scales, and whether variational inference can
  improve the running time.

* Setup
  :PROPERTIES:
  :CUSTOM_ID: setup
  :END:

  #+BEGIN_SRC emacs-lisp :exports none
    (add-to-list 'python-shell-completion-native-disabled-interpreters "jupyter")
    (org-babel-lob-ingest "/home/aksarkar/.emacs.d/org-templates/library.org")
  #+END_SRC

  #+RESULTS:
  : 1

  #+CALL: ipython3(venv="singlecell",partition="mstephens") :exports none :dir /scratch/midway2/aksarkar

  #+RESULTS:
  : Submitted batch job 64094866

  #+BEGIN_SRC ipython
    import itertools as it
    import numpy as np
    import pandas as pd
    import scipy.optimize as so
    import scipy.special as sp
    import scipy.stats as st
    import sys
    import timeit
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[120]:
  :END:

  #+BEGIN_SRC ipython
    %matplotlib inline
    %config InlineBackend.figure_formats = set(['retina'])
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[2]:
  :END:

  #+BEGIN_SRC ipython
    import matplotlib.pyplot as plt
    plt.rcParams['figure.facecolor'] = 'w'
    plt.rcParams['font.family'] = 'Nimbus Sans'
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[3]:
  :END:

* Methods
  :PROPERTIES:
  :CUSTOM_ID: methods
  :END:
** Moment estimation
   :PROPERTIES:
   :CUSTOM_ID: moment
   :END:

   Peccoud and Ycart 1995 derived moment estimators for \(\kr, \kon, \koff\)

   #+BEGIN_SRC ipython
     def fit_poisson_beta_moment(x, **kwargs):
       """Return ln kr, ln kon, ln koff

       Estimate kr, kon, koff using the first three exponential moments (Peccoud &
       Ycart 1995).

       x - array-like [n,]

       """
       moments = np.array([1, x.mean(), (x * (x - 1)).mean(), (x * (x - 1) * (x - 2)).mean()])
       ratios = moments[1:] / moments[:-1]
       kr = (2 * ratios[0] * ratios[2] - ratios[0] * ratios[1] - ratios[1] * ratios[2]) / (ratios[0] - 2 * ratios[1] + ratios[2])
       kon = (2 * ratios[0] * (ratios[2] - ratios[1])) / (ratios[0] * ratios[1] - 2 * ratios[0] * ratios[2] + ratios[1] * ratios[2])
       koff = (2 * (ratios[2] - ratios[1]) * (ratios[0] - ratios[2]) * (ratios[1] - ratios[0])) / ((ratios[0] * ratios[1] - 2 * ratios[0] * ratios[2] + ratios[1] * ratios[2]) * (ratios[0] - 2 * ratios[1] + ratios[2]))
       result = np.array([kr, kon, koff])
       if not (np.isfinite(result).all() and (result > 0).all()):
         raise RuntimeError('moment estimation failed')
       return np.log(result)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[74]:
   :END:

** Maximum likelihood estimation
   :PROPERTIES:
   :CUSTOM_ID: mle
   :END:

   Larsson et al. 2019 use
   [[https://en.wikipedia.org/wiki/Gauss%25E2%2580%2593Jacobi_quadrature][Gauss-Jacobi
   quadrature]] to evaluate the marginal likelihood

   \begin{align*}
     \ell &= \frac{1}{\betafun(\kon, \koff)} \int_0^1 \Pois(x_i; \kr p_i)\, p_i^{\kon - 1} (1 - p_i)^{\koff - 1}\; dp_i\\
     &= \frac{1}{2^{\kon + \koff - 1} \betafun(\kon, \koff)} \int_{-1}^1 \Pois(x_i; \kr \frac{1 + t_i}{2})\, (1 + t_i)^{\kon - 1} (1 - t_i)^{\koff - 1}\; dt_i\\
     &\approx \frac{1}{2^{\kon + \koff - 1} \betafun(\kon, \koff)} \sum_{k=1}^K w_k \Pois(x_i; \kr y_k)
   \end{align*}

   where \(\betafun(\cdot)\) denotes the Beta function, \(y_k\) is the root of
   the Jacobi polynomial of degree \(k\), and \(w_k\) is the associated
   weight. This procedure can be used as a subroutine to numerically optimize
   the marginal likelihood.

   #+BEGIN_SRC ipython
     def poisson_beta_neg_llik(theta, x, n_points=50):
       """Return the negative log likelihood of the data

       theta - [ln k_r, ln k_on, ln k_off]
       n_points - number of points used in numerical integration

       """
       kr, kon, koff = np.exp(theta) + 1e-8
       # Important: Gauss-Jacobi quadrature computes the integral over t ∈ [-1, 1],
       # but we want the integral over p ∈ [0, 1]
       t, w = sp.roots_jacobi(n=n_points, alpha=koff - 1, beta=kon - 1)
       # (n_points, 1)
       p = ((1 + t) / 2).reshape(-1, 1)
       # (1, n_points) @ (n_points, n)
       px = w.reshape(1, -1) @ st.poisson(mu=kr * p).pmf(x.reshape(1, -1))
       # Important: extra 1/2 comes from u-substitution
       return -(np.log(px) - sp.betaln(kon, koff) - (kon + koff - 1) * np.log(2)).sum()

     def fit_poisson_beta_mle(x, init=None, max_iters=1000, n_points=50):
       """Return ln k_r, ln k_on, ln k_off

       x - array-like [n,]
       init - [ln k_r, ln k_on, ln k_off]
       n_points - number of points used in numerical integration

       """
       if init is None:
         try:
           init = fit_poisson_beta_moment(x)
         except RuntimeError:
           init = np.zeros(3)
       opt = so.minimize(poisson_beta_neg_llik, x0=init, args=(x, n_points),
                         method='Nelder-Mead', options={'maxiter': max_iters})
       if not opt.success:
         raise RuntimeError(f'failed to converge: {opt.message}')
       return opt.x
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[111]:
   :END:

** Slice sampling

   Kim & Marioni 2013 add priors

   \begin{align*}
     x_i \mid p_i, \kr &\sim \Pois(p_i \kr)\\
     p_i \mid \kon, \koff &\sim \B(\kon, \koff)\\
     \kr \mid \ar, \br &\sim \Gam(\ar, \br)\\
     \kon \mid \aon, \bon &\sim \Gam(\aon, \bon)\\
     \koff \mid \aoff, \boff &\sim \Gam(\aoff, \boff)
   \end{align*}

   The conditional (log) densities are available, up to constants

   \begin{align*}
     \ln p(p_i \mid \cdot) &= (x_i + \kon - 1) \ln p_i + (\koff - 1)\ln(1 - p_i) - \kr p_i + \const\\
     \ln p(\kr \mid \cdot) &= \sum_i [(x_i + \ar - 1) \ln \kr - (p_i + \br) \kr] + \const\\
     \ln p(\kon \mid \cdot) &= \sum_i [\kon \ln p_i + (\aon - 1)\ln \kon - \bon\kon - \ln\Gamma(\kon) + \ln\Gamma(\kon + \koff)] + \const\\
     \ln p(\koff \mid \cdot) &= \sum_i [\koff \ln (1 - p_i) + (\aoff - 1)\ln \koff - \boff\koff - \ln\Gamma(\koff) + \ln\Gamma(\koff + \koff)] + \const
   \end{align*}

   Although the conditional distributions are non-standard, slice sampling
   (Neal 2003) only requires the target density up to a constant.

** Variational inference
   :PROPERTIES:
   :CUSTOM_ID: vi
   :END:

   To make the model amenable for VI, introduce latent variables \(z_i\)

   \begin{align*}
     x_i \mid z_i, p_i &\sim \Bin(z_i, p_i)\\
     z_i \mid \kr &\sim \Pois(\kr)\\
     p_i \mid \kon, \koff &\sim \B(\kon, \koff)
   \end{align*}

   Then, we have

   \begin{multline*}
     \ln p(x_i, z_i, p_i \mid \kr, \kon, \koff) = (x + \kon - 1) \ln p_i + (z_i - x_i + \koff - 1) \ln(1 - p_i)\\
     + (z_i - x_i) \ln \kr - \kr + x_i \ln \kr - \ln\Gamma(x_i + 1) - \ln\Gamma(z_i - x_i + 1) - \ln\betafun(\kon, \koff)
   \end{multline*}

   where we have added and subtracted \(x_i \ln \kr\) to more easily derive
   coordinate updates

   \begin{align*}
     q^*(z_i - x_i) &= \Pois(\exp(\E{\ln (1 - p_i)} + \ln k_r)) \triangleq \Pois(\mu_i) \\
     q^*(p_i) &= \B(x_i + \kon, \E{z_i - x_i} + \koff) \triangleq \B(\alpha_i, \beta_i)
   \end{align*}

   where expectations are taken with respect to the variational approximation
   \(q\). [[https://en.wikipedia.org/wiki/Beta_distribution#Moments_of_logarithmically_transformed_random_variables][Using
   properties of the Beta distribution]]

   \begin{align*}
     \E{\ln p_i} &= \psi(\alpha_i) - \psi(\alpha_i + \beta_i)\\
     \E{\ln (1 - p_i)} &= \psi(\beta_i) - \psi(\alpha_i + \beta_i)
   \end{align*}   

   where \(\psi\) is the digamma function. The evidence lower bound is

   \begin{multline*}
     \ell = \sum_i (x_i + \kon - \alpha_i) \E{\ln p_i} + (\mu_i + \koff - \beta_i) \E{\ln(1 - p_i)}\\
     + (x_i + \mu_i) \ln \kr - k_r - \mu_i \ln \mu_i + \mu_i - \ln\Gamma(x_i + 1) - \ln\betafun(\kon, \koff) + \ln\betafun(\alpha_i, \beta_i)
   \end{multline*}

   We can derive the Jacobian with respect to \(\kr, \kon, \koff\), leading to
   an analytic update for \(\kr\). We can use Brent's method to numerically
   update \(\kon, \koff\).

   \begin{align*}
     \frac{\partial \ell}{\partial \kr} &= \sum_i \frac{x_i + \mu_i}{\kr} - 1\\
     \kr &:= \frac{1}{n} \sum_i x_i + \mu_i\\
     \frac{\partial \ell}{\partial \kon} &= \sum_i \psi(\alpha_i) - \psi(\alpha_i - \beta_i) - \psi(\kon) + \psi(\kon + \koff)\\
     \frac{\partial \ell}{\partial \koff} &= \sum_i \psi(\beta_i) - \psi(\alpha_i - \beta_i) - \psi(\koff) + \psi(\kon + \koff)
   \end{align*}

   #+BEGIN_SRC ipython
     def poisson_beta_elbo(theta, x, mu, alpha, beta):
       """Return the evidence lower bound

       theta - [ln k_r, ln k_on, ln k_off]
       x - array-like [n,]
       mu - array-like [n,]
       alpha - array-like [n,]
       beta - array-like [n,]

       """
       kr, kon, koff = np.exp(theta)
       return ((x + kon - alpha) * (sp.digamma(alpha) - sp.digamma(alpha + beta))
               + (mu + koff - beta) * (sp.digamma(beta) - sp.digamma(alpha + beta))
               + (x + mu) * np.log(kr) - kr - mu * np.log(mu) + mu - sp.gammaln(x + 1)
               - sp.betaln(kon, koff) + sp.betaln(alpha, beta)).sum()

     def poisson_beta_delbo_dkon(ln_kon, ln_koff, alpha, beta):
       """Return the partial derivative of ELBO wrt kon"""
       return (sp.digamma(alpha)
               - sp.digamma(alpha - beta)
               - sp.digamma(np.exp(ln_kon))
               + sp.digamma(np.exp(ln_kon) + np.exp(ln_koff))).sum()

     def poisson_beta_delbo_dkoff(ln_koff, ln_kon, alpha, beta):
       """Return the partial derivative of ELBO wrt koff"""
       return (sp.digamma(beta)
               - sp.digamma(alpha - beta)
               - sp.digamma(np.exp(ln_koff))
               + sp.digamma(np.exp(ln_kon) + np.exp(ln_koff))).sum()

     def fit_poisson_beta_vi(x, init=None, atol=1e-8, max_iters=1000, verbose=False):
       """Return kr, kon, koff

       init - [ln k_r, ln k_on, ln k_off]

       """
       if init is None:
         init = np.log(fit_poisson_beta_moment(x))
       theta = init
       mu = np.zeros(x.shape)
       alpha = np.ones(x.shape)
       beta = np.ones(x.shape)

       obj = -np.inf
       for t in range(max_iters):
         alpha = x + np.exp(theta[1])
         beta = mu + np.exp(theta[2])
         mu = np.exp(sp.digamma(beta) - sp.digamma(alpha + beta) + theta[0])
         theta[0] = np.log((x + mu).mean())
         opt_kon = so.newton(poisson_beta_delbo_dkon, x0=theta[1], args=(theta[2], alpha, beta))
         if not opt_kon.success:
           raise RuntimeError(f'k_on update failed: {opt_kon.message}')
         theta[1] = opt_kon.x
         opt_koff = so.root_scalar(poisson_beta_delbo_dkoff, x0=theta[2], args=(theta[1], alpha, beta))
         if not opt_koff.success:
           raise RuntimeError(f'k_on update failed: {opt_koff.message}')
         theta[1] = opt_koff.x
         update = poisson_beta_elbo(theta, x, mu, alpha, beta)
         if verbose:
           print(f'Epoch {t}: {update}')
         if abs(obj - update) < atol:
           return theta
         else:
           obj = update
       raise RuntimeError('failed to converge')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[23]:
   :END:

* Results
  :PROPERTIES:
  :CUSTOM_ID: results
  :END:
** Simulation
   :PROPERTIES:
   :CUSTOM_ID: simulation
   :END:

   Make sure the implementations work on one example.

   #+BEGIN_SRC ipython
     def simulate_pois_beta(n, kr=None, kon=None, koff=None, seed=None):
       if seed is not None:
         np.random.seed(seed)
       if kr is None:
         kr = np.random.lognormal(mean=3)
       if kon is None:
         kon = np.random.lognormal()
       if koff is None:
         koff = np.random.lognormal()
       p = np.random.beta(a=kon, b=koff, size=n)
       x = np.random.poisson(lam=kr * p)
       return x, kr, kon, koff
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[5]:
   :END:

   #+BEGIN_SRC ipython
     x, *theta = simulate_pois_beta(n=1000, seed=0)
     theta
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[6]:
   : [117.2199806492514, 1.4920592434019648, 2.661095776728801]
   :END:

   Fit the moment estimator.

   #+BEGIN_SRC ipython
     thetahat_moment = fit_poisson_beta_moment(x)
     np.exp(thetahat_moment)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[8]:
   : array([120.04495743,   1.52142758,   2.72068022])
   :END:

   Fit the MLE.

   #+BEGIN_SRC ipython
     thetahat_mle = fit_poisson_beta_mle(x)
     np.exp(thetahat_mle)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[11]:
   : array([118.92485772,   1.52953485,   2.68994176])
   :END:

   Evaluate more systematically.

   #+BEGIN_SRC ipython
     def evaluate_estimator(n, n_trials):
       sim_result = []
       grid = np.linspace(np.log(.1), np.log(100), 5)
       for (ln_kr, ln_kon, ln_koff) in itertools.product(grid, repeat=3):
         for trial in range(n_trials):
           x, *_ = simulate_pois_beta(n=n, kr=np.exp(ln_kr), kon=np.exp(ln_kon),
                                      koff=np.exp(ln_koff), seed=trial)
           for method in ['moment', 'mle']:
             try:
               thetahat = getattr(sys.modules['__main__'], f'fit_poisson_beta_{method}')(x)
             except RuntimeError:
               thetahat = [np.nan, np.nan, np.nan]
             sim_result.append([ln_kr, ln_kon, ln_koff, method, trial] + list(thetahat))
       return pd.DataFrame(sim_result, columns=['ln_kr', 'ln_kon', 'ln_koff', 'method', 'trial', 'ln_kr_hat', 'ln_kon_hat', 'ln_koff_hat'])
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[67]:
   :END:

   #+BEGIN_SRC ipython :async t
     res = evaluate_estimator(n=100, n_trials=1)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[117]:
   :END:

   Report the fraction of data sets for which estimation failed.

   #+BEGIN_SRC ipython
     res.groupby(['method']).apply(lambda x: x.dropna().shape[0] / x.shape[0])
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[118]:
   #+BEGIN_EXAMPLE
     method
     mle       0.584
     moment    0.296
     dtype: float64
   #+END_EXAMPLE
   :END:

** Running time benchmark

   #+BEGIN_SRC ipython
     def evaluate_running_time(n_trials):
       timing_result = []
       for n in (100, 500, 1000, 5000, 10000):
         # Fix the parameters to something easy
         x, *_ = simulate_pois_beta(n=n, kr=50, kon=1, koff=1, seed=0)
         for m in ('moment', 'mle'):
           res = timeit.repeat(stmt=lambda: getattr(sys.modules['__main__'], f'fit_poisson_beta_{m}')(x),
                               number=1, repeat=n_trials, globals=locals())
           for i, t in enumerate(res):
             timing_result.append([n, m, i, t])
       return pd.DataFrame(timing_result, columns=['n', 'method', 'trial', 'time'])
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[156]:
   :END:

   #+BEGIN_SRC ipython :async t
     timing_res = evaluate_running_time(n_trials=10)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[157]:
   :END:

   #+BEGIN_SRC ipython :ipyfile figure/pois-beta.org/timing.png
     cm = plt.get_cmap('Dark2')
     plt.clf()
     plt.gcf().set_size_inches(3, 3)
     plt.xscale('log')
     plt.yscale('log')
     for i, (m, g) in enumerate(timing_res.groupby('method')):
       t = g.groupby('n')['time'].agg([np.mean, np.std]).reset_index()
       plt.plot(t['n'], t['mean'], lw=1, c=cm(i), label=m)
       plt.scatter(g['n'] + np.random.normal(scale=np.log10(g['n']), size=g.shape[0]), g['time'], c=cm(i), s=2, label=None)
     plt.legend(frameon=False, title='Method')
     plt.xlabel('Sample size')
     plt.ylabel('Running time (s)')
     plt.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[159]:
   [[file:figure/pois-beta.org/timing.png]]
   :END:
