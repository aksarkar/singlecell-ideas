#+TITLE: Massively Parallel Empirical Bayes Poisson Means
#+SETUPFILE: setup.org

* Introduction

  In our prior work ([[https://dx.doi.org/10.1371/journal.pgen.1008045][Sarkar
  et al. 2019]]), we introduced a factor model to capture between donor
  individual variation in the mean and variance of gene expression in a single
  cell type \( 
  \DeclareMathOperator\Gam{Gamma}
  \DeclareMathOperator\Poi{Poisson}
  \DeclareMathOperator\argmin{arg min}
  \newcommand\mf{\mathbf{F}} 
  \newcommand\ml{\mathbf{L}}
  \newcommand\mx{\mathbf{X}}
  \newcommand\vl{\mathbf{l}}
  \newcommand\vx{\mathbf{x}}
  \)

  \begin{align*}
    x_{ij} &\sim \Poi(x_{i+} \lambda_{ij})\\
    \lambda_{ij} &\sim \pi_{ij} \delta_0(\cdot) + (1 - \pi_{ij}) \Gam(\phi_{ij}^{-1}, \mu_{ij}^{-1} \phi_{ij}^{-1})\\
    \ln \mu_{ij} &= (\ml \mf_\mu')_{ij}\\
    \ln \phi_{ij} &= (\ml \mf_\phi')_{ij}\\
    \operatorname{logit} \pi_{ij} &= (\ml \mf_\pi')_{ij}\\
  \end{align*}

  where 

  - \(x_{ij}\) is the number of molecules of gene \(j = 1, \ldots, p\) observed
    in cell \(i = 1, \ldots, n\)
  - \(x_{i+} \triangleq \sum_j x_{ij}\) is the total number of molecules
    observed in sample \(i\)
  - cells are taken from \(m\) donor individuals, \(\ml\) is \(n \times m\),
    each \(\mf_{\cdot}\) is \(p \times m\)
  - assignments of cells to donors (loadings) \(l_{ik} \in \{0, 1\}; k = 1,
    \ldots, m\) are known and fixed.

  We previously implemented maximum likelihood inference of this model via
  batch gradient descent in the Python package
  [[https://www.github.com/aksarkar/scqtl.git][scqtl]]. We have now developed a
  new Python package [[https://www.github.com/aksarkar/mpebpm.git][mpebpm]],
  which should scale inference to much larger data sets. The key improvements
  are optimization using minibatch gradient descent and support for sparse
  matrices. Here, we evaluate the method on simulations.

* Setup
  :PROPERTIES:
  :CUSTOM_ID: setup
  :END:

  #+BEGIN_SRC emacs-lisp :exports none
    (add-to-list 'python-shell-completion-native-disabled-interpreters "jupyter")
    (org-babel-lob-ingest "/home/aksarkar/.emacs.d/org-templates/library.org")
  #+END_SRC

  #+RESULTS:
  : 1

  #+CALL: ipython3(venv="singlecell",partition="gpu2",opts="--gres=gpu:1",memory="8G") :exports none :dir /scratch/midway2/aksarkar/singlecell/

  #+RESULTS:
  : Submitted batch job 66454030

  #+BEGIN_SRC ipython
    import mpebpm
    import numpy as np
    import pandas as pd
    import scipy.sparse as ss
    import scipy.special as sp
    import scqtl
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[51]:
  :END:

  #+BEGIN_SRC ipython
    %matplotlib inline
    %config InlineBackend.figure_formats = set(['retina'])
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[2]:
  :END:

  #+BEGIN_SRC ipython
    import matplotlib.pyplot as plt
    plt.rcParams['figure.facecolor'] = 'w'
    plt.rcParams['font.family'] = 'Nimbus Sans'
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[3]:
  :END:

* Results
** Accuracy of parameter estimation

   We previously evaluated ~scqtl~
   [[https://jdblischak.github.io/singlecell-qtl/zinb.html#orgca44e7f][by
   simulating data from the model]].

   #+BEGIN_SRC ipython
     def evaluate(num_samples, num_mols, num_trials=10, **kwargs):
       # Important: generate all of the samples for each trial in one shot, and use
       # one-hot encoding to get separate estimates
       args = [(num_samples * num_trials, num_mols, log_mu, log_phi, logodds, None, None, None)
               for log_mu in np.linspace(-12, -6, 7)
               for log_phi in np.linspace(-4, 0, 5)
               for logodds in np.linspace(-3, 3, 7)]
       x = np.concatenate([scqtl.simulation.simulate(*a)[0][:,:1] for a in args], axis=1)
       x = ss.csr_matrix(x)
       s = num_mols * np.ones((x.shape[0], 1))
       onehot = np.zeros((num_samples * num_trials, num_trials))
       onehot[np.arange(onehot.shape[0]), np.arange(onehot.shape[0]) // num_samples] = 1
       onehot = ss.csr_matrix(onehot)

       log_mu, neg_log_phi, logodds, _ = mpebpm.ebpm_point_gamma(x, s=s, onehot=onehot, **kwargs)
       result = pd.DataFrame(
         [(a[0] // num_trials, int(a[1]), int(a[2]), int(a[3]), int(a[4]), a[-1], trial)
          for a in args
          for trial in range(num_trials)],
         columns=['num_samples', 'num_mols', 'log_mu', 'log_phi', 'logodds', 'fold', 'trial'])
       result['mean'] = np.exp(result['log_mu'])
       result['var'] = (1 - sp.expit(result['logodds'])) * np.exp(2 * result['log_mu'] + result['log_phi']) + sp.expit(-result['logodds']) * (1 - sp.expit(result['logodds'])) * np.exp(2 * result['log_mu'])

       result['log_mu_hat'] = log_mu.ravel()
       result['log_phi_hat'] = -neg_log_phi.ravel()
       result['logodds_hat'] = logodds.ravel()
       result['mean_hat'] = np.exp(result['log_mu_hat'])
       result['var_hat'] = (1 - sp.expit(result['logodds_hat'])) * np.exp(2 * result['log_mu_hat'] + result['log_phi_hat']) + sp.expit(-result['logodds_hat']) * (1 - sp.expit(result['logodds_hat'])) * np.exp(2 * result['log_mu_hat'])

       diagnostic = []
       for i in range(x.shape[1]):
         for j in range(onehot.shape[1]):
           idx = onehot.A[:,j].astype(bool)
           diagnostic.append(scqtl.diagnostic.diagnostic_test(
             x.A[idx,i].reshape(-1, 1),
             log_mu[j,i],
             -neg_log_phi[j,i],
             -logodds[j,i],
             num_mols,
             np.ones((num_samples, 1))))
       diagnostic = np.array(diagnostic)
       result['ks_d'] = diagnostic[:,0]
       result['ks_p'] = diagnostic[:,1]
       return result
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[53]:
   :END:

   Run the simulation.

   #+BEGIN_SRC ipython :async t
     result = evaluate(num_samples=100, num_mols=int(1e5), batch_size=32, max_epochs=100, verbose=True)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[54]:
   :END:

   Write out the results.

   #+BEGIN_SRC ipython
     result.to_csv('/scratch/midway2/aksarkar/ideas/mpebpm-sim.txt.gz', sep='\t')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[58]:
   :END:

** Goodness of fit

   We previously developed a test for goodness of fit, based on the fact that
   if \(x_{ij} \sim F_{ij}\), then \(F_{ij}(x_{ij}) \sim
   \operatorname{Uniform}(0, 1)\). We applied this test to the distributions
   estimated from the simulated data sets.

** Running time

   Compare the wallclock time of ~scqtl~ against ~mpebpm~ fixing \(p\) and
   increasing \(n\).

** Application to Census of Immune Cells

   The
   [[https://data.humancellatlas.org/explore/projects/cc95ff89-2e68-4a08-a234-480eca21ce79][Census
   of Immune Cells]] is part of the Human Cell Atlas. Currently, it comprises
   scRNA-seq data of 593,844 cells from 16 donors. To demonstrate the scalability of
   ~mpebpm~, fit a Gamma distribution to each gene in each donor.

   Read the sparse data and report its dimensions. (20 seconds)

   #+BEGIN_SRC ipython :async t
     y_csr = ss.load_npz('/scratch/midway2/aksarkar/modes/immune-cell-census.npz')
     y_csr.shape
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[46]:
   :END:

   Read the metadata.

   #+BEGIN_SRC ipython
     genes = pd.read_csv('/scratch/midway2/aksarkar/modes/immune-cell-census-genes.txt.gz', sep='\t', index_col=0)
     donor = pd.Categorical(pd.read_csv('/scratch/midway2/aksarkar/modes/immune-cell-census-samples.txt.gz', sep='\t', index_col=0)['0'])
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[47]:
   :END:
