<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2020-07-03 Fri 23:46 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>ZINB-VAE-GMM</title>
<meta name="generator" content="Org mode">
<meta name="author" content="Abhishek Sarkar">
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link href="bootstrap.min.css" rel="stylesheet" />
<link rel="stylesheet" type="text/css" href="htmlize.css"/>
<link rel="stylesheet" type="text/css" href="main.css"/>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2017 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">ZINB-VAE-GMM</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgf40b2f7">Introduction</a></li>
<li><a href="#setup">Setup</a></li>
<li><a href="#org1b5c4f4">Methods</a></li>
<li><a href="#org6e57704">Results</a></li>
</ul>
</div>
</div>

<div id="outline-container-orgf40b2f7" class="outline-2">
<h2 id="orgf40b2f7">Introduction</h2>
<div class="outline-text-2" id="text-orgf40b2f7">
<p>
Two major strategies for clustering scRNA-seq data are:
</p>

<ol class="org-ol">
<li>Building a \(k\)-nearest neighbor graph on the data, and applying a
community detection algorithm (e.g.,
<a href="https://doi.org/10.1088/1742-5468/2008/10/P10008">Blondel et al. 2008</a>,
<a href="https://arxiv.org/abs/1810.08473">Traag et al. 2018</a>)</li>
<li>Fitting a topic model to the data
(e.g., <a href="https://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1006599">Dey
et al. 2017</a>,
<a href="https://www.nature.com/articles/s41592-019-0367-1">Gonz√°les-Blas et
al. 2019</a>)</li>
</ol>

<p>
The main disadvantage of strategy (1) is that, as commonly applied to
transformed counts, it does not separate measurement error and biological
variation of interest. The main disadvantage of strategy (2) is that it does
not account for transcriptional noise
(<a href="https://doi.org/10.1016/j.cell.2008.09.050">Raj 2008</a>). We
<a href="nbmix.html">previously developed a simple mixture model</a> which could
address this issue. Here, we develop an alternative method based on
<a href="http://ruishu.io/2016/12/25/gmvae/">GMVAE</a>, which has some practical
benefits and can be adapted to this problem.
</p>
</div>
</div>

<div id="outline-container-orgbbadc60" class="outline-2">
<h2 id="setup"><a id="orgbbadc60"></a>Setup</h2>
<div class="outline-text-2" id="text-setup">
<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> numpy <span class="org-keyword">as</span> np
<span class="org-keyword">import</span> scmodes
<span class="org-keyword">import</span> torch
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">%matplotlib inline
%config <span class="org-variable-name">InlineBackend.figure_formats</span> = <span class="org-builtin">set</span>([<span class="org-string">'retina'</span>])
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> matplotlib.pyplot <span class="org-keyword">as</span> plt
<span class="org-variable-name">plt.rcParams</span>[<span class="org-string">'figure.facecolor'</span>] = <span class="org-string">'w'</span>
<span class="org-variable-name">plt.rcParams</span>[<span class="org-string">'font.family'</span>] = <span class="org-string">'Nimbus Sans'</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-org1b5c4f4" class="outline-2">
<h2 id="org1b5c4f4">Methods</h2>
<div class="outline-text-2" id="text-org1b5c4f4">
<p>
We assume \(
  \DeclareMathOperator\E{E}
  \DeclareMathOperator\Gam{Gamma}
  \DeclareMathOperator\Mult{Multinomial}
  \DeclareMathOperator\N{\mathcal{N}}
  \DeclareMathOperator\Pois{Poisson}
  \DeclareMathOperator\diag{diag}
  \DeclareMathOperator\KL{\mathcal{KL}}
  \newcommand\kl[2]{\KL(#1\;\Vert\;#2)}
  \newcommand\xiplus{x_{i+}}
  \newcommand\mi{\mathbf{I}}
  \newcommand\vb{\mathbf{b}}
  \newcommand\vu{\mathbf{u}}
  \newcommand\vx{\mathbf{x}}
  \newcommand\vz{\mathbf{z}}
  \newcommand\vlambda{\boldsymbol{\lambda}}
  \newcommand\vmu{\boldsymbol{\mu}}
  \newcommand\vphi{\boldsymbol{\phi}}
  \newcommand\vpi{\boldsymbol{\pi}}
  \)
</p>

\begin{align}
  x_{ij} \mid \xiplus, \vz_i &\sim \Pois(\xiplus (\lambda(\vz_i))_j)\\
  \vz_i \mid y_i &\sim \N(\mu(y_i), \sigma^2(y_i))\\
  y_i \mid \vpi &\sim \Mult(1, \vpi),
\end{align}

<p>
where
</p>

<ul class="org-ul">
<li>\(x_{ij}\) denotes the number of molecules of gene \(j = 1, \ldots, p\)
observed in cell \(i = 1, \ldots, n\)</li>
<li>\(\xiplus \triangleq \sum_j x_{ij}\) denotes the total number of molecules
observed in cell \(i\)</li>
<li>\(\lambda(\cdot)\) is a fully-connected neural network mapping latent
variables to true gene expression</li>
<li>\(\mu(\cdot), \sigma^2(\cdot)\) are fully-connected neural networks
mapping observations and latent cluster assignments to latent variables</li>
</ul>

<p>
In this model, true gene expression can be represented in an
\(m\)-dimensional space, and within that space belongs to one of \(k\)
clusters, indexed by \(y_i\). In other words, the (marginal) prior on latent
variables \(\vz_i\) is a mixture of Gaussians. To perform inference, introduce a
variational approximation
</p>

\begin{equation}
  q(y_i, \vz_i \mid \vx_i) = q(y_i \mid \vx_i)\, q(\vz_i \mid y_i, \vx_i),
\end{equation}

<p>
which which are parameterized by neural networks. The ELBO
</p>

\begin{equation}
  \ell = \sum_{i, j} \E_q\left[\ln p(x_{ij} \mid \xiplus, \vz_i) + \frac{p(y_i)}{q(y_i \mid \vx_i)} + \frac{p(\vz_i \mid y_i)}{q(\vz_i \mid y_i, \vx_i)}\right],
\end{equation}

<p>
in which the first term must be approximated as a Monte Carlo integral
(Kingma and Welling 2014), the second term is the KL divergence between two
Multinomials (which is analytic), and the final term term is the KL
divergence between a Gaussian and a mixture of Gaussians (which is analytic).
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">class</span> <span class="org-type">FC</span>(torch.nn.Module):
  <span class="org-doc">"""Fully connected layers"""</span>
  <span class="org-keyword">def</span> <span class="org-function-name">__init__</span>(<span class="org-keyword">self</span>, input_dim, hidden_dim=128):
    <span class="org-builtin">super</span>().__init__()
    <span class="org-keyword">self</span>.net = torch.nn.Sequential(
      torch.nn.Linear(input_dim, hidden_dim),
      torch.nn.ReLU(),
      torch.nn.BatchNorm1d(hidden_dim),
      torch.nn.Linear(hidden_dim, hidden_dim),
      torch.nn.ReLU(),
      torch.nn.BatchNorm1d(hidden_dim),
    )

  <span class="org-keyword">def</span> <span class="org-function-name">forward</span>(<span class="org-keyword">self</span>, x):
    <span class="org-keyword">return</span> <span class="org-keyword">self</span>.net(x)

<span class="org-keyword">class</span> <span class="org-type">DeepGaussian</span>(torch.nn.Module):
  <span class="org-doc">"""Gaussian distribution parameterized by FC networks for mean and scale"""</span>
  <span class="org-keyword">def</span> <span class="org-function-name">__init__</span>(<span class="org-keyword">self</span>, input_dim, output_dim, hidden_dim=128):
    <span class="org-builtin">super</span>().__init__()
    <span class="org-keyword">self</span>.net = FC(input_dim, hidden_dim)
    <span class="org-keyword">self</span>.mean = torch.nn.Linear(<span class="org-keyword">self</span>.hidden_dim, output_dim)
    <span class="org-keyword">self</span>.scale = torch.nn.Sequential(torch.nn.Linear(hidden_dim, output_dim), torch.nn.Softplus())

  <span class="org-keyword">def</span> <span class="org-function-name">forward</span>(<span class="org-keyword">self</span>, x):
    <span class="org-variable-name">q</span> = <span class="org-keyword">self</span>.net(x)
    <span class="org-keyword">return</span> <span class="org-keyword">self</span>.mean(q), <span class="org-keyword">self</span>.scale(q)

<span class="org-keyword">class</span> <span class="org-type">DeepCategorical</span>(torch.nn.Module):
  <span class="org-doc">"""Categorical distribution parameterized by FC network for logits"""</span>
  <span class="org-keyword">def</span> <span class="org-function-name">__init__</span>(<span class="org-keyword">self</span>, input_dim, output_dim, hidden_dim=128):
    <span class="org-builtin">super</span>().__init__()
    <span class="org-keyword">self</span>.net = FC(input_dim, hidden_dim)
    <span class="org-keyword">self</span>.logits = torch.nn.Linear(hidden_dim, output_dim)

  <span class="org-keyword">def</span> <span class="org-function-name">forward</span>(<span class="org-keyword">self</span>, x):
    <span class="org-variable-name">q</span> = <span class="org-keyword">self</span>.net(x)
    <span class="org-keyword">return</span> <span class="org-keyword">self</span>.logits(q)

<span class="org-keyword">class</span> <span class="org-type">PGMVAE</span>(torch.nn.Module):
  <span class="org-keyword">def</span> <span class="org-function-name">__init__</span>(<span class="org-keyword">self</span>, input_dim, latent_dim, n_clusters, hidden_dim=128):
    <span class="org-keyword">self</span>.n_clusters = n_clusters
    <span class="org-keyword">self</span>.encoder_y = DeepCategorical(input_dim, n_clusters, hidden_dim)
    <span class="org-keyword">self</span>.encoder_z = DeepGaussian(input_dim + n_clusters, latent_dim, hidden_dim)
    <span class="org-keyword">self</span>.decoder_z = DeepGaussian(n_clusters, latent_dim, hidden_dim)
    <span class="org-keyword">self</span>.decoder_x = torch.nn.Sequential(
      FC(latent_dim),
      torch.nn.Softplus()
    )

  <span class="org-keyword">def</span> <span class="org-function-name">forward</span>(<span class="org-keyword">self</span>, x, s, writer=<span class="org-constant">None</span>, global_step=<span class="org-constant">None</span>):
    <span class="org-comment-delimiter"># </span><span class="org-comment">[batch_size, n_clusters]</span>
    <span class="org-variable-name">logits</span> = <span class="org-keyword">self</span>.encoder_y.forward(x)
    <span class="org-variable-name">probs</span> = torch.sigmoid(logits)
    <span class="org-comment-delimiter"># </span><span class="org-comment">log(sigmoid(x)) = -softplus(-x)</span>
    <span class="org-variable-name">kl_y</span> = probs * (-torch.nn.functional.softplus(-logits) + torch.log(torch.tensor(<span class="org-keyword">self</span>.n_clusters)))

    <span class="org-comment-delimiter"># </span><span class="org-comment">Important: this isn't precisely a KL divergence</span>
    <span class="org-variable-name">kl_z</span> = 0
    <span class="org-variable-name">err</span> = 0
    <span class="org-variable-name">mean</span>, <span class="org-variable-name">scale</span> = <span class="org-keyword">self</span>.encoder_z.forward(torch.cat([x, probs]))
    <span class="org-comment-delimiter"># </span><span class="org-comment">[n_samples, batch_size, latent_dim]</span>
    <span class="org-variable-name">qz</span> = torch.distributions.Normal(mean, scale).rsample(n_samples)
    <span class="org-keyword">for</span> k <span class="org-keyword">in</span> <span class="org-builtin">range</span>(<span class="org-keyword">self</span>.n_clusters):
      <span class="org-variable-name">prior_mean</span>, <span class="org-variable-name">prior_scale</span> = <span class="org-keyword">self</span>.decoder_z.forward(torch.eye(<span class="org-keyword">self</span>.n_clusters)[k])
      <span class="org-variable-name">kl_z</span> += probs[k] * .5 * (2 * torch.log(scale) - 2 * torch.log(prior_scale) + (mean ** 2 + scale ** 2) / prior_scale ** 2)
      <span class="org-comment-delimiter"># </span><span class="org-comment">[n_samples, batch_size]</span>
      <span class="org-variable-name">lam</span> = <span class="org-keyword">self</span>.decoder_x(qz)
      <span class="org-variable-name">err</span> = x * torch.log(s * lam) - s * lam - torch.gammaln(x)
      <span class="org-variable-name">elbo</span> += 

    <span class="org-variable-name">elbo</span> = err - kl_z - kl_y
    <span class="org-keyword">return</span> -elbo

  <span class="org-keyword">def</span> <span class="org-function-name">fit</span>(<span class="org-keyword">self</span>, data, n_epochs=100, n_samples=1, log_dir=<span class="org-constant">None</span>, **kwargs):
    <span class="org-keyword">if</span> torch.cuda.is_available():
      <span class="org-keyword">self</span>.cuda()
    <span class="org-keyword">if</span> log_dir <span class="org-keyword">is</span> <span class="org-keyword">not</span> <span class="org-constant">None</span>:
      <span class="org-variable-name">writer</span> = tb.SummaryWriter(log_dir)
    <span class="org-variable-name">n_samples</span> = torch.Size([n_samples])
    <span class="org-variable-name">opt</span> = torch.optim.RMSprop(<span class="org-keyword">self</span>.parameters(), **kwargs)
    <span class="org-variable-name">global_step</span> = 0
    <span class="org-keyword">for</span> epoch <span class="org-keyword">in</span> <span class="org-builtin">range</span>(n_epochs):
      <span class="org-keyword">for</span> x, s <span class="org-keyword">in</span> data:
        opt.zero_grad()
        <span class="org-variable-name">loss</span> = <span class="org-keyword">self</span>.forward(x, s, n_samples=n_samples, writer=writer, global_step=global_step)
        <span class="org-keyword">if</span> torch.isnan(loss):
          <span class="org-keyword">raise</span> <span class="org-type">RuntimeError</span>(<span class="org-string">'nan loss'</span>)
        loss.backward()
        opt.step()
        <span class="org-variable-name">global_step</span> += 1
    <span class="org-keyword">return</span> <span class="org-keyword">self</span>

</pre>
</div>
</div>
</div>

<div id="outline-container-org6e57704" class="outline-2">
<h2 id="org6e57704">Results</h2>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Abhishek Sarkar</p>
<p class="date">Created: 2020-07-03 Fri 23:46</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
