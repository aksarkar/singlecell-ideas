<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2020-04-30 Thu 16:44 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Mutual nearest neighbors in topic model space</title>
<meta name="generator" content="Org mode">
<meta name="author" content="Abhishek Sarkar">
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link href="bootstrap.min.css" rel="stylesheet" />
<link rel="stylesheet" type="text/css" href="htmlize.css"/>
<link rel="stylesheet" type="text/css" href="main.css"/>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2017 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Mutual nearest neighbors in topic model space</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgef20524">Introduction</a></li>
<li><a href="#setup">Setup</a></li>
<li><a href="#org66b4ca5">Methods</a>
<ul>
<li><a href="#orge084aea">Fast approximate nearest neighbors</a></li>
<li><a href="#org5e90ccb">GMVAE</a></li>
</ul>
</li>
<li><a href="#org6c52ba5">Results</a>
<ul>
<li><a href="#org7bd79a3">iPSC data</a></li>
<li><a href="#org7bd058b">iPSC Drop-Seq data</a></li>
<li><a href="#orge6144f5">NMF</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-orgef20524" class="outline-2">
<h2 id="orgef20524">Introduction</h2>
<div class="outline-text-2" id="text-orgef20524">
<p>
<a href="https://www.ncbi.nlm.nih.gov/pubmed/29608177">Haghverdi et al 2018</a> introduce a method for batch correction based on
identifying <i>mutual nearest neighbors</i>. The key idea of the method is to
identify cells which should have "equal gene expression" across batches, and
compute correction factors based on the observed differences in gene
expression, where "difference" is cosine distance. The key intuition of the
method is "Proper removal of the batch effect should result in the formation
of&#x2026;clusters, one for each cell type, where each cluster contains a mixture
of cells from both batches."
</p>

<blockquote>
<p>
Our use of MNN pairs involves three assumptions: (i) there is at least one
cell population that is present in both batches, (ii) the batch effect is
almost orthogonal to the biological subspace, and (iii) batch effect
variation is much smaller than the biological effect variation between
different cell types
</p>
</blockquote>

<p>
Here, we first investigate the intuition and assumptions underlying MNN
correction using real data with known, estimable batch effects (Sarkar et
al. 2019). Then, we investigate using NMF/LDA to estimate true gene
expression under the generative model
</p>

\begin{align*}
  x_{ij} \mid s_i, \lambda_{ij} &\sim \operatorname{Poisson}(s_i \lambda_{ij})\\
  \lambda_{ij} &= \sum_{k=1}^K l_{ik} f_{jk}
\end{align*}

<p>
and compare MNN applied to this latent space against previous approaches
based on cosine distance on normalized counts, or Euclidean distance in a
principal component subspace.
</p>
</div>
</div>

<div id="outline-container-org2850512" class="outline-2">
<h2 id="setup"><a id="org2850512"></a>Setup</h2>
<div class="outline-text-2" id="text-setup">
<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> anndata
<span class="org-keyword">import</span> numpy <span class="org-keyword">as</span> np
<span class="org-keyword">import</span> pandas <span class="org-keyword">as</span> pd
<span class="org-keyword">import</span> scanpy <span class="org-keyword">as</span> sc
<span class="org-keyword">import</span> sklearn.decomposition <span class="org-keyword">as</span> skd
<span class="org-keyword">import</span> torch
<span class="org-keyword">import</span> torch.utils.data <span class="org-keyword">as</span> td
<span class="org-keyword">import</span> torch.utils.tensorboard <span class="org-keyword">as</span> tb
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">%matplotlib inline
%config <span class="org-variable-name">InlineBackend.figure_formats</span> = <span class="org-builtin">set</span>([<span class="org-string">'retina'</span>])
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> matplotlib.pyplot <span class="org-keyword">as</span> plt
<span class="org-variable-name">plt.rcParams</span>[<span class="org-string">'figure.facecolor'</span>] = <span class="org-string">'w'</span>
<span class="org-variable-name">plt.rcParams</span>[<span class="org-string">'font.family'</span>] = <span class="org-string">'Nimbus Sans'</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-org66b4ca5" class="outline-2">
<h2 id="org66b4ca5">Methods</h2>
<div class="outline-text-2" id="text-org66b4ca5">
</div>
<div id="outline-container-orge084aea" class="outline-3">
<h3 id="orge084aea">Fast approximate nearest neighbors</h3>
<div class="outline-text-3" id="text-orge084aea">
<ul class="org-ul">
<li>M. Aum√ºller, E. Bernhardsson, A. Faithfull: ANN-Benchmarks: A Benchmarking
Tool for Approximate Nearest Neighbor Algorithms. Information
Systems 2019. <a href="10.1016/j.is.2019.02.006">10.1016/j.is.2019.02.006</a> <a href="https://github.com/erikbern/ann-benchmarks">https://github.com/erikbern/ann-benchmarks</a></li>
</ul>
</div>
</div>

<div id="outline-container-org5e90ccb" class="outline-3">
<h3 id="org5e90ccb">GMVAE</h3>
<div class="outline-text-3" id="text-org5e90ccb">
<p>
<a href="http://ruishu.io/2016/12/25/gmvae/">GMVAE</a> is a modification of the semi-supervised generative model first
presented in Kingma &amp; Welling 2014.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">net</span>(input_dim, hidden_dim):
  <span class="org-doc">"""Return fully connected network, ReLU activations, one hidden layer, batch</span>
<span class="org-doc">normalization at each layer</span>

<span class="org-doc">  """</span>
  <span class="org-keyword">return</span> torch.nn.Sequential(
    torch.nn.Linear(input_dim, hidden_dim),
    torch.nn.BatchNorm1d(hidden_dim),
    torch.nn.ReLU(),
    torch.nn.Linear(hidden_dim, hidden_dim),
    torch.nn.BatchNorm1d(hidden_dim),
    torch.nn.ReLU(),
  )  

<span class="org-comment-delimiter"># </span><span class="org-comment">Distributions parameterized by neural nets, to be used for priors and</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">variational approximations.</span>

<span class="org-keyword">class</span> <span class="org-type">Categorical</span>(torch.nn.Module):
  <span class="org-keyword">def</span> <span class="org-function-name">__init__</span>(<span class="org-keyword">self</span>, input_dim, output_dim, hidden_dim=128):
    <span class="org-builtin">super</span>().__init__()
    <span class="org-keyword">self</span>.net = net(input_dim, hidden_dim)
    <span class="org-keyword">self</span>.logits = torch.nn.Linear(hidden_dim, output_dim)

  <span class="org-keyword">def</span> <span class="org-function-name">forward</span>(<span class="org-keyword">self</span>, x):
    <span class="org-variable-name">h</span> = <span class="org-keyword">self</span>.net(x)
    <span class="org-keyword">return</span> <span class="org-keyword">self</span>.logits(h)

<span class="org-keyword">class</span> <span class="org-type">Gaussian</span>(torch.nn.Module):
  <span class="org-keyword">def</span> <span class="org-function-name">__init__</span>(<span class="org-keyword">self</span>, input_dim, output_dim, hidden_dim=128):
    <span class="org-builtin">super</span>().__init__()
    <span class="org-keyword">self</span>.net = net(input_dim, hidden_dim)
    <span class="org-keyword">self</span>.mean = torch.nn.Linear(hidden_dim, output_dim)
    <span class="org-keyword">self</span>.scale = torch.nn.Sequential(torch.nn.Linear(hidden_dim, output_dim), torch.nn.Softplus())

  <span class="org-keyword">def</span> <span class="org-function-name">forward</span>(<span class="org-keyword">self</span>, x):
    <span class="org-variable-name">h</span> = <span class="org-keyword">self</span>.net(x)
    <span class="org-keyword">return</span> <span class="org-keyword">self</span>.mean(h), <span class="org-keyword">self</span>.scale(h)

<span class="org-keyword">class</span> <span class="org-type">Poisson</span>(torch.nn.Module):
  <span class="org-keyword">def</span> <span class="org-function-name">__init__</span>(<span class="org-keyword">self</span>, input_dim, output_dim, hidden_dim=128):
    <span class="org-builtin">super</span>().__init__()
    <span class="org-keyword">self</span>.net = net(input_dim, hidden_dim)
    <span class="org-keyword">self</span>.lam = torch.nn.Sequential(torch.nn.Linear(hidden_dim, output_dim), torch.nn.Softplus())

  <span class="org-keyword">def</span> <span class="org-function-name">forward</span>(<span class="org-keyword">self</span>, x):
    <span class="org-variable-name">h</span> = <span class="org-keyword">self</span>.net(x)
    <span class="org-keyword">return</span> <span class="org-keyword">self</span>.lam(h)

<span class="org-keyword">class</span> <span class="org-type">PVAE</span>(torch.nn.Module):
  <span class="org-keyword">def</span> <span class="org-function-name">__init__</span>(<span class="org-keyword">self</span>, input_dim, latent_dim, covar_dim, hidden_dim=128):
    <span class="org-builtin">super</span>().__init__()
    <span class="org-keyword">self</span>.qz = Gaussian(input_dim, latent_dim)
    <span class="org-keyword">self</span>.px = Poisson(latent_dim + covar_dim, input_dim)
    <span class="org-keyword">self</span>.writer = tb.SummaryWriter()

  <span class="org-keyword">def</span> <span class="org-function-name">forward</span>(<span class="org-keyword">self</span>, s, x, c, n_samples, global_step):
    <span class="org-variable-name">mean</span>, <span class="org-variable-name">scale</span> = <span class="org-keyword">self</span>.qz.forward(x)
    <span class="org-comment-delimiter"># </span><span class="org-comment">[n_samples,]</span>
    <span class="org-variable-name">kl_z</span> = torch.<span class="org-builtin">sum</span>(.5 * (1 - 2 * torch.log(scale) + mean ** 2 + scale ** 2), axis=1)
    <span class="org-keyword">self</span>.writer.add_scalar(<span class="org-string">'loss/kl_z'</span>, kl_z.<span class="org-builtin">sum</span>(), global_step)
    <span class="org-comment-delimiter"># </span><span class="org-comment">[n_samples, batch_size, input_dim]</span>
    <span class="org-variable-name">qz</span> = torch.distributions.Normal(mean, scale).rsample(n_samples)
    <span class="org-comment-delimiter"># </span><span class="org-comment">[n_samples, batch_size, input_dim + covar_dim]</span>
    <span class="org-variable-name">qz</span> = torch.cat([qz, c.unsqueeze(0).expand(qz.shape[0], -1, -1)], dim=-1)
    <span class="org-variable-name">lam</span> = <span class="org-keyword">self</span>.px.forward(qz.reshape([-1, qz.shape[2]])).reshape([qz.shape[0], qz.shape[1], -1])
    <span class="org-variable-name">s</span> = torch.reshape(s, [1, -1, 1])
    <span class="org-comment-delimiter"># </span><span class="org-comment">[n_samples,]</span>
    <span class="org-variable-name">err</span> = torch.mean(torch.<span class="org-builtin">sum</span>(x * torch.log(s * lam) - s * lam + torch.lgamma(x + 1), dim=2), dim=0)
    <span class="org-keyword">self</span>.writer.add_scalar(<span class="org-string">'loss/err'</span>, err.<span class="org-builtin">sum</span>(), global_step)
    <span class="org-variable-name">loss</span> = -torch.<span class="org-builtin">sum</span>(err - kl_z)
    <span class="org-keyword">self</span>.writer.add_scalar(<span class="org-string">'loss/elbo'</span>, loss, global_step)
    <span class="org-keyword">assert</span> <span class="org-keyword">not</span> torch.isnan(loss)
    <span class="org-keyword">return</span> loss

  <span class="org-keyword">def</span> <span class="org-function-name">fit</span>(<span class="org-keyword">self</span>, data, n_epochs, n_samples=10, **kwargs):
    <span class="org-keyword">assert</span> torch.cuda.is_available()
    <span class="org-keyword">self</span>.cuda()
    <span class="org-variable-name">n_samples</span> = torch.Size([n_samples])
    <span class="org-variable-name">opt</span> = torch.optim.RMSprop(<span class="org-keyword">self</span>.parameters(), **kwargs)
    <span class="org-variable-name">global_step</span> = 0
    <span class="org-keyword">for</span> epoch <span class="org-keyword">in</span> <span class="org-builtin">range</span>(n_epochs):
      <span class="org-keyword">for</span> s, x, c <span class="org-keyword">in</span> data:
        <span class="org-variable-name">s</span> = s.cuda()
        <span class="org-variable-name">x</span> = x.cuda()
        <span class="org-variable-name">c</span> = c.cuda()
        opt.zero_grad()
        <span class="org-variable-name">loss</span> = <span class="org-keyword">self</span>.forward(s, x, c, n_samples=n_samples, global_step=global_step)
        <span class="org-keyword">if</span> torch.isnan(loss):
          <span class="org-keyword">raise</span> <span class="org-type">RuntimeError</span>(<span class="org-string">'nan loss'</span>)
        loss.backward()
        opt.step()
        <span class="org-variable-name">global_step</span> += 1
    <span class="org-keyword">return</span> <span class="org-keyword">self</span>

  <span class="org-type">@torch.no_grad</span>()
  <span class="org-keyword">def</span> <span class="org-function-name">latent</span>(<span class="org-keyword">self</span>, data):
    <span class="org-variable-name">res</span> = []
    <span class="org-keyword">for</span> _, x, _ <span class="org-keyword">in</span> data:
      res.append(<span class="org-keyword">self</span>.qz.forward(x.cuda())[0].cpu().numpy())
    <span class="org-variable-name">res</span> = np.vstack(res)
    <span class="org-keyword">return</span> res

<span class="org-keyword">class</span> <span class="org-type">GMVAE</span>(torch.nn.Module):
  r<span class="org-doc">"""Deep generative model</span>

<span class="org-doc">  x_ij \mid s_i, z_i, c_i ~ Pois(s_i [&#955;(z_i, c_i)]_j)</span>
<span class="org-doc">  z_i \mid y_i ~ N(&#956;(y_i), &#963;^2(y_i))</span>
<span class="org-doc">  y_i ~ Mult(1, 1/m, ..., 1/m)</span>

<span class="org-doc">  q(y_i \mid x_i) = Mult(1, &#960;(y_i))</span>
<span class="org-doc">  q(z_i \mid x_i, y_i) = N(m(x_i, y_i), s^2(x_i, y_i))</span>

<span class="org-doc">  where </span>

<span class="org-doc">  x_{ij} is molecule count (cell i, gene j)</span>
<span class="org-doc">  s_i is total molecule count (cell i)</span>
<span class="org-doc">  c_i is covariate vector (cell i)</span>
<span class="org-doc">  z_i is latent representation (cell i)</span>
<span class="org-doc">  y_i is latent label (cell i)</span>
<span class="org-doc">  &#955; is a neural network mapping latent_dim + covar_dim =&gt; input_dim</span>
<span class="org-doc">  &#956;, &#963;^2 are neural nets mapping label_dim =&gt; latent_dim</span>
<span class="org-doc">  &#960; is a neural net mapping input_dim =&gt; label_dim</span>
<span class="org-doc">  m, s^2 are neural nets mapping </span>

<span class="org-doc">  """</span>
  <span class="org-keyword">def</span> <span class="org-function-name">__init__</span>(<span class="org-keyword">self</span>, input_dim, latent_dim, covar_dim, label_dim=0, hidden_dim=128):
    <span class="org-builtin">super</span>().__init__()
    <span class="org-keyword">if</span> label_dim &gt; 0:
      <span class="org-keyword">self</span>.qy = Categorical(input_dim, label_dim)
      <span class="org-keyword">self</span>.py = torch.distributions.Categorical(probs=torch.ones(label_dim).cuda())
      <span class="org-keyword">self</span>.pz = Gaussian(label_dim, hidden_dim)
    <span class="org-keyword">else</span>:
      <span class="org-keyword">self</span>.qy = <span class="org-constant">None</span>
      <span class="org-keyword">self</span>.py = <span class="org-constant">None</span>
      <span class="org-keyword">self</span>.pz = <span class="org-constant">None</span>
    <span class="org-keyword">self</span>.qz = Gaussian(input_dim + label_dim, hidden_dim)
    <span class="org-keyword">self</span>.px = Poisson(latent_dim + covar_dim, hidden_dim)
    <span class="org-keyword">self</span>.writer = tb.SummaryWriter()

  <span class="org-keyword">def</span> <span class="org-function-name">forward</span>(<span class="org-keyword">self</span>, s, x, c, n_samples, global_step):
    <span class="org-keyword">if</span> <span class="org-keyword">self</span>.qy <span class="org-keyword">is</span> <span class="org-keyword">not</span> <span class="org-constant">None</span>:
      <span class="org-variable-name">logits</span> = <span class="org-keyword">self</span>.qy.forward(x)
      <span class="org-comment-delimiter"># </span><span class="org-comment">[batch_size, label_dim]</span>
      <span class="org-variable-name">qy</span> = torch.distributions.Categorical(logits=logits)
      <span class="org-variable-name">kl_y</span> = torch.distributions.kl.kl_divergence(qy, <span class="org-keyword">self</span>.py).<span class="org-builtin">sum</span>()
      <span class="org-keyword">self</span>.writer.add_scalar(<span class="org-string">'loss/kl_y'</span>, kl_y, global_step)
      <span class="org-variable-name">y</span> = torch.nn.functional.one_hot(qy.rsample())
      <span class="org-comment-delimiter"># </span><span class="org-comment">[n_samples, batch_size, latent_dim]</span>
      <span class="org-variable-name">prior_mean</span>, <span class="org-variable-name">prior_scale</span> = <span class="org-keyword">self</span>.pz.forward(qy)
      <span class="org-comment-delimiter"># </span><span class="org-comment">[batch_size, latent_dim]</span>
      <span class="org-variable-name">mean</span>, <span class="org-variable-name">scale</span> = <span class="org-keyword">self</span>.qz.forward(torch.cat([x, y], dim=1))
    <span class="org-keyword">else</span>:
      <span class="org-variable-name">y</span> = torch.empty(0)
      <span class="org-variable-name">kl_y</span> = 0
      <span class="org-variable-name">prior_mean</span> = 0
      <span class="org-variable-name">prior_scale</span> = 1
      <span class="org-variable-name">mean</span>, <span class="org-variable-name">scale</span> = <span class="org-keyword">self</span>.qz.forward(x)

    <span class="org-comment-delimiter"># </span><span class="org-comment">[n_samples, batch_size, latent_dim]</span>
    <span class="org-variable-name">qz</span> = torch.distributions.Normal(mean, scale).rsample(n_samples)
    <span class="org-comment-delimiter"># </span><span class="org-comment">[n_samples,]</span>
    <span class="org-variable-name">kl_z</span> = torch.<span class="org-builtin">sum</span>(.5 * (1 + 2 * torch.log(scale / prior_scale) + ((prior_mean - mean) ** 2 + prior_scale - scale) / scale), axis=1)
    <span class="org-keyword">self</span>.writer.add_scalar(<span class="org-string">'loss/kl_z'</span>, kl.<span class="org-builtin">sum</span>(), global_step)

    <span class="org-comment-delimiter"># </span><span class="org-comment">[n_samples, batch_size, input_dim]</span>
    <span class="org-variable-name">lam</span> = <span class="org-keyword">self</span>.px.forward(qz.reshape([-1, qz.shape[2]])).reshape([qz.shape[0], qz.shape[1], -1])
    <span class="org-variable-name">s</span> = torch.reshape(s, [1, -1, 1])
    <span class="org-comment-delimiter"># </span><span class="org-comment">[n_samples,]</span>
    <span class="org-variable-name">err</span> = torch.mean(torch.<span class="org-builtin">sum</span>(x * torch.log(s * lam) - s * lam + torch.lgamma(x + 1), dim=2), dim=0)
    <span class="org-keyword">self</span>.writer.add_scalar(<span class="org-string">'loss/err'</span>, err.<span class="org-builtin">sum</span>(), global_step)
    <span class="org-variable-name">loss</span> = -torch.<span class="org-builtin">sum</span>(err - kl_y - kl_z)
    <span class="org-keyword">self</span>.writer.add_scalar(<span class="org-string">'loss/elbo'</span>, loss, global_step)
    <span class="org-keyword">assert</span> <span class="org-keyword">not</span> torch.isnan(loss)
    <span class="org-keyword">return</span> loss

  <span class="org-keyword">def</span> <span class="org-function-name">fit</span>(<span class="org-keyword">self</span>, data, n_epochs, n_samples=10, **kwargs):
    <span class="org-keyword">assert</span> torch.cuda.is_available()
    <span class="org-keyword">self</span>.cuda()
    <span class="org-variable-name">n_samples</span> = torch.Size([n_samples])
    <span class="org-variable-name">opt</span> = torch.optim.RMSprop(<span class="org-keyword">self</span>.parameters(), **kwargs)
    <span class="org-variable-name">global_step</span> = 0
    <span class="org-keyword">for</span> epoch <span class="org-keyword">in</span> <span class="org-builtin">range</span>(n_epochs):
      <span class="org-keyword">for</span> s, x, c <span class="org-keyword">in</span> data:
        <span class="org-variable-name">s</span> = s.cuda()
        <span class="org-variable-name">x</span> = x.cuda()
        <span class="org-variable-name">c</span> = c.cuda()
        opt.zero_grad()
        <span class="org-variable-name">loss</span> = <span class="org-keyword">self</span>.forward(s, x, c, n_samples=n_samples, global_step=global_step)
        <span class="org-keyword">if</span> torch.isnan(loss):
          <span class="org-keyword">raise</span> <span class="org-type">RuntimeError</span>(<span class="org-string">'nan loss'</span>)
        loss.backward()
        opt.step()
        <span class="org-variable-name">global_step</span> += 1
    <span class="org-keyword">return</span> <span class="org-keyword">self</span>

  <span class="org-type">@torch.no_grad</span>()
  <span class="org-keyword">def</span> <span class="org-function-name">latent</span>(<span class="org-keyword">self</span>, data):
    <span class="org-variable-name">res</span> = []
    <span class="org-keyword">for</span> _, x, _ <span class="org-keyword">in</span> data:
      <span class="org-variable-name">x</span> = x.cuda()
      <span class="org-variable-name">logits</span> = <span class="org-keyword">self</span>.qy.forward(x)
      torch.distributions.Categorical(logits=logits)
      <span class="org-variable-name">y</span> = torch.nn.functional.one_hot(qy.rsample())
      res.append(<span class="org-keyword">self</span>.qz.forward(torch.cat([x, y], dim=1)).cpu().numpy())
    <span class="org-variable-name">res</span> = np.vstack(res)
    <span class="org-keyword">return</span> res
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-org6c52ba5" class="outline-2">
<h2 id="org6c52ba5">Results</h2>
<div class="outline-text-2" id="text-org6c52ba5">
</div>
<div id="outline-container-org7bd79a3" class="outline-3">
<h3 id="org7bd79a3">iPSC data</h3>
<div class="outline-text-3" id="text-org7bd79a3">
<p>
Load the iPSC data.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">x</span> = anndata.read_h5ad(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-ideas/data/ipsc/ipsc.h5ad'</span>)
</pre>
</div>

<p>
Take all cells from batch 1. A priori, there could be clusters by individual
and plate.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">y</span> = x[x.obs[<span class="org-string">'batch'</span>] == <span class="org-string">'b1'</span>]
</pre>
</div>

<p>
Use the default approach, projecting \(\ln(x+1)\) into a principal component
subspace and computing nearest neighbors.
</p>

<div class="org-src-container">
<pre class="src src-ipython">sc.pp.neighbors(y)
sc.tl.umap(y, copy=<span class="org-constant">False</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Paired'</span>)
plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 2)
fig.set_size_inches(6, 3)
<span class="org-keyword">for</span> i, k <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(y.obs[<span class="org-string">'chip_id'</span>].unique()):
  <span class="org-variable-name">query</span> = y[y.obs[<span class="org-string">'chip_id'</span>] == k].obsm[<span class="org-string">'X_umap'</span>]
  ax[0].scatter(query[:,0], query[:,1], s=1, c=cm(i), label=k)
ax[0].set_title(<span class="org-string">'By donor'</span>)
<span class="org-keyword">for</span> i, k <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(y.obs[<span class="org-string">'experiment'</span>].unique()):
  <span class="org-variable-name">query</span> = y[y.obs[<span class="org-string">'experiment'</span>] == k].obsm[<span class="org-string">'X_umap'</span>]
  ax[1].scatter(query[:,0], query[:,1], s=1, c=cm(i), label=k)
ax[1].set_title(<span class="org-string">'By chip'</span>)
<span class="org-keyword">for</span> a <span class="org-keyword">in</span> ax:
  a.set_xlabel(<span class="org-string">'UMAP 1'</span>)
  a.set_ylabel(<span class="org-string">'UMAP 2'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/mnn.org/ipsc-b1-umap.png" alt="ipsc-b1-umap.png">
</p>
</div>

<p>
Take all cells from NA18507. A priori, there could be clusters by batch.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">y</span> = x[x.obs[<span class="org-string">'chip_id'</span>] == <span class="org-string">'NA18507'</span>]
sc.pp.neighbors(y)
sc.tl.umap(y, copy=<span class="org-constant">False</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Paired'</span>)
plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 2)
fig.set_size_inches(6, 3)
<span class="org-keyword">for</span> i, k <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(y.obs[<span class="org-string">'batch'</span>].unique()):
  <span class="org-variable-name">query</span> = y[y.obs[<span class="org-string">'batch'</span>] == k].obsm[<span class="org-string">'X_umap'</span>]
  ax[0].scatter(query[:,0], query[:,1], s=1, c=cm(i), label=k)
ax[0].set_title(<span class="org-string">'By batch'</span>)
<span class="org-keyword">for</span> i, k <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(y.obs[<span class="org-string">'experiment'</span>].unique()):
  <span class="org-variable-name">query</span> = y[y.obs[<span class="org-string">'experiment'</span>] == k].obsm[<span class="org-string">'X_umap'</span>]
  ax[1].scatter(query[:,0], query[:,1], s=1, c=cm(i), label=k)
ax[1].set_title(<span class="org-string">'By chip'</span>)
<span class="org-keyword">for</span> a <span class="org-keyword">in</span> ax:
  a.set_xlabel(<span class="org-string">'UMAP 1'</span>)
  a.set_ylabel(<span class="org-string">'UMAP 2'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/mnn.org/ipsc-NA18507-umap.png" alt="ipsc-NA18507-umap.png">
</p>
</div>

<p>
Look at the all cells from all individuals measured in both batches 1 and 2.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">ix</span> = <span class="org-builtin">set</span>(x.obs.loc[x.obs[<span class="org-string">'batch'</span>] == <span class="org-string">'b1'</span>,<span class="org-string">'chip_id'</span>]) &amp; <span class="org-builtin">set</span>(x.obs.loc[x.obs[<span class="org-string">'batch'</span>] == <span class="org-string">'b2'</span>,<span class="org-string">'chip_id'</span>])
ix
</pre>
</div>

<pre class="example">
{'NA18507', 'NA18508', 'NA19190'}

</pre>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">y</span> = x[x.obs[<span class="org-string">'chip_id'</span>].isin(ix)]
sc.pp.neighbors(y)
sc.tl.umap(y, copy=<span class="org-constant">False</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Paired'</span>)
plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 2)
fig.set_size_inches(6, 3)
<span class="org-keyword">for</span> i, k <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(y.obs[<span class="org-string">'batch'</span>].unique()):
  <span class="org-variable-name">query</span> = y[y.obs[<span class="org-string">'batch'</span>] == k].obsm[<span class="org-string">'X_umap'</span>]
  ax[0].scatter(query[:,0], query[:,1], s=1, c=cm(i), label=k)
ax[0].set_title(<span class="org-string">'By batch'</span>)
<span class="org-keyword">for</span> i, k <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(y.obs[<span class="org-string">'chip_id'</span>].unique()):
  <span class="org-variable-name">query</span> = y[y.obs[<span class="org-string">'chip_id'</span>] == k].obsm[<span class="org-string">'X_umap'</span>]
  ax[1].scatter(query[:,0], query[:,1], s=1, c=cm(i), label=k)
ax[1].set_title(<span class="org-string">'By individual'</span>)
<span class="org-keyword">for</span> a <span class="org-keyword">in</span> ax:
  a.set_xlabel(<span class="org-string">'UMAP 1'</span>)
  a.set_ylabel(<span class="org-string">'UMAP 2'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/mnn.org/ipsc-b1-b2-umap.png" alt="ipsc-b1-b2-umap.png">
</p>
</div>

<p>
Plot the full data, colored by batch.
</p>

<div class="org-src-container">
<pre class="src src-ipython">sc.pp.neighbors(x)
sc.tl.umap(x, copy=<span class="org-constant">False</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Paired'</span>)
plt.clf()
plt.gcf().set_size_inches(3, 3)
<span class="org-keyword">for</span> i, k <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(x.obs[<span class="org-string">'batch'</span>].unique()):
  <span class="org-variable-name">query</span> = x[x.obs[<span class="org-string">'batch'</span>] == k].obsm[<span class="org-string">'X_umap'</span>]
  plt.scatter(query[:,0], query[:,1], s=1, c=cm(i), label=k)
plt.title(<span class="org-string">'By batch'</span>)
plt.xlabel(<span class="org-string">'UMAP 1'</span>)
plt.ylabel(<span class="org-string">'UMAP 2'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/mnn.org/ipsc-umap.png" alt="ipsc-umap.png">
</p>
</div>
</div>
</div>

<div id="outline-container-org7bd058b" class="outline-3">
<h3 id="org7bd058b">iPSC Drop-Seq data</h3>
<div class="outline-text-3" id="text-org7bd058b">
<p>
Is it the case that Fluidigm C1 batch effects are really smaller? Try
looking at scRNA-seq of iPSCs generated using Drop-Seq (Selewa et al. 2019).
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">x</span> = anndata.read_h5ad(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-ideas/data/czi/drop/czi-ipsc-cm.h5ad'</span>)
</pre>
</div>

<p>
Project \(\ln(x+1)\) into a principal component subspace and compute
nearest neighbors.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">y</span> = x[x.obs[<span class="org-string">'day'</span>] == 0]
sc.pp.filter_genes(y, min_cells=1)
sc.pp.neighbors(y)
sc.tl.umap(y, copy=<span class="org-constant">False</span>)
</pre>
</div>

<p>
Plot the UMAP embedding of the principal component subspace.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Paired'</span>)
plt.clf()
plt.gcf().set_size_inches(3, 3)
<span class="org-keyword">for</span> i, k <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(y.obs[<span class="org-string">'ind'</span>].unique()):
  <span class="org-variable-name">query</span> = y[y.obs[<span class="org-string">'ind'</span>] == k].obsm[<span class="org-string">'X_umap'</span>]
  plt.scatter(query[:,0], query[:,1], s=1, c=cm(i), label=k)
plt.legend(markerscale=8, handletextpad=0, frameon=<span class="org-constant">False</span>)
plt.title(<span class="org-string">'By donor'</span>)
plt.xlabel(<span class="org-string">'UMAP 1'</span>)
plt.ylabel(<span class="org-string">'UMAP 2'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/mnn.org/ipsc-cm-day0-umap.png" alt="ipsc-cm-day0-umap.png">
</p>
</div>

<p>
Fit PVAE.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">s</span> = y.X.<span class="org-builtin">sum</span>(axis=1)
<span class="org-variable-name">rep</span> = pd.get_dummies(y.obs[<span class="org-string">'ind'</span>])
<span class="org-variable-name">dataset</span> = td.TensorDataset(
  torch.tensor(s.A.ravel()),
  torch.tensor(y.X.A),
  torch.tensor(rep.values, dtype=torch.<span class="org-builtin">float</span>))
<span class="org-variable-name">data</span> = td.DataLoader(dataset, batch_size=64, shuffle=<span class="org-constant">True</span>, num_workers=2)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">fit</span> = PVAE(input_dim=y.shape[1], latent_dim=10, covar_dim=2).fit(data, n_epochs=10, lr=1e-2)
</pre>
</div>

<p>
Estimate the GMVAE latent variables \(E[\mathbf{z}_i \mid \mathbf{x}_i
   y_i]\), then compute a UMAP embedding of the latent space.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">y.obsm</span>[<span class="org-string">'pvae'</span>] = fit.latent(td.DataLoader(dataset, batch_size=128, shuffle=<span class="org-constant">False</span>, num_workers=2))
sc.pp.neighbors(y, use_rep=<span class="org-string">'pvae'</span>)
sc.tl.umap(y, copy=<span class="org-constant">False</span>)
</pre>
</div>

<p>
Plot the UMAP embedding.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Paired'</span>)
plt.clf()
plt.gcf().set_size_inches(3, 3)
<span class="org-keyword">for</span> i, k <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(y.obs[<span class="org-string">'ind'</span>].unique()):
  <span class="org-variable-name">query</span> = y[y.obs[<span class="org-string">'ind'</span>] == k].obsm[<span class="org-string">'pvae'</span>]
  plt.scatter(query[:,0], query[:,1], s=1, c=np.array(cm(i)).reshape(1, -1), label=k, zorder=-i)
plt.legend(markerscale=8, handletextpad=0, frameon=<span class="org-constant">False</span>)
plt.title(<span class="org-string">'By donor'</span>)
plt.xlabel(<span class="org-string">'UMAP 1'</span>)
plt.ylabel(<span class="org-string">'UMAP 2'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/mnn.org/pvae-umap.png" alt="pvae-umap.png">
</p>
</div>

<p>
Now, look at fitting PVAE for multiple days and replicates.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">y</span> = x[x.obs[<span class="org-string">'day'</span>].isin([0, 3])]
sc.pp.filter_genes(y, min_cells=1)
sc.pp.neighbors(y)
sc.tl.umap(y, copy=<span class="org-constant">False</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Paired'</span>)
plt.clf()
plt.gcf().set_size_inches(3.5, 2.5)
<span class="org-keyword">for</span> i, (d, k) <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(y.obs.groupby([<span class="org-string">'day'</span>, <span class="org-string">'ind'</span>]).groups.keys()):
  <span class="org-variable-name">query</span> = y[np.logical_and(y.obs[<span class="org-string">'day'</span>] == d, y.obs[<span class="org-string">'ind'</span>] == k)].obsm[<span class="org-string">'X_umap'</span>]
  plt.scatter(query[:,0], query[:,1], s=1, c=np.array(cm(i)).reshape(1, -1), label=f<span class="org-string">'Day {d}/{k}'</span>, alpha=0.1)
<span class="org-variable-name">leg</span> = plt.legend(markerscale=4, handletextpad=0, frameon=<span class="org-constant">False</span>, loc=<span class="org-string">'center left'</span>, bbox_to_anchor=(1, .5))
<span class="org-keyword">for</span> h <span class="org-keyword">in</span> leg.legendHandles:
  h.set_alpha(1)
plt.xlabel(<span class="org-string">'UMAP 1'</span>)
plt.ylabel(<span class="org-string">'UMAP 2'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/mnn.org/ipsc-cm-day0-3.png" alt="ipsc-cm-day0-3.png">
</p>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">s</span> = y.X.<span class="org-builtin">sum</span>(axis=1)
<span class="org-variable-name">rep</span> = pd.get_dummies(y.obs[<span class="org-string">'ind'</span>])
<span class="org-variable-name">dataset</span> = td.TensorDataset(
  torch.tensor(s.A.ravel()),
  torch.tensor(y.X.A),
  torch.tensor(rep.values, dtype=torch.<span class="org-builtin">float</span>))
<span class="org-variable-name">data</span> = td.DataLoader(dataset, batch_size=64, shuffle=<span class="org-constant">True</span>, num_workers=1)
<span class="org-variable-name">fit</span> = PVAE(input_dim=y.shape[1], latent_dim=10, covar_dim=2).fit(data, n_epochs=10, lr=1e-2)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">y.obsm</span>[<span class="org-string">'pvae'</span>] = fit.latent(td.DataLoader(dataset, batch_size=64, shuffle=<span class="org-constant">False</span>, num_workers=1))
sc.pp.neighbors(y, use_rep=<span class="org-string">'pvae'</span>)
sc.tl.umap(y, copy=<span class="org-constant">False</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Paired'</span>)
plt.clf()
plt.gcf().set_size_inches(3.75, 2.5)
<span class="org-keyword">for</span> i, (d, k) <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(y.obs.groupby([<span class="org-string">'day'</span>, <span class="org-string">'ind'</span>]).groups.keys()):
  <span class="org-variable-name">query</span> = y[np.logical_and(y.obs[<span class="org-string">'day'</span>] == d, y.obs[<span class="org-string">'ind'</span>] == k)].obsm[<span class="org-string">'pvae'</span>]
  plt.scatter(query[:,0], query[:,1], s=1, c=np.array(cm(i)).reshape(1, -1), label=f<span class="org-string">'Day {d}/{k}'</span>, alpha=0.5)
plt.legend(markerscale=4, handletextpad=0, frameon=<span class="org-constant">False</span>, loc=<span class="org-string">'center left'</span>, bbox_to_anchor=(1, .5))
plt.xlabel(<span class="org-string">'UMAP 1'</span>)
plt.ylabel(<span class="org-string">'UMAP 2'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/mnn.org/ipsc-cm-day0-3-pvae.png" alt="ipsc-cm-day0-3-pvae.png">
</p>
</div>
</div>
</div>

<div id="outline-container-orge6144f5" class="outline-3">
<h3 id="orge6144f5">NMF</h3>
<div class="outline-text-3" id="text-orge6144f5">
<p>
Instead of PCA, use NMF and normalize to a topic model as the latent space.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">m</span> = skd.NMF(n_components=10, solver=<span class="org-string">'mu'</span>, beta_loss=1, max_iter=1000, verbose=<span class="org-constant">True</span>)
<span class="org-variable-name">l</span> = m.fit_transform(x.X)
<span class="org-variable-name">f</span> = m.components_
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">weights</span> = l * f.<span class="org-builtin">sum</span>(axis=1)
<span class="org-variable-name">topics</span> = f.T / f.<span class="org-builtin">sum</span>(axis=1)
<span class="org-variable-name">s</span> = weights.<span class="org-builtin">sum</span>(axis=1, keepdims=<span class="org-constant">True</span>)
<span class="org-variable-name">weights</span> /= s
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">x.obs</span>[<span class="org-string">'topic_scale'</span>] = s
<span class="org-variable-name">x.obsm</span>[<span class="org-string">'topics'</span>] = weights
<span class="org-variable-name">x.varm</span>[<span class="org-string">'topics'</span>] = topics
x.write(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-ideas/data/ipsc/ipsc.h5ad'</span>)
</pre>
</div>

<p>
<b>TODO:</b> this segfaults.
</p>

<div class="org-src-container">
<pre class="src src-ipython">sc.pp.neighbors(x, use_rep=<span class="org-string">'topics'</span>)
sc.tl.umap(x, copy=<span class="org-constant">False</span>)
</pre>
</div>

<p>
0 - 36d6e328-d507-454e-9ac5-657bf63ae113
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Abhishek Sarkar</p>
<p class="date">Created: 2020-04-30 Thu 16:44</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
