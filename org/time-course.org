#+TITLE: Fully unsupervised topic models of scRNA-seq time course data
#+SETUPFILE: setup.org

* Introduction

  In our prior work ([[https://dx.doi.org/10.1371/journal.pgen.1008045][Sarkar
  et al. 2019]]), we introduced a factor model to capture between donor
  individual variation in the mean and variance of gene expression in a single
  cell type \( 
  \DeclareMathOperator\Gam{Gamma}
  \DeclareMathOperator\Poi{Poisson}
  \DeclareMathOperator\argmin{arg min}
  \newcommand\mf{\mathbf{F}} 
  \newcommand\ml{\mathbf{L}}
  \newcommand\mx{\mathbf{X}}
  \newcommand\vl{\mathbf{l}}
  \newcommand\vx{\mathbf{x}}
  \)

  \begin{align*}
    x_{ij} &\sim \Poi(x_i^+ \lambda_{ij})\\
    \lambda_{ij} &\sim \pi_{ij} \delta_0(\cdot) + (1 - \pi_{ij}) \Gam(\phi_{ij}^{-1}, \mu_{ij}^{-1} \phi_{ij}^{-1})\\
    \ln \mu_{ij} &= (\ml \mf_\mu')_{ij}\\
    \ln \phi_{ij} &= (\ml \mf_\phi')_{ij}\\
    \operatorname{logit} \pi_{ij} &= (\ml \mf_\pi')_{ij}\\
  \end{align*}

  where 

  - \(x_{ij}\) is the number of molecules of gene \(j = 1, \ldots, p\) observed
    in cell \(i = 1, \ldots, n\)
  - cells are taken from \(m\) donor individuals, \(\ml\) is \(n \times m\),
    each \(\mf_{\cdot}\) is \(p \times m\)
  - assignments of cells to donors (loadings) \(l_{ik} \in \{0, 1\}\) are
    known and fixed.

  We are now interested in several lines of questions:

  1. If we analyze this kind of data in a fully unsupervised manner, can we
     recover the assignments of cells to donors? This approach has been
     previously proposed in our specific factor model
     ([[doi:10.1038/s41467-017-02554-5][Risso et al. 2018]]). If not, what do
     we recover?
  2. Can we generalize this analysis approach to data which additionally has
     multiple cell types, and then multiple time points?
  3. Can we implement this approach without forming entire products
     \(\ml\mf'\)? Can we implement each update without looking at the entire
     data \(\mx\)? How much faster can we get than existing methods? Can we
     analyze datasets which are currently impossible to analyze (due to size),
     e.g. Human Cell Atlas? As references, compare against
     [[doi:10.1038/s41592-018-0229-2][scVI]],
     [[https://rawcdn.githack.com/aertslab/cisTopic/8d15fa2813312aa0b20c1042604079558829e947/vignettes/10X_workflow.html#building_the_models][cisTopic]],
     [[https://github.com/stephenslab/fastTopics][fastTopics]]

* Setup
  :PROPERTIES:
  :CUSTOM_ID: setup
  :END:

  #+BEGIN_SRC emacs-lisp :exports none
    (add-to-list 'python-shell-completion-native-disabled-interpreters "jupyter")
    (org-babel-lob-ingest "/home/aksarkar/.emacs.d/org-templates/library.org")
  #+END_SRC

  #+RESULTS:
  : 1

  #+CALL: ipython3(venv="singlecell",partition="gpu2",opts="--gres=gpu:1",memory="16G") :exports none :dir /scratch/midway2/aksarkar/singlecell

  #+RESULTS:
  : Submitted batch job 66342555

  #+BEGIN_SRC ipython
    import anmf
    import anndata
    import numpy as np
    import pandas as pd
    import scanpy as sc
    import scmodes
    import scipy.sparse as ss
    import scipy.stats as st
    import sklearn.decomposition as skd
    import time
    import torch
    import torch.utils.data as td
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[1]:
  :END:

  #+BEGIN_SRC ipython
    %matplotlib inline
    %config InlineBackend.figure_formats = set(['retina'])
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[2]:
  :END:

  #+BEGIN_SRC ipython
    import matplotlib.pyplot as plt
    plt.rcParams['figure.facecolor'] = 'w'
    plt.rcParams['font.family'] = 'Nimbus Sans'
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[3]:
  :END:

* Methods
** Amortized NMF

   Amortized inference
   ([[http://ruishu.io/2017/11/07/amortized-optimization/][Shu 2017]],
   [[https://papers.nips.cc/paper/7692-amortized-inference-regularization.pdf][Shu
   et al. 2018]]) refers to a strategy to efficiently solve a large (possibly
   infinite) collection of optimization problems:

   \[ \theta^* = \argmin_{\theta} f(\theta, \phi), \]

   where \(\phi \in \Phi\) is some context variable. Rather than solving one
   problem for each \(\phi \in \Phi\), we learn a function \(h_\alpha\)
   parameterized by \(\alpha\) which predicts \(\theta^*\) from \(\phi\)

   \[ \alpha^* = \argmin_\alpha E_{\phi \sim \hat{p}(\phi)} f(h_\alpha(\phi), \phi), \]

   where \(\hat{p}(\phi)\) denotes the empirical distribution of \(\phi\) in
   the training data. The function \(h_\alpha\) amortizes inference over the
   training data examples by coupling the optimization problems together, and
   indeed amortizes inference over unseen examples also.

   As a concrete example, NMF
   ([[http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization][Lee
   and Seung 2001]], [[https://dx.doi.org/10.1155/2009/785152][Cemgil 2009]])
   can be written

   \[ \vl_i^* = \argmin_{\vl_i} \sum_j \operatorname{Poisson}(x_{ij}; \sum_k l_{ik} f_{jk})\]

   where \(\vx_i\) is the context variable, and the goal is to learn
   \(h_\alpha\) which maps observations \(\vx_i\) to loadings \(\vl_i\). (Here,
   we are simplifying by holding \(\mf\) fixed. In this setup, we treat the
   \(\vl_i\) as local latent variables, and \(\mf\) as a global latent
   variable. These can be optimized in alternating phases.) The resulting
   optimization problem can be solved by introducing an /encoder network/
   \(h_\alpha\), which has been previously proposed
   ([[doi:10.1038/s41592-018-0229-2][Lopez et al. 2018]],
   [[doi:10.1038/s41467-018-07931-2][Eraslan et al. 2018]]).

   Existing methods have been introduced with the motivation that auto-encoding
   networks can represent non-linear mappings into latent spaces; however, the
   gain in explanatory power of such methods is unclear. Amortized inference
   suggests a simpler, more compelling motivation to explore these methods:
   they enable the use of stochastic optimization methods to analyze large data
   sets (for example, those which do not fit in memory). However, existing
   software implementations have a major limitation: they do not support sparse
   matrices on the GPU, making it either impossible to analyze complete data
   sets, or introducing a major bottleneck in moving minibatches to the GPU. We
   implement this functionality in the Python package ~anmf~.

** Amortized LDA

   NMF is intimately connected to LDA via the Multinomial-Poisson transform
   ([[http://www.jstor.org/stable/2348134][Baker 1994]]). Briefly, if we have
   an MLE for NMF and scale \(\ml\) and \(\mf\) to satisfy the constraints
   \(\sum_k l_{ik} = 1\), \(\sum_k f_{jk} = 1\), we recover an MLE for the
   Multinomial likelihood underlying LDA. This fact suggests an amortized
   inference scheme for maximum likelihood estimation of topic models:

   \begin{align*}
     x_{ij} \mid s_i, \vl_i, \mf &\sim \operatorname{Poisson}(s_i \sum_k l_{ik} f_{jk})\\
     \vl_i &= h(\vx_i)
   \end{align*}

   where \(h\) is a neural network with softmax output. Unlike previous
   approaches for amortized inference in topic models
   ([[https://arxiv.org/pdf/1703.01488][Srivastava et al. 2017]]), we are not
   concerned with recovering an approximate posterior over \(\mathbf{L},
   \mathbf{F}\), simplifying the problem.

** iPSC data

   Read the data.

   #+BEGIN_SRC ipython :async t
     keep_samples = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/quality-single-cells.txt', index_col=0, header=None)
     keep_genes = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/genes-pass-filter.txt', index_col=0, header=None)
     annotations = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-annotation.txt')
     annotations = annotations.loc[keep_samples.values.ravel()]
     header = sorted(set(annotations['chip_id']))
     umi = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-counts.txt.gz', index_col=0).loc[keep_genes.values.ravel(),keep_samples.values.ravel()]
     gene_info = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-genes.txt.gz', index_col=0)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[7]:
   :END:

   Convert to sparse ~h5ad~.

   #+BEGIN_SRC ipython :async t
     del annotations["index"]
     x = anndata.AnnData(ss.csr_matrix(umi.values.T), obs=annotations, var=gene_info.loc[umi.index])
     x.write('/project2/mstephens/aksarkar/projects/singlecell-ideas/data/ipsc/ipsc.h5ad', compression=None, force_dense=False)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[33]:
   :END:

** 10-way mixture

   Read the FACS sorted data sets from
   [[https://dx.doi.org/10.1038/ncomms14049][Zheng et al 2017]].

   #+BEGIN_SRC ipython
     keys = (
       'b_cells',
       'cd34',
       'cd4_t_helper',
       'cd56_nk',
       'cytotoxic_t',
       'memory_t',
       'naive_cytotoxic',
       'naive_t',
       'regulatory_t',
       'cd14_monocytes',
     )
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[71]:
   :END:

   #+BEGIN_SRC ipython :async t
     data = {k: scmodes.dataset.read_10x(f'/project2/mstephens/aksarkar/projects/singlecell-ideas/data/10xgenomics/{k}/filtered_matrices_mex/hg19/', return_adata=True, min_detect=0)
             for k in keys}
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[72]:
   :END:

   Concatenate the data, then take genes with observations in at least 0.1% of
   cells.

   #+BEGIN_SRC ipython :async t
     x = data[keys[0]].concatenate(*[data[k] for k in keys[1:]], join='inner', batch_key='cell_type', batch_categories=keys)
     sc.pp.filter_genes(x, min_cells=1)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[73]:
   :END:

   Report the dimensions.

   #+BEGIN_SRC ipython
     x.shape
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[74]:
   : (94655, 21952)
   :END:

   Write out the data.

   #+BEGIN_SRC ipython
     x.obs = x.obs.rename({0: 'barcode'}, axis=1)
     x.var = x.var.rename({0: 'ensg', 1: 'name'}, axis=1)
     x.write('/scratch/midway2/aksarkar/ideas/zheng-10-way.h5ad')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[75]:
   :END:

** Census of Immune Cells data

   We previously processed the Census of Immune Cells.

   Read the sparse data. (20 seconds)

   #+BEGIN_SRC ipython :async t
     y_csr = ss.load_npz('/scratch/midway2/aksarkar/modes/immune-cell-census.npz')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[46]:
   :END:

   Read the metadata.

   #+BEGIN_SRC ipython
     genes = pd.read_csv('/scratch/midway2/aksarkar/modes/immune-cell-census-genes.txt.gz', sep='\t', index_col=0)
     donor = pd.Categorical(pd.read_csv('/scratch/midway2/aksarkar/modes/immune-cell-census-samples.txt.gz', sep='\t', index_col=0)['0'])
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[47]:
   :END:

* Results
** Simulation

   Simulate a simple example.

   #+BEGIN_SRC ipython
     np.random.seed(0)
     n = 512
     p = 1000
     k = 10
     l = np.random.lognormal(sigma=0.5, size=(n, k))
     f = np.random.lognormal(sigma=0.5, size=(p, k))
     x = np.random.poisson(l.dot(f.T))
     s = x.sum(axis=1)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[4]:
   :END:

   Fit NMF via EM.

   #+BEGIN_SRC ipython :async t
     start = time.time()
     lhat, fhat, loss = scmodes.lra.nmf(x, rank=10, tol=1e-2, max_iters=10000, verbose=True)
     elapsed = time.time() - start
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[6]:
   :END:

   Report the time elapsed (seconds).

   #+BEGIN_SRC ipython
     elapsed
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[7]:
   : 200.11741995811462
   :END:

   Peter Carbonetto previously derived the KKT conditions for the NMF
   objective. Report the maximum absolute KKT residual.

   #+BEGIN_SRC ipython :async t
     A = x / (s.reshape(-1, 1) * lhat.dot(fhat))
     l_resid = abs(fhat.T * ((1 - A).T @ lhat)).max()
     f_resid = abs(lhat * ((1 - A) @ fhat.T)).max()
     l_resid, f_resid
   #+END_SRC

   Report the oracle loss, and the EM loss.

   #+BEGIN_SRC ipython
     -st.poisson(mu=x).logpmf(x).sum(), loss
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[19]:
   : (1104395.014892481, 1356432.0729535678)
   :END:

   Fit ANMF.

   #+BEGIN_SRC ipython
     assert torch.cuda.is_available()
     dense_data = td.TensorDataset(torch.tensor(x, dtype=torch.float).cuda(),
                                   torch.tensor(s, dtype=torch.float).cuda())
     data = td.DataLoader(dense_data, batch_size=64, shuffle=True)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[57]:
   :END:

   #+BEGIN_SRC ipython :async t
     start = time.time()
     fit = (anmf.modules.ANMF(input_dim=x.shape[1])
            .fit(data, max_epochs=400, trace=True, lr=5e-3))
     elapsed = time.time() - start
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[58]:
   :END:

   Report the time elapsed (seconds).

   #+BEGIN_SRC ipython
     elapsed
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[59]:
   : 7.6526994705200195
   :END:

   Plot the optimization trace, focusing on the last 100 iterations.

   #+BEGIN_SRC ipython :ipyfile figure/time-course.org/sim-trace.png
     plt.clf()
     plt.gcf().set_size_inches(4, 2)
     plt.plot(np.array(fit.trace).ravel()[-400:] / (64 * p), lw=1, c='k')
     plt.xlabel('Minibatch')
     plt.ylabel('Per obs neg log lik')
     plt.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[61]:
   [[file:figure/time-course.org/sim-trace.png]]
   :END:

   Report the maximum absolute KKT residual.

   #+BEGIN_SRC ipython :async t
     lhat = np.vstack([fit.loadings(b) for (b, s) in data])
     fhat = fit.factors()
     A = x / (s.reshape(-1, 1) * lhat.dot(fhat))
     l_resid = abs(fhat.T * ((1 - A).T @ lhat)).max()
     f_resid = abs(lhat * ((1 - A) @ fhat.T)).max()
     l_resid, f_resid
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[60]:
   : (0.023548399419082357, 0.029147694241919264)
   :END:

** ANMF on iPSC data

   Read the data.

   #+BEGIN_SRC ipython
     x = anndata.read_h5ad('/project2/mstephens/aksarkar/projects/singlecell-ideas/data/ipsc/ipsc.h5ad')
     s = x.X.sum(axis=1)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[10]:
   :END:

   #+BEGIN_SRC ipython
     sparse_data = anmf.dataset.ExprDataset(x.X, s.A)
     data = td.DataLoader(sparse_data, batch_size=64, shuffle=False, collate_fn=sparse_data.collate_fn)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[23]:
   :END:

   Run ANMF on the data. A priori, we might expect the data to be rank 53
   (equal to the number of donor individuals). Indeed, this is the rank of the
   implicit factor model we originally fit to the data.

   #+BEGIN_SRC ipython :async t
     start = time.time()
     fit = (anmf.modules.ANMF(input_dim=x.shape[1], latent_dim=53)
            .fit(data, max_epochs=40, trace=True, verbose=True, lr=1e-2))
     elapsed = time.time() - start
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[24]:
   :END:

   Report how long the optimization took (minutes).

   #+BEGIN_SRC ipython
     elapsed / 60
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[28]:
   : 2.7217764496803283
   :END:

   Plot the end of the optimization trace.

   #+BEGIN_SRC ipython :ipyfile figure/time-course.org/ipsc-trace.png
     plt.clf()
     plt.gcf().set_size_inches(4, 2)
     plt.plot(np.array(fit.trace).ravel()[-100:] / (128 * x.shape[1]), lw=1, c='k')
     plt.xticks(np.arange(0, 125, 25), np.arange(-100, 25, 25))
     plt.xlabel('Minibatches before maximum')
     plt.ylabel('Neg log lik per obs')
     plt.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[29]:
   [[file:figure/time-course.org/ipsc-trace.png]]
   :END:

   Save the fitted model.

   #+BEGIN_SRC ipython
     torch.save(fit.state_dict(), '/scratch/midway2/aksarkar/ideas/ipsc-anmf.pkl')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[27]:
   :END:

** ANMF on 68K PBMCs

   Read the data, restricting to genes with non-zero observations in at least 1
   cell.

   #+BEGIN_SRC ipython :async t
     x = scmodes.dataset.read_10x('/project2/mstephens/aksarkar/projects/singlecell-ideas/data/10xgenomics/fresh_68k_pbmc_donor_a/filtered_matrices_mex/hg19/', min_detect=0, return_adata=True)
     sc.pp.filter_genes(x, min_cells=1)
     s = x.X.sum(axis=1)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[64]:
   :END:

   Report the dimensions.

   #+BEGIN_SRC ipython
     x.shape
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[65]:
   : (68579, 20387)
   :END:

   Fit rank 10 ANMF.

   #+BEGIN_SRC ipython
     sparse_data = anmf.dataset.ExprDataset(x.X, s.A)
     data = td.DataLoader(sparse_data, batch_size=128, shuffle=False, collate_fn=sparse_data.collate_fn)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[66]:
   :END:

   #+BEGIN_SRC ipython :async t
     start = time.time()
     fit = (anmf.modules.ANMF(input_dim=x.shape[1], latent_dim=10)
            .fit(data, max_epochs=15, trace=True, verbose=True, lr=1e-2))
     elapsed = time.time() - start
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[67]:
   :END:

   Report how long the optimization took (minutes).

   #+BEGIN_SRC ipython
     elapsed / 60
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[69]:
   : 5.1033944328626
   :END:

   Plot the optimization trace, focusing on the tail.

   #+BEGIN_SRC ipython :ipyfile figure/time-course.org/pbmc-trace.png
     plt.clf()
     plt.gcf().set_size_inches(4, 2)
     plt.plot(np.array(fit.trace).ravel()[-2000:] / (128 * x.shape[1]), lw=1, c='k')
     plt.xticks(np.arange(0, 2500, 500), np.arange(-2000, 500, 500))
     plt.xlabel('Minibatches before maximum')
     plt.ylabel('Neg log lik per obs')
     plt.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[70]:
   [[file:figure/time-course.org/pbmc-trace.png]]
   :END:

** ANMF on 10-way mixture

   Read the data.

   #+BEGIN_SRC ipython
     x = anndata.read_h5ad('/scratch/midway2/aksarkar/ideas/zheng-10-way.h5ad')
     s = x.X.sum(axis=1)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[4]:
   :END:

   Fit ANMF.

   #+BEGIN_SRC ipython
     sparse_data = anmf.dataset.ExprDataset(x.X, s.A)
     data = td.DataLoader(sparse_data, batch_size=128, shuffle=False, collate_fn=sparse_data.collate_fn)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[5]:
   :END:

   #+BEGIN_SRC ipython :async t
     start = time.time()
     fit = (anmf.modules.ANMF(input_dim=x.shape[1])
            .fit(data, max_epochs=10, trace=True, verbose=True, lr=5e-3))
     elapsed = time.time() - start
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[7]:
   :END:

   Report how long the optimization took (seconds).

   #+BEGIN_SRC ipython
     elapsed
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[8]:
   : 285.05831122398376
   :END:

   Plot the optimization trace, focusing on the tail.

   #+BEGIN_SRC ipython :ipyfile figure/time-course.org/zheng-10-way-trace.png
     plt.clf()
     plt.gcf().set_size_inches(4, 2)
     plt.plot(np.array(fit.trace).ravel()[-2000:] / (128 * x.shape[1]), lw=1, c='k')
     plt.xticks(np.arange(0, 2500, 500), np.arange(-2000, 500, 500))
     plt.xlabel('Minibatches before maximum')
     plt.ylabel('Neg log lik per obs')
     plt.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[9]:
   [[file:figure/time-course.org/zheng-10-way-trace.png]]
   :END:
