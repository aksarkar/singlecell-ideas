<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2018-10-09 Tue 16:04 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Weighted low rank approximation</title>
<meta name="generator" content="Org mode">
<meta name="author" content="Abhishek Sarkar">
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link href="https://maxcdn.bootstrapcdn.com/bootswatch/3.3.7/cosmo/bootstrap.min.css" rel="stylesheet" />
<link rel="stylesheet" type="text/css" href="https://www.pirilampo.org/styles/readtheorg/css/htmlize.css"/>
<style type="text/css">body {width: 60em; margin:auto} pre.src {overflow:auto}</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2017 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Weighted low rank approximation</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org75d64ac">Introduction</a></li>
<li><a href="#orgc3403ea">Methods</a>
<ul>
<li><a href="#org1010fdb">EM algorithm</a></li>
<li><a href="#orgdd7ac38">Alternative derivation</a></li>
<li><a href="#orgb462a31">Maximizing non-Gaussian likelihoods</a></li>
<li><a href="#orgc035e15">Imputing missing values</a></li>
<li><a href="#org15db90d">Choosing the rank of the approximation</a></li>
</ul>
</li>
<li><a href="#org643a7f6">Results</a>
<ul>
<li><a href="#org31c72cd">Recovering low rank structure</a></li>
<li><a href="#org64afc90">Explaining training data</a></li>
<li><a href="#org622533c">Imputing missing values</a></li>
<li><a href="#orgb6aef17">Convergence</a></li>
<li><a href="#orgc1ad970">Choosing the rank of the approximation</a></li>
<li><a href="#orgd780580">Explaining held out data</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-org75d64ac" class="outline-2">
<h2 id="org75d64ac">Introduction</h2>
<div class="outline-text-2" id="text-org75d64ac">
<p>
We are interested in solving the <i>weighted low-rank approximation problem</i>:
</p>

<p>
\[ \min_{\mathbf{Z}} \sum_{i,j} w_{ij} \left(x_{ij} - z_{ij} \right)^2 \]
</p>

<p>
where \(n \times p\) target matrix \(\mathbf{X}\) and \(n \times p\) weight
matrix \(\mathbf{W}\) are given, and \(\mathbf{Z}\) is constrained to some
rank.
</p>

<p>
Our primary interest is to use WLRA to learn low rank structure in
non-Gaussian data (especially Poisson data, as is generated by
RNA-Seq). Using Taylor expansion of non-Gaussian likelihoods, we can rewrite
the MLE of factor models as the solution to WLRA. The key idea is that the
Taylor expansion is performed around a different value for each observation,
naturally leading to an iterative approach.
</p>

<p>
The methods are implemented in the Python package <a href="https://www.github.com/aksarkar/wlra">wlra</a>.
</p>
</div>
</div>

<div id="outline-container-orgc3403ea" class="outline-2">
<h2 id="orgc3403ea">Methods</h2>
<div class="outline-text-2" id="text-orgc3403ea">
</div>
<div id="outline-container-org1010fdb" class="outline-3">
<h3 id="org1010fdb">EM algorithm</h3>
<div class="outline-text-3" id="text-org1010fdb">
<p>
<a href="https://www.aaai.org/Papers/ICML/2003/ICML03-094.pdf">Srebro and Jaakkola 2003</a> propose an EM algorithm to solve WLRA. The algorithm
is EM in the following sense: suppose the weights \(w_{ij} \in \{0, 1\}\),
corresponding to presence/absence, and suppose \(\mathbf{X} = \mathbf{Z} +
   \mathbf{E}\), where \(\mathbf{Z}\) is low-rank and elements of \(\mathbf{E}\)
are Gaussian.
</p>

<p>
Then, \(E[x_{ij} \mid w_{ij} = 0] = z_{ij}\), naturally giving an EM
algorithm. The E-step fills in \(x_{ij}\) with \(z_{ij}\), and the M-step
estimates \(\mathbf{Z}\) from the filled in \(\mathbf{X}\). The solution to
the M-step is given by the optimal unweighted rank \(k\) approximation,
i.e. truncated SVD, because all the non-zero weights are equal to 1.
</p>

<p>
Conceptually, the method for arbitrary weights is to suppose instead that we
have rational \(w_{ij} \in \{0, 1/N, \ldots, 1\}\). 
</p>

<p>
Then, we can reduce this problem to a problem in 0/1 weights by supposing we
have \(X^{(k)} = Z + E^{(k)}\), \(k \in 1, \ldots, N\), and each entry is
observed in only \(N w_{ij}\) of the \(X^{(k)}\).
</p>

<p>
Then, the M-step becomes:
</p>

<p>
\[ \mathbf{Z}^{(t + 1)} = \mathrm{LRA}_k(\mathbf{W} \circ \mathbf{X} +
   (\mathbf{1} - \mathbf{W}) \circ \mathbf{Z}^{(t)}) \]
</p>

<p>
where \(\mathrm{LRA}_k\) is the unweighted rank \(k\) approximation and
\(\circ\) denotes Hadamard product.
</p>

<p>
Intuitively, the implied E-step corresponds to taking the expectation of
\(z_{ij}^{(k)}\) over the targets \(k\). Clearly, the algorithm generalizes
to any weight matrix where \(0 \leq w_{ij} \leq 1\) are stored in finite
precision. We can support arbitrary weights by scaling by the maximum weight
(this simply scales the objective).
</p>
</div>
</div>

<div id="outline-container-orgdd7ac38" class="outline-3">
<h3 id="orgdd7ac38">Alternative derivation</h3>
<div class="outline-text-3" id="text-orgdd7ac38">
<p>
<a href="https://stephens999.github.io/misc/wSVD.html">Stephens</a> proposes an alternative EM algorithm. Suppose \(\mathbf{X} =
   \mathbf{M} + \mathbf{Z} + \mathbf{E}\) where \(\mathbf{M}\) is low rank,
and:
</p>

<p>
\[ z_{ij} \sim \mathcal{N}(0, \sigma_{ij}^2) \]
</p>

<p>
\[ e_{ij} \sim \mathcal{N}(0, \sigma^{2}) \]
</p>

<p>
Let \(\mathbf{R} = \mathbf{X} - \mathbf{M}\). Then:
</p>

<p>
\[ r_{ij} \mid z_{ij} \sim \mathcal{N}(z_{ij}, \sigma^{2}) \]
</p>

<p>
\[ \mathbb{E}[z_{ij} \mid x_{ij}, m_{ij}, s_{ij}] = \frac{\sigma_{ij}^2}{\sigma^2 +
   \sigma_{ij}^2} (x_{ij} - m_{ij}) = (1 - w_{ij}) (x_{ij} - m_{ij}) \]
</p>

<p>
and the solution to the E step is:
</p>

<p>
\[ \mathbf{Z}^{(t + 1)} = (1 - \mathbf{W}) \circ (\mathbf{X} - \mathbf{M}^{(t)}) \]
</p>

<p>
Given \(\mathbf{Z}^{(t)}\), the solution to the M step is PCA with
homoscedastic errors, i.e. truncated SVD.
</p>

<p>
\[ \mathbf{M}^{(t + 1)} = \mathrm{LRA}_k(\mathbf{X} - \mathbf{Z}) \]
</p>

<p>
\[ = \mathrm{LRA}_k(\mathbf{W} \circ \mathbf{X} + (1 - \mathbf{W}) \circ
   \mathbf{M}^{(t)}) \]
</p>

<p>
In comparison to the algorithm of Srebro and Jaakkola, this algorithm has an
extra parameter \(\sigma^2\). However, we need:
</p>

<p>
\[ \mathbb{V}[x] = s_{ij}^2 = \sigma^{2} + \sigma_{ij}^{2} \]
</p>

<p>
where \(s_{ij}^2\) is a known variance for each observation \(x_{ij}\). If
we let \(\sigma^2 = \min(s_{ij}^2)\), then:
</p>

<p>
\[ w_{ij} = \frac{\min(s_{ij}^2)}{s_{ij}^2} = \frac{1 / s_{ij}^2}{\max(1 / s_{ij}^2)} \]
</p>
</div>
</div>

<div id="outline-container-orgb462a31" class="outline-3">
<h3 id="orgb462a31">Maximizing non-Gaussian likelihoods</h3>
<div class="outline-text-3" id="text-orgb462a31">
<p>
Suppose \(l(\theta) = \ln p(x \mid \theta)\). Then, taking second-order
Taylor expansion about \(\theta_0\):
</p>

<p>
\[ l(\theta) \approx l(\theta_0) + (\theta - \theta_0)\,l'(\theta_0) +
   \frac{(\theta - \theta)^2}{2}\,l''(\theta_0) \]
</p>

<p>
\[ = \frac{l''(\theta_0)}{2} \left[ \theta - \left(\theta_0 -
   \frac{l'(\theta_0)}{l''(\theta_0)}\right)\right]^2 + \mathrm{const}\]
</p>

<p>
where the constant does not depend on \(\theta\). Now, maximizing the
objective is equivalent to maximizing a Gaussian likelihood:
</p>

<p>
\[= \mathcal{N}\left(\theta; \theta_0 -
   \frac{l'(\theta_0)}{l''(\theta_0)}, -\frac{1}{l''(\theta_0)}\right) +
   \mathrm{const} \]
</p>

<p>
Equivalently, minimizing the negative of the objective function is
WLRA. This result makes sense because for fixed \(\sigma^2\), maximizing the
Gaussian likelihood is the same as minimizing the Frobenius norm.
</p>

<p>
The result suggests we can improve \(l\) by optimizing this objective
instead, and suggests an iterative algorithm where we alternate updates
between \(\theta_0\) and \(\theta\).
</p>

<p>
Because this new objective is written in terms of derivatives of the log
likelihood, we can readily write down the required quantities for relevant
distributions:
</p>

<table class="table text-left">


<colgroup>
<col  class="org-left">

<col  class="org-left">

<col  class="org-left">

<col  class="org-left">
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Distribution</th>
<th scope="col" class="org-left">Parameter \(\theta\)</th>
<th scope="col" class="org-left">Target (mean)</th>
<th scope="col" class="org-left">Weight (precision)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Bernoulli</td>
<td class="org-left">\(\mathrm{logit}(p)\)</td>
<td class="org-left">\(\theta + \frac{x - S(\theta)}{S(\theta)(1 - S(\theta))}\)</td>
<td class="org-left">\(\frac{1}{S(\theta)(1 - S(\theta))}\)</td>
</tr>

<tr>
<td class="org-left">Poisson</td>
<td class="org-left">\(\ln\lambda\)</td>
<td class="org-left">\(\theta + x\exp(-\theta) - 1\)</td>
<td class="org-left">\(\exp(-\theta)\)</td>
</tr>
</tbody>
</table>

<p>
where \(S(\cdot)\) denotes the sigmoid function.
</p>
</div>
</div>

<div id="outline-container-orgc035e15" class="outline-3">
<h3 id="orgc035e15">Imputing missing values</h3>
<div class="outline-text-3" id="text-orgc035e15">
<p>
The EM algorithm for WLRA supports missing values by setting \(w_{ij} =
   0\).
</p>

<p>
When maximizing non-Gaussian likelihoods using WLRA, we need to introduce
external weights \(\tilde{w}_{ij} \in \{0, 1\}\) to denote
presence/absence. We can do this by using weights \(\tilde{\mathbf{W}} \circ
   \mathbf{W}\) in each outer iteration.
</p>
</div>
</div>

<div id="outline-container-org15db90d" class="outline-3">
<h3 id="org15db90d">Choosing the rank of the approximation</h3>
<div class="outline-text-3" id="text-org15db90d">
<p>
The most obvious way to pick an optimal rank is to use cross-validation:
hold out some samples, and evaluate the likelihood of the held-out samples
using the estimated low rank structure.
</p>

<p>
Because WLRA supports missing value imputation, we can instead hold out
individual entries of the matrix (increasing the effective size of the
training data) and evaluate the imputation RMSE (<a href="https://cran.r-project.org/web/packages/NNLM/vignettes/Fast-And-Versatile-NMF.html#determine-rank-k-via-missing-value-imputation">Lin 2018</a>). Requiring the
model to be able to reconstruct corrupted inputs is the key idea of
denoising autoencoders (<a href="http://www.iro.umontreal.ca/~lisa/publications2/index.php/publications/show/217">Vincent et al. 2008</a>). Intuitively, for low rank
approximation the true rank should have the best imputation RMSE: lower rank
should underfit, and higher rank should overfit.
</p>
</div>
</div>
</div>

<div id="outline-container-org643a7f6" class="outline-2">
<h2 id="org643a7f6">Results</h2>
<div class="outline-text-2" id="text-org643a7f6">
</div>
<div id="outline-container-org31c72cd" class="outline-3">
<h3 id="org31c72cd">Recovering low rank structure</h3>
<div class="outline-text-3" id="text-org31c72cd">
<p>
We first consider the problem of recovering a planted low rank matrix after
convolving with Gaussian noise, assuming we know the rank.
</p>

<p>
\[ l_{ik} \sim \mathcal{N}(0, 1) \]
\[ f_{kj} \sim \mathcal{N}(0, 1) \]
\[ \mu_{ij} = (\mathbf{L F})_{ij} \]
\[ \sigma^2_{ij} \sim \mathrm{Uniform}(1, \sigma^2_0) \]
\[ x_{ij} \sim \mathcal{N}(\mu_{ij}, \sigma^2_{ij}) \]
</p>

<p>
We assume the noise variances for each observation are known, and use the
inverse variances as the weights.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="org75789dc"><span class="org-keyword">def</span> <span class="org-function-name">wnorm</span>(x, w):
  <span class="org-keyword">return</span> (w * np.square(x)).<span class="org-builtin">sum</span>()

<span class="org-keyword">def</span> <span class="org-function-name">simulate_gaussian</span>(n, p, rank, s0=10, seed=0):
  np.random.seed(seed)
  <span class="org-variable-name">l</span> = np.random.normal(size=(n, rank))
  <span class="org-variable-name">f</span> = np.random.normal(size=(rank, p))
  <span class="org-variable-name">eta</span> = l.dot(f)
  <span class="org-variable-name">noise</span> = np.random.uniform(1, s0, size=eta.shape)
  <span class="org-variable-name">w</span> = 1 / noise
  <span class="org-variable-name">x</span> = np.random.normal(loc=eta, scale=noise)
  <span class="org-keyword">return</span> x, w, eta

<span class="org-keyword">def</span> <span class="org-function-name">rrmse</span>(pred, true):
  <span class="org-keyword">return</span> np.sqrt(np.linalg.norm(pred - true) / np.linalg.norm(true))

<span class="org-keyword">def</span> <span class="org-function-name">score_wlra</span>(x, w, eta, rank):
  <span class="org-keyword">try</span>:
    <span class="org-variable-name">res</span> = wlra.wlra(x, w, rank=rank, max_iters=1000)
    <span class="org-keyword">return</span> rrmse(res, eta)
  <span class="org-keyword">except</span> <span class="org-type">RuntimeError</span>:
    <span class="org-keyword">return</span> np.nan

<span class="org-keyword">def</span> <span class="org-function-name">score_lra</span>(x, eta, rank):
  <span class="org-variable-name">res</span> = wlra.lra(x, rank)
  <span class="org-keyword">return</span> rrmse(res, eta)

<span class="org-keyword">def</span> <span class="org-function-name">evaluate_gaussian_known_rank</span>(num_trials=10):
  <span class="org-variable-name">result</span> = []
  <span class="org-keyword">for</span> s0 <span class="org-keyword">in</span> np.logspace(0, 1, 4):
    <span class="org-keyword">for</span> rank <span class="org-keyword">in</span> <span class="org-builtin">range</span>(1, 4):
      <span class="org-keyword">for</span> trial <span class="org-keyword">in</span> <span class="org-builtin">range</span>(num_trials):
        <span class="org-variable-name">x</span>, <span class="org-variable-name">w</span>, <span class="org-variable-name">eta</span> = simulate_gaussian(n=100, p=1000, rank=rank, s0=s0, seed=trial)
        result.append([rank,
                       s0,
                       trial,
                       score_lra(x, eta, rank=rank),
                       score_wlra(x, w, eta, rank=rank)])
  <span class="org-variable-name">result</span> = pd.DataFrame(result)
  <span class="org-variable-name">result.columns</span> = [<span class="org-string">'rank'</span>, <span class="org-string">'s0'</span>, <span class="org-string">'trial'</span>, <span class="org-string">'LRA'</span>, <span class="org-string">'WLRA'</span>]
  <span class="org-keyword">return</span> result
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">&lt;&lt;imports&gt;&gt;
&lt;&lt;gaussian-reconstruction&gt;&gt;
<span class="org-variable-name">res</span> = evaluate_gaussian_known_rank(num_trials=25)
res.to_csv(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-ideas/data/wlra/gaussian-known-rank.txt.gz'</span>, compression=<span class="org-string">'gzip'</span>, sep=<span class="org-string">'\t'</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-sh">sbatch --partition=broadwl --job-name=gaussian-wlra -n1 -c8
<span class="org-comment-delimiter">#</span><span class="org-comment">!/bin/bash</span>
<span class="org-builtin">source</span> activate singlecell
python /project2/mstephens/aksarkar/projects/singlecell-ideas/code/gaussian-known-rank.py
</pre>
</div>

<pre class="example">
Submitted batch job 50413961

</pre>

<p>
Read the results.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">results_gaussian_known_rank</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-ideas/data/wlra/gaussian-known-rank.txt.gz'</span>, index_col=0)
</pre>
</div>

<p>
Plot the performance.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">T</span> = results_gaussian_known_rank.dropna()

plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, <span class="org-builtin">len</span>(<span class="org-builtin">set</span>(T[<span class="org-string">'rank'</span>])), sharey=<span class="org-constant">True</span>)
fig.set_size_inches(8, 3)
<span class="org-keyword">for</span> i, (rank, data_by_rank) <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(T.groupby([<span class="org-string">'rank'</span>])):
  <span class="org-variable-name">groups</span> = data_by_rank.groupby([<span class="org-string">'s0'</span>])
  <span class="org-keyword">for</span> j, (_, data) <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(groups):
    <span class="org-variable-name">y</span> = data[<span class="org-string">'WLRA'</span>] - data[<span class="org-string">'LRA'</span>]
    <span class="org-variable-name">f</span> = st.gaussian_kde(y)
    <span class="org-variable-name">py</span> = f(y)
    <span class="org-variable-name">x</span> = j + .1 / py.<span class="org-builtin">max</span>() * np.random.uniform(-py, py)
    ax[i].scatter(x, y, c=<span class="org-string">'k'</span>, s=4)
  ax[i].axhline(y=0, c=<span class="org-string">'k'</span>, ls=<span class="org-string">':'</span>, lw=1)
  ax[i].set_title(f<span class="org-string">'Rank = {rank}'</span>)
  ax[i].set_xticks(<span class="org-builtin">range</span>(<span class="org-builtin">len</span>(groups)))
  ax[i].set_xticklabels([<span class="org-string">'{:.2f}'</span>.<span class="org-builtin">format</span>(x) <span class="org-keyword">for</span> x <span class="org-keyword">in</span> groups.groups.keys()])
  ax[i].set_xlabel(<span class="org-string">'$\sigma_0$'</span>)
  ax[i].set_ylabel(<span class="org-string">'Difference in RMSE (LRA)'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/wlra.org/gaussian-known-rank.png" alt="gaussian-known-rank.png">
</p>
</div>
</div>
</div>

<div id="outline-container-org64afc90" class="outline-3">
<h3 id="org64afc90">Explaining training data</h3>
<div class="outline-text-3" id="text-org64afc90">
<p>
We generate a Poisson data matrix with planted row rank structure.
</p>

<p>
\[ \eta_{ij} = (\mathbf{L F})_{ij} \]
\[ x_{ij} \sim \mathrm{Poisson}(\exp(\eta_{ij})) \]
</p>

<p>
We then evaluate the Poisson likelihood of the generated data given the
estimated parameters.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="org89714ae"><span class="org-keyword">def</span> <span class="org-function-name">training_score_oracle</span>(x, eta):
  <span class="org-keyword">return</span> st.poisson(mu=np.exp(eta)).logpmf(x).<span class="org-builtin">sum</span>()

<span class="org-keyword">def</span> <span class="org-function-name">training_score_nmf</span>(x, rank):
  <span class="org-keyword">return</span> st.poisson(mu=nmf(x, rank)).logpmf(x).<span class="org-builtin">sum</span>()

<span class="org-keyword">def</span> <span class="org-function-name">training_score_grad</span>(x, rank):
  <span class="org-keyword">import</span> wlra.grad
  <span class="org-variable-name">m</span> = (wlra.grad.PoissonFA(n_samples=200, n_features=300, n_components=rank)
       .fit(x, atol=1e-3, max_epochs=10000))
  <span class="org-keyword">return</span> st.poisson(mu=np.exp(m.L.dot(m.F))).logpmf(x).<span class="org-builtin">sum</span>()

<span class="org-keyword">def</span> <span class="org-function-name">training_score_plra</span>(x, rank):
  <span class="org-keyword">try</span>:
    <span class="org-keyword">return</span> st.poisson(mu=np.exp(wlra.plra(x, rank=rank, max_outer_iters=100, check_converged=<span class="org-constant">True</span>))).logpmf(x).<span class="org-builtin">sum</span>()
  <span class="org-keyword">except</span> <span class="org-type">RuntimeError</span>:
    <span class="org-keyword">return</span> np.nan

<span class="org-keyword">def</span> <span class="org-function-name">training_score_plra1</span>(x, rank):
  <span class="org-keyword">return</span> st.poisson(mu=np.exp(wlra.plra(x, rank=rank, max_outer_iters=1))).logpmf(x).<span class="org-builtin">sum</span>()

<span class="org-keyword">def</span> <span class="org-function-name">evaluate_training</span>(rank=3, eta_max=2, num_trials=10):
  <span class="org-variable-name">result</span> = []
  <span class="org-keyword">for</span> trial <span class="org-keyword">in</span> <span class="org-builtin">range</span>(num_trials):
    <span class="org-variable-name">x</span>, <span class="org-variable-name">eta</span> = simulate_pois(n=200, p=300, rank=rank, eta_max=eta_max, seed=trial)
    result.append([
      trial,
      training_score_oracle(x, eta),
      training_score_nmf(x, rank),
      training_score_grad(x, rank),
      training_score_plra(x, rank),
      training_score_plra1(x, rank)
    ])
  <span class="org-variable-name">result</span> = pd.DataFrame(result)
  <span class="org-variable-name">result.columns</span> = [<span class="org-string">'trial'</span>, <span class="org-string">'Oracle'</span>, <span class="org-string">'NMF'</span>, <span class="org-string">'Grad'</span>, <span class="org-string">'PLRA'</span>, <span class="org-string">'PLRA1'</span>]
  <span class="org-keyword">return</span> result
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">&lt;&lt;imports&gt;&gt;
&lt;&lt;poisson-imputation&gt;&gt;
&lt;&lt;pois-training&gt;&gt;
<span class="org-variable-name">res</span> = evaluate_training(num_trials=100)
res.to_csv(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-ideas/data/wlra/pois-train-llik.txt.gz'</span>, sep=<span class="org-string">'\t'</span>, compression=<span class="org-string">'gzip'</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-sh">sbatch --partition=mstephens --mem=4G --job-name=pois-train-llik --time=60:00 -n1 -c8
<span class="org-comment-delimiter">#</span><span class="org-comment">!/bin/bash</span>
<span class="org-builtin">source</span> activate singlecell
python /project2/mstephens/aksarkar/projects/singlecell-ideas/code/pois-train-llik.py
</pre>
</div>

<pre class="example">
Submitted batch job 50865668

</pre>

<p>
Read the results.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">pois_train_llik</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-ideas/data/wlra/pois-train-llik.txt.gz'</span>, index_col=0)
</pre>
</div>

<p>
Plot the performance.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">methods</span> = [<span class="org-string">'NMF'</span>, <span class="org-string">'Grad'</span>, <span class="org-string">'PLRA'</span>, <span class="org-string">'PLRA1'</span>]
plt.clf()
plt.gcf().set_size_inches(3, 3)
plt.yscale(<span class="org-string">'symlog'</span>, linthreshy=100)
<span class="org-keyword">for</span> j, method <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(methods):
  <span class="org-variable-name">y</span> = (pois_train_llik[method] - pois_train_llik[<span class="org-string">'Oracle'</span>]).dropna()
  <span class="org-variable-name">f</span> = st.gaussian_kde(y)
  <span class="org-variable-name">py</span> = f(y)
  <span class="org-variable-name">x</span> = j + .2 / py.<span class="org-builtin">max</span>() * np.random.uniform(-py, py)
  plt.scatter(x, y, c=<span class="org-string">'k'</span>, s=4)
plt.axhline(y=0, c=<span class="org-string">'k'</span>, ls=<span class="org-string">':'</span>, lw=1)
plt.xticks(<span class="org-builtin">range</span>(<span class="org-builtin">len</span>(methods)), methods)
plt.xlim(-.5, <span class="org-builtin">len</span>(methods) - .5)
plt.xlabel(<span class="org-string">'Method'</span>)
<span class="org-variable-name">_</span> = plt.ylabel(<span class="org-string">'$\ln\,p(x \mid \hat\eta) - \ln\,p(x \mid \eta)$'</span>)
</pre>
</div>


<div class="figure">
<p><img src="figure/wlra.org/training-llik.png" alt="training-llik.png">
</p>
</div>
</div>
</div>

<div id="outline-container-org622533c" class="outline-3">
<h3 id="org622533c">Imputing missing values</h3>
<div class="outline-text-3" id="text-org622533c">
<p>
We generate a Poisson data matrix with planted row rank structure.
</p>

<p>
\[ \eta_{ij} = (\mathbf{L F})_{ij} \]
\[ x_{ij} \sim \mathrm{Poisson}(\exp(\eta_{ij})) \]
</p>

<p>
We then hold out entries using numpy masked arrays, and impute them using
the Poisson low rank approximation. We compare the imputation accuracy
against non-negative matrix factorization (<a href="https://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf">Lee and Seung 2001</a>,
<a href="https://arxiv.org/abs/1010.1763">Févotte and Idier 2011</a>).
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="org084a2ba"><span class="org-keyword">def</span> <span class="org-function-name">simulate_pois</span>(n, p, rank, eta_max=<span class="org-constant">None</span>, holdout=<span class="org-constant">None</span>, seed=0):
  np.random.seed(seed)
  <span class="org-variable-name">l</span> = np.random.normal(size=(n, rank))
  <span class="org-variable-name">f</span> = np.random.normal(size=(rank, p))
  <span class="org-variable-name">eta</span> = l.dot(f)
  <span class="org-keyword">if</span> eta_max <span class="org-keyword">is</span> <span class="org-keyword">not</span> <span class="org-constant">None</span>:
    <span class="org-comment-delimiter"># </span><span class="org-comment">Scale the maximum value</span>
    <span class="org-variable-name">eta</span> *= eta_max / eta.<span class="org-builtin">max</span>()
  <span class="org-variable-name">x</span> = np.random.poisson(lam=np.exp(eta))
  <span class="org-keyword">if</span> holdout <span class="org-keyword">is</span> <span class="org-keyword">not</span> <span class="org-constant">None</span>:
    <span class="org-variable-name">mask</span> = np.random.uniform(size=(n, p)) &lt; holdout
    <span class="org-variable-name">x</span> = np.ma.masked_array(x, mask=mask)
  <span class="org-keyword">return</span> x, eta

<span class="org-keyword">def</span> <span class="org-function-name">rmse</span>(pred, true):
  <span class="org-keyword">return</span> np.sqrt(np.square(pred - true).mean())

<span class="org-keyword">def</span> <span class="org-function-name">pois_loss</span>(pred, true):
  <span class="org-keyword">return</span> (pred - true * np.log(pred + 1e-8)).mean()

<span class="org-variable-name">losses</span> = [rmse, pois_loss]

<span class="org-keyword">def</span> <span class="org-function-name">loss</span>(pred, true):
  <span class="org-keyword">return</span> [f(pred, true) <span class="org-keyword">for</span> f <span class="org-keyword">in</span> losses]

<span class="org-keyword">def</span> <span class="org-function-name">imputation_score_mean</span>(x):
  <span class="org-doc">"""Mean-impute the data"""</span>
  <span class="org-keyword">return</span> loss(x.mean(), x.data[x.mask])

<span class="org-keyword">def</span> <span class="org-function-name">imputation_score_nmf</span>(x, rank):
  <span class="org-keyword">try</span>:
    <span class="org-variable-name">res</span> = nmf(x, rank, atol=1e-3)
    <span class="org-keyword">return</span> loss(res[x.mask], x.data[x.mask])
  <span class="org-keyword">except</span> <span class="org-type">RuntimeError</span>:
    <span class="org-keyword">return</span> [np.nan <span class="org-keyword">for</span> f <span class="org-keyword">in</span> losses]

<span class="org-keyword">def</span> <span class="org-function-name">imputation_score_plra1</span>(x, rank):
  <span class="org-variable-name">res</span> = np.exp(wlra.pois_lra(x, rank=rank, max_outer_iters=1))
  <span class="org-keyword">return</span> loss(res[x.mask], x.data[x.mask])

<span class="org-keyword">def</span> <span class="org-function-name">imputation_score_plra</span>(x, rank):
  <span class="org-keyword">try</span>:
    <span class="org-variable-name">res</span> = np.exp(wlra.pois_lra(x, rank=rank, max_outer_iters=100, check_converged=<span class="org-constant">True</span>))
    <span class="org-keyword">return</span> loss(res[x.mask], x.data[x.mask])
  <span class="org-keyword">except</span> <span class="org-type">RuntimeError</span>:
    <span class="org-keyword">return</span> [np.nan <span class="org-keyword">for</span> f <span class="org-keyword">in</span> losses]

<span class="org-keyword">def</span> <span class="org-function-name">evaluate_pois_imputation</span>(rank=3, holdout=0.25, eta_max=<span class="org-constant">None</span>, num_trials=10):
  <span class="org-variable-name">result</span> = []
  <span class="org-keyword">for</span> trial <span class="org-keyword">in</span> <span class="org-builtin">range</span>(num_trials):
    <span class="org-variable-name">x</span>, <span class="org-variable-name">eta</span> = simulate_pois(n=200, p=300, rank=rank, eta_max=eta_max,
                           holdout=holdout, seed=trial)
    result.append(<span class="org-builtin">list</span>(itertools.chain.from_iterable(
      [[trial],
       imputation_score_mean(x),
       imputation_score_nmf(x, rank),
       imputation_score_pois_lra(x, rank, max_retries=1),
      ])))
  <span class="org-variable-name">result</span> = pd.DataFrame(result)
  <span class="org-variable-name">result.columns</span> = [<span class="org-string">'trial'</span>, <span class="org-string">'rmse_mean'</span>, <span class="org-string">'pois_loss_mean'</span>, <span class="org-string">'rmse_nmf'</span>,
                    <span class="org-string">'pois_loss_nmf'</span>, <span class="org-string">'rmse_plra'</span>, <span class="org-string">'pois_loss_plra'</span>]
  <span class="org-keyword">return</span> result
</pre>
</div>

<p>
Importantly, we assume \(\eta\) is low rank, and not that \(\exp(\eta)\) is
low rank. It is not the case that if \(\eta\) is low rank, then
\(\exp(\eta)\) is also always low rank.
</p>

<div class="org-src-container">
<pre class="src src-ipython">np.array([(~np.isclose(np.linalg.svd(np.exp(simulate_pois(n=200, p=300, rank=rank, eta_max=3, seed=i)[1]), compute_uv=<span class="org-constant">False</span>), 0)).<span class="org-builtin">sum</span>()
          <span class="org-keyword">for</span> rank <span class="org-keyword">in</span> <span class="org-builtin">range</span>(1, 5) <span class="org-keyword">for</span> i <span class="org-keyword">in</span> <span class="org-builtin">range</span>(10)]).reshape(-1, 10)
</pre>
</div>

<pre class="example">
array([[ 12,  12,  11,  12,  12,  11,  11,  12,  12,  12],
[ 66,  56,  60,  59,  50,  62,  58,  55,  50,  64],
[171, 164, 151, 172, 154, 170, 164, 168, 151, 172],
[200, 200, 200, 200, 200, 200, 200, 200, 200, 200]])
</pre>

<div class="org-src-container">
<pre class="src src-ipython">&lt;&lt;imports&gt;&gt;
&lt;&lt;poisson-imputation&gt;&gt;
<span class="org-variable-name">res</span> = evaluate_pois_imputation(eta_max=2, num_trials=100)
res.to_csv(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-ideas/data/wlra/pois-imputation.txt.gz'</span>, compression=<span class="org-string">'gzip'</span>, sep=<span class="org-string">'\t'</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-sh">sbatch --partition=broadwl --job-name=pois-imputation -n1 -c28 --exclusive
<span class="org-comment-delimiter">#</span><span class="org-comment">!/bin/bash</span>
<span class="org-builtin">source</span> activate singlecell
python /project2/mstephens/aksarkar/projects/singlecell-ideas/code/pois-imputation.py
</pre>
</div>

<pre class="example">
Submitted batch job 50872498

</pre>

<p>
Read the results.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">results_pois_imputation</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-ideas/data/wlra/pois-imputation.txt.gz'</span>, index_col=0)
</pre>
</div>

<p>
Plot the performance.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">T</span> = results_pois_imputation.dropna()

plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 2, sharey=<span class="org-constant">True</span>)
fig.set_size_inches(5.5, 3)
<span class="org-variable-name">methods</span> = [<span class="org-string">'nmf'</span>, <span class="org-string">'plra'</span>]
<span class="org-keyword">for</span> i, (loss_, name) <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(<span class="org-builtin">zip</span>([<span class="org-string">'rmse'</span>, <span class="org-string">'pois_loss'</span>], [<span class="org-string">'RMSE'</span>, <span class="org-string">'Poisson loss'</span>])):
  <span class="org-keyword">for</span> j, method <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(methods):
    <span class="org-variable-name">y</span> = T[f<span class="org-string">'{loss_}_{method}'</span>] - T[f<span class="org-string">'{loss_}_mean'</span>]
    <span class="org-variable-name">f</span> = st.gaussian_kde(y)
    <span class="org-variable-name">py</span> = f(y)
    <span class="org-variable-name">x</span> = j + .2 / py.<span class="org-builtin">max</span>() * np.random.uniform(-py, py)
    ax[i].scatter(x, y, c=<span class="org-string">'k'</span>, s=4)
  ax[i].axhline(y=0, c=<span class="org-string">'k'</span>, ls=<span class="org-string">':'</span>, lw=1)
  ax[i].set_xticks(<span class="org-builtin">range</span>(<span class="org-builtin">len</span>(methods)))
  ax[i].set_xticklabels([x.upper() <span class="org-keyword">for</span> x <span class="org-keyword">in</span> methods])
  ax[i].set_xlim(-.5, 1.5)
  ax[i].set_title(name)
  ax[i].set_xlabel(<span class="org-string">'Method'</span>)
ax[0].set_ylabel(<span class="org-string">'Difference in loss from baseline'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/wlra.org/pois-imputation.png" alt="pois-imputation.png">
</p>
</div>
</div>
</div>

<div id="outline-container-orgb6aef17" class="outline-3">
<h3 id="orgb6aef17">Convergence</h3>
<div class="outline-text-3" id="text-orgb6aef17">
<p>
Extract one of the cases where the algorithm fails to converge.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> pathlib
<span class="org-variable-name">failure_cases</span> = <span class="org-builtin">list</span>(pathlib.Path(<span class="org-string">'/scratch/midway2/aksarkar/ideas/trace/'</span>).glob(<span class="org-string">'*.pkl'</span>))
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">n</span> = 0
<span class="org-keyword">for</span> case <span class="org-keyword">in</span> failure_cases:
  <span class="org-keyword">with</span> <span class="org-builtin">open</span>(case, <span class="org-string">'rb'</span>) <span class="org-keyword">as</span> f:
    <span class="org-variable-name">x</span> = pickle.load(f)
  <span class="org-keyword">try</span>:
    <span class="org-variable-name">_</span> = wlra.pois_lra(x, rank=3, max_outer_iters=50)
  <span class="org-keyword">except</span> <span class="org-type">RuntimeError</span>:
    <span class="org-variable-name">n</span> += 1
n
</pre>
</div>

<pre class="example">
23

</pre>

<p>
Look at the objective function as a function of x and eta.
</p>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 2, sharex=<span class="org-constant">True</span>)
fig.set_size_inches(7, 3)

<span class="org-variable-name">lam</span> = np.logspace(-2, 2, 100)
<span class="org-variable-name">eta</span> = np.log(lam)
<span class="org-variable-name">s2</span> = np.exp(-eta)

<span class="org-keyword">for</span> x <span class="org-keyword">in</span> <span class="org-builtin">range</span>(10):
  <span class="org-variable-name">m</span> = eta + x * np.exp(-eta) - 1
  ax[0].plot(eta, st.norm(loc=m, scale=np.sqrt(s2)).pdf(eta), lw=1, c=colorcet.cm[<span class="org-string">'fire'</span>](x / 10))
ax[0].set_xlabel(<span class="org-string">'$\eta$'</span>)
ax[0].set_ylabel(<span class="org-string">'Approximate objective'</span>)

<span class="org-keyword">for</span> x <span class="org-keyword">in</span> <span class="org-builtin">range</span>(10):
  ax[1].plot(eta, st.poisson(mu=lam).pmf(x), lw=1, c=colorcet.cm[<span class="org-string">'fire'</span>](x / 10))
ax[1].set_xlabel(<span class="org-string">'$\eta$'</span>)
ax[1].set_ylabel(<span class="org-string">'True objective'</span>)

plt.tight_layout()

<span class="org-variable-name">sm</span> = plt.cm.ScalarMappable(cmap=colorcet.cm[<span class="org-string">'fire'</span>], norm=plt.Normalize(vmin=0, vmax=10))
<span class="org-variable-name">sm._A</span> = []
<span class="org-variable-name">cb</span> = plt.colorbar(sm, ax=ax)
cb.set_label(<span class="org-string">'Observation x'</span>)
</pre>
</div>


<div class="figure">
<p><img src="figure/wlra.org/approx.png" alt="approx.png">
</p>
</div>

<p>
Look at the estimates against the true eta for a simple problem.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">x</span>, <span class="org-variable-name">eta</span> = simulate_pois(n=200, p=300, rank=3, eta_max=4)
<span class="org-variable-name">etahat_plra1</span> = wlra.plra(x, rank=3, max_outer_iters=1)
<span class="org-variable-name">etahat_plra2</span> = wlra.plra(x, rank=3, max_outer_iters=2)
<span class="org-variable-name">etahat_plra</span> = wlra.plra(x, rank=3, max_outer_iters=100, check_converged=<span class="org-constant">True</span>)

<span class="org-keyword">import</span> wlra.grad
<span class="org-variable-name">m</span> = wlra.grad.PoissonFA(n_samples=200, n_features=300, n_components=3, log_link=<span class="org-constant">True</span>).fit(x, atol=1e-2, max_epochs=10000)
<span class="org-variable-name">etahat_grad</span> = m.L.dot(m.F)

<span class="org-variable-name">lamhat</span> = nmf(x, 3)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">lim</span> = 4
plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(2, 3)
plt.gcf().set_size_inches(9, 5)

<span class="org-keyword">for</span> a <span class="org-keyword">in</span> ax.ravel():
  a.set_xlim([-lim, lim])
  a.set_ylim([-lim, lim])
  a.set_xlabel(<span class="org-string">'$\eta$'</span>)
  a.set_ylabel(<span class="org-string">'$\hat\eta$'</span>)

<span class="org-keyword">for</span> a <span class="org-keyword">in</span> ax.ravel()[:-2]:
  a.plot([-lim, lim], [-lim, lim], c=<span class="org-string">'r'</span>, lw=1, ls=<span class="org-string">':'</span>)

ax[0][0].scatter(eta, etahat_plra1, c=<span class="org-string">'k'</span>, s=4, alpha=0.1)
ax[0][0].set_title(<span class="org-string">'PLRA1'</span>)

ax[0][1].scatter(eta, etahat_plra2, c=<span class="org-string">'k'</span>, s=4, alpha=0.1)
ax[0][1].set_title(<span class="org-string">'PLRA2'</span>)

ax[0][2].scatter(eta, etahat_plra, c=<span class="org-string">'k'</span>, s=4, alpha=0.1)
ax[0][2].set_title(<span class="org-string">'PLRA'</span>)

ax[1][0].scatter(eta, etahat_grad, c=<span class="org-string">'k'</span>, s=4, alpha=0.1)
ax[1][0].set_title(<span class="org-string">'Grad'</span>)

<span class="org-variable-name">min_</span> = lamhat.<span class="org-builtin">min</span>()
ax[1][1].scatter(np.exp(eta), lamhat, c=<span class="org-string">'k'</span>, s=4, alpha=0.1)
ax[1][1].plot([min_, np.exp(lim)], [min_, np.exp(lim)], c=<span class="org-string">'r'</span>, lw=1, ls=<span class="org-string">':'</span>)
ax[1][1].set_xscale(<span class="org-string">'log'</span>)
ax[1][1].set_yscale(<span class="org-string">'log'</span>)
ax[1][1].set_xlim(min_, np.exp(lim))
ax[1][1].set_ylim(min_, np.exp(lim))
ax[1][1].set_title(<span class="org-string">'NMF'</span>)
ax[1][1].set_xlabel(<span class="org-string">'$\lambda$'</span>)
ax[1][1].set_ylabel(<span class="org-string">'$\hat\lambda$'</span>)

ax[1][2].set_axis_off()
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/wlra.org/etahat.png" alt="etahat.png">
</p>
</div>
</div>
</div>

<div id="outline-container-orgc1ad970" class="outline-3">
<h3 id="orgc1ad970">Choosing the rank of the approximation</h3>
<div class="outline-text-3" id="text-orgc1ad970">
<p>
We generate a Poisson data matrix with planted row rank structure.
</p>

<p>
\[ \eta_{ij} = (\mathbf{L F})_{ij} \]
\[ x_{ij} \sim \mathrm{Poisson}(\exp(\eta_{ij})) \]
</p>

<p>
We then hold out \(p\) fraction of entries, and take the rank which
minimizes the imputation RMSE.
</p>

<p>
As a proof of principle, look at the imputation RMSE as a function of rank
for one simulated data set.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">x</span>, <span class="org-variable-name">eta</span> = simulate_pois(n=200, p=300, rank=3, eta_max=2, seed=0)
<span class="org-variable-name">mask</span> = np.random.uniform(size=x.shape) &lt; 0.25
<span class="org-variable-name">x</span> = np.ma.masked_array(x, mask=mask)

<span class="org-variable-name">query</span> = <span class="org-builtin">range</span>(1, 10)
<span class="org-variable-name">lra_scores</span> = np.array([imputation_score_pois_lra(x, rank) <span class="org-keyword">for</span> rank <span class="org-keyword">in</span> query])

plt.clf()
plt.gcf().set_size_inches(3, 3)
plt.plot(query, lra_scores[:,1], c=<span class="org-string">'k'</span>, lw=1, label=<span class="org-string">'PLRA'</span>)
plt.axvline(x=1 + np.argmin(lra_scores[:,1]), c=<span class="org-string">'k'</span>, lw=1, ls=<span class="org-string">':'</span>)
plt.axhline(y=lra_scores.<span class="org-builtin">min</span>(), c=<span class="org-string">'k'</span>, lw=1, ls=<span class="org-string">':'</span>)

plt.xlabel(<span class="org-string">'Assumed rank'</span>)
<span class="org-variable-name">_</span> = plt.ylabel(<span class="org-string">'Poisson loss of imputed values'</span>)
</pre>
</div>


<div class="figure">
<p><img src="figure/wlra.org/select-rank-example.png" alt="select-rank-example.png">
</p>
</div>
</div>
</div>

<div id="outline-container-orgd780580" class="outline-3">
<h3 id="orgd780580">Explaining held out data</h3>
<div class="outline-text-3" id="text-orgd780580">
<p>
A number of methods have been proposed to estimate low rank structure from
count data. 
</p>

<ul class="org-ul">
<li>Non-negative matrix factorization (<a href="https://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf">Lee and Seung 2001</a>, <a href="https://arxiv.org/abs/1010.1763">Févotte and Idier 2011</a>)</li>
<li>Hierarchical Bayesian Poisson Factorization (<a href="http://www.cs.columbia.edu/~blei/papers/GopalanHofmanBlei2015.pdf">Gopalan et al. 2015</a>)</li>
<li>ZINB-WAVE (<a href="https://www.nature.com/articles/s41467-017-02554-5">Risso et al. 2018</a>)</li>
<li>scVI (<a href="https://www.biorxiv.org/content/early/2018/03/30/292037">Lopez et al 2018</a>)</li>
</ul>

<p>
These methods typically try to learn low rank structure to solve the
<i>collaborative filtering</i> problem: given user \(i\) interacted with item
\(j\), predict what other items they will interact with (<a href="http://yifanhu.net/PUB/cf.pdf">Hu et
al. 2001</a>). Low rank structure in this context corresponds to learning
patterns of user preferences (factors), and the specific preferences of
individual users (loadings).
</p>

<p>
One special feature of the problem is that only <i>implicit feedback</i> is
assumed: we only have observations \(x_{ij} = 1\) when an interaction is
recorded, and \(x_{ij} = 0\) could either reflect negative feedback or
missing data. However, even methods developed specifically for scRNA-Seq
make this assumption even though 0 does not represent a missing value in
that context. It also means that many methods cannot deal with missing data
which is not coded as 0.
</p>

<p>
Here, we instead evaluate methods on their ability to generalize to new
data. We use real data, assuming:
</p>

<p>
\[ x_{ij} \sim \mathrm{Poisson}(R_i \lambda_{ij}) \]
</p>

<p>
\[ \lambda_{ij} = (\mathbf{L F'})_{ij} \]
</p>

<p>
and hold out molecules by randomly thinning the observed counts:
</p>

<p>
\[ y_{ij} \sim \mathrm{Binomial}(x_{ij}, 0.5) \]
</p>

<p>
\[ \tilde{y}_{ij} = x_{ij} - y_{ij} \]
</p>

<p>
This approach leaves the relative abundance of the transcripts unchanged in
expectation, implying that the low rank structure learned in \(\mathbf{Y}\)
should explain the data in \(\tilde{\mathbf{Y}}\).
</p>

<p>
Our metric is then the likelihood of the held-out data. We simply need to
re-scale to account for different size factors \(R_i\).
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="orgb77a649"><span class="org-keyword">def</span> <span class="org-function-name">pois_llik</span>(lam, train, test):
  <span class="org-variable-name">lam</span> *= test.<span class="org-builtin">sum</span>(axis=1, keepdims=<span class="org-constant">True</span>) / train.<span class="org-builtin">sum</span>(axis=1, keepdims=<span class="org-constant">True</span>)
  <span class="org-keyword">return</span> st.poisson(mu=lam).logpmf(test).<span class="org-builtin">sum</span>()

<span class="org-keyword">def</span> <span class="org-function-name">generalization_score_mean</span>(train, test):
  <span class="org-variable-name">lam</span> = np.ones(train.shape) * train.mean(axis=0, keepdims=<span class="org-constant">True</span>)
  <span class="org-keyword">return</span> pois_llik(lam, train, test)

<span class="org-keyword">def</span> <span class="org-function-name">rank_select</span>(train, imputation_score, max_rank, holdout=0.1):
  <span class="org-doc">"""Search for the rank which minimizes imputation loss"""</span>
  <span class="org-variable-name">obj</span> = np.inf
  <span class="org-variable-name">opt</span> = 1
  <span class="org-keyword">for</span> rank <span class="org-keyword">in</span> <span class="org-builtin">range</span>(1, 100):
    <span class="org-comment-delimiter"># </span><span class="org-comment">Randomly hold out 10% of entries to impute</span>
    <span class="org-variable-name">x</span> = np.ma.masked_array(train, np.random.uniform(size=train.shape) &lt; holdout)
    <span class="org-variable-name">res</span> = imputation_score(x, rank)[1]
    <span class="org-keyword">if</span> res &lt; obj:
      <span class="org-variable-name">opt</span> = rank
      <span class="org-variable-name">obj</span> = res
    <span class="org-keyword">else</span>:
      <span class="org-keyword">break</span>
  <span class="org-keyword">return</span> opt

<span class="org-keyword">def</span> <span class="org-function-name">generalization_score_plra</span>(train, test):
  <span class="org-variable-name">opt_rank</span> = rank_select(train, imputation_score_pois_lra, max_rank=10)
  <span class="org-variable-name">lam</span> = np.exp(wlra.pois_lra(train, opt_rank))
  <span class="org-keyword">return</span> pois_llik(lam, train, test)

<span class="org-keyword">def</span> <span class="org-function-name">generalization_score_nmf</span>(train, test):
  <span class="org-variable-name">opt_rank</span> = rank_select(train, imputation_score_nmf, max_rank=100)
  <span class="org-variable-name">lam</span> = nmf(train, opt_rank)
  <span class="org-keyword">return</span> pois_llik(lam, train, test)

<span class="org-keyword">def</span> <span class="org-function-name">generalization_score_hpf</span>(train, test, rank):
  <span class="org-keyword">import</span> scHPF.preprocessing
  <span class="org-keyword">import</span> scHPF.train
  <span class="org-keyword">import</span> tempfile
  <span class="org-keyword">with</span> tempfile.TemporaryDirectory(prefix=<span class="org-string">'/scratch/midway2/aksarkar/ideas/'</span>) <span class="org-keyword">as</span> d:
    <span class="org-comment-delimiter"># </span><span class="org-comment">scHPF assumes genes x cells</span>
    scHPF.preprocessing.split_dataset_hpf(train.T, outdir=d)
    <span class="org-variable-name">opt</span> = scHPF.train.run_trials(
      indir=d, outdir=d, prefix=<span class="org-string">''</span>,
      nfactors=rank, a=0.3, ap=1, bp=1, c=0.3, cp=1, dp=1,
      <span class="org-comment-delimiter"># </span><span class="org-comment">This is broken when we call the API directly</span>
      logging_options={<span class="org-string">'log_phi'</span>: <span class="org-constant">False</span>})
    <span class="org-variable-name">L</span> = np.load(f<span class="org-string">'{opt}/beta_invrate.npy'</span>) * np.load(f<span class="org-string">'{opt}/beta_shape.npy'</span>)
    <span class="org-variable-name">F</span> = np.load(f<span class="org-string">'{opt}/theta_invrate.npy'</span>) * np.load(f<span class="org-string">'{opt}/theta_shape.npy'</span>)
    <span class="org-keyword">return</span> pois_llik(L.dot(F.T), train, test)

<span class="org-keyword">def</span> <span class="org-function-name">generalization_score_scvi</span>(train, test):
  <span class="org-keyword">from</span> scvi.dataset <span class="org-keyword">import</span> GeneExpressionDataset
  <span class="org-keyword">from</span> scvi.inference <span class="org-keyword">import</span> UnsupervisedTrainer
  <span class="org-keyword">from</span> scvi.models <span class="org-keyword">import</span> VAE
  <span class="org-variable-name">data</span> = GeneExpressionDataset(*GeneExpressionDataset.get_attributes_from_matrix(train))
  <span class="org-variable-name">vae</span> = VAE(n_input=train.shape[1])
  <span class="org-variable-name">m</span> = UnsupervisedTrainer(vae, data, verbose=<span class="org-constant">False</span>)
  m.train()
  <span class="org-comment-delimiter"># </span><span class="org-comment">Training permuted the data for minibatching. Unpermute before "imputing"</span>
  <span class="org-comment-delimiter"># </span><span class="org-comment">(estimating lambda)</span>
  <span class="org-variable-name">lam</span> = np.vstack([m.train_set.sequential().imputation(),
                   m.test_set.sequential().imputation()])
  <span class="org-keyword">return</span> pois_llik(lam, train, test)

<span class="org-keyword">def</span> <span class="org-function-name">generalization_score_dca</span>(train, test):
  <span class="org-keyword">import</span> anndata
  <span class="org-keyword">import</span> scanpy.api
  <span class="org-variable-name">data</span> = anndata.AnnData(X=train)
  <span class="org-comment-delimiter"># </span><span class="org-comment">"Denoising" is estimating lambda</span>
  scanpy.api.pp.dca(data, mode=<span class="org-string">'denoise'</span>)
  <span class="org-variable-name">lam</span> = data.X
  <span class="org-keyword">return</span> pois_llik(lam, train, test)

<span class="org-keyword">def</span> <span class="org-function-name">train_test_split</span>(x, p=0.5):
  <span class="org-variable-name">train</span> = np.random.binomial(n=x, p=p, size=x.shape)
  <span class="org-variable-name">test</span> = x - train
  <span class="org-keyword">return</span> train, test

<span class="org-keyword">def</span> <span class="org-function-name">read_ipsc</span>():
  <span class="org-variable-name">keep_samples</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/quality-single-cells.txt'</span>, index_col=0, header=<span class="org-constant">None</span>)
  <span class="org-variable-name">keep_genes</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/genes-pass-filter.txt'</span>, index_col=0, header=<span class="org-constant">None</span>)
  <span class="org-variable-name">x</span> = (pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-counts.txt.gz'</span>, index_col=0)
       .loc[keep_genes.values.ravel(),keep_samples.values.ravel()]
       .values.T)
  <span class="org-keyword">return</span> x

<span class="org-keyword">def</span> <span class="org-function-name">evaluate_generalization</span>(data, num_trials):
  <span class="org-variable-name">result</span> = []
  <span class="org-variable-name">x</span> = data()
  <span class="org-keyword">for</span> trial <span class="org-keyword">in</span> <span class="org-builtin">range</span>(num_trials):
    <span class="org-variable-name">train</span>, <span class="org-variable-name">test</span> = train_test_split(x)
    result.append([trial,
                   generalization_score_mean(train, test),
                   generalization_score_nmf(train, test),
                   generalization_score_hpf(train, test, rank=50),
                   <span class="org-comment-delimiter"># </span><span class="org-comment">generalization_score_plra(train, test),</span>
                   generalization_score_scvi(train, test)
    ])
  <span class="org-variable-name">result</span> = pd.DataFrame(result)
  <span class="org-variable-name">result.columns</span> = [<span class="org-string">'trial'</span>, <span class="org-string">'Mean'</span>, <span class="org-string">'NMF'</span>, <span class="org-string">'HBPF'</span>, <span class="org-string">'scVI'</span>]
  <span class="org-keyword">return</span> result
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">&lt;&lt;imports&gt;&gt;
&lt;&lt;poisson-imputation&gt;&gt;
&lt;&lt;generalization&gt;&gt;
<span class="org-variable-name">res</span> = evaluate_generalization(read_ipsc, 1)
res.to_csv(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-ideas/data/wlra/pois-generalization.txt.gz'</span>, sep=<span class="org-string">'\t'</span>, compression=<span class="org-string">'gzip'</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-sh"><span class="org-comment-delimiter"># </span><span class="org-comment">sbatch --partition=gpu2 --gres=gpu:1 --job-name=pois-generalization --mem=16G --time=60:00 -c4</span>
sbatch --partition=broadwl --job-name=pois-generalization --mem=16G --time=60:00 -n1 -c28 --exclusive
<span class="org-comment-delimiter">#</span><span class="org-comment">!/bin/bash</span>
<span class="org-builtin">source</span> activate singlecell
python /project2/mstephens/aksarkar/projects/singlecell-ideas/code/pois-generalization.py
</pre>
</div>

<pre class="example">
Submitted batch job 50610089

</pre>

<div class="org-src-container">
<pre class="src src-sh">sacct -j 50550386 -o Elapsed
</pre>
</div>

<pre class="example">
   Elapsed 
---------- 
  00:04:49 
  00:04:49 
  00:04:49 

</pre>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Abhishek Sarkar</p>
<p class="date">Created: 2018-10-09 Tue 16:04</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
