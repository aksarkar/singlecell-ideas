#+TITLE: Single cell QTL mapping via sparse multiple regression
#+SETUPFILE: setup.org

* Introduction

  [[https://dx.doi.org/10.1371/journal.pgen.1008045][Sarkar et al. 2019]]
  discovered mean and variance effect QTLs using a modular approach, first
  fitting the model \( 
  \DeclareMathOperator\Gam{Gamma}
  \DeclareMathOperator\N{\mathcal{N}}
  \DeclareMathOperator\Poi{Poisson}
  \DeclareMathOperator\argmin{arg min}
  \newcommand\mf{\mathbf{F}}
  \newcommand\mg{\mathbf{G}}
  \newcommand\ml{\mathbf{L}}
  \newcommand\mx{\mathbf{X}}
  \newcommand\vb{\mathbf{b}}
  \newcommand\vl{\mathbf{l}}
  \newcommand\vx{\mathbf{x}}
  \)

  \begin{align}
    x_{ij} \mid x_{i+}, \lambda_{ij} &\sim \Poi(x_{i+} \lambda_{ij})\\
    \lambda_{ij} \mid \mu_{ij}, \phi_{ij}, \pi_{ij} &\sim \pi_{ij} \delta_0(\cdot) + (1 - \pi_{ij}) \Gam(\phi_{ij}^{-1}, \mu_{ij}^{-1} \phi_{ij}^{-1})\\
    \ln \mu_{ij} &= (\ml \mf_\mu')_{ij}\\
    \ln \phi_{ij} &= (\ml \mf_\phi')_{ij}\\
    \operatorname{logit} \pi_{ij} &= (\ml \mf_\pi')_{ij},
  \end{align}

  where 

  - \(x_{ij}\) is the number of molecules of gene \(j = 1, \ldots, p\) observed
    in cell \(i = 1, \ldots, n\)
  - \(x_{i+} \triangleq \sum_j x_{ij}\) is the total number of molecules
    observed in sample \(i\)
  - cells are taken from \(m\) donor individuals, \(\ml\) is \(n \times m\),
    and each \(\mf_{(\cdot)}\) is \(p \times m\)
  - assignments of cells to donors (loadings) \(l_{ik} \in \{0, 1\}, k = 1,
    \ldots, m\) are known and fixed.

  and then using a standard QTL mapping approach (e.g.,
  [[https://dx.doi.org/10.1038/nature10808][Degner et al. 2012]],
  [[https://dx.doi.org/10.1126/science.1242429][McVicker et al. 2013]]) to
  discover genetic effects on gene expression mean \(E[\lambda_{ij}]\),
  dispersion \(\phi_{ij}\), and variance \(V[\lambda_{ij}]\). Critically, these
  quantities describe the distribution of latent gene expression, removing the
  effect of measurement noise
  ([[https://dx.doi.org/10.1101/2020.04.07.030007][Sarkar and Stephens 2020]]).

  This approach lost substantial power to detect effect on mean gene expression
  compared to bulk RNA-seq on the same (number of) samples
  ([[https://dx.doi.org/10.1101/gr.224436.117][Banovich et al. 2018]]). One
  approach which might increase power would be to incorporate genotype into the
  expression model for gene \(j\)

  \begin{align}
    \lambda_{i} &= \Gam(\phi_{i}^{-1}, \mu_{i}^{-1} \phi_{i}^{-1})\\
    \ln\mu_{i} &= \ml\mg\vb_{\mu} + \vb_{\mu 0}\\
    \ln\phi_{i} &= \ml\mg\vb_{\phi} + \vb_{\phi 0},
  \end{align}

  where \(\mg\) denotes genotype (\(m \times s\)). This approach would increase
  power by pooling samples across individuals with same (similar) genotypes,
  and is similar to previously proposed approaches (e.g.,
  [[https://www.genetics.org/content/188/2/435][Rönnegård and Valdar 2011]],
  [[https://bmcgenet.biomedcentral.com/articles/10.1186/1471-2156-13-63][Rönnegård
  and Valdar 2012]]). To fit this model for \(s > m\), we can further assume a
  sparsity-inducing prior

  \begin{align}
    \vb_{\mu} &\sim \pi_{\mu} \delta_0(\cdot) + (1 - \pi_{\mu}) \N(0, \sigma_{\mu}^2)\\
    \vb_{\phi} &\sim \pi_{\phi} \delta_0(\cdot) + (1 - \pi_{\phi}) \N(0, \sigma_{\phi}^2).
  \end{align}

  The prior is non-conjugate to the likelihood, so we perform variational
  inference (Blei et al. 2017), optimizing a stochastic objective
  ([[https://arxiv.org/abs/1312.6114][Kingma and Welling 2014]],
  [[https://arxiv.org/abs/1401.4082][Rezende et al. 2014]],
  [[http://proceedings.mlr.press/v32/titsias14.html][Titsias and
  Lázaro-Gredilla 2014]],
  [[http://proceedings.mlr.press/v33/ranganath14.html][Ranganath et al. 2014]])
  via gradient descent. We use a local reparameterization
  ([[https://papers.nips.cc/paper/5666-variational-dropout-and-the-local-reparameterization-trick][Kingma
  et al. 2015]]) to speed up convergence
  ([[https://dx.doi.org/10.1101/107623][Park et al. 2017]]).

* Setup
  :PROPERTIES:
  :CUSTOM_ID: setup
  :END:

  #+BEGIN_SRC emacs-lisp :exports none
    (add-to-list 'python-shell-completion-native-disabled-interpreters "jupyter")
    (org-babel-lob-ingest "/home/aksarkar/.emacs.d/org-templates/library.org")
  #+END_SRC

  #+RESULTS:
  : 1

  #+CALL: ipython3(venv="singlecell",partition="mstephens",memory="8G") :exports none :dir /scratch/midway2/aksarkar/singlecell/

  #+RESULTS:
  : Submitted batch job 1009091

  #+BEGIN_SRC ipython
    import anndata
    import numpy as np
    import pandas as pd
    import tabix
    import torch
    import torch.utils.data as td
    import txpred
    import txpred.models.susie
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[12]:
  :END:

  #+BEGIN_SRC ipython
    %matplotlib inline
    %config InlineBackend.figure_formats = set(['retina'])
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[2]:
  :END:

  #+BEGIN_SRC ipython
    import matplotlib.pyplot as plt
    plt.rcParams['figure.facecolor'] = 'w'
    plt.rcParams['font.family'] = 'Nimbus Sans'
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[3]:
  :END:

* Results
** Fine mapping dispersion effects

   The motivation for restricting QTL mapping to 100kb windows was to reduce
   the number of single SNP tests, and therefore increase the power to detect
   associations at a fixed FDR (intuitively, the BH procedure penalizes each
   test based on the total number of tests). Using fast multiple regression
   methods with Bayesian variable selection guarantees, we can simply avoid
   this issue. Read the estimated dispersion parameters.

   #+BEGIN_SRC ipython
     log_phi = pd.read_csv('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design1/zi2-log-phi.txt.gz', index_col=0, sep=' ').T
     log_phi.shape
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[15]:
   : (54, 9957)
   :END:

   #+BEGIN_SRC ipython
     y = log_phi['ENSG00000113558']
     dat = pd.read_csv('/scratch/midway2/aksarkar/singlecell/skp1.vcf.gz', skiprows=2, sep='\t')
     x = dat.filter(like='NA', axis=1).T
     x.columns = dat['POS']
     x = x.sub(x.mean(axis=1), axis=0)
     x = x.div(x.std(axis=1), axis=0)
     x, y = x.align(y, axis=0, join='inner')
     x.shape
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[24]:
   : (54, 5778)
   :END:

   #+BEGIN_SRC ipython
     import rpy2.robjects.packages
     import rpy2.robjects.pandas2ri
     rpy2.robjects.pandas2ri.activate()
     susie = rpy2.robjects.packages.importr('susieR')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[25]:
   :END:

   #+BEGIN_SRC ipython :async t
     fit = susie.susie(x.values, y.values, L=10)
     susie.susie_get_cs(fit, x.values).rx2('cs')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[42]:
   : <rpy2.rinterface.NULLType object at 0x7fb121e3e410> [RTYPES.NILSXP]
   :END:

** /SKP1/

   Read the data.

   #+BEGIN_SRC ipython :async t
     y = anndata.read_h5ad('/project2/mstephens/aksarkar/projects/singlecell-ideas/data/ipsc/ipsc.h5ad')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[47]:
   :END:

   /SKP1/ was the top eQTL discovered in Sarkar et al. 2019. Extract the
   genotypes around the TSS.

   #+BEGIN_SRC ipython
     y.var[y.var['name'] == 'SKP1']
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[6]:
   #+BEGIN_EXAMPLE
     chr      start        end  name strand      source
     index
     ENSG00000113558  hs5  133484633  133512729  SKP1      -  H. sapiens
   #+END_EXAMPLE
   :END:

   #+BEGIN_SRC sh :dir /scratch/midway2/aksarkar/singlecell/
     tabix -h /project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-mapping/yri-120-dosages.vcf.gz chr5:$((133484633-1000000))-$((133484633+1000000)) | gzip >skp1.vcf.gz
   #+END_SRC

   #+RESULTS:

   Read the genotypes, and align it to the individuals with single cell
   measurements.

   #+BEGIN_SRC ipython :async t
     dat = pd.read_csv('/scratch/midway2/aksarkar/singlecell/skp1.vcf.gz', skiprows=2, sep='\t')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[48]:
   :END:

   #+BEGIN_SRC ipython
     L = pd.get_dummies(y.obs['chip_id'])
     x = dat.filter(like='NA', axis=1)
     x.index = dat['POS']
     x = x.sub(x.mean(axis=1), axis=0)
     x = x.div(x.std(axis=1), axis=0)
     L, x = L.align(x, axis=1, join='inner')
     x.T.shape
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[49]:
   : (54, 5778)
   :END:

   Fit the sparse regression model.

   #+BEGIN_SRC ipython :async t
     data = td.DataLoader(
       td.TensorDataset(
         torch.tensor(L.values @ x.values.T, dtype=torch.float),
         torch.tensor(y[:,y.var['name'] == 'SKP1'].X.A, dtype=torch.float),
         torch.tensor(y.obs['mol_hs'].values, dtype=torch.float)),
       batch_size=128, shuffle=True)
     fit = (txpred.models.susie.PoisGamRegression(
       mean_prior=txpred.models.susie.SpikeSlab(x.shape[0], fit_intercept=True),
       inv_disp_prior=txpred.models.susie.SpikeSlab(x.shape[0], fit_intercept=True))
            .fit(data, n_epochs=80, trace=True))
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   :END:

   #+BEGIN_SRC ipython :ipyfile figure/scqtl.org/skp1.png
     plt.clf()
     fig, ax = plt.subplots(2, 1, sharex=True, sharey=True)
     fig.set_size_inches(4, 3)
     grid = np.array(x.index) / 1e6
     pip = torch.sigmoid(fit.mean_prior.logits.detach()).numpy().ravel()
     ax[0].scatter(grid[pip > 1e-2], pip[pip > 1e-2], s=2, c='k')
     for t in grid[pip > .5]:
       for a in ax:
         a.axvline(x=t, lw=.5, c='0.7', zorder=-1)
     ax[0].set_ylabel(r'PIP $\ln(\mu)$')
     pip = torch.sigmoid(fit.inv_disp_prior.logits.detach()).numpy().ravel()
     ax[1].scatter(grid[pip > 1e-2], pip[pip > 1e-2], s=2, c='k')
     for t in grid[pip > .5]:
       for a in ax:
         a.axvline(x=t, lw=.5, c='0.7', zorder=-1)
     ax[1].set_xlabel('Position (MB)')
     ax[1].set_ylabel(r'PIP $\ln(\phi)$')
     t = y.var.loc[y.var['name'] == 'SKP1', 'start'][0] / 1e6
     for a in ax:
       a.axvline(x=t + .1, c='b', lw=.5, zorder=3)
       a.axvline(x=t - .1, c='b', lw=.5, zorder=3)
     ax[0].set_title('SKP1')
     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[94]:
   [[file:figure/scqtl.org/skp1.png]]
   :END:
