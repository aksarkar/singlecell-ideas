<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2021-01-26 Tue 13:37 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Normalizing flows for EBPM</title>
<meta name="generator" content="Org mode">
<meta name="author" content="Abhishek Sarkar">
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link href="bootstrap.min.css" rel="stylesheet" />
<link rel="stylesheet" type="text/css" href="htmlize.css"/>
<link rel="stylesheet" type="text/css" href="main.css"/>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2017 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Normalizing flows for EBPM</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org788bee6">Introduction</a></li>
<li><a href="#setup">Setup</a></li>
<li><a href="#orgeb9df02">Methods</a>
<ul>
<li><a href="#org799ab3c">Normalizing flow for density estimation</a></li>
<li><a href="#orgef00a68">Normalizing flow for empirical Bayes</a></li>
</ul>
</li>
<li><a href="#org26165be">Results</a>
<ul>
<li><a href="#org74ca7c6">Example of normalizing flow</a></li>
<li><a href="#orga548a6a">Example of Empirical Bayes Normal Means</a></li>
<li><a href="#orgd0277f9">Example of Empirical Bayes Poisson Means</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-org788bee6" class="outline-2">
<h2 id="org788bee6">Introduction</h2>
<div class="outline-text-2" id="text-org788bee6">
<p>
Fitting an expression model to observed scRNA-seq data at a single gene can
be thought of as solving an empirical Bayes problem (Sarkar and Stephens
2020). \(
  \DeclareMathOperator\Pois{Poisson}
  \DeclareMathOperator\Gam{Gamma}
  \DeclareMathOperator\E{E}
  \DeclareMathOperator\N{\mathcal{N}}
  \newcommand\abs[1]{\left\vert #1 \right\vert}
  \)
</p>

\begin{align}
  x_i \mid s_i, \lambda_i &\sim \Pois(s_i \lambda_i)\\
  \lambda_i &\sim g(\cdot) \in \mathcal{G},
\end{align}

<p>
where \(i = 1, \ldots, n\) indexes samples. Assuming \(\mathcal{G}\) is the
family of Gamma distributions yields analytic gradients and admits
<a href="mpebpm.html">fast implementation on GPUs</a>. However, the fitted model
can fail to accurately describe expression variation at some genes.
</p>

<p>
In contrast, the family of non-parametric unimodal distributions
(Stephens 2016) could be sufficient for all but a minority of genes. In
practice, this family is approximated as the family of mixture of uniform
distributions with fixed endpoints, including the common mode. Then,
inference in this model can be achieved by a combination of convex
optimization (over mixture weights, given the mode) and line search (over the
mode, as an outer loop). However, in practice this approach is expensive and
cumbersome to parallelize for large data sets.
</p>

<p>
One idea which could bridge the gap between these approaches (in both
computational cost and flexibility) is <i>normalizing flows</i> (reviewed in
<a href="https://arxiv.org/abs/1912.02762">Papamakarios et al. 2019</a>). The key idea
of normalizing flows is to apply a series of invertible, differentiable
transformations \(T_1, \ldots, T_K\) to a tractable density, in order to
obtain a different density
</p>

\begin{align}
  x &= (T_K \circ \cdots \circ T_1)(u)\\
  f_x(x) &= f_u(u) \prod_{k=1}^{K} \det \abs{J_k(\cdot)},
\end{align}

<p>
where \(J_k\) is the Jacobian of \(T_k^{-1}\). If the \(T_k\) have free
parameters, gradients with respect to those parameters are available,
allowing the transformations to be learned from the data. Here, we
investigate using flows to define a flexible family of priors, and use that
family to fit expression models to scRNA-seq data.
</p>
</div>
</div>

<div id="outline-container-orgc920dae" class="outline-2">
<h2 id="setup"><a id="orgc920dae"></a>Setup</h2>
<div class="outline-text-2" id="text-setup">
<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> numpy <span class="org-keyword">as</span> np
<span class="org-keyword">import</span> pandas <span class="org-keyword">as</span> pd
<span class="org-keyword">import</span> scipy.stats <span class="org-keyword">as</span> st
<span class="org-keyword">import</span> torch
<span class="org-keyword">import</span> torch.utils.tensorboard <span class="org-keyword">as</span> tb
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">%matplotlib inline
%config <span class="org-variable-name">InlineBackend.figure_formats</span> = <span class="org-builtin">set</span>([<span class="org-string">'retina'</span>])
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> matplotlib.pyplot <span class="org-keyword">as</span> plt
<span class="org-variable-name">plt.rcParams</span>[<span class="org-string">'figure.facecolor'</span>] = <span class="org-string">'w'</span>
<span class="org-variable-name">plt.rcParams</span>[<span class="org-string">'font.family'</span>] = <span class="org-string">'Nimbus Sans'</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-orgeb9df02" class="outline-2">
<h2 id="orgeb9df02">Methods</h2>
<div class="outline-text-2" id="text-orgeb9df02">
</div>
<div id="outline-container-org799ab3c" class="outline-3">
<h3 id="org799ab3c">Normalizing flow for density estimation</h3>
<div class="outline-text-3" id="text-org799ab3c">
<p>
Suppose we have observations \(x_1, \ldots, x_n\) drawn from \(f^*\). One
can estimate \(f_x\) by maximizing the likelihood of the data
</p>

\begin{align}
  &\max_{f_x} \E_{f^*}[\ln f_x(x)]\\
  = &\max_{T_1, \ldots, T_K} \E_{f^*}\left[\ln f_u(T(x)) + \sum_{k=1}^K \ln\det J_k(\cdot)\right]\\
  = &\max_{T_1, \ldots, T_K} \frac{1}{n} \sum_i\left[\ln f_u(T(x_i)) + \sum_{k=1}^K \ln\det J_k(\cdot)\right],
\end{align}

<p>
where \(T = T_K \circ \cdots \circ T_1\) is the mapping from \(x \in
   \mathcal{X} \rightarrow u \in \mathcal{U}\), and \(f_u\) is the density of
some simple distribution (e.g., standard Gaussian). This optimization
problem can be readily solved using automatic differentiation and gradient
descent.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">class</span> <span class="org-type">PlanarFlow</span>(torch.nn.Module):
  <span class="org-comment-delimiter"># </span><span class="org-comment">Rezende and Mohamed 2015</span>
  <span class="org-keyword">def</span> <span class="org-function-name">__init__</span>(<span class="org-keyword">self</span>, n_features):
    <span class="org-builtin">super</span>().__init__()
    <span class="org-keyword">self</span>.weight = torch.nn.Parameter(torch.zeros([n_features, 1]))
    torch.nn.init.xavier_normal_(<span class="org-keyword">self</span>.weight)
    <span class="org-keyword">self</span>.bias = torch.nn.Parameter(torch.zeros([1]))
    <span class="org-keyword">self</span>.post_act = torch.nn.Parameter(torch.zeros([n_features, 1]))
    torch.nn.init.xavier_normal_(<span class="org-keyword">self</span>.post_act)

  <span class="org-keyword">def</span> <span class="org-function-name">forward</span>(<span class="org-keyword">self</span>, x):
    <span class="org-comment-delimiter"># </span><span class="org-comment">x is [batch_size, n_features]</span>
    <span class="org-variable-name">pre_act</span> = x @ <span class="org-keyword">self</span>.weight + <span class="org-keyword">self</span>.bias
    <span class="org-comment-delimiter"># </span><span class="org-comment">This is required to invert the flow</span>
    <span class="org-variable-name">post_act</span> = <span class="org-keyword">self</span>.post_act + <span class="org-keyword">self</span>.weight / (<span class="org-keyword">self</span>.weight.T @ <span class="org-keyword">self</span>.weight) * (-1 + torch.nn.functional.softplus(<span class="org-keyword">self</span>.weight.T @ <span class="org-keyword">self</span>.post_act) - <span class="org-keyword">self</span>.weight.T @ <span class="org-keyword">self</span>.post_act)
    <span class="org-variable-name">out</span> = x + torch.sigmoid(pre_act) @ post_act.T
    <span class="org-variable-name">log_det</span> = torch.log(torch.<span class="org-builtin">abs</span>(1 + torch.sigmoid(pre_act) * torch.sigmoid(-pre_act) @ <span class="org-keyword">self</span>.weight.T @ post_act))
    <span class="org-keyword">assert</span> <span class="org-keyword">not</span> torch.isnan(log_det).<span class="org-builtin">any</span>()
    <span class="org-keyword">return</span> out, log_det

<span class="org-keyword">class</span> <span class="org-type">NormalizingFlow</span>(torch.nn.Module):
  <span class="org-comment-delimiter"># </span><span class="org-comment">https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#Sequential</span>
  <span class="org-keyword">def</span> <span class="org-function-name">__init__</span>(<span class="org-keyword">self</span>, flows):
    <span class="org-builtin">super</span>().__init__()
    <span class="org-keyword">self</span>.flows = torch.nn.ModuleList(flows)

  <span class="org-keyword">def</span> <span class="org-function-name">forward</span>(<span class="org-keyword">self</span>, x):
    <span class="org-variable-name">log_det</span> = torch.zeros(x.shape)
    <span class="org-keyword">for</span> f <span class="org-keyword">in</span> <span class="org-keyword">self</span>.flows:
      <span class="org-variable-name">x</span>, <span class="org-variable-name">l</span> = f.forward(x)
      <span class="org-variable-name">log_det</span> += l
    <span class="org-keyword">return</span> x, log_det

<span class="org-keyword">class</span> <span class="org-type">DensityEstimator</span>(torch.nn.Module):
  <span class="org-keyword">def</span> <span class="org-function-name">__init__</span>(<span class="org-keyword">self</span>, n_features, K):
    <span class="org-builtin">super</span>().__init__()
    <span class="org-comment-delimiter"># </span><span class="org-comment">Important: here the flow maps x in ambient measure to u in base measure</span>
    <span class="org-keyword">self</span>.flow = NormalizingFlow([PlanarFlow(n_features) <span class="org-keyword">for</span> _ <span class="org-keyword">in</span> <span class="org-builtin">range</span>(K)])

  <span class="org-keyword">def</span> <span class="org-function-name">forward</span>(<span class="org-keyword">self</span>, x):
    <span class="org-variable-name">loss</span> = -<span class="org-keyword">self</span>.log_prob(x).mean()
    <span class="org-keyword">assert</span> loss &gt; 0
    <span class="org-keyword">return</span> loss

  <span class="org-keyword">def</span> <span class="org-function-name">fit</span>(<span class="org-keyword">self</span>, x, n_epochs, log_dir=<span class="org-constant">None</span>, **kwargs):
    <span class="org-keyword">if</span> log_dir <span class="org-keyword">is</span> <span class="org-keyword">not</span> <span class="org-constant">None</span>:
      <span class="org-variable-name">writer</span> = tb.SummaryWriter(log_dir)
    <span class="org-variable-name">opt</span> = torch.optim.RMSprop(<span class="org-keyword">self</span>.parameters(), **kwargs)
    <span class="org-variable-name">global_step</span> = 0
    <span class="org-keyword">for</span> _ <span class="org-keyword">in</span> <span class="org-builtin">range</span>(n_epochs):
      opt.zero_grad()
      <span class="org-variable-name">loss</span> = <span class="org-keyword">self</span>.forward(x)
      <span class="org-keyword">if</span> log_dir <span class="org-keyword">is</span> <span class="org-keyword">not</span> <span class="org-constant">None</span>:
        writer.add_scalar(<span class="org-string">'loss'</span>, loss, global_step)
      <span class="org-keyword">if</span> torch.isnan(loss):
        <span class="org-keyword">raise</span> <span class="org-type">RuntimeError</span>
      loss.backward()
      opt.step()
      <span class="org-variable-name">global_step</span> += 1
    <span class="org-keyword">return</span> <span class="org-keyword">self</span>

  <span class="org-keyword">def</span> <span class="org-function-name">log_prob</span>(<span class="org-keyword">self</span>, x):
    <span class="org-variable-name">u</span>, <span class="org-variable-name">log_det</span> = <span class="org-keyword">self</span>.flow.forward(x)
    <span class="org-variable-name">l</span> = torch.distributions.Normal(loc=0., scale=1.).log_prob(u) + log_det
    <span class="org-keyword">return</span> l
</pre>
</div>
</div>
</div>

<div id="outline-container-orgef00a68" class="outline-3">
<h3 id="orgef00a68">Normalizing flow for empirical Bayes</h3>
<div class="outline-text-3" id="text-orgef00a68">
<p>
Now consider the EBPM problem
</p>

\begin{align}
  x_i \mid s_i, \lambda_i &\sim \Pois(s_i \lambda_i)\\
  \lambda_i &\sim g(\cdot) = g_0(\cdot) \prod_k \det \abs{J^g_k}
\end{align}

<p>
where \(i = 1, \ldots, n\), and \(g_0(\cdot) = \N(\cdot; 0, 1)\) for
simplicity. One can estimate \(g\) by maximizing the marginal likelihood
</p>

\begin{align}
   &\max_g \sum_i \ln p(x_i \mid s_i, g)\\
   \geq &\max_{g, q} \E_{\lambda_i \sim q}\left[\sum_i \ln p(x_i | s_i, \lambda_i) + \ln g(\lambda_i) - \ln q(\lambda_i)\right]\\
   = &\max_{T_g, q} \E_{\lambda_i \sim q}\left[\sum_i \ln p(x_i | s_i, \lambda_i) + \ln g_0(T_g(\lambda_i)) + \sum_k \ln\det\abs{J^g_k} - \ln q(\lambda_i)\right]\\
\end{align}

<p>
where \(T_g = T^g_K \circ \cdots \circ T^g_1\) maps \(g\) to a base measure
and \(J^g_k\) denotes the Jacobian of \(T^g_k\). It is straightforward to
show that, holding \(g\) fixed, the optimal \(q\) is the true posterior
\(p(\lambda_i \mid x_i, s_i, g)\) (e.g., Neal and Hinton 1998). In order to
ensure \(q\) is flexible enough to capture the true posterior, suppose it
too is represened by a normalizing flow
</p>

\begin{equation}
  q(\cdot) = q_0(\cdot) \prod_k \det\abs{J^q_k},
\end{equation}

<p>
where \(J^q_k\) denotes the Jacobian of the transform \(T^q_k\). In order to
make sampling easy, suppose \(T_q = T^q_K \circ \cdots \circ T^q_1\) maps
the base measure \(q_0(u_i \mid x_i)\) to \(q\). Then, the optimization
problem is
</p>

\begin{equation}
  \max_{T_g, T_q} \E_{u_i \sim q_0}\left[\sum_i \ln p(x_i | s_i, T_q(u_i)) + \ln g_0(T_g(T_q(u_i))) + \sum_k \ln\det\abs{J^g_k} - \ln q_0(u_i) + \sum_k \ln\det\abs{J^q_k}\right].
\end{equation}

<p>
<i>Remark</i> It is critical that \(u_i \sim q_0\) depends on \(x_i\) in the
variational approximation. Rezende and Mohamed 2015 propose using amortized
inference; however, in the context of this problem, a simpler alternative
could be a log-Gamma posterior.
</p>

<p>
<i>Remark</i> Since the transformation \(T_q\) maps \(u_i \in \mathcal{U}\) to
\(\lambda_i \in \Lambda\), the sign of the determinants of the Jacobian
needs to be inverted.
</p>

<p>
Since \(T_g, T_q\) are differentiable, this problem can be solved by
replacing the expectation with a Monte Carlo integral (e.g., Kingma and
Welling 2014), and then using automatic differentiation and gradient descent
to optimize the resulting stochastic objective.
</p>

<p>
<i>Remark</i> When reducing problems in scRNA-seq data analysis to EBPM, we are
primarily interested in the estimated prior \(\hat{g}\). Depending on the
choice of flow, obtaining expectations with respect to \(\hat{g}\) might be
difficult. One possibility is to approximate these expectations by
discretizing \(\hat{g}\) and taking weighted sums.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">class</span> <span class="org-type">EBNM</span>(torch.nn.Module):
  <span class="org-keyword">def</span> <span class="org-function-name">__init__</span>(<span class="org-keyword">self</span>, K, scale):
    <span class="org-builtin">super</span>().__init__()
    <span class="org-keyword">self</span>.scale = scale
    <span class="org-keyword">self</span>.p0 = torch.distributions.Normal(loc=0., scale=1.)
    <span class="org-keyword">self</span>.pz = NormalizingFlow([PlanarFlow(1) <span class="org-keyword">for</span> _ <span class="org-keyword">in</span> <span class="org-builtin">range</span>(K)])
    <span class="org-keyword">self</span>.qz = NormalizingFlow([PlanarFlow(1) <span class="org-keyword">for</span> _ <span class="org-keyword">in</span> <span class="org-builtin">range</span>(K)])

  <span class="org-keyword">def</span> <span class="org-function-name">forward</span>(<span class="org-keyword">self</span>, x, n_samples):
    <span class="org-variable-name">q0</span> = torch.distributions.Normal(
      loc=x / (1 + <span class="org-keyword">self</span>.scale ** 2),
      scale=torch.sqrt(1 / (1 + <span class="org-keyword">self</span>.scale ** 2)))
    <span class="org-variable-name">u</span> = q0.rsample([1]).squeeze(-1)
    <span class="org-variable-name">z</span>, <span class="org-variable-name">log_det_q</span> = <span class="org-keyword">self</span>.qz.forward(u.T)
    <span class="org-variable-name">w</span>, <span class="org-variable-name">log_det_p</span> = <span class="org-keyword">self</span>.pz.forward(z)
    <span class="org-comment-delimiter"># </span><span class="org-comment">Important: qz is forward transforms, so we need to invert the sign of</span>
    <span class="org-comment-delimiter"># </span><span class="org-comment">log_det_q</span>
    <span class="org-variable-name">elbo</span> = (torch.distributions.Normal(z, <span class="org-keyword">self</span>.scale).log_prob(x)
            + <span class="org-keyword">self</span>.p0.log_prob(w) + log_det_p
            - (q0.log_prob(u.T) - log_det_q)).<span class="org-builtin">sum</span>()
    <span class="org-keyword">assert</span> elbo &lt;= 0
    <span class="org-keyword">return</span> -elbo

  <span class="org-keyword">def</span> <span class="org-function-name">fit</span>(<span class="org-keyword">self</span>, x, n_epochs, n_samples=1, log_dir=<span class="org-constant">None</span>, **kwargs):
    <span class="org-keyword">if</span> log_dir <span class="org-keyword">is</span> <span class="org-keyword">not</span> <span class="org-constant">None</span>:
      <span class="org-variable-name">writer</span> = tb.SummaryWriter(log_dir)
    <span class="org-variable-name">n_samples</span> = torch.Size([n_samples])
    <span class="org-variable-name">opt</span> = torch.optim.RMSprop(<span class="org-keyword">self</span>.parameters(), **kwargs)
    <span class="org-variable-name">global_step</span> = 0
    <span class="org-keyword">for</span> _ <span class="org-keyword">in</span> <span class="org-builtin">range</span>(n_epochs):
      opt.zero_grad()
      <span class="org-variable-name">loss</span> = <span class="org-keyword">self</span>.forward(x, n_samples)
      <span class="org-keyword">if</span> log_dir <span class="org-keyword">is</span> <span class="org-keyword">not</span> <span class="org-constant">None</span>:
        writer.add_scalar(<span class="org-string">'loss'</span>, loss, global_step)
      <span class="org-keyword">if</span> torch.isnan(loss):
        <span class="org-keyword">raise</span> <span class="org-type">RuntimeError</span>
      loss.backward()
      opt.step()
      <span class="org-variable-name">global_step</span> += 1
    <span class="org-keyword">return</span> <span class="org-keyword">self</span>

  <span class="org-type">@torch.no_grad</span>()
  <span class="org-keyword">def</span> <span class="org-function-name">fitted_g</span>(<span class="org-keyword">self</span>, z, log=<span class="org-constant">True</span>):
    <span class="org-variable-name">u</span>, <span class="org-variable-name">log_det</span> = <span class="org-keyword">self</span>.pz.forward(z)
    <span class="org-variable-name">log_prob</span> = <span class="org-keyword">self</span>.p0.log_prob(u) + log_det
    <span class="org-keyword">if</span> log:
      <span class="org-keyword">return</span> log_prob.numpy()
    <span class="org-keyword">else</span>:
      <span class="org-keyword">return</span> torch.exp(log_prob).numpy()
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">class</span> <span class="org-type">EBPM</span>(torch.nn.Module):
  <span class="org-keyword">def</span> <span class="org-function-name">__init__</span>(<span class="org-keyword">self</span>, K):
    <span class="org-builtin">super</span>().__init__()
    <span class="org-keyword">self</span>.pz = NormalizingFlow([PlanarFlow(1) <span class="org-keyword">for</span> _ <span class="org-keyword">in</span> <span class="org-builtin">range</span>(K)])
    <span class="org-keyword">self</span>.qz = NormalizingFlow([PlanarFlow(1) <span class="org-keyword">for</span> _ <span class="org-keyword">in</span> <span class="org-builtin">range</span>(K)])

  <span class="org-keyword">def</span> <span class="org-function-name">forward</span>(<span class="org-keyword">self</span>, x, s, n_samples):
    <span class="org-variable-name">u</span> = N.rsample([x.shape[0], 1])
    <span class="org-variable-name">log_lam</span>, <span class="org-variable-name">log_det_q</span> = <span class="org-keyword">self</span>.qz.forward(u)
    <span class="org-variable-name">lam</span> = torch.exp(log_lam)
    <span class="org-variable-name">log_det_q</span> += log_lam
    <span class="org-variable-name">w</span>, <span class="org-variable-name">log_det_p</span> = <span class="org-keyword">self</span>.pz.forward(lam)
    <span class="org-comment-delimiter"># </span><span class="org-comment">Important: qz is forward transforms, so we need to invert the sign of</span>
    <span class="org-comment-delimiter"># </span><span class="org-comment">log_det_q</span>
    <span class="org-variable-name">kl</span> = N.log_prob(w) + log_det_p - (N.log_prob(u) - log_det_q)
    <span class="org-keyword">assert</span> (kl &gt; 0).<span class="org-builtin">all</span>()
    <span class="org-variable-name">elbo</span> = (torch.distributions.Poisson(s * lam).log_prob(x) - kl).<span class="org-builtin">sum</span>()
    <span class="org-keyword">assert</span> elbo &lt;= 0
    <span class="org-keyword">return</span> -elbo

  <span class="org-keyword">def</span> <span class="org-function-name">fit</span>(<span class="org-keyword">self</span>, x, s, n_epochs, n_samples=1, log_dir=<span class="org-constant">None</span>, **kwargs):
    <span class="org-keyword">if</span> log_dir <span class="org-keyword">is</span> <span class="org-keyword">not</span> <span class="org-constant">None</span>:
      <span class="org-variable-name">writer</span> = tb.SummaryWriter(log_dir)
    <span class="org-variable-name">n_samples</span> = torch.Size([n_samples])
    <span class="org-variable-name">opt</span> = torch.optim.RMSprop(<span class="org-keyword">self</span>.parameters(), **kwargs)
    <span class="org-variable-name">global_step</span> = 0
    <span class="org-keyword">for</span> _ <span class="org-keyword">in</span> <span class="org-builtin">range</span>(n_epochs):
      opt.zero_grad()
      <span class="org-variable-name">loss</span> = <span class="org-keyword">self</span>.forward(x, s, n_samples)
      <span class="org-keyword">if</span> log_dir <span class="org-keyword">is</span> <span class="org-keyword">not</span> <span class="org-constant">None</span>:
        writer.add_scalar(<span class="org-string">'loss'</span>, loss, global_step)
      <span class="org-keyword">if</span> torch.isnan(loss):
        <span class="org-keyword">raise</span> <span class="org-type">RuntimeError</span>
      loss.backward()
      opt.step()
      <span class="org-variable-name">global_step</span> += 1
    <span class="org-keyword">return</span> <span class="org-keyword">self</span>

  <span class="org-keyword">def</span> <span class="org-function-name">fitted_g</span>(<span class="org-keyword">self</span>, lam):
    <span class="org-variable-name">u</span>, <span class="org-variable-name">log_det</span> = <span class="org-keyword">self</span>.pz.forward(lam)
    <span class="org-keyword">return</span> N.log_prob(u) + log_det

</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-org26165be" class="outline-2">
<h2 id="org26165be">Results</h2>
<div class="outline-text-2" id="text-org26165be">
</div>
<div id="outline-container-org74ca7c6" class="outline-3">
<h3 id="org74ca7c6">Example of normalizing flow</h3>
<div class="outline-text-3" id="text-org74ca7c6">
<p>
Draw data from a scale mixture of Gaussians.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">rng</span> = np.random.default_rng(1)
<span class="org-variable-name">n</span> = 1000
<span class="org-variable-name">pi</span> = np.array([0.3, 0.7])
<span class="org-variable-name">scale</span> = np.array([0.1, 0.4])
<span class="org-variable-name">z</span> = rng.uniform(size=(n, 1)) &lt; pi[0]
<span class="org-variable-name">x</span> = rng.normal(scale=scale @ np.hstack([z, ~z]).T)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Dark2'</span>)
plt.clf()
plt.gcf().set_size_inches(2.5, 2.5)
plt.hist(x, bins=19, density=<span class="org-constant">True</span>, color=<span class="org-string">'0.7'</span>)
<span class="org-variable-name">grid</span> = np.linspace(x.<span class="org-builtin">min</span>(), x.<span class="org-builtin">max</span>(), 5000)
<span class="org-variable-name">mixcdf</span> = st.norm(scale=scale).cdf(grid.reshape(-1, 1)) @ pi
plt.plot(grid[:-1], np.diff(mixcdf) / (grid[1] - grid[0]), lw=1, c=<span class="org-string">'k'</span>)
plt.xlabel(<span class="org-string">'Observation'</span>)
plt.ylabel(<span class="org-string">'Density'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/flows.org/ex0.png" alt="ex0.png">
</p>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">K</span> = 2
<span class="org-variable-name">run</span> = 0
<span class="org-variable-name">lr</span> = 1e-2
<span class="org-variable-name">n_epochs</span> = 4000
torch.manual_seed(run)
<span class="org-variable-name">m</span> = (DensityEstimator(n_features=1, K=K)
     .fit(torch.tensor(x.reshape(-1, 1), dtype=torch.<span class="org-builtin">float</span>),
          n_epochs=n_epochs,
          lr=lr,
          log_dir=f<span class="org-string">'/scratch/midway2/aksarkar/singlecell/runs/nf-{K}-{lr}-{n_epochs}-{run}'</span>))
</pre>
</div>

<p>
Plot the fit.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">grid</span> = np.linspace(x.<span class="org-builtin">min</span>(), x.<span class="org-builtin">max</span>(), 5000)
<span class="org-variable-name">mixcdf</span> = st.norm(scale=scale).cdf(grid.reshape(-1, 1)) @ pi
<span class="org-keyword">with</span> torch.no_grad():
  <span class="org-variable-name">flow</span> = np.exp(m.log_prob(torch.tensor(grid.reshape(-1, 1), dtype=torch.<span class="org-builtin">float</span>)).numpy())

<span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Dark2'</span>)
plt.clf()
plt.gcf().set_size_inches(3.5, 2.5)
plt.hist(x, bins=19, density=<span class="org-constant">True</span>, color=<span class="org-string">'0.7'</span>)
plt.plot(grid[:-1], np.diff(mixcdf) / (grid[1] - grid[0]), lw=1, c=<span class="org-string">'k'</span>, label=<span class="org-string">'Simulated'</span>)
plt.plot(grid, flow, lw=1, c=<span class="org-string">'r'</span>, label=<span class="org-string">'Fit'</span>)
plt.legend(frameon=<span class="org-constant">False</span>, loc=<span class="org-string">'center left'</span>, bbox_to_anchor=(1, .5))
plt.xlabel(<span class="org-string">'Observation'</span>)
plt.ylabel(<span class="org-string">'Density'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/flows.org/ex0-fit.png" alt="ex0-fit.png">
</p>
</div>
</div>
</div>

<div id="outline-container-orga548a6a" class="outline-3">
<h3 id="orga548a6a">Example of Empirical Bayes Normal Means</h3>
<div class="outline-text-3" id="text-orga548a6a">
<p>
For EBNM, a simple choice of \(q_0\) is 
</p>

\begin{equation}
  u_i \mid x_i, s_i^2 \sim \N\left(\frac{1}{1 + s_i^2} x_i, \frac{1}{1 + 1 / s_i^2}\right),
\end{equation}

<p>
which is the exact posterior under the simple model
</p>

\begin{align}
  x_i \mid u_i, s_i^2 &\sim \N(u_i, s_i^2)\\
  u_i &\sim \N(0, 1).
\end{align}

<p>
Draw from a scale mixture of Gaussians convolved with a Gaussian.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">rng</span> = np.random.default_rng(1)
<span class="org-variable-name">n</span> = 1000
<span class="org-variable-name">s</span> = 1.
<span class="org-variable-name">pi</span> = np.array([0.7, 0.3])
<span class="org-variable-name">scale</span> = np.array([0.1, 0.4])
<span class="org-variable-name">z</span> = rng.uniform(size=(n, 1)) &lt; pi[0]
<span class="org-variable-name">mu</span> = rng.normal(scale=scale @ np.hstack([z, ~z]).T)
<span class="org-variable-name">x</span> = rng.normal(mu, 0.05)
</pre>
</div>

<p>
Plot the simulated data.
</p>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(2, 1)
fig.set_size_inches(4, 4)
ax[0].hist(x, bins=27, color=<span class="org-string">'0.7'</span>, density=<span class="org-constant">True</span>)
ax[0].set_xlabel(<span class="org-string">'Observation'</span>)
ax[0].set_ylabel(<span class="org-string">'Density'</span>)
<span class="org-variable-name">grid</span> = np.linspace(mu.<span class="org-builtin">min</span>(), mu.<span class="org-builtin">max</span>(), 1000)
<span class="org-variable-name">mixcdf</span> = st.norm(scale=scale).cdf(grid.reshape(-1, 1)) @ pi
ax[1].hist(mu, bins=31, color=<span class="org-string">'0.7'</span>, density=<span class="org-constant">True</span>)
ax[1].plot(grid[:-1], np.diff(mixcdf) / (grid[1] - grid[0]), lw=1, c=<span class="org-string">'k'</span>)
ax[1].set_xlabel(<span class="org-string">'Latent variable'</span>)
ax[1].set_ylabel(<span class="org-string">'Density'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/flows.org/ebnm-ex.png" alt="ebnm-ex.png">
</p>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">K</span> = 3
<span class="org-variable-name">run</span> = 10
<span class="org-variable-name">lr</span> = 1e-2
<span class="org-variable-name">n_epochs</span> = 1600
torch.manual_seed(run)
<span class="org-variable-name">m</span> = (EBNM(K=K, scale=torch.tensor(0.05))
     .fit(torch.tensor(x.reshape(-1, 1), dtype=torch.<span class="org-builtin">float</span>),
          n_epochs=n_epochs,
          lr=lr,
          log_dir=f<span class="org-string">'/scratch/midway2/aksarkar/singlecell/runs/ebnm-nf-{K}-{lr}-{n_epochs}-{run}'</span>))
</pre>
</div>

<p>
Plot the fitted prior against the true prior.
</p>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
plt.gcf().set_size_inches(3.5, 2.5)
<span class="org-variable-name">grid</span> = np.linspace(mu.<span class="org-builtin">min</span>(), mu.<span class="org-builtin">max</span>(), 1000)
<span class="org-variable-name">mixcdf</span> = st.norm(scale=scale).cdf(grid.reshape(-1, 1)) @ pi
plt.plot(grid[:-1], np.diff(mixcdf) / (grid[1] - grid[0]), lw=1, c=<span class="org-string">'k'</span>, label=<span class="org-string">'Simulated'</span>)
plt.plot(grid, m.fitted_g(torch.tensor(grid, dtype=torch.<span class="org-builtin">float</span>).reshape(-1, 1), log=<span class="org-constant">False</span>), lw=1, c=<span class="org-string">'r'</span>, label=<span class="org-string">'Fit'</span>)
plt.legend(frameon=<span class="org-constant">False</span>, loc=<span class="org-string">'center left'</span>, bbox_to_anchor=(1, .5))
plt.xlabel(<span class="org-string">'Latent variable'</span>)
plt.ylabel(<span class="org-string">'Density'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/flows.org/ebnm-fitted-g.png" alt="ebnm-fitted-g.png">
</p>
</div>

<p>
Plot the approximate posterior against the true latent variables.
</p>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
plt.gcf().set_size_inches(2.5, 2.5)
<span class="org-variable-name">q0</span> = st.norm(x.reshape(-1, 1) / (1 + .05**2), np.sqrt(1 / (1 + 1 / .05**2)))
<span class="org-keyword">with</span> torch.no_grad():
  <span class="org-variable-name">samples</span> = np.stack([m.qz.forward(torch.tensor(q0.rvs(), dtype=torch.<span class="org-builtin">float</span>))[0].numpy() <span class="org-keyword">for</span> _ <span class="org-keyword">in</span> <span class="org-builtin">range</span>(100)]).squeeze()
  <span class="org-variable-name">muhat</span> = samples.mean(axis=0)
plt.scatter(mu, muhat, s=1, c=<span class="org-string">'k'</span>)
<span class="org-variable-name">lim</span> = [mu.<span class="org-builtin">min</span>(), mu.<span class="org-builtin">max</span>()]
plt.plot(lim, lim, lw=1, ls=<span class="org-string">':'</span>, c=<span class="org-string">'r'</span>)
plt.xlim(lim)
plt.ylim(lim)
plt.xlabel(<span class="org-string">'Latent variable'</span>)
plt.ylabel(<span class="org-string">'Estimate'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/flows.org/ebnm-q.png" alt="ebnm-q.png">
</p>
</div>

<p>
Compute the marginal log likelihood of the data under the oracle prior, and
under the estimated prior.
</p>

<div class="org-src-container">
<pre class="src src-ipython">pd.Series({
  <span class="org-string">'oracle'</span>: (st.norm(scale=np.sqrt(1 + scale ** 2)).pdf(x.reshape(-1, 1)) @ pi).<span class="org-builtin">sum</span>(),
  <span class="org-string">'ebnm-nf'</span>: (st.norm(loc=grid).pdf(x.reshape(-1, 1)) @ m.fitted_g(torch.tensor(grid, dtype=torch.<span class="org-builtin">float</span>).reshape(-1, 1), log=<span class="org-constant">False</span>)).<span class="org-builtin">sum</span>()
})
</pre>
</div>

<pre class="example">
oracle       271.383956
ebnm-nf    96719.294528
dtype: float64
</pre>
</div>
</div>

<div id="outline-container-orgd0277f9" class="outline-3">
<h3 id="orgd0277f9">Example of Empirical Bayes Poisson Means</h3>
<div class="outline-text-3" id="text-orgd0277f9">
<p>
For EBPM, a simple choice of \(q_0\) is 
</p>

\begin{equation}
  u_i \mid x_i, s_i \sim \Gam(1 + x_i, 1 + s_i),
\end{equation}

<p>
where the Gamma distribution is parameterized by shape and rate, which is
the exact posterior under the simple model
</p>

\begin{align}
  x_i \mid s_i, \lambda_i &\sim \Pois(s_i \lambda_i)\\
  u_i &\sim \Gam(1, 1).
\end{align}

<p>
For simplicity, it is easier to work with a prior (and approximate
posterior) on \(\ln u_i\).
</p>

<p>
Draw data from a Poisson convolved with a Gamma.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">rng</span> = np.random.default_rng(1)
<span class="org-variable-name">n</span> = 1000
<span class="org-variable-name">s</span> = np.full(n, 1e4)
<span class="org-variable-name">log_mean</span> = -8
<span class="org-variable-name">log_inv_disp</span> = 1
<span class="org-variable-name">lam</span> = rng.gamma(shape=np.exp(log_inv_disp), scale=np.exp(log_mean - log_inv_disp), size=n)
<span class="org-variable-name">x</span> = rng.poisson(s * lam)
</pre>
</div>

<p>
Plot the simulated data.
</p>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(2, 1)
fig.set_size_inches(4, 4)
<span class="org-variable-name">grid</span> = np.arange(x.<span class="org-builtin">max</span>() + 1)
ax[0].hist(x, bins=grid, color=<span class="org-string">'0.7'</span>, density=<span class="org-constant">True</span>)
ax[0].plot(grid + 0.5, st.nbinom(n=np.exp(log_inv_disp), p=1 / (1 + s[0] * np.exp(log_mean - log_inv_disp))).pmf(grid), lw=1, c=<span class="org-string">'k'</span>)
ax[0].set_xlabel(<span class="org-string">'Observed count'</span>)
ax[0].set_ylabel(<span class="org-string">'Density'</span>)
<span class="org-variable-name">grid</span> = np.linspace(0, (x / s).<span class="org-builtin">max</span>(), 1000)
ax[1].plot(grid, st.gamma(a=np.exp(log_inv_disp), scale=np.exp(log_mean - log_inv_disp)).pdf(grid), lw=1, c=<span class="org-string">'k'</span>)
ax[1].set_xlabel(<span class="org-string">'Latent variable'</span>)
ax[1].set_ylabel(<span class="org-string">'Density'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/flows.org/ex1.png" alt="ex1.png">
</p>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">K</span> = 2
<span class="org-variable-name">run</span> = 2
<span class="org-variable-name">lr</span> = 1e-2
<span class="org-variable-name">n_epochs</span> = 8000
torch.manual_seed(run)
<span class="org-variable-name">m</span> = (EBPM(K=K)
     .fit(torch.tensor(x.reshape(-1, 1), dtype=torch.<span class="org-builtin">float</span>),
          torch.tensor(s.reshape(-1, 1), dtype=torch.<span class="org-builtin">float</span>),
          n_epochs=n_epochs,
          lr=lr,
          log_dir=f<span class="org-string">'/scratch/midway2/aksarkar/singlecell/runs/ebpm-nf-{K}-{lr}-{n_epochs}-{run}'</span>))
</pre>
</div>

<p>
0 - c7f81a67-eafd-460f-90f5-257874e6e1ed
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Abhishek Sarkar</p>
<p class="date">Created: 2021-01-26 Tue 13:37</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
