<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2021-01-08 Fri 12:06 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Differential variance analysis</title>
<meta name="generator" content="Org mode">
<meta name="author" content="Abhishek Sarkar">
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link href="bootstrap.min.css" rel="stylesheet" />
<link rel="stylesheet" type="text/css" href="htmlize.css"/>
<link rel="stylesheet" type="text/css" href="main.css"/>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2017 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Differential variance analysis</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#intro">Introduction</a></li>
<li><a href="#setup">Setup</a></li>
<li><a href="#methods">Methods</a>
<ul>
<li><a href="#poisson-beta">Mechanistic basis of effects on variance</a></li>
<li><a href="#diff-var">Frequentist approaches to detect differential variance</a></li>
<li><a href="#org9a9b5ea">Bayesian approaches to detect differential variance</a></li>
<li><a href="#data">Data sets</a></li>
<li><a href="#orge7cc758">STRUCTURE plot</a></li>
</ul>
</li>
<li><a href="#results">Results</a>
<ul>
<li><a href="#mean-disp-rel">Mean-dispersion relationship</a></li>
<li><a href="#null-sim">Null simulation</a></li>
<li><a href="#org8c5fc24">Fitting kinetic models directly</a></li>
<li><a href="#org65b70b2">Variational inference</a></li>
<li><a href="#org7280499">Slice within Gibbs</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-org688f67f" class="outline-2">
<h2 id="intro"><a id="org688f67f"></a>Introduction</h2>
<div class="outline-text-2" id="text-intro">
<p>
We previously hypothesized that QTLs could disrupt the mechanisms controlling
the variance of gene expression, and therefore reveal new insights into the
genetic regulation of differentiation and disease. To investigate this
hypothesis, we directly observed gene expression variance across 53
individuals using scRNA-seq, and sought to identify dispersion QTLs (dQTLs)
which could alter the variance of gene expression across cells within a
single individual, independently of altering the mean expression. However, we
failed to discover such QTLs, and demonstrated that variance QTLs can be
explained by effects on mean expression
(<a href="https://dx.doi.org/10.1371/journal.pgen.1008045">Sarkar et al. 2019</a>). \(
  \DeclareMathOperator\Poi{Poisson}
  \DeclareMathOperator\B{Beta}
  \DeclareMathOperator\E{E}
  \DeclareMathOperator\Gam{Gamma}
  \DeclareMathOperator\KL{\mathcal{KL}}
  \DeclareMathOperator\N{Normal}
  \DeclareMathOperator\V{V}
  \newcommand\delmu{\delta_{\mu}}
  \newcommand\delphi{\delta_{\phi}}
  \newcommand\const{\mathrm{const}}
  \newcommand\kr{k_r}
  \newcommand\kon{k_{\text{on}}}
  \newcommand\koff{k_{\text{off}}}
  \newcommand\vtheta{\boldsymbol{\theta}}
  \newcommand\vx{\mathbf{x}}
  \newcommand\vs{\mathbf{s}}
  \newcommand\vz{\mathbf{z}}
  \)
</p>

<p>
Before attempting to map dQTLs in a followup study, we need to produce
evidence that there are such effects to find in human cell types.
<i>Essentially no studies demonstrate mechanistic evidence of such effects in
human tissues.</i> Many studies measure gene expression variance in human
tissues without estimating the effect of perturbation
(e.g. <a href="https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.0040309">Raj
et al. 2006</a>, <a href="https://doi.org/10.1038/nature12172">Shalek et al. 2013</a>;
<a href="https://doi.org/10.1038/nature13437">Shalek et
al. 2014</a>). <a href="https://science.sciencemag.org/content/355/6332/1433.long">Martinez-Jimenez
et al. 2018</a> (re-analyzed by
<a href="https://www.cell.com/cell-systems/fulltext/S2405-4712(18)30278-3">Eling et
al. 2018</a>) showed that aging changes variability of mouse CD4+ cell gene
expression; however, clearly this perturbation is a complex combination of
environmental factors. (This data was not generated using UMIs, further
complicating the analysis.) The strongest evidence is presented by
<a href="http://dx.doi.org/10.1038/nbt.2642">Wills et al. 2013</a>, who demonstrated
that perturbation of human naive B lymphocytes with a GSK3 inhibitor leads to
changes in variance of downstream gene expression (as measured by qPCR).
</p>

<p>
Here, we develop a method to test for differential variance, and use existing
scRNA-seq datasets to ask whether there are effects which can alter gene
expression variance <i>independent</i> of altering mean gene expression.
</p>
</div>
</div>

<div id="outline-container-org07ce98a" class="outline-2">
<h2 id="setup"><a id="org07ce98a"></a>Setup</h2>
<div class="outline-text-2" id="text-setup">
<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> anndata
<span class="org-keyword">import</span> itertools <span class="org-keyword">as</span> it
<span class="org-keyword">import</span> numpy <span class="org-keyword">as</span> np
<span class="org-keyword">import</span> pandas <span class="org-keyword">as</span> pd
<span class="org-keyword">import</span> pickle
<span class="org-keyword">import</span> poisbeta
<span class="org-keyword">import</span> scanpy <span class="org-keyword">as</span> sc
<span class="org-keyword">import</span> scipy.io <span class="org-keyword">as</span> si
<span class="org-keyword">import</span> scipy.special <span class="org-keyword">as</span> sp
<span class="org-keyword">import</span> scipy.stats <span class="org-keyword">as</span> st
<span class="org-keyword">import</span> scmodes
<span class="org-keyword">import</span> sklearn.decomposition <span class="org-keyword">as</span> skd
<span class="org-keyword">import</span> sqlite3
<span class="org-keyword">import</span> torch
<span class="org-keyword">import</span> torch.utils.tensorboard <span class="org-keyword">as</span> tb
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">%matplotlib inline
%config <span class="org-variable-name">InlineBackend.figure_formats</span> = <span class="org-builtin">set</span>([<span class="org-string">'retina'</span>])
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> colorcet
<span class="org-keyword">import</span> matplotlib.pyplot <span class="org-keyword">as</span> plt
<span class="org-variable-name">plt.rcParams</span>[<span class="org-string">'figure.facecolor'</span>] = <span class="org-string">'w'</span>
<span class="org-variable-name">plt.rcParams</span>[<span class="org-string">'font.family'</span>] = <span class="org-string">'Nimbus Sans'</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-org41d64dc" class="outline-2">
<h2 id="methods"><a id="org41d64dc"></a>Methods</h2>
<div class="outline-text-2" id="text-methods">
</div>
<div id="outline-container-orgce88f0d" class="outline-3">
<h3 id="poisson-beta"><a id="orgce88f0d"></a>Mechanistic basis of effects on variance</h3>
<div class="outline-text-3" id="text-poisson-beta">
<p>
A simple model for transcriptional regulation is the <i>telegraph model</i>
(<a href="https://www.sciencedirect.com/science/article/pii/S0040580985710271">Peccoud
and Ycart 1995</a>,
<a href="https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.0040309">Raj
et al. 2006</a>, <a href="https://dx.doi.org/10.1186/gb-2013-14-1-r7">Kim and Marioni
2013</a>, <a href="https://dx.doi.org/10.1126/science.1216379">Munsky et al. 2013</a>).
The steady state of this model can be characterized as a
<a href="pois-beta.html">Poisson-Beta distribution</a>
</p>

\begin{align}
  m_i \mid \kr, p_i &\sim \Poi(\kr p_i)\\
  p_i \mid \kon, \koff &\sim \B(\kon, \koff),
\end{align}

<p>
where:
</p>

<ul class="org-ul">
<li>\(m_i\) is the number of molecules in sample \(i\) (considering one gene)</li>
<li>\(\kr\) is the rate of transcription</li>
<li>\(\kon\) is the rate of off \(\rightarrow\) on promoter switching</li>
<li>\(\koff\) is the rate of on \(\rightarrow\) off promoter switching</li>
</ul>

<p>
and rates are scaled by the decay rate. Under this model
</p>

\begin{align}
  E[m_i] &= \kr \frac{\kon}{\kon + \koff}\\
  V[m_i] &= \kr \frac{\kon}{\kon + \koff} + \kr^2 \frac{\kon\koff}{(\kon + \koff)^2 (\kon + \koff + 1)}\\
  \text{Fano factor} &= 1 + \frac{\kr\koff}{(\kon + \koff) (\kon + \koff + 1)}
\end{align}

<p>
It follows that an effect which changes variance <i>while leaving the mean
unchanged</i> must leave \(p_i\) unchanged, i.e. equally scale \(\kon\) and
\(\koff\). This fact raises several questions:
</p>

<ul class="org-ul">
<li>Do we know of such a biological mechanism?</li>
<li>Should we expect to find one?</li>
<li>Does such an effect require convergence of multiple biological mechanisms?</li>
<li>This model assumes <i>all cells are described by the same kinetic
parameters.</i> What if subsets of cells change kinetic parameters in
different directions?</li>
</ul>

<p>
The Poisson-Beta model describes the process which generates the molecules
present in each cell; however, it does not include measurement error. To
model measurement error, assume there are \(m_{ij}\) molecules from gene
\(j\) in cell \(i\), with \(m_i^+\) molecules total, but we only observe a
random sample \(x_{i1}, \ldots, x_{ip}\) of the available molecules, in
which case
</p>

<p>
\[ x_{i1}, \ldots, x_{ip} \mid x_i^+ \sim \operatorname{Multinomial}(x_i^+, \lambda_{i1}, \ldots, \lambda_{ip}) \]
</p>

<p>
where \(x_i^+ \triangleq \sum_j x_{ij}\), \(\lambda_{ij} \triangleq
   m_{ij} / m_i^+\), and we have assumed that \(x_i^+ \ll m_i^+\). This can be
transformed into a Poisson model (Baker 1994)
</p>

<p>
\[ x_{ij} \sim \Poi(x_i^+ \lambda_{ij}), \qquad j = 1, \ldots, p \]
</p>

<p>
Therefore, observed molecule counts \(x_{ij}\) are generated as Poisson
sampling on top of a Poisson-Beta generative process
</p>

\begin{align}
  x_{ij} &\sim \Poi(x_i^+ m_{ij} / m_i^+)\\
  m_{ij} \mid \kr^{(j)}, p_{ij} &\sim \Poi(\kr^{(j)} p_{ij})\\
  p_{ij} \mid \kon^{(j)}, \koff^{(j)} &\sim \B(\kon^{(j)}, \koff^{(j)}).
\end{align}

<p>
However, focusing on gene \(j\), we have
</p>

<p>
\[ x_{ij} \sim \Poi(x_i^+ m_{ij} / m_i^+) \approx \operatorname{Binomial}(m_{ij}, x_i^+ / m_i^+). \]
</p>

<p>
Using a well-known result about binomial sampling of a Poisson-distributed
random variable
(e.g. <a href="https://www.biorxiv.org/content/10.1101/758524v1">Gerard 2019</a>), we
have
</p>

<p>
\[ x_{ij} \sim \Poi(x_i^+ \kr^{(j)} p_{ij} / m_i^+). \]
</p>

<p>
There is a degree of freedom in the Multinomial-Poisson transform, namely
that the Poisson rates can be arbitrarily scaled. If we scale the rates by
\(m_i^+\), we recover the previously proposed model of Kim and
Marioni 2013. Now suppose we fit a Poisson-Gamma model to the observed data,
meaning we assume variation in \(\lambda_{ij}\) is due to stochastic
transcription and follows a Gamma distribution
</p>

\begin{align}
  x_{ij} \mid x_i^+, \lambda_{ij} &\sim \Poi(x_i^+ \lambda_{ij}) \\
  \lambda_{ij} \mid \mu_j, \phi_j &\sim \Gam(\phi_j^{-1}, \phi_j^{-1} \mu_j^{-1})
\end{align}

<p>
where:
</p>

<ul class="org-ul">
<li>\(\mu\) denotes the latent mean gene expression</li>
<li>\(\phi\) denotes the overdispersion parameter (relative to Poisson)</li>
</ul>

<p>
Under the Poisson-Gamma model for gene \(j\):
</p>

\begin{align}
  E[x_i] &= x_i^+ \mu \\
  V[x_i] &= x_i^+ \mu + (x_i^+ \mu)^2 \phi \\
  \text{Fano factor} &= 1 + \phi
\end{align}

<p>
These results suggest the mean \(\mu\) and overdispersion \(\phi\) will be
related to each other, through their dependence on \(\kr, \kon,
   \koff\). This possibility raises several questions:
</p>

<ul class="org-ul">
<li>If we do find effects on \(\phi\), should we expect them to be independent
of effects on \(\mu\)?</li>
<li><a href="https://dx.doi.org/10.1126/science.1216379">Eling et al. 2018</a> claim
data demonstrate this relationship, and propose to correct the
relationship between \(\mu\) and \(\phi\).</li>
<li>Will this approach find something other than differences in kinetic
parameters? Why not just test for parameter differences in that model?</li>
</ul>
</div>
</div>

<div id="outline-container-orge396a29" class="outline-3">
<h3 id="diff-var"><a id="orge396a29"></a>Frequentist approaches to detect differential variance</h3>
<div class="outline-text-3" id="text-diff-var">
<p>
The simplest approach begins from a generative model for the data
</p>

\begin{align}
  x_{ij} \mid x_i^+ &\sim \Poi(x_i^+ \lambda_{ij}) \\
  \lambda_{ij} \mid z_i &\sim g_{z_i}(\cdot),
\end{align}

<p>
where
</p>

<ul class="org-ul">
<li>\(x_{ij}\) is the number of molecules in sample \(i\) mapping to gene
\(j\)</li>
<li>\(x_i^+\) is the total number of molecules in sample \(i\)</li>
<li>\(z_i\) is an indicator variable denoting which group sample \(i\) belongs
to</li>
</ul>

<p>
Under this model, the variance in group \(k\) is \(\V(g_{k})\). A natural
approach would be to estimate likelihood ratios comparing the model above to
the null model:
</p>

\begin{align}
  x_{ij} \mid x_i^+ &\sim \operatorname{Poisson}(x_i^+ \lambda_{ij}) \\
  \lambda_{ij} &\sim g_0(\cdot);
\end{align}

<p>
however, this model comparison will also be significant for differentially
expressed genes which do not have significant changes in variance. One
alternative is to assume a parametric form on \(g\), e.g.
</p>

\begin{equation}
  g_{k} = \Gam\left(\frac{1}{\phi_k}, \frac{1}{\mu_k \phi_k}\right),
\end{equation}

<p>
where the Gamma distribution is parameterized by shape and rate, estimate
differences in \(\phi_k\) or \(V[g_k] = \mu_k^2 \phi_k\), and bootstrap to
get \(p\)-values. There are three challenges in this approach:
</p>

<ol class="org-ol">
<li><i>Boundary values</i>. In real data, we should expect \(\pi = 0\) (Negative Binomial
marginal distribution), and even \(\phi = \infty\) ([Zero-inflated] Poisson
marginal distribution) for some genes.</li>

<li><i>Mean-variance relationship</i>. Eling et al. 2018 claim there is a
relationship even between \(\mu\) and \(\phi\) in the deconvolved Gamma
distributions.</li>

<li><i>Computational cost</i>. The bootstraps are expensive, and it is difficult
to take a hierarchical approach in this setting due to lack of conjugacy.</li>
</ol>

<div class="org-src-container">
<pre class="src src-ipython" id="orgf5fde8e"><span class="org-keyword">def</span> <span class="org-function-name">est_var</span>(x, s):
  <span class="org-keyword">if</span> x.<span class="org-builtin">max</span>() == 0:
    <span class="org-keyword">return</span> 0
  <span class="org-keyword">else</span>:
    <span class="org-variable-name">g</span> = scmodes.ebpm.ebpm_gamma(x, s)
    <span class="org-keyword">if</span> <span class="org-keyword">not</span> np.isfinite(g[1]):
      <span class="org-keyword">return</span> np.exp(g[0])
    <span class="org-keyword">else</span>:
      <span class="org-keyword">return</span> np.exp(2 * g[0] + g[1])

<span class="org-keyword">def</span> <span class="org-function-name">est_diff_var</span>(x, s, z):
  <span class="org-variable-name">var1</span> = est_var(x[z], s[z])
  <span class="org-variable-name">var2</span> = est_var(x[~z], s[~z])
  <span class="org-keyword">return</span> var2 - var1

<span class="org-keyword">def</span> <span class="org-function-name">diff_var_bootstrap</span>(x, s, z, random_state=<span class="org-constant">None</span>, n_bootstrap=1000):
  <span class="org-keyword">if</span> random_state <span class="org-keyword">is</span> <span class="org-keyword">not</span> <span class="org-constant">None</span>:
    np.random.seed(random_state)
  <span class="org-variable-name">theta</span> = est_diff_var(x, s, z)
  <span class="org-variable-name">B</span> = []
  <span class="org-keyword">for</span> trial <span class="org-keyword">in</span> <span class="org-builtin">range</span>(n_bootstrap):
    B.append(est_diff_var(x, s, np.random.permutation(z)))
  <span class="org-variable-name">B</span> = np.array(B)
  <span class="org-keyword">if</span> theta &lt; 0:
    <span class="org-keyword">return</span> theta, (B &lt; theta).mean()
  <span class="org-keyword">else</span>:
    <span class="org-keyword">return</span> theta, (B &gt; theta).mean()
</pre>
</div>

<p>
As a second alternative, for each gene \(j\) we can estimate \(g_0\) via
Empirical Bayes, and then recover the posterior distribution:
</p>

\begin{align}
  \hat{g} &= \hat\pi \delta_0(\cdot) + (1 - \hat\pi)\operatorname{Gamma}(\frac{1}{\hat\phi}, \frac{1}{\hat\mu \hat\phi})\\
  p(\lambda_i \mid x_1, \ldots, x_n, x_1^+, \ldots, x_n^+, \hat{g}) &= \hat\pi \delta_0(\cdot) + (1 - \hat\pi)\operatorname{Gamma}\left(\cdot; x_i +
   \frac{1}{\hat\phi}, x_i^+ + \frac{1}{\hat\mu \hat\phi}\right)
\end{align}

<p>
We can then take sample variances of latent gene expression values
\(E[\lambda_i \mid \cdot]\) across groups.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">post_mean</span>(x, s, log_mu, neg_log_phi):
  <span class="org-keyword">return</span> (x + np.exp(neg_log_phi)) / (s + np.exp(-log_mu + neg_log_phi))

<span class="org-keyword">def</span> <span class="org-function-name">diff_var_pm</span>(x, s, z):
  <span class="org-variable-name">g1</span> = scmodes.ebpm.ebpm_gamma(x[z], s[z])
  <span class="org-variable-name">lam1</span> = post_mean(x, s, *g1[:-1])
  <span class="org-variable-name">g2</span> = scmodes.ebpm.ebpm_gamma(x[~z], s[~z])
  <span class="org-variable-name">lam2</span> = post_mean(x, s, *g2[:-1])
</pre>
</div>
</div>
</div>

<div id="outline-container-org9a9b5ea" class="outline-3">
<h3 id="org9a9b5ea">Bayesian approaches to detect differential variance</h3>
<div class="outline-text-3" id="text-org9a9b5ea">
<p>
In a Bayesian framework, we re-parameterize the model in terms of the
difference in mean \(\delmu\) and dispersion \(\delphi\), and estimate the
posterior distribution \(p(\delmu, \delphi \mid \vx, \vs, \vz)\). Using all
of the data to estimate a Gamma expression model \(\mu_0, \phi_0\), we can
form informative priors on \(\vtheta = (\ln\mu, \ln\phi, \delmu, \delphi)\).
</p>

\begin{align}
  x_i \mid s_i, \lambda_i &\sim \Poi(s_i \lambda_i)\\
  \lambda_i \mid z_i, \mu, \phi, \delmu, \delphi &\sim \Gam(\exp(-\ln\phi - z_i \delphi), \exp(-\ln\mu - \ln\phi - z_i (\delmu + \delphi)))\\
  \ln \mu &\sim \N(\ln\hat\mu_0, 1)\\
  \ln \phi &\sim \N(\ln\hat\phi_0, 1)\\
  \delmu &\sim \N(0, 1)\\
  \delphi &\sim \N(0, 1).
\end{align}
</div>

<div id="outline-container-org93f7fdd" class="outline-4">
<h4 id="org93f7fdd">Variational inference</h4>
<div class="outline-text-4" id="text-org93f7fdd">
<p>
We can use variational inference to obtain an approximate posterior
\(q(\delphi) \approx p(\delphi \mid \vx, \vs, \vz)\), assuming 
</p>

\begin{equation}
  q(\vtheta) = q(\ln\mu)q(\ln\phi)q(\delmu)q(\delphi)
\end{equation}

<p>
It is easy to show that the optimal \(q\) &#x2013; assuming no constraints &#x2013; are
non-standard distributions, so we will instead assume each \(q\) is in the
same family as the corresponding prior.
</p>

<p>
<i>Remark:</i> Assuming each factor \(q\) is in some simple parametric family
might lead to underestimates of the posterior variance, which will need to
be verified.
</p>

<p>
To address the fact that the prior is non-conjugate, we can optimize a
stochastic objective whose expectation is the ELBO,
</p>

\begin{equation}
  \ell = \E_{q(\vtheta)}\left[\sum_i \ln p(x_i \mid s_i, z_i, \vtheta)\right] - \sum_{\theta \in \vtheta} \KL(q(\theta) \Vert p(\theta)),
\end{equation}

<p>
where the KL terms are analytic, and the first expectation is computed using
a Monte Carlo integral and a differentiable re-parameterization (Kingma and
Welling 2014).
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="orgc0b259e"><span class="org-keyword">class</span> <span class="org-type">DiffVarExprModel</span>(torch.nn.Module):
  <span class="org-keyword">def</span> <span class="org-function-name">__init__</span>(<span class="org-keyword">self</span>, prior_log_mean, prior_log_inv_disp, prior_scale=1.):
    <span class="org-keyword">assert</span> prior_scale &gt; 0
    <span class="org-builtin">super</span>().__init__()
    <span class="org-keyword">self</span>.prior_log_mean = prior_log_mean
    <span class="org-keyword">self</span>.prior_log_inv_disp = prior_log_inv_disp
    <span class="org-keyword">self</span>.prior_diff = torch.distributions.Normal(loc=0., scale=prior_scale)

    <span class="org-keyword">self</span>.q_log_mean_mean = torch.nn.Parameter(<span class="org-keyword">self</span>.prior_log_mean.loc)
    <span class="org-keyword">self</span>.q_log_mean_raw_scale = torch.nn.Parameter(torch.zeros([1]))
    <span class="org-keyword">self</span>.q_log_inv_disp_mean = torch.nn.Parameter(<span class="org-keyword">self</span>.prior_log_inv_disp.loc)
    <span class="org-keyword">self</span>.q_log_inv_disp_raw_scale = torch.nn.Parameter(torch.zeros([1]))

    <span class="org-comment-delimiter"># </span><span class="org-comment">Important: this is actually N(0, ln(2))</span>
    <span class="org-keyword">self</span>.q_diff_mean_mean = torch.nn.Parameter(torch.zeros([1]))
    <span class="org-keyword">self</span>.q_diff_mean_raw_scale = torch.nn.Parameter(torch.zeros([1]))
    <span class="org-keyword">self</span>.q_diff_inv_disp_mean = torch.nn.Parameter(torch.zeros([1]))
    <span class="org-keyword">self</span>.q_diff_inv_disp_raw_scale = torch.nn.Parameter(torch.zeros([1]))

  <span class="org-keyword">def</span> <span class="org-function-name">forward</span>(<span class="org-keyword">self</span>, x, s, z, n_samples, writer=<span class="org-constant">None</span>, global_step=<span class="org-constant">None</span>):
    <span class="org-variable-name">softplus</span> = torch.nn.functional.softplus
    <span class="org-variable-name">_kl</span> = torch.distributions.kl.kl_divergence

    <span class="org-comment-delimiter"># </span><span class="org-comment">We need to keep these around to draw samples</span>
    <span class="org-variable-name">q_log_mean</span> = torch.distributions.Normal(loc=<span class="org-keyword">self</span>.q_log_mean_mean, scale=softplus(<span class="org-keyword">self</span>.q_log_mean_raw_scale))
    <span class="org-variable-name">q_log_inv_disp</span> = torch.distributions.Normal(loc=<span class="org-keyword">self</span>.q_log_inv_disp_mean, scale=softplus(<span class="org-keyword">self</span>.q_log_inv_disp_raw_scale))
    <span class="org-variable-name">q_diff_mean</span> = torch.distributions.Normal(loc=<span class="org-keyword">self</span>.q_diff_mean_mean, scale=softplus(<span class="org-keyword">self</span>.q_diff_mean_raw_scale))
    <span class="org-variable-name">q_diff_inv_disp</span> = torch.distributions.Normal(loc=<span class="org-keyword">self</span>.q_diff_inv_disp_mean, scale=softplus(<span class="org-keyword">self</span>.q_diff_inv_disp_raw_scale))

    <span class="org-comment-delimiter"># </span><span class="org-comment">These can be computed without sampling</span>
    <span class="org-variable-name">kl_mean</span> = _kl(q_log_mean, <span class="org-keyword">self</span>.prior_log_mean)
    <span class="org-variable-name">kl_inv_disp</span> = _kl(q_log_inv_disp, <span class="org-keyword">self</span>.prior_log_inv_disp)
    <span class="org-variable-name">kl_diff_mean</span> = _kl(q_diff_mean, <span class="org-keyword">self</span>.prior_diff)
    <span class="org-variable-name">kl_diff_inv_disp</span> = _kl(q_diff_inv_disp, <span class="org-keyword">self</span>.prior_diff)

    <span class="org-comment-delimiter"># </span><span class="org-comment">[n_samples, 1]</span>
    <span class="org-variable-name">log_mean</span> = q_log_mean.rsample([n_samples])
    <span class="org-variable-name">log_inv_disp</span> = q_log_inv_disp.rsample([n_samples])
    <span class="org-variable-name">diff_mean</span> = q_diff_mean.rsample([n_samples])
    <span class="org-variable-name">diff_inv_disp</span> = q_diff_inv_disp.rsample([n_samples])
    <span class="org-variable-name">a</span> = torch.exp(log_inv_disp.T - z @ diff_inv_disp.T)
    <span class="org-variable-name">b</span> = torch.exp(-log_mean.T + log_inv_disp.T - z @ (diff_mean + diff_inv_disp).T)
    <span class="org-keyword">assert</span> torch.isfinite(a).<span class="org-builtin">all</span>()
    <span class="org-keyword">assert</span> torch.isfinite(b).<span class="org-builtin">all</span>()
    <span class="org-variable-name">err</span> = (x * torch.log(s / b)
           - x * torch.log(1 + s / b)
           - a * torch.log(1 + s / b)
           + torch.lgamma(x + a)
           - torch.lgamma(a)
           - torch.lgamma(x + 1)).mean(dim=1).<span class="org-builtin">sum</span>()
    <span class="org-keyword">assert</span> err &lt;= 0
    <span class="org-variable-name">elbo</span> = err - kl_mean - kl_inv_disp - kl_diff_mean - kl_diff_inv_disp
    <span class="org-keyword">if</span> writer <span class="org-keyword">is</span> <span class="org-keyword">not</span> <span class="org-constant">None</span>:
      writer.add_scalar(<span class="org-string">'loss/err'</span>, err, global_step)
      writer.add_scalar(<span class="org-string">'loss/kl_mean'</span>, kl_mean, global_step)
      writer.add_scalar(<span class="org-string">'loss/kl_inv_disp'</span>, kl_inv_disp, global_step)
      writer.add_scalar(<span class="org-string">'loss/kl_diff_mean'</span>, kl_diff_mean, global_step)
      writer.add_scalar(<span class="org-string">'loss/kl_diff_inv_disp'</span>, kl_diff_inv_disp, global_step)
    <span class="org-keyword">return</span> -elbo

  <span class="org-keyword">def</span> <span class="org-function-name">fit</span>(<span class="org-keyword">self</span>, x, s, z, n_epochs, n_samples=1, log_dir=<span class="org-constant">None</span>, **kwargs):
    <span class="org-variable-name">n</span> = x.shape[0]
    <span class="org-keyword">assert</span> x.shape == (n, 1)
    <span class="org-keyword">assert</span> s.shape == x.shape
    <span class="org-keyword">assert</span> z.shape == x.shape
    <span class="org-keyword">assert</span> n_samples &gt;= 1
    <span class="org-keyword">if</span> log_dir <span class="org-keyword">is</span> <span class="org-keyword">not</span> <span class="org-constant">None</span>:
      <span class="org-variable-name">writer</span> = tb.SummaryWriter(log_dir)
    <span class="org-keyword">else</span>:
      <span class="org-variable-name">writer</span> = <span class="org-constant">None</span>
    <span class="org-variable-name">global_step</span> = 0
    <span class="org-variable-name">opt</span> = torch.optim.RMSprop(<span class="org-keyword">self</span>.parameters(), **kwargs)
    <span class="org-keyword">for</span> epoch <span class="org-keyword">in</span> <span class="org-builtin">range</span>(n_epochs):
      opt.zero_grad()
      <span class="org-variable-name">loss</span> = <span class="org-keyword">self</span>.forward(x, s, z, n_samples, writer, global_step)
      <span class="org-keyword">if</span> torch.isnan(loss):
        <span class="org-keyword">raise</span> <span class="org-type">RuntimeError</span>(<span class="org-string">'nan loss'</span>)
      loss.backward()
      opt.step()
      <span class="org-variable-name">global_step</span> += 1
    <span class="org-keyword">return</span> <span class="org-keyword">self</span>

  <span class="org-type">@property</span>
  <span class="org-keyword">def</span> <span class="org-function-name">log_mean</span>(<span class="org-keyword">self</span>):
    <span class="org-keyword">return</span> <span class="org-keyword">self</span>.q_log_mean_mean.detach().numpy(), np.log1p(np.exp(<span class="org-keyword">self</span>.q_log_mean_raw_scale.detach().numpy()))

  <span class="org-type">@property</span>
  <span class="org-keyword">def</span> <span class="org-function-name">diff_mean</span>(<span class="org-keyword">self</span>):
    <span class="org-keyword">return</span> <span class="org-keyword">self</span>.q_diff_mean_mean.detach().numpy(), np.log1p(np.exp(<span class="org-keyword">self</span>.q_diff_mean_raw_scale.detach().numpy()))

  <span class="org-type">@property</span>
  <span class="org-keyword">def</span> <span class="org-function-name">lfsr_diff_mean</span>(<span class="org-keyword">self</span>):
    <span class="org-variable-name">loc</span>, <span class="org-variable-name">scale</span> = <span class="org-keyword">self</span>.diff_mean
    <span class="org-keyword">return</span> <span class="org-builtin">min</span>(st.norm(loc, scale).cdf(0), st.norm(loc, scale).sf(0))

  <span class="org-type">@property</span>
  <span class="org-keyword">def</span> <span class="org-function-name">log_inv_disp</span>(<span class="org-keyword">self</span>):
    <span class="org-keyword">return</span> <span class="org-keyword">self</span>.q_log_inv_disp_mean.detach().numpy(), np.log1p(np.exp(<span class="org-keyword">self</span>.q_log_inv_disp_raw_scale.detach().numpy()))

  <span class="org-type">@property</span>
  <span class="org-keyword">def</span> <span class="org-function-name">diff_inv_disp</span>(<span class="org-keyword">self</span>):
    <span class="org-keyword">return</span> <span class="org-keyword">self</span>.q_diff_inv_disp_mean.detach().numpy(), np.log1p(np.exp(<span class="org-keyword">self</span>.q_diff_inv_disp_raw_scale.detach().numpy()))

  <span class="org-type">@property</span>
  <span class="org-keyword">def</span> <span class="org-function-name">lfsr_diff_inv_disp</span>(<span class="org-keyword">self</span>):
    <span class="org-variable-name">loc</span>, <span class="org-variable-name">scale</span> = <span class="org-keyword">self</span>.diff_inv_disp
    <span class="org-keyword">return</span> <span class="org-builtin">min</span>(st.norm(loc, scale).cdf(0), st.norm(loc, scale).sf(0))
</pre>
</div>
</div>
</div>

<div id="outline-container-orgbe415fd" class="outline-4">
<h4 id="orgbe415fd">MCMC</h4>
<div class="outline-text-4" id="text-orgbe415fd">
<p>
To check whether VI understates the posterior variance, or if the posterior
is actually multi-modal and VI only captures one mode, we can compare
against a collapsed Gibbs sampler, using slice sampling to sample from
non-standard conditional densities
(<a href="https://projecteuclid.org/euclid.aos/1056562461">Neal 2003</a>).
</p>

<p>
Letting
</p>

\begin{align}
  a_i &\triangleq \exp(-\ln\phi - z_i \delphi)\\
  b_i &\triangleq \exp(-\ln\mu - \ln\phi - z_i (\delmu + \delphi)),
\end{align}

<p>
and marginalizing over \(\lambda_i\), the log joint probability
</p>

\begin{multline}
  \ln p(x_i, \vtheta \mid s_i, z_i) = x_i \ln \left(\frac{s_i}{s_i + b_i}\right) + a_i \ln \left(\frac{b_i}{s_i + b_i}\right) + \ln\Gamma(x_i + a_i) - \ln\Gamma(a_i) - \ln\Gamma(x_i + 1)\\
  - \frac{(\ln\mu - \ln\mu_0)^2}{2} - \frac{(\ln\phi - \ln\phi_0)^2}{2} - \frac{\delmu^2}{2} - \frac{\delphi^2}{2} + \const,
\end{multline}

<p>
and the log conditional probabilities (up to a constant) are
</p>

\begin{align}
  \ln p(\ln\mu \mid \cdot) &= \sum_i u_i - (\ln \mu - \ln\mu_0)^2 / 2 + \const\\
  \ln p(\ln\phi \mid \cdot) &= \sum_i u_i + v_i - (\ln \phi - \ln\phi_0)^2 / 2 + \const\\
  \ln p(\delmu \mid \cdot) &= \sum_i u_i - \delmu^2 / 2 + \const\\
  \ln p(\delphi \mid \cdot) &= \sum_i u_i + v_i - \delphi^2 / 2 + \const,
\end{align}

<p>
where
</p>

\begin{align}
  u_i &\triangleq -x_i \ln (s_i + b_i) + a_i \ln b_i - a_i \ln (s_i + b_i)\\
  v_i &\triangleq \ln\Gamma(x_i + a_i) - \ln\Gamma(a_i)
\end{align}

<div class="org-src-container">
<pre class="src src-ipython" id="org4079dd5"><span class="org-keyword">def</span> <span class="org-function-name">slice_sample</span>(f, init, width=0.1, max_steps=100, bounds=<span class="org-constant">None</span>, **kwargs):
  <span class="org-doc">"""Return samples from the density proportional to exp(f)</span>

<span class="org-doc">  f - log density (up to a constant)</span>
<span class="org-doc">  init - initial value</span>
<span class="org-doc">  width - typical width of f</span>
<span class="org-doc">  max_steps - maximum number of times to step out</span>
<span class="org-doc">  bounds - support of f</span>
<span class="org-doc">  kwargs - additional arguments to f</span>

<span class="org-doc">  """</span>
  <span class="org-keyword">if</span> bounds <span class="org-keyword">is</span> <span class="org-constant">None</span>:
    <span class="org-variable-name">bounds</span> = (-np.inf, np.inf)
  <span class="org-comment-delimiter"># </span><span class="org-comment">Auxiliary variable defines the slice</span>
  <span class="org-variable-name">z</span> = f(init, **kwargs) - np.random.exponential()
  <span class="org-comment-delimiter"># </span><span class="org-comment">Step out</span>
  <span class="org-variable-name">left</span> = init - width * np.random.uniform()
  <span class="org-variable-name">right</span> = left + width
  <span class="org-variable-name">left_steps</span> = <span class="org-builtin">int</span>(np.random.uniform() * max_steps)
  <span class="org-keyword">for</span> _ <span class="org-keyword">in</span> <span class="org-builtin">range</span>(left_steps):
    <span class="org-keyword">if</span> left &lt; bounds[0] <span class="org-keyword">or</span> z &lt; f(left, **kwargs):
      <span class="org-keyword">break</span>
    <span class="org-variable-name">left</span> -= width
  <span class="org-variable-name">left</span> = np.clip(left, *bounds)
  <span class="org-keyword">for</span> _ <span class="org-keyword">in</span> <span class="org-builtin">range</span>(max_steps - left_steps):
    <span class="org-keyword">if</span> right &gt; bounds[1] <span class="org-keyword">or</span> z &lt; f(right, **kwargs):
      <span class="org-keyword">break</span>
    <span class="org-variable-name">right</span> += width
  <span class="org-variable-name">right</span> = np.clip(right, *bounds)
  <span class="org-comment-delimiter"># </span><span class="org-comment">Step in</span>
  <span class="org-keyword">while</span> right &gt; left:
    <span class="org-variable-name">proposal</span> = left + np.random.uniform() * (right - left)
    <span class="org-keyword">if</span> z &lt; f(proposal, **kwargs):
      <span class="org-keyword">return</span> proposal
    <span class="org-keyword">elif</span> proposal &lt; init:
      <span class="org-variable-name">left</span> = proposal
    <span class="org-keyword">else</span>:
      <span class="org-variable-name">right</span> = proposal
  <span class="org-keyword">raise</span> <span class="org-type">RuntimeError</span>(<span class="org-string">'failed to find an acceptable sample'</span>)

<span class="org-keyword">def</span> <span class="org-function-name">_safe_log</span>(x, eps=1e-8):
  <span class="org-keyword">return</span> np.log(x + eps)

<span class="org-keyword">def</span> <span class="org-function-name">_cond_logpdf</span>(init, u, prior_mean=0):
  <span class="org-keyword">return</span> (u - (init - prior_mean) ** 2 / 2).<span class="org-builtin">sum</span>()

<span class="org-keyword">def</span> <span class="org-function-name">diff_var_mcmc</span>(x, s, z, n_samples, sigma0=10, verbose=<span class="org-constant">False</span>, trace=<span class="org-constant">False</span>):
  <span class="org-keyword">assert</span> sigma0 &gt; 0
  <span class="org-keyword">assert</span> n_samples &gt; 0
  <span class="org-variable-name">init</span> = scmodes.ebpm.ebpm_gamma(x, s)
  <span class="org-variable-name">log_mean</span> = init[0]
  <span class="org-keyword">if</span> np.isfinite(init[1]):
    <span class="org-variable-name">log_inv_disp</span> = init[1]
  <span class="org-keyword">else</span>:
    <span class="org-keyword">raise</span> <span class="org-type">RuntimeError</span>(<span class="org-string">"data are not consistent with non-zero variance"</span>)
  <span class="org-variable-name">diff_log_mean</span> = 0
  <span class="org-variable-name">diff_log_inv_disp</span> = 0
  <span class="org-variable-name">F_log_mean</span> = st.norm(loc=init[0])
  <span class="org-variable-name">F_log_inv_disp</span> = st.norm(loc=init[1])
  <span class="org-variable-name">F_diff_log_mean</span> = st.norm(scale=sigma0)
  <span class="org-variable-name">F_diff_log_inv_disp</span> = st.norm(scale=sigma0)
  <span class="org-variable-name">samples</span> = []
  <span class="org-variable-name">log_joint_trace</span> = []
  <span class="org-keyword">for</span> i <span class="org-keyword">in</span> <span class="org-builtin">range</span>(n_samples):
    samples.append((log_mean, log_inv_disp, diff_log_mean, diff_log_inv_disp))
    <span class="org-variable-name">a</span> = np.exp(log_inv_disp - z * diff_log_inv_disp)
    <span class="org-variable-name">b</span> = np.exp(-log_mean + log_inv_disp - z * (diff_log_mean + diff_log_inv_disp))
    <span class="org-variable-name">u</span> = -x * _safe_log(s + b) - a * _safe_log(s + b) + a * _safe_log(b)
    <span class="org-variable-name">v</span> = sp.gammaln(x + a) - sp.gammaln(a)
    <span class="org-variable-name">log_mean</span> = slice_sample(_cond_logpdf, init=log_mean, u=u, prior_mean=F_log_mean.mean())
    <span class="org-variable-name">log_inv_disp</span> = slice_sample(_cond_logpdf, init=log_inv_disp, u=u + v, prior_mean=F_log_inv_disp.mean())
    <span class="org-variable-name">diff_log_mean</span> = slice_sample(_cond_logpdf, init=diff_log_mean, u=u)
    <span class="org-variable-name">diff_log_inv_disp</span> = slice_sample(_cond_logpdf, init=diff_log_inv_disp, u=u + v)
    <span class="org-keyword">if</span> trace <span class="org-keyword">or</span> verbose:
      <span class="org-variable-name">log_joint</span> = (st.nbinom(n=a, p=1 / (1 + s / b)).logpmf(x).<span class="org-builtin">sum</span>()
                   + F_log_mean.logpdf(log_mean)
                   + F_log_inv_disp.logpdf(log_inv_disp)
                   + F_diff_log_mean.logpdf(diff_log_mean)
                   + F_diff_log_inv_disp.logpdf(diff_log_inv_disp))
      <span class="org-keyword">assert</span> np.isfinite(log_joint)
      <span class="org-keyword">if</span> trace:
        log_joint_trace.append(log_joint)
      <span class="org-keyword">if</span> verbose:
        <span class="org-keyword">print</span>(f<span class="org-string">'sample {i}: {log_joint}'</span>)
  <span class="org-keyword">return</span> np.array(samples).reshape(-1, 4), log_joint_trace
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-orga85492c" class="outline-3">
<h3 id="data"><a id="orga85492c"></a>Data sets</h3>
<div class="outline-text-3" id="text-data">
<p>
For null simulations, we use homogeneous populations of sorted cells (Zheng
et al. 2017):
</p>

<ul class="org-ul">
<li>Cytotoxic T cells</li>
<li>B cells</li>
</ul>

<p>
As a positive control, we re-analyze
<a href="https://www.ebi.ac.uk/arrayexpress/experiments/E-MTAB-4888/">unstimulated
and stimulated naive and effector memory CD4+ T cells</a> from young and old
mice from two divergent species (Martinez-Jimenez et al. 2018)
</p>

<div class="org-src-container">
<pre class="src src-sh">sbatch --partition=mstephens
<span class="org-comment-delimiter">#</span><span class="org-comment">!/bin/bash</span>
<span class="org-builtin">test</span> -f E-MTAB-4888.processed.1.zip || curl -O https://www.ebi.ac.uk/arrayexpress/files/E-MTAB-4888/E-MTAB-4888.processed.1.zip
<span class="org-builtin">test</span> -f raw_data.txt || unzip E-MTAB-4888.processed.1.zip
mkdir -p /project2/mstephens/aksarkar/projects/singlecell-ideas/data/E-MTAB-4888
gzip &lt;raw_data.txt &gt;/project2/mstephens/aksarkar/projects/singlecell-ideas/data/E-MTAB-4888/counts.txt.gz
</pre>
</div>

<pre class="example">
Submitted batch job 61624943

</pre>

<p>
We then analyze single cell RNA-seq time course data through differentiation
of iPSCs into cardiomyocytes (Selewa et al. 2019).
</p>
</div>
</div>

<div id="outline-container-orge7cc758" class="outline-3">
<h3 id="orge7cc758">STRUCTURE plot</h3>
<div class="outline-text-3" id="text-orge7cc758">
<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">plot_structure</span>(weights, ax=<span class="org-constant">None</span>, idx=<span class="org-constant">None</span>):
  <span class="org-keyword">if</span> ax <span class="org-keyword">is</span> <span class="org-constant">None</span>:
    <span class="org-variable-name">ax</span> = plt.gca()
  <span class="org-variable-name">prop</span> = np.cumsum(weights, axis=1)
  <span class="org-keyword">if</span> idx <span class="org-keyword">is</span> <span class="org-constant">None</span>:
    <span class="org-variable-name">idx</span> = np.lexsort(weights.T)
  <span class="org-keyword">for</span> i <span class="org-keyword">in</span> <span class="org-builtin">range</span>(prop.shape[1]):
    <span class="org-keyword">if</span> i &gt; 0:
      <span class="org-variable-name">bot</span> = prop[idx,i - 1]
    <span class="org-keyword">else</span>:
      <span class="org-variable-name">bot</span> = <span class="org-constant">None</span>
    ax.bar(np.arange(prop.shape[0]), weights[idx,i], bottom=bot, color=f<span class="org-string">'C{i}'</span>, width=1, label=f<span class="org-string">'Topic {i + 1}'</span>)
  ax.set_xlim(0, weights.shape[0])
  ax.set_xticks([])
  ax.set_ylim(0, 1)
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-org70446ab" class="outline-2">
<h2 id="results"><a id="org70446ab"></a>Results</h2>
<div class="outline-text-2" id="text-results">
</div>
<div id="outline-container-orgea903e8" class="outline-3">
<h3 id="mean-disp-rel"><a id="orgea903e8"></a>Mean-dispersion relationship</h3>
<div class="outline-text-3" id="text-mean-disp-rel">
<p>
Use scRNA-seq of 9,957 genes in 5,597 iPSCs from 53 donors (Sarkar et
al. 2019) to examine the relationship between mean and dispersion.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">log_mu</span> = pd.read_csv(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design1/zi2-log-mu.txt.gz'</span>, sep=<span class="org-string">' '</span>, index_col=0)
<span class="org-variable-name">log_phi</span> = pd.read_csv(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design1/zi2-log-phi.txt.gz'</span>, sep=<span class="org-string">' '</span>, index_col=0)
<span class="org-variable-name">logodds</span> = pd.read_csv(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design1/zi2-logodds.txt.gz'</span>, sep=<span class="org-string">' '</span>, index_col=0)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
plt.gcf().set_size_inches(3, 3)
<span class="org-variable-name">H</span> = np.histogram2d(log_mu.values.ravel(), log_phi.values.ravel(), bins=200)
<span class="org-variable-name">X</span>, <span class="org-variable-name">Y</span> = np.meshgrid(H[1], H[2])
plt.contour(X[1:,1:].T, Y[1:,1:].T, H[0], cmap=colorcet.cm[<span class="org-string">'fire'</span>], linewidths=1)
plt.xlabel(<span class="org-string">'$\ln(\mu)$'</span>)
plt.ylabel(<span class="org-string">'$\ln(\phi)$'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/diff-var.org/log-mu-vs-log-phi.png" alt="log-mu-vs-log-phi.png">
</p>
</div>

<p>
Look at zeros and dispersion.
</p>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
plt.gcf().set_size_inches(3, 3)
<span class="org-variable-name">H</span> = np.histogram2d(logodds.values.ravel(), log_phi.values.ravel(), bins=200)
<span class="org-variable-name">X</span>, <span class="org-variable-name">Y</span> = np.meshgrid(H[1], H[2])
plt.contour(X[1:,1:].T, Y[1:,1:].T, H[0], cmap=colorcet.cm[<span class="org-string">'fire'</span>], linewidths=1)
plt.xlabel(<span class="org-string">'$\mathrm{logit}(\pi)$'</span>)
plt.ylabel(<span class="org-string">'$\ln(\mu)$'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/diff-var.org/logodds-vs-log-phi.png" alt="logodds-vs-log-phi.png">
</p>
</div>

<p>
We previously looked at the relationship between estimated parameters,
averaging over all 53 individuals.
</p>


<div class="figure">
<p><img src="https://jdblischak.github.io/singlecell-qtl/figure/zinb.org/joint-distribution.png" alt="joint-distribution.png">
</p>
</div>
</div>
</div>

<div id="outline-container-org9c1ac43" class="outline-3">
<h3 id="null-sim"><a id="org9c1ac43"></a>Null simulation</h3>
<div class="outline-text-3" id="text-null-sim">
<p>
Randomly split a homogeneous sample into two groups to generate null data
sets.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">null_sim</span>(dat, n1, n2, seed=0):
  <span class="org-variable-name">query</span> = sc.pp.subsample(dat, n_obs=n1 + n2, random_state=seed, copy=<span class="org-constant">True</span>)
  <span class="org-variable-name">z</span> = np.zeros(n1 + n2).astype(<span class="org-builtin">bool</span>)
  <span class="org-variable-name">z</span>[:n1] = 1
  <span class="org-keyword">return</span> query, z

<span class="org-keyword">def</span> <span class="org-function-name">evaluate_null_type1</span>(dat, n=100, min_detect=0.01, n_bootstrap=1000, n_trials=10):
  <span class="org-variable-name">result</span> = []
  <span class="org-keyword">for</span> trial <span class="org-keyword">in</span> <span class="org-builtin">range</span>(n_trials):
    <span class="org-variable-name">x</span>, <span class="org-variable-name">z</span> = null_sim(dat, n, n, seed=trial)
    <span class="org-variable-name">s</span> = x.X.<span class="org-builtin">sum</span>(axis=1).A.ravel()
    <span class="org-keyword">for</span> j <span class="org-keyword">in</span> <span class="org-builtin">range</span>(x.shape[1]):
      <span class="org-variable-name">diff</span>, <span class="org-variable-name">p</span> = diff_var_bootstrap(x.X[:,j].A.ravel(), s, z, n_bootstrap=n_bootstrap)
      result.append(pd.Series({<span class="org-string">'trial'</span>: trial, <span class="org-string">'gene'</span>: j, <span class="org-string">'n'</span>: n, <span class="org-string">'diff'</span>: diff, <span class="org-string">'p'</span>: p}))
  <span class="org-variable-name">result</span> = pd.DataFrame(result)
  <span class="org-keyword">return</span> result
</pre>
</div>

<p>
Read 10X v3 data, which has higher sequencing depth.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">dat</span> = anndata.read_h5ad(<span class="org-string">'/scratch/midway2/aksarkar/modes/10k_pbmc_v3.h5ad'</span>)
</pre>
</div>

<p>
First, apply our approach to a single example.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">n1</span> = 500
<span class="org-variable-name">n2</span> = 500
<span class="org-variable-name">x</span>, <span class="org-variable-name">z</span> = null_sim(dat, n1, n2, seed=1)
<span class="org-variable-name">s</span> = x.X.<span class="org-builtin">sum</span>(axis=1).A.ravel()
sc.pp.filter_genes(x, min_cells=0.01 * (n1 + n2))
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">est_diff_var(x.X[:,0].A.ravel(), s, z)
</pre>
</div>

<pre class="example">
2.3068812267492158e-11

</pre>

<div class="org-src-container">
<pre class="src src-ipython">diff_var_bootstrap(x.X[:,0].A.ravel(), s, z, random_state=1)
</pre>
</div>

<p>
Test the benchmark.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">test_result</span> = evaluate_null_type1(dat, n=500, n_trials=1)
test_result.head()
</pre>
</div>

<p>
0 - f39f19f1-0b74-45a9-bb8b-18a9a4a3227b
</p>
</div>
</div>

<div id="outline-container-org8c5fc24" class="outline-3">
<h3 id="org8c5fc24">Fitting kinetic models directly</h3>
<div class="outline-text-3" id="text-org8c5fc24">
<p>
Instead of assuming variance of gene expression is driven by stochastic
transcription and asking whether the variance changes after some
perturbation, we could attempt to ask directly whether the underlying
stochastic process changes.
</p>
</div>

<div id="outline-container-org702bb0f" class="outline-4">
<h4 id="org702bb0f">iPSC data</h4>
<div class="outline-text-4" id="text-org702bb0f">
<p>
We previously identified 5 variance QTLs in iPSCs, but also found they were
all mean QTLs.
</p>

<p>
Read the data.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">x</span> = anndata.read_h5ad(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-ideas/data/ipsc/ipsc.h5ad'</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">vqtls</span> = pd.read_csv(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-mapping/variance.txt.gz'</span>, sep=<span class="org-string">' '</span>, index_col=0)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">vqtls.sort_values(<span class="org-string">'p_beta'</span>).head(n=5)
</pre>
</div>

<pre class="example">
chr      start        end strand  num_vars  distance  \
gene
ENSG00000184674  chr22   24384681   24384680      -       477       0.0
ENSG00000113558   chr5  133512730  133512729      -       673   -2801.0
ENSG00000132507  chr17    7210319    7210318      +       496    5218.0
ENSG00000148680  chr10   92617672   92617671      -       669   47915.0
ENSG00000131469  chr17   41150291   41150290      +       515  -32887.0

id var_chr    var_start      var_end  \
gene
ENSG00000184674   esv2669470.chr22.24343050   chr22   24343050.0   24397301.0
ENSG00000113558   rs13356194.chr5.133515530    chr5  133515530.0  133515530.0
ENSG00000132507     rs1054378.chr17.7215536   chr17    7215536.0    7215536.0
ENSG00000148680  rs138704692.chr10.92569757   chr10   92569757.0   92569757.0
ENSG00000131469   rs12947287.chr17.41117404   chr17   41117404.0   41117404.0

df    dummy         a        b     p_nominal      beta  \
gene
ENSG00000184674  51.0  43.2504  1.033960  59.1680  4.192900e-11 -1.182590
ENSG00000113558  51.0  39.5051  1.066360  37.1031  1.387570e-10 -0.796700
ENSG00000132507  51.0  42.2941  1.069530  73.9335  7.525920e-10 -1.214550
ENSG00000148680  51.0  42.9914  1.035830  43.6306  4.632030e-09 -1.044440
ENSG00000131469  51.0  34.8434  0.902717  23.8075  8.796860e-11 -0.848284

p_empirical        p_beta
gene
ENSG00000184674       0.0001  4.266760e-08
ENSG00000113558       0.0001  2.371110e-07
ENSG00000132507       0.0001  6.094700e-07
ENSG00000148680       0.0001  2.088450e-06
ENSG00000131469       0.0001  7.746900e-06
</pre>

<div class="org-src-container">
<pre class="src src-ipython">x.var.loc[<span class="org-string">'ENSG00000184674'</span>]
</pre>
</div>

<pre class="example">
chr             hs22
start       24376133
end         24384680
name           GSTT1
strand             -
source    H. sapiens
Name: ENSG00000184674, dtype: object
</pre>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">with</span> sqlite3.connect(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/browser/browser.db'</span>) <span class="org-keyword">as</span> conn:
  <span class="org-variable-name">geno</span> = pd.read_sql(<span class="org-string">'select * from mean_qtl_geno where gene = ?'</span>, con=conn, params=(<span class="org-string">'ENSG00000184674'</span>,)).set_index(<span class="org-string">'ind'</span>)[<span class="org-string">'value'</span>]
</pre>
</div>

<p>
Read the previously fitted point-Gamma distribution for each individual.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">log_mu</span> = pd.read_csv(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design1/zi2-log-mu.txt.gz'</span>, sep=<span class="org-string">' '</span>, index_col=0)
<span class="org-variable-name">log_phi</span> = pd.read_csv(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design1/zi2-log-phi.txt.gz'</span>, sep=<span class="org-string">' '</span>, index_col=0)
<span class="org-variable-name">logodds</span> = pd.read_csv(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design1/zi2-logodds.txt.gz'</span>, sep=<span class="org-string">' '</span>, index_col=0)
</pre>
</div>

<p>
Plot the observed data and the fitted distributions.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">query</span> = x.X.tocsc()[:,(x.var[<span class="org-string">'name'</span>] == <span class="org-string">'GSTT1'</span>).values].A.ravel()
<span class="org-variable-name">lam</span> = query / x.obs[<span class="org-string">'mol_hs'</span>]
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Dark2'</span>)
<span class="org-variable-name">k</span> = <span class="org-string">'ENSG00000184674'</span>

plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(2, 1)
fig.set_size_inches(6, 4)
<span class="org-variable-name">bins</span> = np.arange(query.<span class="org-builtin">max</span>() + 1)
ax[0].hist(query, bins=bins, color=<span class="org-string">'k'</span>)
ax[0].set_xlabel(<span class="org-string">'Number of molecules'</span>)
ax[0].set_ylabel(<span class="org-string">'Number of cells'</span>)
ax[0].set_title(x.var.loc[k, <span class="org-string">'name'</span>])
ax[0].set_xticks(bins[::2])

<span class="org-variable-name">grid</span> = np.linspace(0, lam.<span class="org-builtin">max</span>(), 1000)
<span class="org-keyword">for</span> i <span class="org-keyword">in</span> log_mu.columns:
  <span class="org-keyword">if</span> i <span class="org-keyword">in</span> geno.index:
    <span class="org-variable-name">t</span> = log_mu.loc[k, i], log_phi.loc[k, i], logodds.loc[k, i]
    <span class="org-variable-name">F</span> = st.gamma(a=np.exp(-t[1]), scale=np.exp(t[0] + t[1])).cdf(grid)
    <span class="org-comment-delimiter"># </span><span class="org-comment">Important: we need to round the imputed dosages</span>
    ax[1].plot(grid, F, lw=1, alpha=0.35, c=cm(<span class="org-builtin">int</span>(np.<span class="org-builtin">round</span>(geno.loc[i]))))
<span class="org-variable-name">dummy</span> = [plt.Line2D([0], [0], c=cm(i)) <span class="org-keyword">for</span> i <span class="org-keyword">in</span> <span class="org-builtin">range</span>(3)]
ax[1].legend(dummy, np.arange(3), title=vqtls.loc[k, <span class="org-string">'id'</span>].split(<span class="org-string">'.'</span>)[0], frameon=<span class="org-constant">False</span>)
ax[1].set_xlabel(<span class="org-string">'Latent gene expression'</span>)
ax[1].set_ylabel(<span class="org-string">'CDF'</span>)
ax[1].set_xlim(0, 7e-5)

fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/diff-var.org/ipsc-gstt1.png" alt="ipsc-gstt1.png">
</p>
</div>

<p>
Estimate the posterior distribution of kinetic parameters for each individual.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">k_params</span> = <span class="org-builtin">dict</span>()
<span class="org-keyword">for</span> i <span class="org-keyword">in</span> geno.index:
  <span class="org-keyword">print</span>(f<span class="org-string">'Fitting individual {i}'</span>)
  <span class="org-variable-name">keep</span> = (x.obs[<span class="org-string">'chip_id'</span>] == i).values.ravel()
  <span class="org-variable-name">xg</span> = query[keep]
  <span class="org-variable-name">sg</span> = x.obs[<span class="org-string">'mol_hs'</span>].values[keep]
  <span class="org-variable-name">samples</span>, <span class="org-variable-name">_</span> = poisbeta.fit_poisson_beta_mcmc(xg, n_samples=1000, ar=1, br=xg.<span class="org-builtin">max</span>(), aon=1, bon=100, aoff=1, boff=100)
  <span class="org-variable-name">k_params</span>[i] = samples
</pre>
</div>

<p>
Write out the samples.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">with</span> <span class="org-builtin">open</span>(<span class="org-string">'/scratch/midway2/aksarkar/ideas/ipsc-gstt1-post.pkl'</span>, <span class="org-string">'wb'</span>) <span class="org-keyword">as</span> f:
  pickle.dump(k_params, f)
</pre>
</div>

<p>
Plot the 95% credible intervals for each parameter in each individual,
against genotype.
</p>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 3)
fig.set_size_inches(8, 3)
<span class="org-variable-name">n</span> = geno.shape[0]
<span class="org-keyword">for</span> j, (a, t) <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(<span class="org-builtin">zip</span>(ax, [<span class="org-string">'$\log\,k_r$'</span>, <span class="org-string">'$\log\,k_{\mathrm{on}}$'</span>, <span class="org-string">'$\log\,k_{\mathrm{off}}$'</span>])):
  <span class="org-variable-name">y</span> = np.array([np.log(k_params[i][-800:,j]).mean() <span class="org-keyword">for</span> i <span class="org-keyword">in</span> geno.index])
  <span class="org-variable-name">ci</span> = np.array([np.percentile(np.log(k_params[i][-800:,j]), [2.5, 97.5]) <span class="org-keyword">for</span> i <span class="org-keyword">in</span> geno.index]).T
  ax[j].errorbar(x=geno.values + np.random.normal(scale=0.1, size=n),
                 y=y,
                 yerr=<span class="org-builtin">abs</span>(ci - y),
                 fmt=<span class="org-string">'.'</span>,
                 c=<span class="org-string">'k'</span>,
                 ecolor=<span class="org-string">'0.7'</span>,
                 elinewidth=1,
                 ms=4,
  )
  ax[j].set_xlabel(<span class="org-string">'Imputed dosage'</span>)
  ax[j].set_ylabel(t)
  fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/diff-var.org/ipsc-gstt1-params.png" alt="ipsc-gstt1-params.png">
</p>
</div>

<p>
Plot one of the joint posteriors.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">names</span> = [<span class="org-string">'$\log\,k_r$'</span>, <span class="org-string">'$\log\,k_{\mathrm{on}}$'</span>, <span class="org-string">'$\log\,k_{\mathrm{off}}$'</span>]
plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 3)
fig.set_size_inches(8, 3)
<span class="org-keyword">for</span> (s, t), a <span class="org-keyword">in</span> <span class="org-builtin">zip</span>(it.combinations(<span class="org-builtin">range</span>(3), 2), ax):
  <span class="org-variable-name">x</span> = np.array([np.log(k_params[<span class="org-string">'NA18852'</span>][-800:,s]) <span class="org-keyword">for</span> i <span class="org-keyword">in</span> geno.index])
  <span class="org-variable-name">y</span> = np.array([np.log(k_params[<span class="org-string">'NA18852'</span>][-800:,t]) <span class="org-keyword">for</span> i <span class="org-keyword">in</span> geno.index])
  a.hexbin(x=x, y=y, gridsize=15, cmap=colorcet.cm[<span class="org-string">'gray_r'</span>])
  a.set_xlabel(names[s])
  a.set_ylabel(names[t])
  a.set_title(<span class="org-string">'NA18852'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/diff-var.org/ipsc-gstt1-joint.png" alt="ipsc-gstt1-joint.png">
</p>
</div>

<p>
Repeat the analysis for the rest of the vQTLs.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">k</span> = <span class="org-string">'ENSG00000113558'</span>
<span class="org-keyword">with</span> sqlite3.connect(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/browser/browser.db'</span>) <span class="org-keyword">as</span> conn:
  <span class="org-variable-name">geno</span> = pd.read_sql(<span class="org-string">'select * from mean_qtl_geno where gene = ?'</span>, con=conn, params=(k,)).set_index(<span class="org-string">'ind'</span>)[<span class="org-string">'value'</span>]
<span class="org-variable-name">query</span> = x.X.tocsc()[:,<span class="org-builtin">list</span>(x.var.index).index(k)].A.ravel()
<span class="org-variable-name">k_params</span> = <span class="org-builtin">dict</span>()
<span class="org-keyword">for</span> i <span class="org-keyword">in</span> geno.index:
  <span class="org-keyword">print</span>(f<span class="org-string">'Fitting individual {i}'</span>)
  <span class="org-variable-name">keep</span> = (x.obs[<span class="org-string">'chip_id'</span>] == i).values.ravel()
  <span class="org-variable-name">xg</span> = query[keep]
  <span class="org-variable-name">sg</span> = x.obs[<span class="org-string">'mol_hs'</span>].values[keep]
  <span class="org-variable-name">samples</span>, <span class="org-variable-name">_</span> = poisbeta.fit_poisson_beta_mcmc(xg, n_samples=1000, ar=1, br=xg.<span class="org-builtin">max</span>(), aon=1, bon=100, aoff=1, boff=100)
  <span class="org-variable-name">k_params</span>[i] = samples
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 3)
fig.set_size_inches(8, 3)
<span class="org-variable-name">n</span> = geno.shape[0]
<span class="org-keyword">for</span> j, (a, t) <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(<span class="org-builtin">zip</span>(ax, [<span class="org-string">'$\log\,k_r$'</span>, <span class="org-string">'$\log\,k_{\mathrm{on}}$'</span>, <span class="org-string">'$\log\,k_{\mathrm{off}}$'</span>])):
  <span class="org-variable-name">y</span> = np.array([np.log(k_params[i][-800:,j]).mean() <span class="org-keyword">for</span> i <span class="org-keyword">in</span> geno.index])
  <span class="org-variable-name">ci</span> = np.array([np.percentile(np.log(k_params[i][-800:,j]), [2.5, 97.5]) <span class="org-keyword">for</span> i <span class="org-keyword">in</span> geno.index]).T
  ax[j].errorbar(x=geno.values + np.random.normal(scale=0.1, size=n),
                 y=y,
                 yerr=<span class="org-builtin">abs</span>(ci - y),
                 fmt=<span class="org-string">'.'</span>,
                 c=<span class="org-string">'k'</span>,
                 ecolor=<span class="org-string">'0.7'</span>,
                 elinewidth=1,
                 ms=4,
  )
  ax[j].set_title(x.var.loc[<span class="org-string">'ENSG00000113558'</span>, <span class="org-string">'name'</span>])
  ax[j].set_xlabel(<span class="org-string">'Imputed dosage'</span>)
  ax[j].set_ylabel(t)
  fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/diff-var.org/ipsc-skp1.png" alt="ipsc-skp1.png">
</p>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">k</span> = <span class="org-string">'ENSG00000132507'</span>
<span class="org-keyword">with</span> sqlite3.connect(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/browser/browser.db'</span>) <span class="org-keyword">as</span> conn:
  <span class="org-variable-name">geno</span> = pd.read_sql(<span class="org-string">'select * from mean_qtl_geno where gene = ?'</span>, con=conn, params=(k,)).set_index(<span class="org-string">'ind'</span>)[<span class="org-string">'value'</span>]
<span class="org-variable-name">query</span> = x.X.tocsc()[:,<span class="org-builtin">list</span>(x.var.index).index(k)].A.ravel()
<span class="org-variable-name">k_params</span> = <span class="org-builtin">dict</span>()
<span class="org-keyword">for</span> i <span class="org-keyword">in</span> geno.index:
  <span class="org-keyword">print</span>(f<span class="org-string">'Fitting individual {i}'</span>)
  <span class="org-variable-name">keep</span> = (x.obs[<span class="org-string">'chip_id'</span>] == i).values.ravel()
  <span class="org-variable-name">xg</span> = query[keep]
  <span class="org-variable-name">sg</span> = x.obs[<span class="org-string">'mol_hs'</span>].values[keep]
  <span class="org-variable-name">samples</span>, <span class="org-variable-name">_</span> = poisbeta.fit_poisson_beta_mcmc(xg, n_samples=1000, ar=1, br=xg.<span class="org-builtin">max</span>(), aon=1, bon=100, aoff=1, boff=100)
  <span class="org-variable-name">k_params</span>[i] = samples
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 3)
fig.set_size_inches(8, 3)
<span class="org-variable-name">n</span> = geno.shape[0]
<span class="org-keyword">for</span> j, (a, t) <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(<span class="org-builtin">zip</span>(ax, [<span class="org-string">'$\log\,k_r$'</span>, <span class="org-string">'$\log\,k_{\mathrm{on}}$'</span>, <span class="org-string">'$\log\,k_{\mathrm{off}}$'</span>])):
  <span class="org-variable-name">y</span> = np.array([np.log(k_params[i][-800:,j]).mean() <span class="org-keyword">for</span> i <span class="org-keyword">in</span> geno.index])
  <span class="org-variable-name">ci</span> = np.array([np.percentile(np.log(k_params[i][-800:,j]), [2.5, 97.5]) <span class="org-keyword">for</span> i <span class="org-keyword">in</span> geno.index]).T
  ax[j].errorbar(x=geno.values + np.random.normal(scale=0.1, size=n),
                 y=y,
                 yerr=<span class="org-builtin">abs</span>(ci - y),
                 fmt=<span class="org-string">'.'</span>,
                 c=<span class="org-string">'k'</span>,
                 ecolor=<span class="org-string">'0.7'</span>,
                 elinewidth=1,
                 ms=4,
  )
  ax[j].set_title(x.var.loc[k, <span class="org-string">'name'</span>])
  ax[j].set_xlabel(<span class="org-string">'Imputed dosage'</span>)
  ax[j].set_ylabel(t)
  fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/diff-var.org/ipsc-ENSG00000132507.png" alt="ipsc-ENSG00000132507.png">
</p>
</div>
</div>
</div>

<div id="outline-container-orgb279d01" class="outline-4">
<h4 id="orgb279d01">iPSC-CM time course data</h4>
<div class="outline-text-4" id="text-orgb279d01">
<p>
<a href="https://www.nature.com/articles/s41598-020-58327-6">Selewa et al. 2019</a>
generated Drop-Seq of iPSCs differentiating into cardiomyocytes at days 0,
1, 3, 7, and 15. Read the data.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">counts</span> = si.mmread(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-ideas/data/czi/drop/counts.mtx.gz'</span>)
<span class="org-variable-name">genes</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-ideas/data/czi/drop/genes.txt'</span>, header=<span class="org-constant">None</span>)
<span class="org-variable-name">genes.columns</span> = [<span class="org-string">'name'</span>]
<span class="org-variable-name">cells</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-ideas/data/czi/drop/cells.txt'</span>, sep=<span class="org-string">' '</span>)
<span class="org-variable-name">cells.columns</span> = [<span class="org-string">'barcode'</span>, <span class="org-string">'day'</span>, <span class="org-string">'ind'</span>]
<span class="org-variable-name">cells</span>[<span class="org-string">'day'</span>] = pd.Categorical(cells[<span class="org-string">'day'</span>])
<span class="org-variable-name">x</span> = anndata.AnnData(counts.T.tocsc(), obs=cells, var=genes)
x.write(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-ideas/data/czi/drop/czi-ipsc-cm.h5ad'</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">y</span> = x[x.obs[<span class="org-string">'ind'</span>] == <span class="org-string">'Rep1'</span>]
sc.pp.filter_genes(y, min_counts=.01 * y.shape[0])
<span class="org-variable-name">y.var</span> = y.var.reset_index(drop=<span class="org-constant">True</span>)
y.shape
y.write(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-ideas/data/czi/drop/czi-ipsc-cm-rep1.h5ad'</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">y</span> = anndata.read_h5ad(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-ideas/data/czi/drop/czi-ipsc-cm-rep1.h5ad'</span>)
</pre>
</div>

<p>
Fit a fully unsupervised NMF model to achieve two goals:
</p>

<ol class="org-ol">
<li>Merge cells across time points which are in the same cell state</li>
<li>Cluster cells within/across time points to separate cell states</li>
</ol>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">m</span> = skd.NMF(n_components=10, beta_loss=1, solver=<span class="org-string">'mu'</span>, init=<span class="org-string">'random'</span>, max_iter=1000)
<span class="org-variable-name">l</span> = m.fit_transform(y.X)
<span class="org-variable-name">f</span> = m.components_
</pre>
</div>

<p>
Normalize the NMF solution into a topic model.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">weights</span> = l * f.<span class="org-builtin">sum</span>(axis=1)
<span class="org-variable-name">scale</span> = weights.<span class="org-builtin">sum</span>(axis=1, keepdims=<span class="org-constant">True</span>)
<span class="org-variable-name">weights</span> /= scale
<span class="org-variable-name">topics</span> = f / f.<span class="org-builtin">sum</span>(axis=1, keepdims=<span class="org-constant">True</span>)
</pre>
</div>

<p>
Write out the topic model.
</p>

<div class="org-src-container">
<pre class="src src-ipython">np.save(<span class="org-string">'/scratch/midway2/aksarkar/ideas/ipsc-cm-topic-weights.npz'</span>, weights)
np.save(<span class="org-string">'/scratch/midway2/aksarkar/ideas/ipsc-cm-topics.npz'</span>, topics)
np.save(<span class="org-string">'/scratch/midway2/aksarkar/ideas/ipsc-cm-topic-scale.npz'</span>, scale)
</pre>
</div>

<p>
First, compute the correlation matrix between the topic weights and the day
of the protocol.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">r</span> = np.corrcoef(pd.get_dummies(y.obs[<span class="org-string">'day'</span>]).T, weights.T)[:10,:4]
plt.clf()
plt.gcf().set_size_inches(4, 4)
plt.imshow(r, cmap=colorcet.cm[<span class="org-string">'coolwarm'</span>], vmin=-1, vmax=1)
plt.xlabel(<span class="org-string">'Day'</span>)
plt.xticks(np.arange(4), y.obs[<span class="org-string">'day'</span>].cat.categories)
plt.ylabel(<span class="org-string">'Topic'</span>)
plt.yticks(np.arange(10), 1 + np.arange(10))
<span class="org-variable-name">cb</span> = plt.colorbar()
cb.set_label(<span class="org-string">'Correlation'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/diff-var.org/ipsc-cm-day-topic-corr.png" alt="ipsc-cm-day-topic-corr.png">
</p>
</div>

<p>
We might expect (from a pseudotime perspective) that there are cells
&ldquo;between&rdquo; days, which would have non-trivial topic weight in multiple
days. If there were such cells, we might expect weak positive correlations
between multiple days and topics; however, we find only one such topic.
</p>

<p>
Interestingly, we also find multiple topics correlated with each day. To
begin with, examine topics 0 and 4 (which correlate with day 0), and look
at expression of pluritpotency markers in cells with maximal topic weight
in those topics.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">lam</span> = l @ f
<span class="org-variable-name">lam</span> /= y.X.<span class="org-builtin">sum</span>(axis=1)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Dark2'</span>)
plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 3, sharey=<span class="org-constant">True</span>)
fig.set_size_inches(7, 3)
<span class="org-keyword">for</span> a, k <span class="org-keyword">in</span> <span class="org-builtin">zip</span>(ax, [<span class="org-string">'POU5F1'</span>, <span class="org-string">'SOX2'</span>, <span class="org-string">'NANOG'</span>]):
  <span class="org-variable-name">idx</span> = y.var[y.var[<span class="org-string">'name'</span>] == k].reset_index(drop=<span class="org-constant">True</span>).index.astype(<span class="org-builtin">int</span>)
  <span class="org-keyword">for</span> j, topic <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>([0, 4]):
    <span class="org-variable-name">query</span> = lam[np.where(np.argmax(weights, 1) == j)[0],idx]
    a.hist(query, density=<span class="org-constant">True</span>, bins=np.linspace(0, 2e-3, 25), alpha=0.5, color=cm(j), label=f<span class="org-string">'Topic {topic}'</span>)
  a.set_xlabel(<span class="org-string">'Latent gene expression'</span>)
  a.set_title(k)
ax[0].set_ylabel(<span class="org-string">'Density'</span>)
ax[-1].legend(frameon=<span class="org-constant">False</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/diff-var.org/ipsc-cm-day-0-markers-topic-0-4.png" alt="ipsc-cm-day-0-markers-topic-0-4.png">
</p>
</div>

<p>
Based on the topic model, we might safely begin by just analyzing each time
point as a homogeneous collection of cells. Estimate gene expression mean
and variance for each gene in each time point.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">point_gamma_res</span> = <span class="org-builtin">dict</span>()
<span class="org-keyword">for</span> k <span class="org-keyword">in</span> (0, 1, 3, 7):
  <span class="org-variable-name">point_gamma_res</span>[k] = scmodes.ebpm.sgd.ebpm_point_gamma(
    y.X[y.obs[<span class="org-string">'day'</span>] == k],
    lr=1e-2, batch_size=128, max_epochs=20, verbose=<span class="org-constant">True</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">res</span> = pd.concat({k: pd.DataFrame(np.vstack(v[:-1]).T,
                                 columns=[<span class="org-string">'log_mu'</span>, <span class="org-string">'neg_log_phi'</span>, <span class="org-string">'logodds'</span>])
                  <span class="org-keyword">for</span> k, v <span class="org-keyword">in</span> point_gamma_res.items()}, axis=<span class="org-string">'columns'</span>)
res.to_csv(<span class="org-string">'/scratch/midway2/aksarkar/ideas/ipsc-cm-ebpm-point-gamma.txt.gz'</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">res</span> = pd.read_csv(<span class="org-string">'/scratch/midway2/aksarkar/ideas/ipsc-cm-ebpm-point-gamma.txt.gz'</span>, index_col=0, header=[0, 1])
</pre>
</div>

<p>
Look at the trajectories of dispersion over time.
</p>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
plt.gcf().set_size_inches(3, 3)
<span class="org-keyword">for</span> k, z <span class="org-keyword">in</span> res.loc(axis=1)[:,<span class="org-string">'neg_log_phi'</span>].iterrows():
  <span class="org-keyword">if</span> z.mean() &gt; -10:
    plt.plot(z.values.ravel(), c=<span class="org-string">'k'</span>, lw=1, alpha=0.1)
plt.xlabel(<span class="org-string">'Day'</span>)
plt.xticks(np.arange(4), [0, 1, 3, 7])
plt.ylabel(<span class="org-string">'$-\log\ \phi$'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/diff-var.org/ipsc-cm-log-phi.png" alt="ipsc-cm-log-phi.png">
</p>
</div>

<p>
Look at the trajectories of variance over time.
</p>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
plt.gcf().set_size_inches(3, 3)
<span class="org-keyword">for</span> k, z <span class="org-keyword">in</span> res.iterrows():
  <span class="org-variable-name">var</span> = np.exp(2 * z.loc[:,<span class="org-string">'log_mu'</span>] - z.loc[:,<span class="org-string">'neg_log_phi'</span>])
  <span class="org-keyword">if</span> np.median(var) &gt; 1e-6:
    plt.plot(var.values.ravel(), c=<span class="org-string">'k'</span>, lw=1, alpha=0.25)
plt.xlabel(<span class="org-string">'Day'</span>)
plt.xticks(np.arange(4), [0, 1, 3, 7])
plt.ylabel(<span class="org-string">'Variance of latent gene expression'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/diff-var.org/ipsc-cm-var.png" alt="ipsc-cm-var.png">
</p>
</div>

<p>
Extract some interesting examples.
</p>

<div class="org-src-container">
<pre class="src src-ipython">res[np.logical_and(res.loc(axis=1)[<span class="org-string">'0'</span>, <span class="org-string">'neg_log_phi'</span>] &lt; 2, res.loc(axis=1)[<span class="org-string">'1'</span>, <span class="org-string">'neg_log_phi'</span>] &gt; 2)].head()
</pre>
</div>

<pre class="example">
0                               1                               3  \
log_mu neg_log_phi   logodds    log_mu neg_log_phi   logodds    log_mu
38  -8.624235    1.224293 -8.834169 -7.599703    5.335293 -9.648245 -7.419324
127 -8.624235    1.224293 -8.834169 -7.599703    5.335293 -9.648245 -7.419324
254 -8.624235    1.224293 -8.834169 -7.599703    5.335293 -9.648245 -7.419324
286 -8.624235    1.224293 -8.834169 -7.599703    5.335293 -9.648245 -7.419324
320 -8.624235    1.224293 -8.834169 -7.599703    5.335293 -9.648245 -7.419324

7
neg_log_phi   logodds    log_mu neg_log_phi   logodds
38     5.027008 -9.271143 -7.770187      2.8829 -8.345497
127    5.027008 -9.271143 -7.770187      2.8829 -8.345497
254    5.027008 -9.271143 -7.770187      2.8829 -8.345497
286    5.027008 -9.271143 -7.770187      2.8829 -8.345497
320    5.027008 -9.271143 -7.770187      2.8829 -8.345497
</pre>

<p>
<i>NDUFAB1</i> appears to show a reduction in variance between days 0 and 1, and
then a subsequent gain in variance between days 3 and 7.
</p>

<div class="org-src-container">
<pre class="src src-ipython">y.var.iloc[38]
</pre>
</div>

<pre class="example">
name        NDUFAB1
n_counts       5571
Name: 49, dtype: object
</pre>

<p>
Plot the data and the fitted distributions
</p>

<div class="org-src-container">
<pre class="src src-ipython">plt.set_cmap(<span class="org-string">'Dark2'</span>)
plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(2, 1)
fig.set_size_inches(5, 4)

<span class="org-variable-name">days</span> = [0, 1, 3, 7]
<span class="org-variable-name">query</span> = y.X[:,38].A.ravel()
<span class="org-variable-name">bins</span> = np.arange(query.<span class="org-builtin">max</span>() + 1)
<span class="org-keyword">for</span> i, k <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(days):
  ax[0].hist(query[y.obs[<span class="org-string">'day'</span>] == k], bins=bins, color=f<span class="org-string">'C{i}'</span>, alpha=0.5)
ax[0].set_xlabel(<span class="org-string">'Number of molecules'</span>)
ax[0].set_ylabel(<span class="org-string">'Number of cells'</span>)

<span class="org-variable-name">grid</span> = np.linspace(0, 1e-3, 1000)
<span class="org-keyword">for</span> i, k <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>([<span class="org-builtin">str</span>(d) <span class="org-keyword">for</span> d <span class="org-keyword">in</span> days]):
  <span class="org-variable-name">log_mu</span> = res.loc[38, (k, <span class="org-string">'log_mu'</span>)]
  <span class="org-variable-name">neg_log_phi</span> = res.loc[38, (k, <span class="org-string">'neg_log_phi'</span>)]
  <span class="org-variable-name">logodds</span> = res.loc[38, (k, <span class="org-string">'logodds'</span>)]
  <span class="org-variable-name">F</span> = st.gamma(a=np.exp(neg_log_phi), scale=np.exp(log_mu - neg_log_phi)).cdf(grid)
  <span class="org-variable-name">F</span> = sp.expit(logodds) + sp.expit(-logodds) * F
  ax[1].plot(grid, F, lw=1, color=f<span class="org-string">'C{i}'</span>, label=f<span class="org-string">'Day {k}'</span>)
ax[1].legend(frameon=<span class="org-constant">False</span>)
ax[1].set_xlabel(<span class="org-string">'Latent gene expression'</span>)
ax[1].set_ylabel(<span class="org-string">'CDF'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/diff-var.org/ipsc-cm-NDUFAB1.png" alt="ipsc-cm-NDUFAB1.png">
</p>
</div>

<p>
Look at examples with the opposite trajectory.
</p>

<div class="org-src-container">
<pre class="src src-ipython">res[np.logical_and(res.loc(axis=1)[<span class="org-string">'0'</span>, <span class="org-string">'neg_log_phi'</span>] &gt; 8, res.loc(axis=1)[<span class="org-string">'1'</span>, <span class="org-string">'neg_log_phi'</span>] &lt; 4)].head()
</pre>
</div>

<pre class="example">
0                              1                               3  \
log_mu neg_log_phi   logodds   log_mu neg_log_phi   logodds    log_mu
5851 -4.983784    8.014603 -3.454462 -4.98496    3.259651 -1.306795 -5.037562
6641 -4.983784    8.014603 -3.454462 -4.98496    3.259651 -1.306795 -5.037562

7
neg_log_phi   logodds    log_mu neg_log_phi   logodds
5851    3.340554 -1.143311 -5.236426    7.055284 -2.009873
6641    3.340554 -1.143311 -5.236426    7.055284 -2.009873
</pre>

<div class="org-src-container">
<pre class="src src-ipython">x.var.iloc[[5851, 6641]]
</pre>
</div>

<pre class="example">
name
5851   PRCP
6641  UBXN4
</pre>

<div class="org-src-container">
<pre class="src src-ipython">plt.set_cmap(<span class="org-string">'Dark2'</span>)
plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(2, 1)
fig.set_size_inches(5, 4)

<span class="org-variable-name">days</span> = [0, 1, 3, 7]
<span class="org-variable-name">query</span> = y.X[:,(y.var[<span class="org-string">'name'</span>] == <span class="org-string">'PRCP'</span>).values].A.ravel()
<span class="org-variable-name">bins</span> = np.arange(query.<span class="org-builtin">max</span>() + 1)
<span class="org-keyword">for</span> i, k <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(days):
  ax[0].hist(query[y.obs[<span class="org-string">'day'</span>] == k], bins=bins, color=f<span class="org-string">'C{i}'</span>, alpha=0.5)
ax[0].set_xlabel(<span class="org-string">'Number of molecules'</span>)
ax[0].set_ylabel(<span class="org-string">'Number of cells'</span>)
ax[0].set_title(x.var.iloc[idx][0])

<span class="org-variable-name">grid</span> = np.linspace(0, 1e-2, 1000)
<span class="org-keyword">for</span> i, k <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>([<span class="org-builtin">str</span>(d) <span class="org-keyword">for</span> d <span class="org-keyword">in</span> days]):
  <span class="org-variable-name">log_mu</span> = res.loc[idx, (k, <span class="org-string">'log_mu'</span>)]
  <span class="org-variable-name">neg_log_phi</span> = res.loc[idx, (k, <span class="org-string">'neg_log_phi'</span>)]
  <span class="org-variable-name">logodds</span> = res.loc[idx, (k, <span class="org-string">'logodds'</span>)]
  <span class="org-variable-name">F</span> = st.gamma(a=np.exp(neg_log_phi), scale=np.exp(log_mu - neg_log_phi)).cdf(grid)
  <span class="org-variable-name">F</span> = sp.expit(logodds) + sp.expit(-logodds) * F
  ax[1].plot(grid, F, lw=1, color=f<span class="org-string">'C{i}'</span>, label=f<span class="org-string">'Day {k}'</span>)
ax[1].legend(frameon=<span class="org-constant">False</span>)
ax[1].set_xlabel(<span class="org-string">'Latent gene expression'</span>)
ax[1].set_ylabel(<span class="org-string">'CDF'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/diff-var.org/ipsc-cm-PRCP.png" alt="ipsc-cm-PRCP.png">
</p>
</div>

<p>
Fit kinetic models for <i>PRCP</i> gene expression at each time point.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">idx</span> = 5851
<span class="org-variable-name">k_params</span> = <span class="org-builtin">dict</span>()
<span class="org-variable-name">traces</span> = <span class="org-builtin">dict</span>()
<span class="org-keyword">for</span> k <span class="org-keyword">in</span> [0, 1, 3, 7]:
  <span class="org-keyword">print</span>(f<span class="org-string">'Fitting day {k}'</span>)
  <span class="org-variable-name">query</span> = y.X[:,idx].A.ravel()
  <span class="org-comment-delimiter"># </span><span class="org-comment">Default hyperparameters follow Kim &amp; Marioni 2013</span>
  <span class="org-variable-name">samples</span>, <span class="org-variable-name">trace</span> = poisbeta.fit_poisson_beta_mcmc(query, n_samples=1000)
  <span class="org-variable-name">k_params</span>[k] = samples
  <span class="org-variable-name">traces</span>[k] = trace
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">with</span> <span class="org-builtin">open</span>(<span class="org-string">'/scratch/midway2/aksarkar/ideas/ipsc-cm-mcmc.pkl'</span>, <span class="org-string">'wb'</span>) <span class="org-keyword">as</span> f:
  pickle.dump((k_params, traces), f)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">with</span> <span class="org-builtin">open</span>(<span class="org-string">'/scratch/midway2/aksarkar/ideas/ipsc-cm-mcmc.pkl'</span>, <span class="org-string">'rb'</span>) <span class="org-keyword">as</span> f:
  <span class="org-variable-name">k_params</span>, <span class="org-variable-name">traces</span> = pickle.load(f)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">mcmc_res</span> = pd.concat({k: pd.DataFrame(k_params[k], columns=[<span class="org-string">'log_k_r'</span>, <span class="org-string">'log_k_on'</span>, <span class="org-string">'log_k_koff'</span>]) <span class="org-keyword">for</span> k <span class="org-keyword">in</span> k_params}, axis=<span class="org-string">'columns'</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 3)
fig.set_size_inches(7, 3)
<span class="org-keyword">for</span> a, k, t <span class="org-keyword">in</span> <span class="org-builtin">zip</span>(ax, [<span class="org-string">'log_k_r'</span>, <span class="org-string">'log_k_on'</span>, <span class="org-string">'log_k_koff'</span>], [<span class="org-string">'$\log\ k_r$'</span>, <span class="org-string">'$\log\ k_{\mathrm{on}}$'</span>, <span class="org-string">'$\log\ k_{\mathrm{off}}$'</span>]):
  <span class="org-variable-name">samples</span> = mcmc_res.loc(axis=1)[:,k][-800:]
  <span class="org-variable-name">pm</span> = samples.mean()
  a.errorbar(x=np.arange(4),
             y=pm,
             yerr=<span class="org-builtin">abs</span>(np.percentile(samples, [2.5, 97.5], axis=0) - pm.values.reshape(1, -1)),
             fmt=<span class="org-string">'.'</span>,
             c=<span class="org-string">'k'</span>,
             ecolor=<span class="org-string">'0.7'</span>,
             elinewidth=1,
             ms=4,)
  a.set_xlabel(<span class="org-string">'Day'</span>)
  a.set_xticks(np.arange(4))
  a.set_xticklabels([0, 1, 3, 7])
  a.set_ylabel(t)
  a.set_title(y.var.loc[<span class="org-builtin">str</span>(idx), <span class="org-string">'name'</span>])
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/diff-var.org/ipsc-cm-post.png" alt="ipsc-cm-post.png">
</p>
</div>

<p>
Look at the most variable gene.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">order</span> = np.argsort(-np.median(np.exp(2 * res.loc(axis=1)[:,<span class="org-string">'log_mu'</span>].values - res.loc(axis=1)[:,<span class="org-string">'neg_log_phi'</span>].values), axis=1))[:10]
y.var.loc[order]
</pre>
</div>

<pre class="example">
name  n_counts
10227   MALAT1   53633.0
3178    EXOSC8    5236.0
8226    RPL35A   36692.0
8929    RPL37A   29614.0
9331   MT-ND4L   12695.0
8645    COMMD6    6545.0
2993     RPS25   11410.0
10449   MRPS21    9434.0
7942     DDX10    1003.0
2640   SELENOK    2380.0
</pre>

<div class="org-src-container">
<pre class="src src-ipython">plt.set_cmap(<span class="org-string">'Dark2'</span>)
plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(2, 1)
fig.set_size_inches(5, 4)

<span class="org-variable-name">idx</span> = 10227
<span class="org-variable-name">days</span> = [0, 1, 3, 7]
<span class="org-variable-name">query</span> = y.X[:,idx].A.ravel()
<span class="org-variable-name">bins</span> = np.arange(query.<span class="org-builtin">max</span>() + 1)
<span class="org-keyword">for</span> i, k <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(days):
  <span class="org-variable-name">h</span>, <span class="org-variable-name">e</span> = np.histogram(query[y.obs[<span class="org-string">'day'</span>] == k], bins=bins)
  ax[0].plot(bins[:-1] + 0.5, h, color=f<span class="org-string">'C{i}'</span>, lw=1)
ax[0].set_xlabel(<span class="org-string">'Number of molecules'</span>)
ax[0].set_ylabel(<span class="org-string">'Number of cells'</span>)
ax[0].set_title(y.var.iloc[idx][<span class="org-string">'name'</span>])

<span class="org-variable-name">lam</span> = query / y.X.<span class="org-builtin">sum</span>(axis=1).A.ravel()
<span class="org-variable-name">grid</span> = np.linspace(lam.<span class="org-builtin">min</span>(), lam.<span class="org-builtin">max</span>(), 1000)
<span class="org-keyword">for</span> i, k <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>([<span class="org-builtin">str</span>(d) <span class="org-keyword">for</span> d <span class="org-keyword">in</span> days]):
  <span class="org-variable-name">log_mu</span> = res.loc[idx, (k, <span class="org-string">'log_mu'</span>)]
  <span class="org-variable-name">neg_log_phi</span> = res.loc[idx, (k, <span class="org-string">'neg_log_phi'</span>)]
  <span class="org-variable-name">logodds</span> = res.loc[idx, (k, <span class="org-string">'logodds'</span>)]
  <span class="org-variable-name">F</span> = st.gamma(a=np.exp(neg_log_phi), scale=np.exp(log_mu - neg_log_phi)).cdf(grid)
  <span class="org-variable-name">F</span> = sp.expit(logodds) + sp.expit(-logodds) * F
  ax[1].plot(grid, F, lw=1, color=f<span class="org-string">'C{i}'</span>, label=f<span class="org-string">'Day {k}'</span>)
ax[1].legend(frameon=<span class="org-constant">False</span>)
ax[1].set_xlabel(<span class="org-string">'Latent gene expression'</span>)
ax[1].set_ylabel(<span class="org-string">'CDF'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/diff-var.org/ipsc-cm-COMMD6.png" alt="ipsc-cm-COMMD6.png">
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org65b70b2" class="outline-3">
<h3 id="org65b70b2">Variational inference</h3>
<div class="outline-text-3" id="text-org65b70b2">
</div>
<div id="outline-container-orgdeb8a55" class="outline-4">
<h4 id="orgdeb8a55">Null simulation</h4>
<div class="outline-text-4" id="text-orgdeb8a55">
<p>
Simulate from the model, fixing \(\delmu = \delphi = 0\).
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">rng</span> = np.random.default_rng(1)
<span class="org-variable-name">n</span> = 500
<span class="org-variable-name">log_mean</span> = -10
<span class="org-variable-name">log_inv_disp</span> = -0.5
<span class="org-variable-name">lam</span> = rng.gamma(shape=np.exp(log_inv_disp), scale=np.exp(log_mean - log_inv_disp), size=2 * n)
<span class="org-variable-name">s</span> = np.full(2 * n, 1e5)
<span class="org-variable-name">x</span> = rng.poisson(s * lam)
<span class="org-variable-name">z</span> = np.zeros(2 * n, dtype=<span class="org-builtin">bool</span>)
<span class="org-variable-name">z</span>[:n] = 1
</pre>
</div>

<p>
Plot the simulated data.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Dark2'</span>)
plt.clf()
plt.gcf().set_size_inches(4, 2)
<span class="org-variable-name">grid</span> = np.arange(x.<span class="org-builtin">max</span>() + 2)
plt.hist(x[z], bins=grid, color=cm(0), density=<span class="org-constant">True</span>, alpha=0.35)
plt.hist(x[~z], bins=grid, color=cm(1), density=<span class="org-constant">True</span>, alpha=0.25)
plt.plot(grid + .5, st.nbinom(n=np.exp(log_inv_disp), p=1 / (1 + s[0] * np.exp(log_mean - log_inv_disp))).pmf(grid), c=<span class="org-string">'k'</span>, lw=1)
plt.xlabel(<span class="org-string">'Number of molecules'</span>)
plt.ylabel(<span class="org-string">'Density'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/diff-var.org/vi-ex0.png" alt="vi-ex0.png">
</p>
</div>

<p>
Fit the model.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">n_epochs</span> = 2000
<span class="org-variable-name">n_samples</span> = 10
<span class="org-variable-name">run</span> = 0
<span class="org-variable-name">fit0</span> = scmodes.ebpm.ebpm_gamma(x, s)
torch.manual_seed(run)
<span class="org-variable-name">m</span> = DiffVarExprModel(
  prior_log_mean=torch.distributions.Normal(loc=fit0[0], scale=1.),
  prior_log_inv_disp=torch.distributions.Normal(loc=fit0[1], scale=1.))
m.fit(
  torch.tensor(x.reshape(-1, 1), dtype=torch.<span class="org-builtin">float</span>),
  torch.tensor(s.reshape(-1, 1), dtype=torch.<span class="org-builtin">float</span>),
  torch.tensor(z.reshape(-1, 1), dtype=torch.<span class="org-builtin">float</span>),
  n_epochs=n_epochs,
  n_samples=n_samples,
  log_dir=f<span class="org-string">'/scratch/midway2/aksarkar/singlecell/runs/diffvar/dev_nb_{n_epochs}_{n_samples}_{run}'</span>)
</pre>
</div>

<pre class="example">
DiffVarExprModel()

</pre>

<p>
Plot the fit.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Dark2'</span>)
plt.clf()
plt.gcf().set_size_inches(4, 2)
<span class="org-variable-name">grid</span> = np.arange(x.<span class="org-builtin">max</span>() + 2)
plt.hist(x[z], bins=grid, color=cm(0), density=<span class="org-constant">True</span>, alpha=0.35)
plt.plot(grid + .5, st.nbinom(n=np.exp(m.log_inv_disp[0]), p=1 / (1 + s[0] * np.exp(m.log_mean[0] - m.log_inv_disp[0]))).pmf(grid), c=cm(0), lw=1)
plt.hist(x[~z], bins=grid, color=cm(1), density=<span class="org-constant">True</span>, alpha=0.25)
plt.plot(grid + .5, st.nbinom(n=np.exp(m.log_inv_disp[0] - m.diff_inv_disp[0]), p=1 / (1 + s[0] * np.exp(m.log_mean[0] - m.log_inv_disp[0] - m.diff_mean[0] - m.diff_inv_disp[0]))).pmf(grid), c=cm(1), lw=1, ls=<span class="org-string">'--'</span>)
plt.xlabel(<span class="org-string">'Number of molecules'</span>)
plt.ylabel(<span class="org-string">'Density'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/diff-var.org/vi-ex0-fit.png" alt="vi-ex0-fit.png">
</p>
</div>

<p>
Report the LFSR for DE/DD.
</p>

<div class="org-src-container">
<pre class="src src-ipython">pd.Series({
  <span class="org-string">'de'</span>: m.lfsr_diff_mean[0],
  <span class="org-string">'dd'</span>: m.lfsr_diff_inv_disp[0]
})
</pre>
</div>

<pre class="example">
de    0.426195
dd    0.305537
dtype: float64
</pre>
</div>
</div>

<div id="outline-container-orgd43e5d6" class="outline-4">
<h4 id="orgd43e5d6">DE</h4>
<div class="outline-text-4" id="text-orgd43e5d6">
<p>
Simulate data under differential expression, but not differential
dispersion.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">rng</span> = np.random.default_rng(1)
<span class="org-variable-name">n</span> = 500
<span class="org-variable-name">z</span> = np.zeros(2 * n, dtype=<span class="org-builtin">bool</span>)
<span class="org-variable-name">z</span>[:n] = 1
<span class="org-variable-name">log_mean</span> = -10
<span class="org-variable-name">log_inv_disp</span> = -0.5
<span class="org-variable-name">diff_mean</span> = np.log(2)
<span class="org-variable-name">lam</span> = rng.gamma(shape=np.exp(log_inv_disp), scale=np.exp(log_mean - log_inv_disp - z * diff_mean), size=2 * n)
<span class="org-variable-name">s</span> = np.full(2 * n, 1e5)
<span class="org-variable-name">x</span> = rng.poisson(s * lam)
</pre>
</div>

<p>
Plot the simulated data.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Dark2'</span>)
plt.clf()
plt.gcf().set_size_inches(4, 2)
<span class="org-variable-name">grid</span> = np.arange(x.<span class="org-builtin">max</span>() + 2)
plt.hist(x[z], bins=grid, color=cm(0), density=<span class="org-constant">True</span>, alpha=0.35)
plt.plot(grid + .5, st.nbinom(n=np.exp(log_inv_disp), p=1 / (1 + s[0] * np.exp(log_mean - log_inv_disp - diff_mean))).pmf(grid), c=cm(0), lw=1)
plt.hist(x[~z], bins=grid, color=cm(1), density=<span class="org-constant">True</span>, alpha=0.25)
plt.plot(grid + .5, st.nbinom(n=np.exp(log_inv_disp), p=1 / (1 + s[0] * np.exp(log_mean - log_inv_disp))).pmf(grid), c=cm(1), lw=1)
plt.xlabel(<span class="org-string">'Number of molecules'</span>)
plt.ylabel(<span class="org-string">'Density'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/diff-var.org/vi-ex1.png" alt="vi-ex1.png">
</p>
</div>

<p>
Fit the model.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">n_epochs</span> = 2000
<span class="org-variable-name">n_samples</span> = 10
<span class="org-variable-name">run</span> = 0
<span class="org-variable-name">fit0</span> = scmodes.ebpm.ebpm_gamma(x, s)
torch.manual_seed(run)
<span class="org-variable-name">m</span> = DiffVarExprModel(
  prior_log_mean=torch.distributions.Normal(loc=fit0[0], scale=1.),
  prior_log_inv_disp=torch.distributions.Normal(loc=fit0[1], scale=1.))
m.fit(
  torch.tensor(x.reshape(-1, 1), dtype=torch.<span class="org-builtin">float</span>),
  torch.tensor(s.reshape(-1, 1), dtype=torch.<span class="org-builtin">float</span>),
  torch.tensor(z.reshape(-1, 1), dtype=torch.<span class="org-builtin">float</span>),
  n_epochs=n_epochs,
  n_samples=n_samples,
  log_dir=f<span class="org-string">'/scratch/midway2/aksarkar/singlecell/runs/diffvar/dev_nb_2fold_{n_epochs}_{n_samples}_{run}'</span>)
</pre>
</div>

<pre class="example">
DiffVarExprModel()

</pre>

<p>
Plot the fit.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Dark2'</span>)
plt.clf()
plt.gcf().set_size_inches(4, 2)
<span class="org-variable-name">grid</span> = np.arange(x.<span class="org-builtin">max</span>() + 2)
plt.hist(x[z], bins=grid, color=cm(0), density=<span class="org-constant">True</span>, alpha=0.35)
plt.plot(grid + .5, st.nbinom(
  n=np.exp(m.log_inv_disp[0] + m.diff_inv_disp[0]),
  p=1 / (1 + s[0] * np.exp(m.log_mean[0] - m.log_inv_disp[0] + m.diff_mean[0] + m.diff_inv_disp[0]))).pmf(grid), c=cm(0), lw=1)
plt.hist(x[~z], bins=grid, color=cm(1), density=<span class="org-constant">True</span>, alpha=0.25)
plt.plot(grid + .5, st.nbinom(
  n=np.exp(m.log_inv_disp[0]),
  p=1 / (1 + s[0] * np.exp(m.log_mean[0] - m.log_inv_disp[0]))).pmf(grid), c=cm(1), lw=1, ls=<span class="org-string">'--'</span>)
plt.xlabel(<span class="org-string">'Number of molecules'</span>)
plt.ylabel(<span class="org-string">'Density'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/diff-var.org/vi-ex1-fit.png" alt="vi-ex1-fit.png">
</p>
</div>

<p>
Report the lfsr for DE/DD.
</p>

<div class="org-src-container">
<pre class="src src-ipython">pd.Series({
  <span class="org-string">'de'</span>: m.lfsr_diff_mean[0],
  <span class="org-string">'dd'</span>: m.lfsr_diff_inv_disp[0]
})
</pre>
</div>

<pre class="example">
de    3.836553e-25
dd    3.287435e-01
dtype: float64
</pre>
</div>
</div>

<div id="outline-container-org4b0df7e" class="outline-4">
<h4 id="org4b0df7e">DD</h4>
<div class="outline-text-4" id="text-org4b0df7e">
<p>
Simulate data under differential dispersion, but not differential
expression.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">rng</span> = np.random.default_rng(1)
<span class="org-variable-name">n</span> = 500
<span class="org-variable-name">z</span> = np.zeros(2 * n, dtype=<span class="org-builtin">bool</span>)
<span class="org-variable-name">z</span>[:n] = 1
<span class="org-variable-name">log_mean</span> = -10
<span class="org-variable-name">log_inv_disp</span> = -0.5
<span class="org-variable-name">diff_inv_disp</span> = np.log(2)
<span class="org-variable-name">lam</span> = rng.gamma(shape=np.exp(log_inv_disp + z * diff_inv_disp), scale=np.exp(log_mean - log_inv_disp - z * diff_inv_disp), size=2 * n)
<span class="org-variable-name">s</span> = np.full(2 * n, 1e5)
<span class="org-variable-name">x</span> = rng.poisson(s * lam)
</pre>
</div>

<p>
Plot the simulated data.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Dark2'</span>)
plt.clf()
plt.gcf().set_size_inches(4, 2)
<span class="org-variable-name">grid</span> = np.arange(x.<span class="org-builtin">max</span>() + 2)
plt.hist(x[z], bins=grid, color=cm(0), density=<span class="org-constant">True</span>, alpha=0.35)
plt.plot(grid + .5, st.nbinom(n=np.exp(log_inv_disp + diff_inv_disp), p=1 / (1 + s[0] * np.exp(log_mean - log_inv_disp - diff_inv_disp))).pmf(grid), c=cm(0), lw=1)
plt.hist(x[~z], bins=grid, color=cm(1), density=<span class="org-constant">True</span>, alpha=0.25)
plt.plot(grid + .5, st.nbinom(n=np.exp(log_inv_disp), p=1 / (1 + s[0] * np.exp(log_mean - log_inv_disp))).pmf(grid), c=cm(1), lw=1)
plt.xlabel(<span class="org-string">'Number of molecules'</span>)
plt.ylabel(<span class="org-string">'Density'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/diff-var.org/vi-ex2.png" alt="vi-ex2.png">
</p>
</div>

<p>
Fit the model.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">n_epochs</span> = 2000
<span class="org-variable-name">n_samples</span> = 10
<span class="org-variable-name">run</span> = 0
<span class="org-variable-name">fit0</span> = scmodes.ebpm.ebpm_gamma(x, s)
torch.manual_seed(run)
<span class="org-variable-name">m</span> = DiffVarExprModel(
  prior_log_mean=torch.distributions.Normal(loc=fit0[0], scale=1.),
  prior_log_inv_disp=torch.distributions.Normal(loc=fit0[1], scale=1.))
m.fit(
  torch.tensor(x.reshape(-1, 1), dtype=torch.<span class="org-builtin">float</span>),
  torch.tensor(s.reshape(-1, 1), dtype=torch.<span class="org-builtin">float</span>),
  torch.tensor(z.reshape(-1, 1), dtype=torch.<span class="org-builtin">float</span>),
  n_epochs=n_epochs,
  n_samples=n_samples,
  log_dir=f<span class="org-string">'/scratch/midway2/aksarkar/singlecell/runs/diffvar/dev_nb_dd_2fold_{n_epochs}_{n_samples}_{run}'</span>)
</pre>
</div>

<pre class="example">
DiffVarExprModel()

</pre>

<p>
Plot the fit.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Dark2'</span>)
plt.clf()
plt.gcf().set_size_inches(4, 2)
<span class="org-variable-name">grid</span> = np.arange(x.<span class="org-builtin">max</span>() + 2)
plt.hist(x[z], bins=grid, color=cm(0), density=<span class="org-constant">True</span>, alpha=0.35)
plt.plot(grid + .5, st.nbinom(
  n=np.exp(m.log_inv_disp[0] - m.diff_inv_disp[0]),
  p=1 / (1 + s[0] * np.exp(m.log_mean[0] - m.log_inv_disp[0] + m.diff_mean[0] + m.diff_inv_disp[0]))).pmf(grid), c=cm(0), lw=1)
plt.hist(x[~z], bins=grid, color=cm(1), density=<span class="org-constant">True</span>, alpha=0.25)
plt.plot(grid + .5, st.nbinom(
  n=np.exp(m.log_inv_disp[0]),
  p=1 / (1 + s[0] * np.exp(m.log_mean[0] - m.log_inv_disp[0]))).pmf(grid), c=cm(1), lw=1, ls=<span class="org-string">'--'</span>)
plt.xlabel(<span class="org-string">'Number of molecules'</span>)
plt.ylabel(<span class="org-string">'Density'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/diff-var.org/vi-ex2-fit.png" alt="vi-ex2-fit.png">
</p>
</div>

<p>
Report the lfsr for DE/DD.
</p>

<div class="org-src-container">
<pre class="src src-ipython">pd.Series({
  <span class="org-string">'de'</span>: m.lfsr_diff_mean[0],
  <span class="org-string">'dd'</span>: m.lfsr_diff_inv_disp[0]
})
</pre>
</div>

<pre class="example">
de    3.532345e-01
dd    6.212102e-22
dtype: float64
</pre>
</div>
</div>

<div id="outline-container-orgfc114a5" class="outline-4">
<h4 id="orgfc114a5">Simulation study</h4>
<div class="outline-text-4" id="text-orgfc114a5">
<p>
Look at power at fixed lfsr.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">simulate</span>(n_samples, n_mols, log_mean, log_inv_disp, diff_log_mean, diff_log_inv_disp, seed):
  np.random.seed(seed)
  <span class="org-variable-name">z</span> = np.zeros(2 * n_samples)
  <span class="org-variable-name">z</span>[n_samples:] = 1
  <span class="org-variable-name">s</span> = n_mols * np.ones(2 * n_samples)
  <span class="org-variable-name">a</span> = np.exp(log_inv_disp - z * diff_log_inv_disp)
  <span class="org-variable-name">b</span> = np.exp(-log_mean + log_inv_disp - z * (diff_log_mean + diff_log_inv_disp))
  <span class="org-variable-name">x</span> = st.nbinom(n=a, p=1 / (1 + n_mols / b)).rvs(2 * n_samples)
  <span class="org-keyword">return</span> x, s, z

<span class="org-keyword">def</span> <span class="org-function-name">fit_dev</span>(x, s, z, n_epochs=2000, n_samples=10):
  <span class="org-variable-name">init</span> = scmodes.ebpm.ebpm_gamma(x, s)
  torch.manual_seed(run)
  <span class="org-variable-name">m</span> = DiffVarExprModel(
    prior_log_mean=torch.distributions.Normal(loc=init[0], scale=1.),
    prior_log_inv_disp=torch.distributions.Normal(loc=init[1], scale=1.))
  m.fit(
    torch.tensor(x.reshape(-1, 1), dtype=torch.<span class="org-builtin">float</span>),
    torch.tensor(s.reshape(-1, 1), dtype=torch.<span class="org-builtin">float</span>),
    torch.tensor(z.reshape(-1, 1), dtype=torch.<span class="org-builtin">float</span>),
    n_epochs=n_epochs,
    n_samples=n_samples)
  <span class="org-keyword">return</span> m

<span class="org-keyword">def</span> <span class="org-function-name">evaluate_lfsr_diff_mean</span>(n_samples, n_mols, n_trials):
  <span class="org-variable-name">result</span> = []
  <span class="org-keyword">for</span> log_mean <span class="org-keyword">in</span> np.linspace(-12, -6, 7):
    <span class="org-keyword">for</span> log_inv_disp <span class="org-keyword">in</span> np.logspace(-4, 0, 5):
      <span class="org-keyword">for</span> diff_log_mean <span class="org-keyword">in</span> np.linspace(0, np.log(2), 5):
        <span class="org-keyword">for</span> trial <span class="org-keyword">in</span> <span class="org-builtin">range</span>(n_trials):
          <span class="org-variable-name">x</span>, <span class="org-variable-name">s</span>, <span class="org-variable-name">z</span> = simulate(n_samples, n_mols, log_mean, log_inv_disp, diff_log_mean, diff_log_inv_disp=0, seed=trial)
          <span class="org-variable-name">m</span> = fit_dev(x, s, z)
          result.append([n_samples, n_mols, log_mean, log_inv_disp, diff_log_mean, 0., trial, m.lfsr_diff_mean[0]])
  <span class="org-variable-name">result</span> = pd.DataFrame(result)
  <span class="org-variable-name">result.columns</span> = [<span class="org-string">'n_samples'</span>, <span class="org-string">'n_mols'</span>, <span class="org-string">'log_mean'</span>, <span class="org-string">'log_inv_disp'</span>, <span class="org-string">'diff_log_mean'</span>, <span class="org-string">'diff_log_inv_disp'</span>, <span class="org-string">'trial'</span>, <span class="org-string">'lfsr'</span>]
  <span class="org-keyword">return</span> result
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">%%timeit -n1 -r1
<span class="org-variable-name">res</span> = evaluate_lfsr_diff_mean(500, 1e5, 1)
res.to_csv(<span class="org-string">'temp.txt.gz'</span>, sep=<span class="org-string">'\t'</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-org7280499" class="outline-3">
<h3 id="org7280499">Slice within Gibbs</h3>
<div class="outline-text-3" id="text-org7280499">
</div>
<div id="outline-container-orgd011c08" class="outline-4">
<h4 id="orgd011c08">Null example</h4>
<div class="outline-text-4" id="text-orgd011c08">
<p>
Simulate from the model.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">rng</span> = np.random.default_rng(1)
<span class="org-variable-name">n</span> = 500
<span class="org-variable-name">log_mean</span> = -10
<span class="org-variable-name">log_inv_disp</span> = -0.5
<span class="org-variable-name">lam</span> = rng.gamma(shape=np.exp(log_inv_disp), scale=np.exp(log_mean - log_inv_disp), size=2 * n)
<span class="org-variable-name">s</span> = 1e5
<span class="org-variable-name">x</span> = rng.poisson(s * lam)
<span class="org-variable-name">z</span> = np.zeros(2 * n, dtype=<span class="org-builtin">bool</span>)
<span class="org-variable-name">z</span>[:n] = 1
</pre>
</div>

<p>
Plot the simulated data.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Dark2'</span>)
plt.clf()
plt.gcf().set_size_inches(4, 2)
<span class="org-variable-name">grid</span> = np.arange(x.<span class="org-builtin">max</span>() + 2)
plt.hist(x[z], bins=grid, color=cm(0), density=<span class="org-constant">True</span>, alpha=0.35)
plt.hist(x[~z], bins=grid, color=cm(1), density=<span class="org-constant">True</span>, alpha=0.25)
plt.plot(grid + .5, st.nbinom(n=np.exp(log_inv_disp), p=1 / (1 + s * np.exp(log_mean - log_inv_disp))).pmf(grid), c=<span class="org-string">'k'</span>, lw=1)
plt.xlabel(<span class="org-string">'Number of molecules'</span>)
plt.ylabel(<span class="org-string">'Density'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/diff-var.org/mcmc-ex0.png" alt="mcmc-ex0.png">
</p>
</div>

<p>
Draw samples from the posterior.
</p>

<div class="org-src-container">
<pre class="src src-ipython">%%timeit -n1 -r1
<span class="org-variable-name">samples</span>, <span class="org-variable-name">trace</span> = diff_var_mcmc(x, s, z, n_samples=2000, trace=<span class="org-constant">True</span>)
</pre>
</div>

<p>
Plot the log joint over the Markov chain.
</p>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
plt.gcf().set_size_inches(4, 2)
plt.plot(trace, lw=1, c=<span class="org-string">'k'</span>)
plt.xlabel(<span class="org-string">'Sample'</span>)
plt.ylabel(<span class="org-string">'Log joint prob'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/diff-var.org/mcmc-ex0-trace.png" alt="mcmc-ex0-trace.png">
</p>
</div>

<p>
Plot the estimated posterior distribution, including reference points at the
(pooled) MLE.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">mle</span> = scmodes.ebpm.ebpm_gamma(x, s)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 4)
fig.set_size_inches(7.5, 2.5)
<span class="org-keyword">for</span> i, a <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(ax):
  a.hist(samples[-500:,i], bins=7, density=<span class="org-constant">True</span>, color=<span class="org-string">'0.7'</span>)
ax[0].axvline(x=log_mean, lw=1, c=<span class="org-string">'k'</span>)
ax[0].axvline(x=mle[0], lw=1, c=<span class="org-string">'r'</span>)
ax[1].axvline(x=log_inv_disp, lw=1, c=<span class="org-string">'k'</span>)
ax[1].axvline(x=mle[1], lw=1, c=<span class="org-string">'r'</span>)
ax[2].axvline(x=0, lw=1, c=<span class="org-string">'k'</span>)
ax[3].axvline(x=0, lw=1, c=<span class="org-string">'k'</span>)
<span class="org-keyword">for</span> a <span class="org-keyword">in</span> ax[:-1]:
  <span class="org-variable-name">lim</span> = a.get_xlim()
  <span class="org-variable-name">grid</span> = np.linspace(lim[0], lim[1], 1000)
ax[0].set_xlabel(r<span class="org-string">'$\ln\mu$'</span>)
ax[0].set_ylabel(<span class="org-string">'Density'</span>)
ax[1].set_xlabel(r<span class="org-string">'$\ln\phi$'</span>)
ax[2].set_xlabel(r<span class="org-string">'$\delta_{\mu}$'</span>)
ax[3].set_xlabel(r<span class="org-string">'$\delta_{\phi}$'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/diff-var.org/mcmc-ex0-post.png" alt="mcmc-ex0-post.png">
</p>
</div>

<p>
Fit VI.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">m</span> = DiffVarExprModel(
  prior_log_mean=torch.distributions.Normal(loc=mle[0], scale=1.),
  prior_log_inv_disp=torch.distributions.Normal(loc=mle[1], scale=1.))
m.fit(
  torch.tensor(x.reshape(-1, 1), dtype=torch.<span class="org-builtin">float</span>),
  torch.tensor(np.ones((x.shape[0], 1)) * s, dtype=torch.<span class="org-builtin">float</span>),
  torch.tensor(z.reshape(-1, 1), dtype=torch.<span class="org-builtin">float</span>),
  n_epochs=2000,
  n_samples=10)
</pre>
</div>

<pre class="example">
DiffVarExprModel()

</pre>

<p>
Compare the approximate posterior to the sampled posterior.
</p>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 4)
fig.set_size_inches(7.5, 2.5)
<span class="org-keyword">for</span> i, a <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(ax):
  a.hist(samples[-1000:,i], bins=9, density=<span class="org-constant">True</span>, color=<span class="org-string">'0.7'</span>)
<span class="org-keyword">for</span> k, a <span class="org-keyword">in</span> <span class="org-builtin">zip</span>([<span class="org-string">'log_mean'</span>, <span class="org-string">'log_inv_disp'</span>, <span class="org-string">'diff_mean'</span>, <span class="org-string">'diff_inv_disp'</span>], ax):
  <span class="org-variable-name">lim</span> = a.get_xlim()
  <span class="org-variable-name">grid</span> = np.linspace(lim[0], lim[1], 1000)
  <span class="org-variable-name">f</span> = st.norm(*<span class="org-builtin">getattr</span>(m, k)).pdf(grid)
  a.plot(grid, f, lw=1, c=<span class="org-string">'k'</span>)
ax[0].set_xlabel(r<span class="org-string">'$\ln\mu$'</span>)
ax[0].set_ylabel(<span class="org-string">'Density'</span>)
ax[1].set_xlabel(r<span class="org-string">'$\ln\phi$'</span>)
ax[2].set_xlabel(r<span class="org-string">'$\delta_{\mu}$'</span>)
ax[3].set_xlabel(r<span class="org-string">'$\delta_{\phi}$'</span>)
fig.tight_layout()

</pre>
</div>


<div class="figure">
<p><img src="figure/diff-var.org/mcmc-vs-vi-ex0.png" alt="mcmc-vs-vi-ex0.png">
</p>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Abhishek Sarkar</p>
<p class="date">Created: 2021-01-08 Fri 12:06</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
