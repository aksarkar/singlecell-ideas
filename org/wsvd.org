#+TITLE: Factor analysis for single cell data
#+SETUPFILE: setup.org
#+OPTIONS: toc:t

* Setup                                                            :noexport:

  #+BEGIN_SRC emacs-lisp
    (org-babel-lob-ingest "/home/aksarkar/.emacs.d/org-templates/library.org")
  #+END_SRC

  #+RESULTS:
  : 1

  #+CALL: ipython3(memory="32G",venv="singlecell") :dir /scratch/midway2/aksarkar/singlecell

  #+RESULTS:
  :RESULTS:
  Submitted batch job 48655196
  :END:

  #+BEGIN_SRC ipython
    %matplotlib inline
    %config InlineBackend.figure_formats = set(['retina'])
    import colorcet
    import matplotlib.pyplot as plt
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[1]:
  :END:

  #+NAME: imports
  #+BEGIN_SRC ipython
    import numpy as np
    import pandas as pd
    import pickle
    import scipy.io as si
    import scipy.sparse as ss
    import scipy.special as sp
    import scipy.stats as st
    import sklearn.decomposition as skd
    import sklearn.linear_model as sklm
    import sklearn.preprocessing as skp
  #+END_SRC

  #+RESULTS: imports
  :RESULTS:
  # Out[2]:
  :END:

* Introduction

  Suppose we want to fit a linear model with heteroscedastic errors:

  \[ y \sim N(X \beta, \Sigma) \]

  where \(y\) is \(n \times 1\), \(X\) is \(n \times p\), and \(\Sigma =
  \mathrm{diag}(\sigma^2_1, \ldots, \sigma^2_n)\).

  We can estimate \(\beta\) via weighted least squares:

  \[ \hat\beta = (X' W X)^{-1} X' W y \]

  where \(W = \Sigma^{-1}\).

  Now suppose \(y\) is not Gaussian, but we assume a generalized linear model:

  \[ \eta = X \beta \]

  \[ E[y \mid x] = \mu = g^{-1}(\eta) \]

  In this case, we can estimate \(\beta\) via iterative reweighted least
  squares (Nelder and Wedderbun 1972).

  The key idea is to perform approximate Newton-Raphson updates (by
  approximating the Hessian). Introduce an auxiliary response:

  \[ z = \eta + (y - \mu) \left(\frac{\partial \mu}{\partial \eta}\right) \]

  and associated weight:

  \[ w = \frac{1}{V[y]} \left(\frac{\partial \mu}{\partial \eta}\right)^2 \]

  IRLS consists of repeatedly applying the updates:

  1. Given \(z, w\), update \(\beta \leftarrow (X' W X)^{-1} X' W z\)
  2. Given \(\beta\), update \(z, w\)

  Can we generalize these ideas to perform PCA/factor analysis (Tipping 1999)?
  Suppose:

  \[ x_i = W z_i + \mu_i + \epsilon_i \]

  where \(x_i\) is an observed \(p\)-vector, \(z_i\) is a latent \(q\)-vector,
  and \(q \ll p\).

  
  If we assume \(i = 1, \ldots, n, z_i \sim N(0, 1), \epsilon_i \sim N(0,
  \sigma^2)\), then for fixed \(\sigma^2\), the MLE of \(W\) is the the top
  \(q\) left singular vectors of \(X\). This follows from the fact that
  maximizing the likelihood is equivalent to minimizing \(\Vert X - W
  \Vert^2_2\) subject to \(W\) having rank \(q\), and the truncated SVD is the
  optimal rank \(q\) approximation (Eckhart and Young 1936).

  Further, the MAP estimates are (Tipping 1999):

  \[ \hat{\mu}_i = \frac{1}{n} \sum_j x_{ij} \]

  \[ \hat{\sigma^2} = \frac{1}{p - q} \sum_{j = q + 1}^{p} \lambda_j \]

  \[ \hat{W} = U (\Lambda - \sigma^2 I)^{1/2} \]

  where \(U, \Lambda\) are the top \(q\) eigenvectors (eigenvalues) of \(X' X\)

  If the errors are heteroscedastic, can we generalize the weighted least
  squares idea to a weighted SVD? Suppose:

  \[ X = U D V' + Z + E \]

  \[ z_{ij} \sim N(0, s_{ij}^2) \]

  \[ e_{ij} \sim N(0, \tau^{-1}) \]

  Given \(Z\), the problem becomes PCA of \((X - Z)\) with homoscedastic
  errors.

  Let \(R = X - UDV'\). Then, given \(U, D, V, S, \tau\):

  \[ r_{ij} \mid z_{ij} \sim N(r_{ij}, \tau^{-1}) \]

  \[ z_{ij} \mid \cdot \sim N(\mu_1, \tau_1^{-1}) \]

  \[ \mu_1 = r_{ij} \tau / \tau_1 \]

  \[ \tau_1 = \tau + 1 / s_{ij}^2 \]

  This idea generalizes the basic iteration in the Soft-Impute algorithm for
  matrix completion (Mazumder 2010), by representing missing entries as
  \(s_{ij} = \infty\).

  Now suppose the data \(X\) is not Gaussian. Can we generalize IRLS to an
  iterative reweighted factor analysis, using our weighted SVD? Suppose \(Y\)
  is the auxiliary response:

  1. Given \(Y, W\), update \(U, D, V\)
  2. Given \(U, D, V\), update \(Y, W\)

* Methods
** Iterative reweighted factor analysis

  #+BEGIN_SRC ipython
    def wsvd(x, s, n_components, prior_prec=1, max_iters=10, verbose=False):
      n, p = x.shape
      z = np.zeros((n, p))
      pca = skd.PCA(n_components=n_components)
      obj = float('-inf')
      for i in range(max_iters):
        u, d, vt = pca._fit(x - z)
        r = x - np.einsum('ij,j,jk->ik', u, d, vt)
        posterior_prec = prior_prec + 1 / s ** 2
        z = r * prior_prec / posterior_prec
        update = st.norm(scale=np.sqrt(s ** 2 + 1 / prior_prec)).logpdf(r).mean()
        if verbose:
          print(f'wsvd [{i}] = {update}')
        if update < obj or np.isclose(update, obj):
          return u, d, vt
        else:
          obj = update
      raise RuntimeError('failed to converge')

    def pois_llik(y, lam):
      return y * np.log(lam) - lam - sp.gammaln(y + 1)

    def exp(x):
      """Numerically safe exp"""
      return np.exp(np.clip(x, 0, 700))

    def pois_svd(x, max_outer_iters=10, verbose=False, **kwargs):
      mu = x.mean(axis=0, keepdims=True) * np.ones(x.shape)
      eta = np.log(mu)
      obj = float('-inf')
      for i in range(max_outer_iters):
        y = eta + (x - mu) / mu
        w = 1 / mu
        u, d, vt = wsvd(y, w, verbose=verbose, **kwargs)
        eta = np.einsum('ij,j,jk->ik', u, d, vt)
        mu = exp(eta)
        update = pois_llik(x, mu).mean()
        if verbose:
          print(f'pois_svd [{i}] = {update}')
        if update < obj or np.isclose(update, obj):
          return u, d, vt
        else:
          obj = update
      raise RuntimeError('failed to converge')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[74]:
  :END:

** Srebro and Jaakkola 2003

   Srebro and Jaakola 2003 propose an EM algorithm to solve the weighted
   low-rank approximation problem:

   \[ \min_{\mathbf{Z}} \sum_{i,j} w_{ij} \left(x_{ij} - z_{ij} \right)^2 \]

   where target matrix \(\mathbf{X}\) and weight matrix \(\mathbf{W}\) are
   given, and \(\mathbf{Z}\) is constrained to some rank.

   The algorithm is EM in the following sense: suppose the weights \(w_{ij} \in
   \{0, 1\}\), corresponding to presence/absence, and suppose \(\mathbf{X} =
   \mathbf{Z} + \mathbf{E}\), where \(\mathbf{Z}\) is low-rank and elements of
   \(\mathbf{E}\) are Gaussian.

   Then, \(E[x_{ij} \mid w_{ij} = 0] = z_{ij}\), naturally giving an EM
   algorithm. The E-step fills in \(x_{ij}\) with \(z_{ij}\), and the M-step
   estimates \(\mathbf{Z}\) from the filled in \(\mathbf{X}\). The solution to
   the M-step is given by the optimal unweighted rank \(k\) approximation,
   i.e. truncated SVD, because all the non-zero weights are equal to 1.

   Conceptually, the method for arbitrary weights is to suppose instead that we
   have rational \(w_{ij} \in \{0, 1/N, \ldots, 1\}\). 

   Then, we can reduce this problem to a problem in 0/1 weights by supposing we
   have \(X^{(k)} = Z + E^{(k)}\), \(k \in 1, \ldots, N\), and each entry is
   observed in only \(N w_{ij}\) of the \(X^{(k)}\).

   Then, the M-step becomes:

   \[ \mathbf{Z}^{(t + 1)} = \mathrm{LRA}_k(\mathbf{W} \circ \mathbf{X} +
   (\mathbf{1} - \mathbf{W}) \circ \mathbf{Z}^{(t)}) \]

   where \(\mathrm{LRA}_k\) is the unweighted rank \(k\) approximation and
   \(\circ\) denotes Hadamard product.

   Intuitively, the implied E-step corresponds to taking the expectation of
   \(z_{ij}^{(k)}\) over the targets \(k\). Clearly, the algorithm generalizes
   to any weight matrix where \(0 \leq w_{ij} \leq 1\) are stored in finite
   precision.

   #+BEGIN_SRC ipython
     def wlra(x, w, rank, max_iters=10, verbose=False):
       n, p = x.shape
       low_rank = x
       pca = skd.PCA(n_components=rank)
       obj = float('inf')
       for i in range(max_iters):
         u, d, vt = pca._fit(w * x + (1 - w) * low_rank)
         low_rank = np.einsum('ij,j,jk->ik', u, d, vt)
         update = (w * np.square(x - low_rank)).mean()
         if verbose:
           print(f'wsvd [{i}] = {update}')
         if update > obj or np.isclose(update, obj):
           return low_rank
         else:
           obj = update
       raise RuntimeError('failed to converge')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[27]:
   :END:

   Srebro and Jaakkola apply this approach to solve low rank approximation
   minimizing the logistic loss (where entries are e.g. probability a user will
   prefer an item).

   #+BEGIN_SRC ipython
     def logistic_lra(x, rank, max_outer_iters=1000, verbose=False):
       pass
   #+END_SRC

   We extend this approach to instead maximize the Poisson log-likelihood.

   \[ \ln p(x \mid \lambda) = l(\lambda) = x \ln \lambda - \lambda +
   \ln\Gamma(x + 1) \]

   Taking a second-order Taylor expansion:

   \[ l(\lambda) \approx \left(\frac{x}{\lambda_0} - 1\right)(\lambda -
   \lambda_0) - \frac{x}{2\lambda_0^2}(\lambda - \lambda_0)^2 +
   \mathrm{const}\]

   where the constant does not depend on \(\lambda\).

   \[ l(\lambda) \approx -\frac{x}{2\lambda_0^2} \left[\lambda - \left(1 +
   \frac{1}{\lambda_0 x}\right)\right]^2 + \mathrm{const}\]

   Clearly, minimizing the negative of this objective function for each entry
   \(x_{ij}\) is a weighted low-rank approximation problem.  This result
   suggests an iterative algorithm, where we take successive Taylor
   approximations about different parameter estimates for each entry.

   #+BEGIN_SRC ipython
     def pois_lra(x, n_components, max_outer_iters=10, max_iters=1000, verbose=False):
       n, p = x.shape
       lam = x.copy()
       for i in range(max_outer_iters):
         w = x / (2 * np.square(lam))
         target = 1 + 1 / (lam * x)
         u, d, vt = srebro_wsvd(target, w, n_components)
         low_rank = np.einsum('ij,j,jk->ik', u, d, vt)
         lam = (target - 1) * x
         update = pois_llik(x, lam).mean()
         if verbose:
           print(f'srebro_pois_svd [{i}]: {update}')
       return lam
   #+END_SRC

** First order optimization

  Directly optimize the log likelihood via gradient descent:

  \[ x_{ij} \sim \mathrm{Poisson}(\lambda_{ij}) \]

  \[ \lambda_{ij} = \sum_k L_{ik} F_{kj} \]

  For simplicity, don't put any constraints on \(L, F\).

  #+BEGIN_SRC ipython
    import torch

    class PoissonFA(torch.nn.Module):
      def __init__(self, n_samples, n_features, n_components):
        super().__init__()
        self.l = torch.randn([n_samples, n_components], requires_grad=True)
        self.f = torch.randn([n_components, n_features], requires_grad=True)

      def forward(self, x):
        log_lam = torch.matmul(self.l, self.f)
        return -torch.mean(x * log_lam - torch.exp(log_lam) + sp.gammaln(x + 1))

      def fit(self, x, max_epochs=1000, verbose=False, **kwargs):
        x = torch.tensor(x, dtype=torch.float)
        opt = torch.optim.Adam([self.l, self.f], **kwargs)
        for i in range(max_epochs):
          opt.zero_grad()
          loss = self.forward(x)
          if verbose and not i % 100:
            print(f'Epoch {i} = {loss}')
          loss.backward()
          opt.step()
        return self

    def pois_fa(x, n_components, **kwargs):
      n, p = x.shape
      res = PoissonFA(n, p, n_components).fit(x, max_epochs=2000, lr=5e-2)
      return res.l.detach().numpy(), np.ones(n_components), res.f.detach().numpy()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[99]:
  :END:

** Naive factor analysis

  The naive approach would be to compute the Poisson MLE \(\hat\lambda_{ij}\)
  once and then factorize the resulting matrix. But without further
  assumptions, the MLE is \(\hat\lambda_{ij} = x_{ij}\)

  #+NAME: naive-pois-svd
  #+BEGIN_SRC ipython
    def naive_pois_svd(x, n_components, **kwargs):
      return skd.PCA(n_components=n_components)._fit(x)
  #+END_SRC

  #+RESULTS: naive-pois-svd
  :RESULTS:
  # Out[80]:
  :END:

* Simulation
** Gaussian reconstruction

   Reproduce the experiment of Srebro and Jaakkola 2003.

   #+BEGIN_SRC ipython
     def simulate_gaussian(num_samples, num_features, rank, nsr=1, seed=None):
       if seed is None:
         seed = 0
       np.random.seed(seed)
       l = np.random.normal(size=(num_samples, rank))
       f = np.random.normal(size=(rank, num_features))
       lf = l.dot(f)
       noise_scale = np.random.uniform(1, nsr, size=(num_samples, num_features))
       e = np.random.normal(scale=noise_scale, size=(num_samples, num_features))
       x = lf + e
       return x, lf, noise_scale
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[32]:
   :END:

** Poisson matrix completion

  Generate a Poisson data matrix assuming the rate matrix is low rank. Evaluate
  the quality of the fit using RRMSE.

  #+NAME: simulation
  #+BEGIN_SRC ipython
    def simulate_poisson(num_samples, num_features, rank, seed=None):
      if seed is None:
        seed = 0
      np.random.seed(seed)
      l = np.random.normal(scale=0.7, size=(num_samples, rank))
      f = np.random.normal(scale=0.7, size=(rank, num_features))
      lf = l.dot(f)
      e = np.random.normal(scale=lf.std(), size=(num_samples, num_features))
      x = np.random.poisson(lam=np.exp(lf + e))
      return x, lf

    def reconstruct(res):
      return np.einsum('ij,j,jk->ik', *res)

    def rrmse(pred, true):
      return np.sqrt(np.linalg.norm(pred - true) / np.linalg.norm(true))
  #+END_SRC

  #+RESULTS: simulation
  :RESULTS:
  # Out[44]:
  :END:

  Test the simulation framework.

  #+BEGIN_SRC ipython
    x, lf = simulate(num_samples=100, num_features=1000, rank=1, seed=0)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[16]:
  :END:

  #+BEGIN_SRC ipython
    res = pois_svd(x, n_components=1)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[55]:
  :END:

  #+BEGIN_SRC ipython
    rrmse(reconstruct(res), lf)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[56]:
  : 0.7758305792529097
  :END:

  Compute the naive estimate.

  #+BEGIN_SRC ipython
    rrmse(reconstruct(naive_pois_svd(x, 1)), lf)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[58]:
  : 1.1425364246860807
  :END:

  Evaluate the methods systematically.

  #+NAME: main
  #+BEGIN_SRC ipython
    def evaluate(num_trials=10):
      result = []
      for rank in range(1, 4):
        for n_components in range(1, 4):
          for method in (pois_svd, pois_fa, naive_pois_svd):
            for trial in range(num_trials):
              try:
                x, lam = simulate(num_samples=100, num_features=1000, rank=rank, seed=trial)
                res = method(x, n_components=n_components, max_outer_iters=1000, max_iters=1000)
                rrmse_ = rrmse(reconstruct(res), lam)
                llik = pois_llik(x, np.exp(reconstruct(res))).mean()
              except:
                rrmse_ = None
                llik = None
              result.append((rank, trial, method.__name__, n_components, rrmse_, llik))
      result = pd.DataFrame(result)
      result.columns = ['rank', 'trial', 'method', 'n_components', 'rrmse', 'llik']
      return result
  #+END_SRC

  #+RESULTS: main
  :RESULTS:
  # Out[72]:
  :END:

  #+BEGIN_SRC ipython :async t
    result = evaluate(num_trials=5)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

  Serialize the results.

  #+BEGIN_SRC ipython
    result.to_csv('pois-svd-simulation.txt.gz', sep='\t', compression='gzip')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[107]:
  :END:

  Read the results.

  #+BEGIN_SRC ipython
    result = pd.read_table('pois-svd-simulation.txt.gz', index_col=0)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[83]:
  :END:

  #+BEGIN_SRC ipython :ipyfile figure/wsvd.org/rrmse.png
    plt.clf()
    result[result['rank'] == result['n_components']].boxplot(column='rrmse', by=['rank', 'method'], figsize=(3, 3), grid=False, rot=90)
    plt.ylabel('Relative RMSE')
    plt.title('')
    plt.suptitle('')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[109]:
  : Text(0.5,0.98,'')
  [[file:figure/wsvd.org/rrmse.png]]
  :END:

  Does the WSVD reveal the underlying rank?

  #+BEGIN_SRC ipython :ipyfile figure/wsvd.org/llik.png
    plt.clf()
    result[result['method'] == 'pois_svd'].dropna().boxplot(column='llik', by=['rank', 'n_components'], figsize=(3, 5), grid=False, rot=90)
    plt.ylabel('Per sample log likelihood')
    plt.yscale('symlog', linthreshy=1e-14)
    plt.title('')
    plt.suptitle('')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[110]:
  : Text(0.5,0.98,'')
  [[file:figure/wsvd.org/llik.png]]
  :END:

  For each data set, how often does the model with number of components equal
  to the ground truth have the highest likelihood?

  #+BEGIN_SRC ipython
    (result[result['method'] == 'pois_svd']
     .groupby(['rank', 'trial'])
     .apply(lambda x: x.loc[x['llik'].idxmax, 'n_components'])
     .reset_index())
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[125]:
  #+BEGIN_EXAMPLE
    rank  trial  0
    0      1      0  1
    1      1      1  1
    2      1      2  1
    3      1      3  1
    4      1      4  1
    5      2      0  2
    6      2      1  2
    7      2      2  3
    8      2      3  2
    9      2      4  2
    10     3      0  3
    11     3      1  3
    12     3      2  3
    13     3      3  3
    14     3      4  3
  #+END_EXAMPLE
  :END:

* Application to single cell data

  What are the latent factors we expect to find in single cell data?

  1. Unwanted variation
  2. Cell types/subpopulations

  Of these, cell types are of the most interest, because we can then read off
  the relevant genes from the factors.

  To test whether the method recovers cell type, we use known mixtures of
  sorted PBMCs from 10X Genomics.

  #+BEGIN_SRC ipython
    b_cells = si.mmread('/project2/mstephens/aksarkar/projects/singlecell-ideas/data/10xgenomics/cd19+_b_cells/filtered_matrices_mex/hg19/matrix.mtx')
    t_cells = si.mmread('/project2/mstephens/aksarkar/projects/singlecell-ideas/data/10xgenomics/cd8+_cytotoxic_t_cells/filtered_matrices_mex/hg19/matrix.mtx')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[71]:
  :END:

  #+BEGIN_SRC ipython
    genes = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-ideas/data/10xgenomics/cd19+_b_cells/filtered_matrices_mex/hg19/genes.tsv', header=None)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[116]:
  :END:

  #+BEGIN_SRC ipython
    b_cell_subset = b_cells.tocsc()[:,:500]
    t_cell_subset = t_cells.tocsc()[:,:500]
    mix = ss.hstack([b_cell_subset, t_cell_subset], format='csr')
    keep = (mix.sum(axis=1) > 0).A.ravel()
    genes = genes.loc[keep]
    mix = mix[keep].A.T
    mix /= mix.sum(axis=1, keepdims=True)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[118]:
  :END:

  #+BEGIN_SRC ipython :async t
    res = pois_svd(mix, n_components=10, verbose=True)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  2 - 8b2e7d2c-614d-46bb-aaff-4a9a5a4420d0
  :END:

  #+BEGIN_SRC ipython
    res0 = skd.PCA(n_components=10).fit(mix)  
  #+END_SRC

  #+BEGIN_SRC ipython
    Y = np.zeros(mix.shape[0])
    Y[:mix.shape[0] // 2] = 1
    m0 = sklm.LogisticRegressionCV(fit_intercept=True).fit(res0.transform(mix), Y)
    m0.coef_
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[105]:
  #+BEGIN_EXAMPLE
    array([[ 5.53867438e-04, -1.16538042e-04,  1.97613135e-05,
    -7.26492033e-06, -8.97671870e-06,  6.80991734e-06,
    -3.30404299e-06,  1.02224040e-05, -2.31100318e-06,
    7.68760237e-07]])
  #+END_EXAMPLE
  :END:

  #+BEGIN_SRC ipython
    m0.score(res0.transform(mix), Y)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[106]:
  : 1.0
  :END:

  #+BEGIN_SRC ipython
    scaler = skp.StandardScaler()
    m1 = sklm.LogisticRegressionCV(fit_intercept=True).fit(scaler.fit_transform(mix), Y)
    m1.score(scaler.transform(mix), Y)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[129]:
  : 1.0
  :END:
