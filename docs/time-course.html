<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2020-03-13 Fri 12:40 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Fully unsupervised topic models of scRNA-seq time course data</title>
<meta name="generator" content="Org mode">
<meta name="author" content="Abhishek Sarkar">
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link href="bootstrap.min.css" rel="stylesheet" />
<link rel="stylesheet" type="text/css" href="htmlize.css"/>
<link rel="stylesheet" type="text/css" href="main.css"/>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2017 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Fully unsupervised topic models of scRNA-seq time course data</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgbb76e5c">Introduction</a></li>
<li><a href="#setup">Setup</a></li>
<li><a href="#org1860a88">Methods</a>
<ul>
<li><a href="#org7d4bcf5">Amortized NMF</a></li>
<li><a href="#org7c79f06">Amortized LDA</a></li>
<li><a href="#orga68114a">iPSC data</a></li>
<li><a href="#org042cb98">10-way mixture</a></li>
<li><a href="#org6dd7c96">Census of Immune Cells data</a></li>
</ul>
</li>
<li><a href="#org1cce9ff">Results</a>
<ul>
<li><a href="#org13ff0f7">Simulation</a></li>
<li><a href="#orgb7e7231">ANMF on 68K PBMCs</a></li>
<li><a href="#org8ec4008">ANMF on 10-way mixture</a></li>
<li><a href="#org687e657">ANMF on iPSC data</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-orgbb76e5c" class="outline-2">
<h2 id="orgbb76e5c">Introduction</h2>
<div class="outline-text-2" id="text-orgbb76e5c">
<p>
In our prior work (<a href="https://dx.doi.org/10.1371/journal.pgen.1008045">Sarkar
et al. 2019</a>), we introduced a factor model to capture between donor
individual variation in the mean and variance of gene expression in a single
cell type \( 
  \DeclareMathOperator\Gam{Gamma}
  \DeclareMathOperator\Poi{Poisson}
  \DeclareMathOperator\argmin{arg min}
  \newcommand\mf{\mathbf{F}} 
  \newcommand\ml{\mathbf{L}}
  \newcommand\mx{\mathbf{X}}
  \newcommand\vl{\mathbf{l}}
  \newcommand\vx{\mathbf{x}}
  \)
</p>

\begin{align*}
  x_{ij} &\sim \Poi(x_i^+ \lambda_{ij})\\
  \lambda_{ij} &\sim \pi_{ij} \delta_0(\cdot) + (1 - \pi_{ij}) \Gam(\phi_{ij}^{-1}, \mu_{ij}^{-1} \phi_{ij}^{-1})\\
  \ln \mu_{ij} &= (\ml \mf_\mu')_{ij}\\
  \ln \phi_{ij} &= (\ml \mf_\phi')_{ij}\\
  \operatorname{logit} \pi_{ij} &= (\ml \mf_\pi')_{ij}\\
\end{align*}

<p>
where 
</p>

<ul class="org-ul">
<li>\(x_{ij}\) is the number of molecules of gene \(j = 1, \ldots, p\) observed
in cell \(i = 1, \ldots, n\)</li>
<li>cells are taken from \(m\) donor individuals, \(\ml\) is \(n \times m\),
each \(\mf_{\cdot}\) is \(p \times m\)</li>
<li>assignments of cells to donors (loadings) \(l_{ik} \in \{0, 1\}\) are
known and fixed.</li>
</ul>

<p>
We are now interested in several lines of questions:
</p>

<ol class="org-ol">
<li>If we analyze this kind of data in a fully unsupervised manner, can we
recover the assignments of cells to donors? This approach has been
previously proposed in our specific factor model
(<a href="10.1038/s41467-017-02554-5">Risso et al. 2018</a>). If not, what do
we recover?</li>
<li>Can we generalize this analysis approach to data which additionally has
multiple cell types, and then multiple time points?</li>
<li>Can we implement this approach without forming entire products
\(\ml\mf'\)? Can we implement each update without looking at the entire
data \(\mx\)? How much faster can we get than existing methods? Can we
analyze datasets which are currently impossible to analyze (due to size),
e.g. Human Cell Atlas? As references, compare against
<a href="10.1038/s41592-018-0229-2">scVI</a>,
<a href="https://rawcdn.githack.com/aertslab/cisTopic/8d15fa2813312aa0b20c1042604079558829e947/vignettes/10X_workflow.html#building_the_models">cisTopic</a>,
<a href="https://github.com/stephenslab/fastTopics">fastTopics</a></li>
</ol>
</div>
</div>

<div id="outline-container-orgaef0b05" class="outline-2">
<h2 id="setup"><a id="orgaef0b05"></a>Setup</h2>
<div class="outline-text-2" id="text-setup">
<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> anmf
<span class="org-keyword">import</span> anndata
<span class="org-keyword">import</span> numpy <span class="org-keyword">as</span> np
<span class="org-keyword">import</span> pandas <span class="org-keyword">as</span> pd
<span class="org-keyword">import</span> scanpy <span class="org-keyword">as</span> sc
<span class="org-keyword">import</span> scmodes
<span class="org-keyword">import</span> scipy.sparse <span class="org-keyword">as</span> ss
<span class="org-keyword">import</span> scipy.stats <span class="org-keyword">as</span> st
<span class="org-keyword">import</span> sklearn.decomposition <span class="org-keyword">as</span> skd
<span class="org-keyword">import</span> time
<span class="org-keyword">import</span> torch
<span class="org-keyword">import</span> torch.utils.data <span class="org-keyword">as</span> td
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">%matplotlib inline
%config <span class="org-variable-name">InlineBackend.figure_formats</span> = <span class="org-builtin">set</span>([<span class="org-string">'retina'</span>])
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> colorcet
<span class="org-keyword">import</span> matplotlib.pyplot <span class="org-keyword">as</span> plt
<span class="org-variable-name">plt.rcParams</span>[<span class="org-string">'figure.facecolor'</span>] = <span class="org-string">'w'</span>
<span class="org-variable-name">plt.rcParams</span>[<span class="org-string">'font.family'</span>] = <span class="org-string">'Nimbus Sans'</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-org1860a88" class="outline-2">
<h2 id="org1860a88">Methods</h2>
<div class="outline-text-2" id="text-org1860a88">
</div>
<div id="outline-container-org7d4bcf5" class="outline-3">
<h3 id="org7d4bcf5">Amortized NMF</h3>
<div class="outline-text-3" id="text-org7d4bcf5">
<p>
Amortized inference
(<a href="http://ruishu.io/2017/11/07/amortized-optimization/">Shu 2017</a>,
<a href="https://papers.nips.cc/paper/7692-amortized-inference-regularization.pdf">Shu
et al. 2018</a>) refers to a strategy to efficiently solve a large (possibly
infinite) collection of optimization problems:
</p>

<p>
\[ \theta^* = \argmin_{\theta} f(\theta, \phi), \]
</p>

<p>
where \(\phi \in \Phi\) is some context variable. Rather than solving one
problem for each \(\phi \in \Phi\), we learn a function \(h_\alpha\)
parameterized by \(\alpha\) which predicts \(\theta^*\) from \(\phi\)
</p>

<p>
\[ \alpha^* = \argmin_\alpha E_{\phi \sim \hat{p}(\phi)} f(h_\alpha(\phi), \phi), \]
</p>

<p>
where \(\hat{p}(\phi)\) denotes the empirical distribution of \(\phi\) in
the training data. The function \(h_\alpha\) amortizes inference over the
training data examples by coupling the optimization problems together, and
indeed amortizes inference over unseen examples also.
</p>

<p>
As a concrete example, NMF
(<a href="http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization">Lee
and Seung 2001</a>, <a href="https://dx.doi.org/10.1155/2009/785152">Cemgil 2009</a>)
can be written
</p>

<p>
\[ \vl_i^* = \argmin_{\vl_i} \sum_j \operatorname{Poisson}(x_{ij}; \sum_k l_{ik} f_{jk})\]
</p>

<p>
where \(\vx_i\) is the context variable, and the goal is to learn
\(h_\alpha\) which maps observations \(\vx_i\) to loadings \(\vl_i\). (Here,
we are simplifying by holding \(\mf\) fixed. In this setup, we treat the
\(\vl_i\) as local latent variables, and \(\mf\) as a global latent
variable. These can be optimized in alternating phases.) The resulting
optimization problem can be solved by introducing an <i>encoder network</i>
\(h_\alpha\), which has been previously proposed
(<a href="10.1038/s41592-018-0229-2">Lopez et al. 2018</a>,
<a href="10.1038/s41467-018-07931-2">Eraslan et al. 2018</a>).
</p>

<p>
Existing methods have been introduced with the motivation that auto-encoding
networks can represent non-linear mappings into latent spaces; however, the
gain in explanatory power of such methods is unclear. Amortized inference
suggests a simpler, more compelling motivation to explore these methods:
they enable the use of stochastic optimization methods to analyze large data
sets (for example, those which do not fit in memory). However, existing
software implementations have a major limitation: they do not support sparse
matrices on the GPU, making it either impossible to analyze complete data
sets, or introducing a major bottleneck in moving minibatches to the GPU. We
implement this functionality in the Python package <code>anmf</code>.
</p>
</div>
</div>

<div id="outline-container-org7c79f06" class="outline-3">
<h3 id="org7c79f06">Amortized LDA</h3>
<div class="outline-text-3" id="text-org7c79f06">
<p>
NMF is intimately connected to LDA via the Multinomial-Poisson transform
(<a href="http://www.jstor.org/stable/2348134">Baker 1994</a>). Briefly, if we have
an MLE for NMF and scale \(\ml\) and \(\mf\) to satisfy the constraints
\(\sum_k l_{ik} = 1\), \(\sum_k f_{jk} = 1\), we recover an MLE for the
Multinomial likelihood underlying LDA. This fact suggests an amortized
inference scheme for maximum likelihood estimation of topic models:
</p>

\begin{align*}
  x_{ij} \mid s_i, \vl_i, \mf &\sim \operatorname{Poisson}(s_i \sum_k l_{ik} f_{jk})\\
  \vl_i &= h(\vx_i)
\end{align*}

<p>
where \(h\) is a neural network with softmax output. Unlike previous
approaches for amortized inference in topic models
(<a href="https://arxiv.org/pdf/1703.01488">Srivastava et al. 2017</a>), we are not
concerned with recovering an approximate posterior over \(\mathbf{L},
   \mathbf{F}\), simplifying the problem.
</p>
</div>
</div>

<div id="outline-container-orga68114a" class="outline-3">
<h3 id="orga68114a">iPSC data</h3>
<div class="outline-text-3" id="text-orga68114a">
<p>
Read the data.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">keep_samples</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/quality-single-cells.txt'</span>, index_col=0, header=<span class="org-constant">None</span>)
<span class="org-variable-name">keep_genes</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/genes-pass-filter.txt'</span>, index_col=0, header=<span class="org-constant">None</span>)
<span class="org-variable-name">annotations</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-annotation.txt'</span>)
<span class="org-variable-name">annotations</span> = annotations.loc[keep_samples.values.ravel()]
<span class="org-variable-name">header</span> = <span class="org-builtin">sorted</span>(<span class="org-builtin">set</span>(annotations[<span class="org-string">'chip_id'</span>]))
<span class="org-variable-name">umi</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-counts.txt.gz'</span>, index_col=0).loc[keep_genes.values.ravel(),keep_samples.values.ravel()]
<span class="org-variable-name">gene_info</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-genes.txt.gz'</span>, index_col=0)
</pre>
</div>

<p>
Convert to sparse <code>h5ad</code>.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">del</span> annotations[<span class="org-string">"index"</span>]
<span class="org-variable-name">x</span> = anndata.AnnData(ss.csr_matrix(umi.values.T), obs=annotations, var=gene_info.loc[umi.index])
x.write(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-ideas/data/ipsc/ipsc.h5ad'</span>, compression=<span class="org-constant">None</span>, force_dense=<span class="org-constant">False</span>)
</pre>
</div>
</div>
</div>

<div id="outline-container-org042cb98" class="outline-3">
<h3 id="org042cb98">10-way mixture</h3>
<div class="outline-text-3" id="text-org042cb98">
<p>
Read the FACS sorted data sets from
<a href="https://dx.doi.org/10.1038/ncomms14049">Zheng et al 2017</a>.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">keys</span> = (
  <span class="org-string">'b_cells'</span>,
  <span class="org-string">'cd34'</span>,
  <span class="org-string">'cd4_t_helper'</span>,
  <span class="org-string">'cd56_nk'</span>,
  <span class="org-string">'cytotoxic_t'</span>,
  <span class="org-string">'memory_t'</span>,
  <span class="org-string">'naive_cytotoxic'</span>,
  <span class="org-string">'naive_t'</span>,
  <span class="org-string">'regulatory_t'</span>,
  <span class="org-string">'cd14_monocytes'</span>,
)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">data</span> = {k: scmodes.dataset.read_10x(f<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-ideas/data/10xgenomics/{k}/filtered_matrices_mex/hg19/'</span>, return_adata=<span class="org-constant">True</span>, min_detect=0)
        <span class="org-keyword">for</span> k <span class="org-keyword">in</span> keys}
</pre>
</div>

<p>
Concatenate the data, then take genes with observations in at least 0.1% of
cells.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">x</span> = data[keys[0]].concatenate(*[data[k] <span class="org-keyword">for</span> k <span class="org-keyword">in</span> keys[1:]], join=<span class="org-string">'inner'</span>, batch_key=<span class="org-string">'cell_type'</span>, batch_categories=keys)
sc.pp.filter_genes(x, min_cells=1)
</pre>
</div>

<p>
Report the dimensions.
</p>

<div class="org-src-container">
<pre class="src src-ipython">x.shape
</pre>
</div>

<pre class="example">
(94655, 21952)

</pre>

<p>
Write out the data.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">x.obs</span> = x.obs.rename({0: <span class="org-string">'barcode'</span>}, axis=1)
<span class="org-variable-name">x.var</span> = x.var.rename({0: <span class="org-string">'ensg'</span>, 1: <span class="org-string">'name'</span>}, axis=1)
x.write(<span class="org-string">'/scratch/midway2/aksarkar/ideas/zheng-10-way.h5ad'</span>)
</pre>
</div>
</div>
</div>

<div id="outline-container-org6dd7c96" class="outline-3">
<h3 id="org6dd7c96">Census of Immune Cells data</h3>
<div class="outline-text-3" id="text-org6dd7c96">
<p>
We previously processed the Census of Immune Cells.
</p>

<p>
Read the sparse data. (20 seconds)
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">y_csr</span> = ss.load_npz(<span class="org-string">'/scratch/midway2/aksarkar/modes/immune-cell-census.npz'</span>)
</pre>
</div>

<p>
Read the metadata.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">genes</span> = pd.read_csv(<span class="org-string">'/scratch/midway2/aksarkar/modes/immune-cell-census-genes.txt.gz'</span>, sep=<span class="org-string">'\t'</span>, index_col=0)
<span class="org-variable-name">donor</span> = pd.Categorical(pd.read_csv(<span class="org-string">'/scratch/midway2/aksarkar/modes/immune-cell-census-samples.txt.gz'</span>, sep=<span class="org-string">'\t'</span>, index_col=0)[<span class="org-string">'0'</span>])
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-org1cce9ff" class="outline-2">
<h2 id="org1cce9ff">Results</h2>
<div class="outline-text-2" id="text-org1cce9ff">
</div>
<div id="outline-container-org13ff0f7" class="outline-3">
<h3 id="org13ff0f7">Simulation</h3>
<div class="outline-text-3" id="text-org13ff0f7">
<p>
Simulate a simple example.
</p>

<div class="org-src-container">
<pre class="src src-ipython">np.random.seed(0)
<span class="org-variable-name">n</span> = 512
<span class="org-variable-name">p</span> = 1000
<span class="org-variable-name">k</span> = 10
<span class="org-variable-name">l</span> = np.random.lognormal(sigma=0.5, size=(n, k))
<span class="org-variable-name">f</span> = np.random.lognormal(sigma=0.5, size=(p, k))
<span class="org-variable-name">x</span> = np.random.poisson(l @ f.T)
<span class="org-variable-name">s</span> = x.<span class="org-builtin">sum</span>(axis=1)
</pre>
</div>

<p>
Fit NMF via EM. Report the time elapsed (minutes).
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">start</span> = time.time()
<span class="org-variable-name">lhat0</span>, <span class="org-variable-name">fhat0</span>, <span class="org-variable-name">loss0</span> = scmodes.lra.nmf(x, rank=10, tol=1e-3, max_iters=50000, verbose=<span class="org-constant">True</span>)
<span class="org-variable-name">elapsed</span> = time.time() - start
elapsed / 60
</pre>
</div>

<pre class="example">
13.413497316837312

</pre>

<p>
Peter Carbonetto previously derived the KKT conditions for the NMF
objective. Report the maximum absolute KKT residual.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-comment-delimiter"># </span><span class="org-comment">Important: our NMF implementation does not remove the size factor</span>
<span class="org-variable-name">A</span> = x / (lhat0 @ fhat0.T)
<span class="org-variable-name">l_resid</span> = <span class="org-builtin">abs</span>(fhat0 * ((1 - A).T @ lhat0)).<span class="org-builtin">max</span>()
<span class="org-variable-name">f_resid</span> = <span class="org-builtin">abs</span>(lhat0 * ((1 - A) @ fhat0)).<span class="org-builtin">max</span>()
l_resid, f_resid
</pre>
</div>

<pre class="example">
(0.05866832853326247, 0.09743250873968519)

</pre>

<p>
Fit ANMF. Report the time elapsed (seconds).
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">assert</span> torch.cuda.is_available()
<span class="org-variable-name">xt</span> = torch.tensor(x, dtype=torch.<span class="org-builtin">float</span>).cuda()
<span class="org-variable-name">dense_data</span> = td.TensorDataset(xt, torch.tensor(s, dtype=torch.<span class="org-builtin">float</span>).cuda())
<span class="org-variable-name">data</span> = td.DataLoader(dense_data, batch_size=64, shuffle=<span class="org-constant">False</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">start</span> = time.time()
<span class="org-variable-name">fit</span> = (anmf.modules.ANMF(input_dim=x.shape[1])
       .fit(data, max_epochs=800, trace=<span class="org-constant">True</span>, lr=5e-4))
<span class="org-variable-name">elapsed</span> = time.time() - start
elapsed
</pre>
</div>

<pre class="example">
15.24153757095337

</pre>

<p>
Plot the optimization trace, focusing on the last 100 minibatches.
</p>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
plt.gcf().set_size_inches(4, 2)
plt.plot(np.array(fit.trace).ravel()[-400:] / (64 * p), lw=1, c=<span class="org-string">'k'</span>)
plt.xticks(np.arange(0, 500, 100), np.arange(-400, 100, 100))
plt.xlabel(<span class="org-string">'Minibatch before end'</span>)
plt.ylabel(<span class="org-string">'Per obs neg log lik'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/time-course.org/sim-trace.png" alt="sim-trace.png">
</p>
</div>

<p>
Recover the loadings and factors.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">lhat</span> = fit.loadings(xt)
<span class="org-variable-name">fhat</span> = fit.factors()
</pre>
</div>

<p>
Report the maximum absolute KKT residual.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">A</span> = x / (s.reshape(-1, 1) * lhat.dot(fhat))
<span class="org-variable-name">l_resid</span> = <span class="org-builtin">abs</span>(fhat.T * ((1 - A).T @ lhat)).<span class="org-builtin">max</span>()
<span class="org-variable-name">f_resid</span> = <span class="org-builtin">abs</span>(lhat * ((1 - A) @ fhat.T)).<span class="org-builtin">max</span>()
l_resid, f_resid
</pre>
</div>

<pre class="example">
(0.000967120470937001, 0.07753114624327002)

</pre>

<p>
Report the loss achieved by each algorithm.
</p>

<div class="org-src-container">
<pre class="src src-ipython">pd.Series({
  <span class="org-string">'oracle'</span>: -st.poisson(mu=l @ f.T).logpmf(x).<span class="org-builtin">sum</span>(),
  <span class="org-string">'em'</span>: loss0,
  <span class="org-string">'anmf'</span>: -st.poisson(mu=s.reshape(-1, 1) * lhat @ fhat).logpmf(x).<span class="org-builtin">sum</span>(),
})
</pre>
</div>

<pre class="example">
oracle    1.364353e+06
em        1.356402e+06
anmf      1.385015e+06
dtype: float64
</pre>

<p>
The log likelihood achieved by ANMF is lower than the oracle. Recent results
(<a href="https://arxiv.org/abs/1801.03558">Cremer et al 2020</a>) suggest this is to
be expected: there is an <i>amortization gap</i> due to the fact that we are not
directly optimizing the likelihood with respect to the local latent variable
\(\vl_i\) for each observation \(\vx_i\). To investigate this gap, fix the
factors to the ground truth, and see whether ANMF recovers the loadings.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">start</span> = time.time()
<span class="org-variable-name">fit</span> = anmf.modules.ANMF(input_dim=x.shape[1], latent_dim=10)
<span class="org-comment-delimiter"># </span><span class="org-comment">y = ln(1 + exp(x))</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">exp(y) - 1 = exp(x)</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">ln(exp(y) - 1) = x</span>
<span class="org-variable-name">fit.decoder._f.data</span> = torch.tensor(np.log(np.exp(f.T) - 1), dtype=torch.<span class="org-builtin">float</span>)
<span class="org-variable-name">fit.decoder._f.requires_grad</span> = <span class="org-constant">False</span>
fit.fit(data, max_epochs=1600, trace=<span class="org-constant">True</span>, lr=1e-3)
<span class="org-variable-name">elapsed</span> = time.time() - start
elapsed
</pre>
</div>

<pre class="example">
30.25163698196411

</pre>

<p>
Compare the fit fixing the ground truth \(\mf\) to the oracle.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">lhat</span> = fit.loadings(xt)
<span class="org-variable-name">fhat</span> = fit.factors()
pd.Series({
  <span class="org-string">'oracle'</span>: -st.poisson(mu=l @ f.T).logpmf(x).mean(),
  <span class="org-string">'anmf'</span>: -st.poisson(mu=s.reshape(-1, 1) * lhat @ fhat).logpmf(x).mean(),
})
</pre>
</div>

<pre class="example">
oracle    2.664753
anmf      2.676659
dtype: float64
</pre>

<p>
Plot the estimated loadings against the true loadings.
</p>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(2, 5, sharex=<span class="org-constant">True</span>, sharey=<span class="org-constant">True</span>)
fig.set_size_inches(8, 4)
<span class="org-variable-name">lim</span> = [0, 3]
<span class="org-keyword">for</span> i, a <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(ax.ravel()):
  a.scatter(np.sqrt(l[:,i]), np.sqrt(s * lhat[:,i]), s=1, c=<span class="org-string">'k'</span>)
  a.plot(lim, lim, lw=1, ls=<span class="org-string">':'</span>, c=<span class="org-string">'r'</span>)
  a.set_title(f<span class="org-string">'Factor {i}'</span>)
  a.set_xlim(lim)
  a.set_ylim(lim)
<span class="org-keyword">for</span> a <span class="org-keyword">in</span> ax:
  a[0].set_ylabel(<span class="org-string">'Sqrt est loading'</span>)
<span class="org-keyword">for</span> a <span class="org-keyword">in</span> ax.T:
  a[-1].set_xlabel(<span class="org-string">'Sqrt true loading'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/time-course.org/sim-lhat.png" alt="sim-lhat.png">
</p>
</div>

<p>
Check whether this is an algorithmic problem by trying batch gradient
descent instead.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">data</span> = td.DataLoader(dense_data, batch_size=x.shape[0], shuffle=<span class="org-constant">False</span>)
<span class="org-variable-name">fit</span> = anmf.modules.ANMF(input_dim=x.shape[1], latent_dim=10)
<span class="org-variable-name">fit.decoder._f.data</span> = torch.tensor(np.log(np.exp(f.T) - 1), dtype=torch.<span class="org-builtin">float</span>)
<span class="org-variable-name">fit.decoder._f.requires_grad</span> = <span class="org-constant">False</span>
fit.fit(data, max_epochs=6400, trace=<span class="org-constant">True</span>, lr=1e-3)

<span class="org-variable-name">lhat</span> = fit.loadings(xt)
<span class="org-variable-name">fhat</span> = fit.factors()
pd.Series({
  <span class="org-string">'oracle'</span>: -st.poisson(mu=l @ f.T).logpmf(x).mean(),
  <span class="org-string">'anmf'</span>: -st.poisson(mu=s.reshape(-1, 1) * lhat @ fhat).logpmf(x).mean(),
})
</pre>
</div>

<pre class="example">
oracle    2.664753
anmf      2.675015
dtype: float64
</pre>
</div>
</div>

<div id="outline-container-orgb7e7231" class="outline-3">
<h3 id="orgb7e7231">ANMF on 68K PBMCs</h3>
<div class="outline-text-3" id="text-orgb7e7231">
<p>
Read the data, restricting to genes with non-zero observations in at least 1
cell.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">x</span> = scmodes.dataset.read_10x(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-ideas/data/10xgenomics/fresh_68k_pbmc_donor_a/filtered_matrices_mex/hg19/'</span>, min_detect=0, return_adata=<span class="org-constant">True</span>)
sc.pp.filter_genes(x, min_cells=1)
<span class="org-variable-name">s</span> = x.X.<span class="org-builtin">sum</span>(axis=1)
</pre>
</div>

<p>
Report the dimensions.
</p>

<div class="org-src-container">
<pre class="src src-ipython">x.shape
</pre>
</div>

<pre class="example">
(68579, 20387)

</pre>

<p>
Fit rank 10 ANMF.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">sparse_data</span> = anmf.dataset.ExprDataset(x.X, s.A)
<span class="org-variable-name">data</span> = td.DataLoader(sparse_data, batch_size=128, shuffle=<span class="org-constant">False</span>, collate_fn=sparse_data.collate_fn)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">start</span> = time.time()
<span class="org-variable-name">fit</span> = (anmf.modules.ANMF(input_dim=x.shape[1], latent_dim=10)
       .fit(data, max_epochs=15, trace=<span class="org-constant">True</span>, verbose=<span class="org-constant">True</span>, lr=1e-2))
<span class="org-variable-name">elapsed</span> = time.time() - start
</pre>
</div>

<p>
Report how long the optimization took (minutes).
</p>

<div class="org-src-container">
<pre class="src src-ipython">elapsed / 60
</pre>
</div>

<pre class="example">
5.1033944328626

</pre>

<p>
Plot the optimization trace, focusing on the tail.
</p>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
plt.gcf().set_size_inches(4, 2)
plt.plot(np.array(fit.trace).ravel()[-2000:] / (128 * x.shape[1]), lw=1, c=<span class="org-string">'k'</span>)
plt.xticks(np.arange(0, 2500, 500), np.arange(-2000, 500, 500))
plt.xlabel(<span class="org-string">'Minibatches before maximum'</span>)
plt.ylabel(<span class="org-string">'Neg log lik per obs'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/time-course.org/pbmc-trace.png" alt="pbmc-trace.png">
</p>
</div>
</div>
</div>

<div id="outline-container-org8ec4008" class="outline-3">
<h3 id="org8ec4008">ANMF on 10-way mixture</h3>
<div class="outline-text-3" id="text-org8ec4008">
<p>
Read the data.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">x</span> = anndata.read_h5ad(<span class="org-string">'/scratch/midway2/aksarkar/ideas/zheng-10-way.h5ad'</span>)
<span class="org-variable-name">s</span> = x.X.<span class="org-builtin">sum</span>(axis=1)
</pre>
</div>

<p>
Fit ANMF. Report how long the optimization took (minutes).
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">sparse_data</span> = anmf.dataset.ExprDataset(x.X, s.A)
<span class="org-variable-name">data</span> = td.DataLoader(sparse_data, batch_size=64, shuffle=<span class="org-constant">False</span>, collate_fn=sparse_data.collate_fn)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">start</span> = time.time()
<span class="org-variable-name">fit</span> = (anmf.modules.ANMF(input_dim=x.shape[1])
       .fit(data, max_epochs=10, trace=<span class="org-constant">True</span>, verbose=<span class="org-constant">True</span>, lr=5e-3))
<span class="org-variable-name">elapsed</span> = time.time() - start
elapsed / 60
</pre>
</div>

<pre class="example">
5.154516808191935

</pre>

<p>
Save the fitted model.
</p>

<div class="org-src-container">
<pre class="src src-ipython">torch.save(fit.state_dict(), <span class="org-string">'/scratch/midway2/aksarkar/ideas/zheng-10-way-anmf.pkl'</span>)
</pre>
</div>

<p>
Load the fitted model.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">fit</span> = anmf.modules.ANMF(input_dim=x.shape[1])
fit.load_state_dict(torch.load(<span class="org-string">'/scratch/midway2/aksarkar/ideas/zheng-10-way-anmf.pkl'</span>))
fit.cuda()
</pre>
</div>

<p>
Plot the optimization trace, focusing on the tail.
</p>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
plt.gcf().set_size_inches(4, 2)
plt.plot(np.array(fit.trace).ravel()[-2000:] / (128 * x.shape[1]), lw=1, c=<span class="org-string">'k'</span>)
plt.xticks(np.arange(0, 2500, 500), np.arange(-2000, 500, 500))
plt.xlabel(<span class="org-string">'Minibatches before maximum'</span>)
plt.ylabel(<span class="org-string">'Neg log lik per obs'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/time-course.org/zheng-10-way-trace.png" alt="zheng-10-way-trace.png">
</p>
</div>

<p>
Get the loadings/factors.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">l</span> = np.vstack([fit.loadings(b) <span class="org-keyword">for</span> b, s <span class="org-keyword">in</span> data])
<span class="org-variable-name">f</span> = fit.factors()
</pre>
</div>

<p>
Normalize into a topic model.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">w</span> = l * f.<span class="org-builtin">sum</span>(axis=1)
<span class="org-variable-name">w</span> /= w.<span class="org-builtin">sum</span>(axis=1, keepdims=<span class="org-constant">True</span>)
</pre>
</div>

<p>
Get the cell type identities.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">onehot</span> = pd.get_dummies(x.obs[<span class="org-string">'cell_type'</span>])
</pre>
</div>

<p>
Estimate the correlation between the topic weights and the cell types.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">r</span> = np.corrcoef(w.T, onehot.T)
plt.clf()
plt.gcf().set_size_inches(4, 4)
plt.imshow(r[:10,10:], cmap=colorcet.cm[<span class="org-string">'coolwarm'</span>], vmin=-1, vmax=1)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/time-course.org/zheng-10-way-loadings.png" alt="zheng-10-way-loadings.png">
</p>
</div>
</div>
</div>

<div id="outline-container-org687e657" class="outline-3">
<h3 id="org687e657">ANMF on iPSC data</h3>
<div class="outline-text-3" id="text-org687e657">
<p>
Read the data.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">x</span> = anndata.read_h5ad(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-ideas/data/ipsc/ipsc.h5ad'</span>)
<span class="org-variable-name">s</span> = x.X.<span class="org-builtin">sum</span>(axis=1)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">sparse_data</span> = anmf.dataset.ExprDataset(x.X, s.A)
<span class="org-variable-name">data</span> = td.DataLoader(sparse_data, batch_size=64, shuffle=<span class="org-constant">False</span>, collate_fn=sparse_data.collate_fn)
</pre>
</div>

<p>
Run ANMF on the data. A priori, we might expect the data to be rank 53
(equal to the number of donor individuals). Indeed, this is the rank of the
implicit factor model we originally fit to the data.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">start</span> = time.time()
<span class="org-variable-name">fit</span> = (anmf.modules.ANMF(input_dim=x.shape[1], latent_dim=53)
       .fit(data, max_epochs=40, trace=<span class="org-constant">True</span>, verbose=<span class="org-constant">True</span>, lr=1e-2))
<span class="org-variable-name">elapsed</span> = time.time() - start
</pre>
</div>

<p>
Report how long the optimization took (minutes).
</p>

<div class="org-src-container">
<pre class="src src-ipython">elapsed / 60
</pre>
</div>

<pre class="example">
2.7217764496803283

</pre>

<p>
Plot the end of the optimization trace.
</p>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
plt.gcf().set_size_inches(4, 2)
plt.plot(np.array(fit.trace).ravel()[-100:] / (128 * x.shape[1]), lw=1, c=<span class="org-string">'k'</span>)
plt.xticks(np.arange(0, 125, 25), np.arange(-100, 25, 25))
plt.xlabel(<span class="org-string">'Minibatches before maximum'</span>)
plt.ylabel(<span class="org-string">'Neg log lik per obs'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/time-course.org/ipsc-trace.png" alt="ipsc-trace.png">
</p>
</div>

<p>
Save the fitted model.
</p>

<div class="org-src-container">
<pre class="src src-ipython">torch.save(fit.state_dict(), <span class="org-string">'/scratch/midway2/aksarkar/ideas/ipsc-anmf.pkl'</span>)
</pre>
</div>

<p>
Load the fitted model.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">fit</span> = anmf.modules.ANMF(input_dim=x.shape[1], latent_dim=53)     
fit.load_state_dict(torch.load(<span class="org-string">'/scratch/midway2/aksarkar/ideas/ipsc-anmf.pkl'</span>))
fit.cuda()
</pre>
</div>

<pre class="example">
ANMF(
(encoder): Encoder(
(net): Sequential(
(0): Linear(in_features=9957, out_features=128, bias=True)
(1): ReLU()
(2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): Linear(in_features=128, out_features=53, bias=True)
(4): Softplus(beta=1, threshold=20)
)
)
(decoder): Pois()
)
</pre>

<p>
Look at the correlation between the loadings and the donor individuals.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">l</span> = np.vstack([fit.loadings(b) <span class="org-keyword">for</span> b, s <span class="org-keyword">in</span> data])
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">keep_samples</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/quality-single-cells.txt'</span>, index_col=0, header=<span class="org-constant">None</span>)
<span class="org-variable-name">annotations</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-annotation.txt'</span>)
<span class="org-variable-name">annotations</span> = annotations.loc[keep_samples.values.ravel()]
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">onehot</span> = pd.get_dummies(annotations[<span class="org-string">'chip_id'</span>])
<span class="org-variable-name">r</span> = np.corrcoef(l.T, onehot.T)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
plt.gcf().set_size_inches(4, 4)
plt.imshow(r, vmin=-1, vmax=1, cmap=colorcet.cm[<span class="org-string">'coolwarm'</span>])
plt.xlabel(<span class="org-string">'Estimated loadings'</span>)
plt.ylabel(<span class="org-string">'Donor individual'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/time-course.org/ipsc-loadings.png" alt="ipsc-loadings.png">
</p>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Abhishek Sarkar</p>
<p class="date">Created: 2020-03-13 Fri 12:40</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
