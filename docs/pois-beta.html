<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2020-02-17 Mon 11:53 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Poisson-Beta model</title>
<meta name="generator" content="Org mode">
<meta name="author" content="Abhishek Sarkar">
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link href="bootstrap.min.css" rel="stylesheet" />
<link rel="stylesheet" type="text/css" href="htmlize.css"/>
<link rel="stylesheet" type="text/css" href="main.css"/>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2017 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Poisson-Beta model</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgc65a620">Introduction</a></li>
<li><a href="#setup">Setup</a></li>
<li><a href="#methods">Methods</a>
<ul>
<li><a href="#moment">Moment estimation</a></li>
<li><a href="#mle">Maximum likelihood estimation</a></li>
<li><a href="#mcmc">Slice-within-Gibbs sampling</a></li>
<li><a href="#vi">Variational inference</a></li>
<li><a href="#org5ff99fe">Simulation</a></li>
</ul>
</li>
<li><a href="#results">Results</a>
<ul>
<li><a href="#simulation">Simulation</a></li>
<li><a href="#orgfaf81f2">Running time benchmark</a></li>
<li><a href="#org2a8e2d9">Impact of size factors</a></li>
<li><a href="#org1a579b3">Log likelihood surface</a></li>
<li><a href="#org2d20171">Impact of initialization</a></li>
<li><a href="#orgfb20ae8">Slice-within-Gibbs sampling</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-orgc65a620" class="outline-2">
<h2 id="orgc65a620">Introduction</h2>
<div class="outline-text-2" id="text-orgc65a620">
<p>
One idealized model for transcriptional regulation is the <i>telegraph model</i>
(Peccoud and Ycart 1995, Raj et al. 2006, Kim and Marioni 2013, Munsky et
al. 2013), whose steady state is described by \(
  \newcommand\Bin{\operatorname{Binomial}}
  \newcommand\B{\operatorname{Beta}}
  \newcommand\E[1]{\left\langle #1 \right\rangle}
  \newcommand\Gam{\operatorname{Gamma}}
  \newcommand\Pois{\operatorname{Poisson}}
  \newcommand\aoff{a_{\text{off}}}
  \newcommand\aon{a_{\text{on}}}
  \newcommand\ar{a_r}
  \newcommand\betafun{\operatorname{B}}
  \newcommand\boff{b_{\text{off}}}
  \newcommand\bon{b_{\text{on}}}
  \newcommand\br{b_r}
  \newcommand\const{\mathrm{const}}
  \newcommand\koff{k_{\text{off}}}
  \newcommand\kon{k_{\text{on}}}
  \newcommand\kr{k_r}
  \)
</p>

\begin{align*}
  x_i \mid p_i, \kr &\sim \Pois(p_i \kr)\\
  p_i \mid \kon, \koff &\sim \B(\kon, \koff)
\end{align*}

<p>
where \(x_i\) is the number of mRNA molecules in cell \(i=1, \ldots, n\)
(considering only one gene), \(\kon\) is the rate of off\(\rightarrow\)on
promoter switching, \(\koff\) is the rate of on\(\rightarrow\)off promoter
switching, and \(\kr\) is the rate of mRNA synthesis. (All rates are scaled
relative to the mRNA decay rate.)
</p>

<p>
The inference goal is to estimate \(\kr, \kon, \koff\) given data \(x\)
assumed to be at steady state. The marginal likelihood \(p(x_i \mid \kr,
  \kon, \koff)\) does have a closed form; however, it involves the
<a href="http://mathworld.wolfram.com/HypergeometricFunction.html">confluent
hypergeometric function of the first kind</a> (Raj et al 2006), which is
difficult to evaluate. To avoid this challenge, Kim and Marioni 2013 develop
an MCMC scheme to sample from the posterior \(p(\kr, \kon, \koff \mid x_1,
  \ldots, x_n)\), and Larsson et al. 2019 use numerical integration to evaluate
the marginal likelihood. 
</p>

<p>
We collected these different inference algorithms into a Python package
<a href="https://www.github.com/aksarkar/poisbeta">poisbeta</a>. Here, we investigate
how the running time of the MLE procedure (using numerical integration)
scales, and whether variational inference can improve the running time. We
also investigate the effect of size factors (Kim and Marioni 2013) and
initialization on the estimates.
</p>
</div>
</div>

<div id="outline-container-orgc673021" class="outline-2">
<h2 id="setup"><a id="orgc673021"></a>Setup</h2>
<div class="outline-text-2" id="text-setup">
<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> colorcet
<span class="org-keyword">import</span> itertools <span class="org-keyword">as</span> it
<span class="org-keyword">import</span> numpy <span class="org-keyword">as</span> np
<span class="org-keyword">import</span> pandas <span class="org-keyword">as</span> pd
<span class="org-keyword">import</span> poisbeta
<span class="org-keyword">import</span> scipy.optimize <span class="org-keyword">as</span> so
<span class="org-keyword">import</span> scipy.special <span class="org-keyword">as</span> sp
<span class="org-keyword">import</span> scipy.stats <span class="org-keyword">as</span> st
<span class="org-keyword">import</span> sys
<span class="org-keyword">import</span> timeit
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">%matplotlib inline
%config <span class="org-variable-name">InlineBackend.figure_formats</span> = <span class="org-builtin">set</span>([<span class="org-string">'retina'</span>])
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> matplotlib.pyplot <span class="org-keyword">as</span> plt
<span class="org-variable-name">plt.rcParams</span>[<span class="org-string">'figure.facecolor'</span>] = <span class="org-string">'w'</span>
<span class="org-variable-name">plt.rcParams</span>[<span class="org-string">'font.family'</span>] = <span class="org-string">'Nimbus Sans'</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-org7800341" class="outline-2">
<h2 id="methods"><a id="org7800341"></a>Methods</h2>
<div class="outline-text-2" id="text-methods">
</div>
<div id="outline-container-org5c935d4" class="outline-3">
<h3 id="moment"><a id="org5c935d4"></a>Moment estimation</h3>
<div class="outline-text-3" id="text-moment">
<p>
Peccoud and Ycart 1995 derived moment estimators for \(\kr, \kon, \koff\)
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">fit_poisson_beta_moment</span>(x, **kwargs):
  <span class="org-doc">"""Return ln kr, ln kon, ln koff</span>

<span class="org-doc">  Estimate kr, kon, koff using the first three exponential moments (Peccoud &amp;</span>
<span class="org-doc">  Ycart 1995).</span>

<span class="org-doc">  x - array-like [n,]</span>

<span class="org-doc">  """</span>
  <span class="org-variable-name">moments</span> = np.array([1, x.mean(), (x * (x - 1)).mean(), (x * (x - 1) * (x - 2)).mean()])
  <span class="org-variable-name">ratios</span> = moments[1:] / moments[:-1]
  <span class="org-variable-name">kr</span> = (2 * ratios[0] * ratios[2] - ratios[0] * ratios[1] - ratios[1] * ratios[2]) / (ratios[0] - 2 * ratios[1] + ratios[2])
  <span class="org-variable-name">kon</span> = (2 * ratios[0] * (ratios[2] - ratios[1])) / (ratios[0] * ratios[1] - 2 * ratios[0] * ratios[2] + ratios[1] * ratios[2])
  <span class="org-variable-name">koff</span> = (2 * (ratios[2] - ratios[1]) * (ratios[0] - ratios[2]) * (ratios[1] - ratios[0])) / ((ratios[0] * ratios[1] - 2 * ratios[0] * ratios[2] + ratios[1] * ratios[2]) * (ratios[0] - 2 * ratios[1] + ratios[2]))
  <span class="org-variable-name">result</span> = np.array([kr, kon, koff])
  <span class="org-keyword">if</span> <span class="org-keyword">not</span> (np.isfinite(result).<span class="org-builtin">all</span>() <span class="org-keyword">and</span> (result &gt; 0).<span class="org-builtin">all</span>()):
    <span class="org-keyword">raise</span> <span class="org-type">RuntimeError</span>(<span class="org-string">'moment estimation failed'</span>)
  <span class="org-keyword">return</span> np.log(result)
</pre>
</div>
</div>
</div>

<div id="outline-container-org85740f7" class="outline-3">
<h3 id="mle"><a id="org85740f7"></a>Maximum likelihood estimation</h3>
<div class="outline-text-3" id="text-mle">
<p>
Larsson et al. 2019 use
<a href="https://en.wikipedia.org/wiki/Gauss%E2%80%93Jacobi_quadrature">Gauss-Jacobi
quadrature</a> to evaluate the marginal likelihood
</p>

\begin{align*}
  \ell &= \frac{1}{\betafun(\kon, \koff)} \int_0^1 \Pois(x_i; \kr p_i)\, p_i^{\kon - 1} (1 - p_i)^{\koff - 1}\; dp_i\\
  &= \frac{1}{2^{\kon + \koff - 1} \betafun(\kon, \koff)} \int_{-1}^1 \Pois(x_i; \kr \frac{1 + t_i}{2})\, (1 + t_i)^{\kon - 1} (1 - t_i)^{\koff - 1}\; dt_i\\
  &\approx \frac{1}{2^{\kon + \koff - 1} \betafun(\kon, \koff)} \sum_{k=1}^K w_k \Pois(x_i; \kr y_k)
\end{align*}

<p>
where \(\betafun(\cdot)\) denotes the Beta function, \(y_k\) is the root of
the Jacobi polynomial of degree \(k\), and \(w_k\) is the associated
weight. This procedure can be used as a subroutine to numerically optimize
the marginal likelihood.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">poisson_beta_logpmf</span>(theta, x, s, n_points=50):
  <span class="org-variable-name">kr</span>, <span class="org-variable-name">kon</span>, <span class="org-variable-name">koff</span> = np.exp(theta) + 1e-8
  <span class="org-comment-delimiter"># </span><span class="org-comment">Important: Gauss-Jacobi quadrature computes the integral over t &#8712; [-1, 1],</span>
  <span class="org-comment-delimiter"># </span><span class="org-comment">but we want the integral over p &#8712; [0, 1]</span>
  <span class="org-variable-name">t</span>, <span class="org-variable-name">w</span> = sp.roots_jacobi(n=n_points, alpha=koff - 1, beta=kon - 1)
  <span class="org-comment-delimiter"># </span><span class="org-comment">(n_points, 1)</span>
  <span class="org-variable-name">p</span> = ((1 + t) / 2).reshape(-1, 1)
  <span class="org-comment-delimiter"># </span><span class="org-comment">(1, n_points) @ (n_points, n)</span>
  <span class="org-variable-name">px</span> = w.reshape(1, -1) @ st.poisson(mu=s * kr * p).pmf(x.reshape(1, -1))
  <span class="org-comment-delimiter"># </span><span class="org-comment">Important: extra 1/2 comes from u-substitution</span>
  <span class="org-keyword">return</span> np.log(px) - sp.betaln(kon, koff) - (kon + koff - 1) * np.log(2)

<span class="org-keyword">def</span> <span class="org-function-name">poisson_beta_pmf</span>(theta, x, s, n_points=50):
  <span class="org-keyword">return</span> np.exp(poisson_beta_logpmf(theta, x, s, n_points))

<span class="org-keyword">def</span> <span class="org-function-name">poisson_beta_neg_llik</span>(theta, x, s, n_points=50):
  <span class="org-doc">"""Return the negative log likelihood of the data</span>

<span class="org-doc">  theta - [ln k_r, ln k_on, ln k_off]</span>
<span class="org-doc">  n_points - number of points used in numerical integration</span>

<span class="org-doc">  """</span>
  <span class="org-keyword">return</span> -poisson_beta_logpmf(theta, x, s, n_points).<span class="org-builtin">sum</span>()

<span class="org-keyword">def</span> <span class="org-function-name">fit_poisson_beta_mle</span>(x, s=<span class="org-constant">None</span>, init=<span class="org-constant">None</span>, max_iters=1000, n_points=50):
  <span class="org-doc">"""Return ln k_r, ln k_on, ln k_off</span>

<span class="org-doc">  x - array-like [n,]</span>
<span class="org-doc">  init - [ln k_r, ln k_on, ln k_off]</span>
<span class="org-doc">  n_points - number of points used in numerical integration</span>

<span class="org-doc">  """</span>
  <span class="org-keyword">if</span> init <span class="org-keyword">is</span> <span class="org-constant">None</span>:
    <span class="org-keyword">try</span>:
      <span class="org-variable-name">init</span> = fit_poisson_beta_moment(x)
    <span class="org-keyword">except</span> <span class="org-type">RuntimeError</span>:
      <span class="org-variable-name">init</span> = np.zeros(3)
  <span class="org-keyword">if</span> s <span class="org-keyword">is</span> <span class="org-constant">None</span>:
    <span class="org-variable-name">s</span> = 1
  <span class="org-variable-name">opt</span> = so.minimize(poisson_beta_neg_llik, x0=init, args=(x, s, n_points),
                    method=<span class="org-string">'Nelder-Mead'</span>, options={<span class="org-string">'maxiter'</span>: max_iters})
  <span class="org-keyword">if</span> <span class="org-keyword">not</span> opt.success:
    <span class="org-keyword">raise</span> <span class="org-type">RuntimeError</span>(f<span class="org-string">'failed to converge: {opt.message}'</span>)
  <span class="org-keyword">return</span> opt.x
</pre>
</div>
</div>
</div>

<div id="outline-container-org860fe85" class="outline-3">
<h3 id="mcmc"><a id="org860fe85"></a>Slice-within-Gibbs sampling</h3>
<div class="outline-text-3" id="text-mcmc">
<p>
Kim &amp; Marioni 2013 add priors
</p>

\begin{align*}
  \kr \mid \ar, \br &\sim \Gam(\ar, \br)\\
  \kon \mid \aon, \bon &\sim \Gam(\aon, \bon)\\
  \koff \mid \aoff, \boff &\sim \Gam(\aoff, \boff)
\end{align*}

<p>
The conditional (log) densities are available, up to constants
</p>

\begin{align*}
  \ln p(p_i \mid \cdot) &= (x_i + \kon - 1) \ln p_i + (\koff - 1)\ln(1 - p_i) - \kr p_i + \const\\
  \ln p(\kr \mid \cdot) &= \sum_i [(x_i + \ar - 1) \ln \kr - (p_i + \br) \kr] + \const\\
  \ln p(\kon \mid \cdot) &= \sum_i [\kon \ln p_i + (\aon - 1)\ln \kon - \bon\kon - \ln\Gamma(\kon) + \ln\Gamma(\kon + \koff)] + \const\\
  \ln p(\koff \mid \cdot) &= \sum_i [\koff \ln (1 - p_i) + (\aoff - 1)\ln \koff - \boff\koff - \ln\Gamma(\koff) + \ln\Gamma(\kon + \koff)] + \const
\end{align*}

<p>
Although the conditional distributions are non-standard,
<a href="https://en.wikipedia.org/wiki/Slice_sampling">slice sampling</a>
(<a href="https://projecteuclid.org/euclid.aos/1056562461">Neal 2003</a>) only
requires the target density up to a constant.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">_slice_sample</span>(f, init, width=0.1, max_steps=100, **kwargs):
  <span class="org-doc">"""Return samples from the density proportional to exp(f)</span>

<span class="org-doc">  f - log density (up to a constant)</span>
<span class="org-doc">  init - initial value</span>
<span class="org-doc">  kwargs - additional arguments to f</span>

<span class="org-doc">  """</span>
  <span class="org-comment-delimiter"># </span><span class="org-comment">Auxiliary variable defines the slice</span>
  <span class="org-variable-name">z</span> = f(init, **kwargs) - np.random.exponential()
  <span class="org-comment-delimiter"># </span><span class="org-comment">Step out</span>
  <span class="org-variable-name">left</span> = init - width * np.random.uniform()
  <span class="org-variable-name">right</span> = left + width
  <span class="org-variable-name">left_steps</span> = <span class="org-builtin">int</span>(np.random.uniform() * max_steps)
  <span class="org-keyword">for</span> _ <span class="org-keyword">in</span> <span class="org-builtin">range</span>(left_steps):
    <span class="org-keyword">if</span> z &lt; f(left, **kwargs):
      <span class="org-keyword">break</span>
    <span class="org-variable-name">left</span> -= width
  <span class="org-keyword">for</span> _ <span class="org-keyword">in</span> <span class="org-builtin">range</span>(max_steps - left_steps):
    <span class="org-keyword">if</span> z &lt; f(right, **kwargs):
      <span class="org-keyword">break</span>
    <span class="org-variable-name">right</span> += width
  <span class="org-comment-delimiter"># </span><span class="org-comment">Step in</span>
  <span class="org-keyword">while</span> right &gt; left:
    <span class="org-variable-name">proposal</span> = left + np.random.uniform() * (right - left)
    <span class="org-keyword">if</span> z &lt; f(proposal, **kwargs):
      <span class="org-keyword">return</span> proposal
    <span class="org-keyword">elif</span> proposal &lt; init:
      <span class="org-variable-name">left</span> = proposal
    <span class="org-keyword">else</span>:
      <span class="org-variable-name">right</span> = proposal
  <span class="org-keyword">raise</span> <span class="org-type">RuntimeError</span>(<span class="org-string">'failed to find an acceptable sample'</span>)

<span class="org-keyword">def</span> <span class="org-function-name">_cond_logpdf_p</span>(p, x, kr, kon, koff):
  <span class="org-keyword">return</span> (x + kon - 1) * np.log(p) + (koff - 1) * np.log(1 - p) - kr * p

<span class="org-keyword">def</span> <span class="org-function-name">_cond_logpdf_kr</span>(kr, x, p, ar, br):
  <span class="org-keyword">return</span> ((x + ar - 1) * np.log(kr) - (p + br) * kr).<span class="org-builtin">sum</span>()

<span class="org-keyword">def</span> <span class="org-function-name">_cond_logpdf_kon</span>(kon, p, koff, aon, bon):
  <span class="org-keyword">return</span> (kon * np.log(p) + (aon - 1) * np.log(kon)
          - bon * kon - sp.gammaln(kon) + sp.gammaln(kon + koff)).<span class="org-builtin">sum</span>()

<span class="org-keyword">def</span> <span class="org-function-name">_cond_logpdf_koff</span>(koff, p, on, aoff, boff):
  <span class="org-keyword">return</span> (koff * np.log(1 - p) + (aoff - 1) * np.log(koff)
          - boff * koff - sp.gammaln(koff) + sp.gammaln(on + koff)).<span class="org-builtin">sum</span>()

<span class="org-keyword">def</span> <span class="org-function-name">fit_poisson_beta_mcmc</span>(x, n_samples, ar, br, aon, bon, aoff, boff, verbose=<span class="org-constant">False</span>):
  <span class="org-doc">"""Return samples from the posterior p(kon, koff, kr | x)</span>

<span class="org-doc">  x - counts (n,)</span>
<span class="org-doc">  n_samples - number of samples to draw</span>
<span class="org-doc">  ar, br - prior parameters kr ~ Gamma(ar, br)</span>
<span class="org-doc">  aon, bon - prior parameters kon ~ Gamma(aon, bon)</span>
<span class="org-doc">  aoff, boff - prior parameters koff ~ Gamma(aoff, boff)</span>

<span class="org-doc">  """</span>
  <span class="org-variable-name">samples</span> = []
  <span class="org-comment-delimiter"># </span><span class="org-comment">Important: these are fixed</span>
  <span class="org-variable-name">Fr</span> = st.gamma(a=ar, scale=1 / br)
  <span class="org-variable-name">Fon</span> = st.gamma(a=aon, scale=1 / bon)
  <span class="org-variable-name">Foff</span> = st.gamma(a=aoff, scale=1 / boff)
  <span class="org-comment-delimiter"># </span><span class="org-comment">Draw initial sample from the prior</span>
  <span class="org-variable-name">kr</span> = Fr.rvs(size=1)
  <span class="org-variable-name">kon</span> = Fon.rvs(size=1)
  <span class="org-variable-name">koff</span> = Foff.rvs(size=1)
  <span class="org-variable-name">p</span> = st.beta(a=kon, b=koff).rvs(size=x.shape)
  <span class="org-keyword">for</span> t <span class="org-keyword">in</span> <span class="org-builtin">range</span>(n_samples):
    samples.append((kr, kon, koff))
    <span class="org-keyword">for</span> i <span class="org-keyword">in</span> <span class="org-builtin">range</span>(x.shape[0]):
      <span class="org-variable-name">p</span>[i] = _slice_sample(_cond_logpdf_p, init=p[i], x=x[i], kr=kr, kon=kon, koff=koff)
    <span class="org-variable-name">kr</span> = _slice_sample(_cond_logpdf_kr, init=kr, x=x, p=p, ar=ar, br=br)
    <span class="org-variable-name">kon</span> = _slice_sample(_cond_logpdf_kon, init=kon, p=p, koff=koff, aon=aon, bon=bon)
    <span class="org-variable-name">koff</span> = _slice_sample(_cond_logpdf_koff, init=koff, p=p, kon=kon, aoff=aoff, boff=boff)
    <span class="org-keyword">if</span> verbose:
      <span class="org-variable-name">log_joint</span> = (st.poisson(mu=kr * p).logpmf(x).<span class="org-builtin">sum</span>()
                   + st.beta(a=kon, b=koff).logpdf(p).<span class="org-builtin">sum</span>()
                   + Fr.logpdf(kr)
                   + Fon.logpdf(kon)
                   + Foff.logpdf(koff))
      <span class="org-keyword">print</span>(f<span class="org-string">'sample {t}: {log_joint}'</span>)
  <span class="org-keyword">return</span> samples
</pre>
</div>
</div>
</div>

<div id="outline-container-orgafe29a8" class="outline-3">
<h3 id="vi"><a id="orgafe29a8"></a>Variational inference</h3>
<div class="outline-text-3" id="text-vi">
<p>
To make the model amenable for VI, introduce latent variables \(z_i\)
</p>

\begin{align*}
  x_i \mid z_i, p_i &\sim \Bin(z_i, p_i)\\
  z_i \mid \kr &\sim \Pois(\kr)\\
  p_i \mid \kon, \koff &\sim \B(\kon, \koff)
\end{align*}

<p>
Then, we have
</p>

\begin{multline*}
  \ln p(x_i, z_i, p_i \mid \kr, \kon, \koff) = (x + \kon - 1) \ln p_i + (z_i - x_i + \koff - 1) \ln(1 - p_i)\\
  + (z_i - x_i) \ln \kr - \kr + x_i \ln \kr - \ln\Gamma(x_i + 1) - \ln\Gamma(z_i - x_i + 1) - \ln\betafun(\kon, \koff)
\end{multline*}

<p>
where we have added and subtracted \(x_i \ln \kr\) to more easily derive
coordinate updates
</p>

\begin{align*}
  q^*(z_i - x_i) &= \Pois(\exp(\E{\ln (1 - p_i)} + \ln k_r)) \triangleq \Pois(\mu_i) \\
  q^*(p_i) &= \B(x_i + \kon, \E{z_i - x_i} + \koff) \triangleq \B(\alpha_i, \beta_i)
\end{align*}

<p>
where expectations are taken with respect to the variational approximation
\(q\). <a href="https://en.wikipedia.org/wiki/Beta_distribution#Moments_of_logarithmically_transformed_random_variables">Using
properties of the Beta distribution</a>
</p>

\begin{align*}
  \E{\ln p_i} &= \psi(\alpha_i) - \psi(\alpha_i + \beta_i)\\
  \E{\ln (1 - p_i)} &= \psi(\beta_i) - \psi(\alpha_i + \beta_i)
\end{align*}   

<p>
where \(\psi\) is the digamma function. The evidence lower bound is
</p>

\begin{multline*}
  \ell = \sum_i (x_i + \kon - \alpha_i) \E{\ln p_i} + (\mu_i + \koff - \beta_i) \E{\ln(1 - p_i)}\\
  + (x_i + \mu_i) \ln \kr - k_r - \mu_i \ln \mu_i + \mu_i - \ln\Gamma(x_i + 1) - \ln\betafun(\kon, \koff) + \ln\betafun(\alpha_i, \beta_i)
\end{multline*}

<p>
We can derive the Jacobian with respect to \(\kr, \kon, \koff\), leading to
an analytic update for \(\kr\). We can use Brent's method to numerically
update \(\kon, \koff\).
</p>

\begin{align*}
  \frac{\partial \ell}{\partial \kr} &= \sum_i \frac{x_i + \mu_i}{\kr} - 1\\
  \kr &:= \frac{1}{n} \sum_i x_i + \mu_i\\
  \frac{\partial \ell}{\partial \kon} &= \sum_i \psi(\alpha_i) - \psi(\alpha_i - \beta_i) - \psi(\kon) + \psi(\kon + \koff)\\
  \frac{\partial \ell}{\partial \koff} &= \sum_i \psi(\beta_i) - \psi(\alpha_i - \beta_i) - \psi(\koff) + \psi(\kon + \koff)
\end{align*}

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">poisson_beta_elbo</span>(theta, x, mu, alpha, beta):
  <span class="org-doc">"""Return the evidence lower bound</span>

<span class="org-doc">  theta - [ln k_r, ln k_on, ln k_off]</span>
<span class="org-doc">  x - array-like [n,]</span>
<span class="org-doc">  mu - array-like [n,]</span>
<span class="org-doc">  alpha - array-like [n,]</span>
<span class="org-doc">  beta - array-like [n,]</span>

<span class="org-doc">  """</span>
  <span class="org-variable-name">kr</span>, <span class="org-variable-name">kon</span>, <span class="org-variable-name">koff</span> = np.exp(theta)
  <span class="org-keyword">return</span> ((x + kon - alpha) * (sp.digamma(alpha) - sp.digamma(alpha + beta))
          + (mu + koff - beta) * (sp.digamma(beta) - sp.digamma(alpha + beta))
          + (x + mu) * np.log(kr) - kr - mu * np.log(mu) + mu - sp.gammaln(x + 1)
          - sp.betaln(kon, koff) + sp.betaln(alpha, beta)).<span class="org-builtin">sum</span>()

<span class="org-keyword">def</span> <span class="org-function-name">poisson_beta_delbo_dkon</span>(ln_kon, ln_koff, alpha, beta):
  <span class="org-doc">"""Return the partial derivative of ELBO wrt kon"""</span>
  <span class="org-keyword">return</span> (sp.digamma(alpha)
          - sp.digamma(alpha - beta)
          - sp.digamma(np.exp(ln_kon))
          + sp.digamma(np.exp(ln_kon) + np.exp(ln_koff))).<span class="org-builtin">sum</span>()

<span class="org-keyword">def</span> <span class="org-function-name">poisson_beta_delbo_dkoff</span>(ln_koff, ln_kon, alpha, beta):
  <span class="org-doc">"""Return the partial derivative of ELBO wrt koff"""</span>
  <span class="org-keyword">return</span> (sp.digamma(beta)
          - sp.digamma(alpha - beta)
          - sp.digamma(np.exp(ln_koff))
          + sp.digamma(np.exp(ln_kon) + np.exp(ln_koff))).<span class="org-builtin">sum</span>()

<span class="org-keyword">def</span> <span class="org-function-name">fit_poisson_beta_vi</span>(x, init=<span class="org-constant">None</span>, atol=1e-8, max_iters=1000, verbose=<span class="org-constant">False</span>):
  <span class="org-doc">"""Return kr, kon, koff</span>

<span class="org-doc">  init - [ln k_r, ln k_on, ln k_off]</span>

<span class="org-doc">  """</span>
  <span class="org-keyword">if</span> init <span class="org-keyword">is</span> <span class="org-constant">None</span>:
    <span class="org-variable-name">init</span> = np.log(fit_poisson_beta_moment(x))
  <span class="org-variable-name">theta</span> = init
  <span class="org-variable-name">mu</span> = np.zeros(x.shape)
  <span class="org-variable-name">alpha</span> = np.ones(x.shape)
  <span class="org-variable-name">beta</span> = np.ones(x.shape)

  <span class="org-variable-name">obj</span> = -np.inf
  <span class="org-keyword">for</span> t <span class="org-keyword">in</span> <span class="org-builtin">range</span>(max_iters):
    <span class="org-variable-name">alpha</span> = x + np.exp(theta[1])
    <span class="org-variable-name">beta</span> = mu + np.exp(theta[2])
    <span class="org-variable-name">mu</span> = np.exp(sp.digamma(beta) - sp.digamma(alpha + beta) + theta[0])
    <span class="org-variable-name">theta</span>[0] = np.log((x + mu).mean())
    <span class="org-variable-name">opt_kon</span> = so.newton(poisson_beta_delbo_dkon, x0=theta[1], args=(theta[2], alpha, beta))
    <span class="org-keyword">if</span> <span class="org-keyword">not</span> opt_kon.success:
      <span class="org-keyword">raise</span> <span class="org-type">RuntimeError</span>(f<span class="org-string">'k_on update failed: {opt_kon.message}'</span>)
    <span class="org-variable-name">theta</span>[1] = opt_kon.x
    <span class="org-variable-name">opt_koff</span> = so.root_scalar(poisson_beta_delbo_dkoff, x0=theta[2], args=(theta[1], alpha, beta))
    <span class="org-keyword">if</span> <span class="org-keyword">not</span> opt_koff.success:
      <span class="org-keyword">raise</span> <span class="org-type">RuntimeError</span>(f<span class="org-string">'k_on update failed: {opt_koff.message}'</span>)
    <span class="org-variable-name">theta</span>[1] = opt_koff.x
    <span class="org-variable-name">update</span> = poisson_beta_elbo(theta, x, mu, alpha, beta)
    <span class="org-keyword">if</span> verbose:
      <span class="org-keyword">print</span>(f<span class="org-string">'Epoch {t}: {update}'</span>)
    <span class="org-keyword">if</span> <span class="org-builtin">abs</span>(obj - update) &lt; atol:
      <span class="org-keyword">return</span> theta
    <span class="org-keyword">else</span>:
      <span class="org-variable-name">obj</span> = update
  <span class="org-keyword">raise</span> <span class="org-type">RuntimeError</span>(<span class="org-string">'failed to converge'</span>)
</pre>
</div>
</div>
</div>

<div id="outline-container-org5ff99fe" class="outline-3">
<h3 id="org5ff99fe">Simulation</h3>
<div class="outline-text-3" id="text-org5ff99fe">
<p>
Draw data from the basic model.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="org06bcd76"><span class="org-keyword">def</span> <span class="org-function-name">simulate_pois_beta</span>(n, s=<span class="org-constant">None</span>, kr=<span class="org-constant">None</span>, kon=<span class="org-constant">None</span>, koff=<span class="org-constant">None</span>, seed=<span class="org-constant">None</span>):
  <span class="org-keyword">if</span> seed <span class="org-keyword">is</span> <span class="org-keyword">not</span> <span class="org-constant">None</span>:
    np.random.seed(seed)
  <span class="org-keyword">if</span> kr <span class="org-keyword">is</span> <span class="org-constant">None</span>:
    <span class="org-variable-name">kr</span> = np.random.lognormal(mean=3)
  <span class="org-keyword">if</span> kon <span class="org-keyword">is</span> <span class="org-constant">None</span>:
    <span class="org-variable-name">kon</span> = np.random.lognormal()
  <span class="org-keyword">if</span> koff <span class="org-keyword">is</span> <span class="org-constant">None</span>:
    <span class="org-variable-name">koff</span> = np.random.lognormal()
  <span class="org-keyword">if</span> s <span class="org-keyword">is</span> <span class="org-constant">None</span>:
    <span class="org-variable-name">s</span> = 1
  <span class="org-variable-name">p</span> = np.random.beta(a=kon, b=koff, size=n)
  <span class="org-variable-name">x</span> = np.random.poisson(lam=s * kr * p)
  <span class="org-keyword">return</span> x, kr, kon, koff
</pre>
</div>

<p>
Draw data from a model with size factors.
</p>

\begin{align*}
  x_i \mid s_i, p_i, \kr &\sim \Pois(s_i p_i \kr)\\
  p_i \mid \kon, \koff &\sim \B(\kon, \koff)
\end{align*}

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">trial</span>(n, mean_size, var_size, kr, kon, koff, seed):
  np.random.seed(seed)
  <span class="org-variable-name">disp</span> = (var_size - mean_size) / (mean_size * mean_size)
  <span class="org-variable-name">s</span> = np.random.poisson(lam=mean_size, size=n) * np.random.gamma(shape=1 / disp, scale=disp, size=n)
  x, *<span class="org-variable-name">theta</span> = simulate_pois_beta(n=n, s=s, kr=kr / mean_size, kon=kon, koff=koff, seed=1)
  <span class="org-variable-name">theta0</span> = fit_poisson_beta_moment(x)
  <span class="org-variable-name">theta0</span>[0] -= np.log(s).mean()
  <span class="org-variable-name">theta1</span> = fit_poisson_beta_mle(x)
  <span class="org-variable-name">theta1</span>[0] -= np.log(s).mean()
  <span class="org-variable-name">theta2</span> = fit_poisson_beta_mle(x, s, init=theta0)
  <span class="org-keyword">return</span> x, s, [np.log(theta), theta0, theta1, theta2]
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-org8f429f7" class="outline-2">
<h2 id="results"><a id="org8f429f7"></a>Results</h2>
<div class="outline-text-2" id="text-results">
</div>
<div id="outline-container-orged64f38" class="outline-3">
<h3 id="simulation"><a id="orged64f38"></a>Simulation</h3>
<div class="outline-text-3" id="text-simulation">
<p>
Make sure the implementations work on one example.
</p>

<div class="org-src-container">
<pre class="src src-ipython">x, *<span class="org-variable-name">theta</span> = simulate_pois_beta(n=1000, seed=0)
theta
</pre>
</div>

<pre class="example">
[117.2199806492514, 1.4920592434019648, 2.661095776728801]

</pre>

<p>
Fit the moment estimator.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">thetahat_moment</span> = fit_poisson_beta_moment(x)
np.exp(thetahat_moment)
</pre>
</div>

<pre class="example">
array([120.04495743,   1.52142758,   2.72068022])

</pre>

<p>
Fit the MLE.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">thetahat_mle</span> = fit_poisson_beta_mle(x)
np.exp(thetahat_mle)
</pre>
</div>

<pre class="example">
array([118.92485772,   1.52953485,   2.68994176])

</pre>

<p>
Evaluate more systematically.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">evaluate_estimator</span>(n, n_trials):
  <span class="org-variable-name">sim_result</span> = []
  <span class="org-variable-name">grid</span> = np.linspace(np.log(.1), np.log(100), 5)
  <span class="org-keyword">for</span> (ln_kr, ln_kon, ln_koff) <span class="org-keyword">in</span> itertools.product(grid, repeat=3):
    <span class="org-keyword">for</span> trial <span class="org-keyword">in</span> <span class="org-builtin">range</span>(n_trials):
      x, *<span class="org-variable-name">_</span> = simulate_pois_beta(n=n, kr=np.exp(ln_kr), kon=np.exp(ln_kon),
                                 koff=np.exp(ln_koff), seed=trial)
      <span class="org-keyword">for</span> method <span class="org-keyword">in</span> [<span class="org-string">'moment'</span>, <span class="org-string">'mle'</span>]:
        <span class="org-keyword">try</span>:
          <span class="org-variable-name">thetahat</span> = <span class="org-builtin">getattr</span>(sys.modules[<span class="org-string">'__main__'</span>], f<span class="org-string">'fit_poisson_beta_{method}'</span>)(x)
        <span class="org-keyword">except</span> <span class="org-type">RuntimeError</span>:
          <span class="org-variable-name">thetahat</span> = [np.nan, np.nan, np.nan]
        sim_result.append([ln_kr, ln_kon, ln_koff, method, trial] + <span class="org-builtin">list</span>(thetahat))
  <span class="org-keyword">return</span> pd.DataFrame(sim_result, columns=[<span class="org-string">'ln_kr'</span>, <span class="org-string">'ln_kon'</span>, <span class="org-string">'ln_koff'</span>, <span class="org-string">'method'</span>, <span class="org-string">'trial'</span>, <span class="org-string">'ln_kr_hat'</span>, <span class="org-string">'ln_kon_hat'</span>, <span class="org-string">'ln_koff_hat'</span>])
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">res</span> = evaluate_estimator(n=100, n_trials=1)
</pre>
</div>

<p>
Report the fraction of data sets for which estimation failed.
</p>

<div class="org-src-container">
<pre class="src src-ipython">res.groupby([<span class="org-string">'method'</span>]).<span class="org-builtin">apply</span>(<span class="org-keyword">lambda</span> x: x.dropna().shape[0] / x.shape[0])
</pre>
</div>

<pre class="example">
method
mle       0.584
moment    0.296
dtype: float64
</pre>
</div>
</div>

<div id="outline-container-orgfaf81f2" class="outline-3">
<h3 id="orgfaf81f2">Running time benchmark</h3>
<div class="outline-text-3" id="text-orgfaf81f2">
<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">evaluate_running_time</span>(n_trials):
  <span class="org-variable-name">timing_result</span> = []
  <span class="org-keyword">for</span> n <span class="org-keyword">in</span> (100, 500, 1000, 5000, 10000):
    <span class="org-comment-delimiter"># </span><span class="org-comment">Fix the parameters to something easy</span>
    x, *<span class="org-variable-name">_</span> = simulate_pois_beta(n=n, kr=50, kon=1, koff=1, seed=0)
    <span class="org-keyword">for</span> m <span class="org-keyword">in</span> (<span class="org-string">'moment'</span>, <span class="org-string">'mle'</span>):
      <span class="org-variable-name">res</span> = timeit.repeat(stmt=<span class="org-keyword">lambda</span>: <span class="org-builtin">getattr</span>(sys.modules[<span class="org-string">'__main__'</span>], f<span class="org-string">'fit_poisson_beta_{m}'</span>)(x),
                          number=1, repeat=n_trials, <span class="org-builtin">globals</span>=<span class="org-builtin">locals</span>())
      <span class="org-keyword">for</span> i, t <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(res):
        timing_result.append([n, m, i, t])
  <span class="org-keyword">return</span> pd.DataFrame(timing_result, columns=[<span class="org-string">'n'</span>, <span class="org-string">'method'</span>, <span class="org-string">'trial'</span>, <span class="org-string">'time'</span>])
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">timing_res</span> = evaluate_running_time(n_trials=10)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Dark2'</span>)
plt.clf()
plt.gcf().set_size_inches(3, 3)
plt.xscale(<span class="org-string">'log'</span>)
plt.yscale(<span class="org-string">'log'</span>)
<span class="org-keyword">for</span> i, (m, g) <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(timing_res.groupby(<span class="org-string">'method'</span>)):
  <span class="org-variable-name">t</span> = g.groupby(<span class="org-string">'n'</span>)[<span class="org-string">'time'</span>].agg([np.mean, np.std]).reset_index()
  plt.plot(t[<span class="org-string">'n'</span>], t[<span class="org-string">'mean'</span>], lw=1, c=cm(i), label=m)
  plt.scatter(g[<span class="org-string">'n'</span>] + np.random.normal(scale=np.log10(g[<span class="org-string">'n'</span>]), size=g.shape[0]), g[<span class="org-string">'time'</span>], c=cm(i), s=2, label=<span class="org-constant">None</span>)
plt.legend(frameon=<span class="org-constant">False</span>, title=<span class="org-string">'Method'</span>)
plt.xlabel(<span class="org-string">'Sample size'</span>)
plt.ylabel(<span class="org-string">'Running time (s)'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/pois-beta.org/timing.png" alt="timing.png">
</p>
</div>
</div>
</div>
<div id="outline-container-org2a8e2d9" class="outline-3">
<h3 id="org2a8e2d9">Impact of size factors</h3>
<div class="outline-text-3" id="text-org2a8e2d9">
<p>
Simulate a simple example with non-trivial size factor variation. Plot the
estimated distributions, as well as the difference between the fitted Beta
distributions and the ground truth.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">x</span>, <span class="org-variable-name">s</span>, <span class="org-variable-name">res</span> = trial(n=1000, mean_size=10000, var_size=250000, kr=32, kon=.5, koff=.25, seed=0)

<span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Dark2'</span>)
plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(2, 1)
fig.set_size_inches(6, 4)

<span class="org-variable-name">grid</span> = np.arange(x.<span class="org-builtin">max</span>() + 1)
ax[0].hist(x, bins=grid, color=<span class="org-string">'0.5'</span>)

<span class="org-keyword">for</span> i, (log_theta, label) <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(<span class="org-builtin">zip</span>(res, [<span class="org-string">'Ground truth'</span>, <span class="org-string">'Moment'</span>, <span class="org-string">'MLE (posthoc)'</span>, <span class="org-string">'MLE'</span>])):
  <span class="org-variable-name">p</span> = np.array([poisson_beta_pmf(log_theta, i, s).mean() <span class="org-keyword">for</span> i <span class="org-keyword">in</span> grid[:-1]])
  ax[0].plot(.5 + grid[:-1], n * p, lw=1, c=cm(i), label=label)
ax[0].legend(frameon=<span class="org-constant">False</span>, bbox_to_anchor=(1, .5), loc=<span class="org-string">'center left'</span>)
ax[0].set_xlabel(<span class="org-string">'Number of molecules'</span>)
ax[0].set_ylabel(<span class="org-string">'Number of cells'</span>)

<span class="org-variable-name">grid</span> = np.linspace(0, 1, 1000)
<span class="org-variable-name">fp</span> = st.beta(a=kon, b=koff).logpdf(grid)
<span class="org-keyword">for</span> i, (log_theta, label) <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(<span class="org-builtin">zip</span>(res, [<span class="org-string">'Ground truth'</span>, <span class="org-string">'Moment'</span>, <span class="org-string">'MLE (posthoc)'</span>, <span class="org-string">'MLE'</span>])):
  <span class="org-keyword">if</span> i &gt; 0:
    ax[1].plot(grid, fp - st.beta(a=np.exp(log_theta[1]), b=np.exp(log_theta[2])).logpdf(grid), lw=1, c=cm(i))
ax[1].axhline(y=0, lw=1, ls=<span class="org-string">':'</span>, c=<span class="org-string">'k'</span>)
ax[1].set_xlabel(<span class="org-string">'$p_i$'</span>)
ax[1].set_ylabel(<span class="org-string">'Diff log density\nfrom ground truth'</span>)

fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/pois-beta.org/size-ex.png" alt="size-ex.png">
</p>
</div>

<p>
Compare the log likelihoods of the fitted models.
</p>

<div class="org-src-container">
<pre class="src src-ipython">pd.Series({k: poisson_beta_neg_llik(log_theta, x, s) <span class="org-keyword">for</span> k, log_theta 
           <span class="org-keyword">in</span> <span class="org-builtin">zip</span>([<span class="org-string">'ground_truth'</span>, <span class="org-string">'moment'</span>, <span class="org-string">'mle_poshoc'</span>, <span class="org-string">'mle'</span>], res)})
</pre>
</div>

<pre class="example">
ground_truth    3692.394156
moment          3690.490051
mle_poshoc      3690.382941
mle             3690.376902
dtype: float64
</pre>

<p>
Look at some more examples.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">x</span>, <span class="org-variable-name">s</span>, <span class="org-variable-name">res</span> = trial(n=<span class="org-builtin">int</span>(1e3), mean_size=1e5, var_size=4e5, kr=64, kon=2, koff=.5, seed=1)

<span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Dark2'</span>)
plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(2, 1)
fig.set_size_inches(6, 4)

<span class="org-variable-name">grid</span> = np.arange(x.<span class="org-builtin">max</span>() + 1)
ax[0].hist(x, bins=grid, color=<span class="org-string">'0.5'</span>)

<span class="org-keyword">for</span> i, (log_theta, label) <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(<span class="org-builtin">zip</span>(res, [<span class="org-string">'Ground truth'</span>, <span class="org-string">'Moment'</span>, <span class="org-string">'MLE (posthoc)'</span>, <span class="org-string">'MLE'</span>])):
  <span class="org-variable-name">p</span> = np.array([poisson_beta_pmf(log_theta, i, s).mean() <span class="org-keyword">for</span> i <span class="org-keyword">in</span> grid[:-1]])
  ax[0].plot(.5 + grid[:-1], n * p, lw=1, c=cm(i), label=label)
ax[0].legend(frameon=<span class="org-constant">False</span>, bbox_to_anchor=(1, .5), loc=<span class="org-string">'center left'</span>)
ax[0].set_xlabel(<span class="org-string">'Number of molecules'</span>)
ax[0].set_ylabel(<span class="org-string">'Number of cells'</span>)

<span class="org-variable-name">grid</span> = np.linspace(0, 1, 1000)
<span class="org-variable-name">fp</span> = st.beta(a=kon, b=koff).logpdf(grid)
<span class="org-keyword">for</span> i, (log_theta, label) <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(<span class="org-builtin">zip</span>(res, [<span class="org-string">'Ground truth'</span>, <span class="org-string">'Moment'</span>, <span class="org-string">'MLE (posthoc)'</span>, <span class="org-string">'MLE'</span>])):
  <span class="org-keyword">if</span> i &gt; 0:
    ax[1].plot(grid, fp - st.beta(a=np.exp(log_theta[1]), b=np.exp(log_theta[2])).logpdf(grid), lw=1, c=cm(i))
ax[1].set_xlabel(<span class="org-string">'$p_i$'</span>)
ax[1].set_ylabel(<span class="org-string">'Diff log density\nfrom ground truth'</span>)

fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/pois-beta.org/size-ex2.png" alt="size-ex2.png">
</p>
</div>

<div class="org-src-container">
<pre class="src src-ipython">pd.Series({k: poisson_beta_neg_llik(log_theta, x, s) <span class="org-keyword">for</span> k, log_theta 
           <span class="org-keyword">in</span> <span class="org-builtin">zip</span>([<span class="org-string">'ground_truth'</span>, <span class="org-string">'moment'</span>, <span class="org-string">'mle_poshoc'</span>, <span class="org-string">'mle'</span>], res)})
</pre>
</div>

<pre class="example">
ground_truth    4053.912218
moment          4052.601052
mle_poshoc      4052.557805
mle             4052.557227
dtype: float64
</pre>
</div>
</div>

<div id="outline-container-org1a579b3" class="outline-3">
<h3 id="org1a579b3">Log likelihood surface</h3>
<div class="outline-text-3" id="text-org1a579b3">
<p>
Simulate a single example.
</p>

<div class="org-src-container">
<pre class="src src-ipython">x, *<span class="org-variable-name">theta</span> = simulate_pois_beta(n=1000, seed=0)
<span class="org-variable-name">log_theta</span> = np.log(np.array(theta))
<span class="org-variable-name">log_theta_hat</span> = fit_poisson_beta_mle(x)
</pre>
</div>

<p>
Plot the simulated data.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Dark2'</span>)
plt.clf()
plt.gcf().set_size_inches(4, 2)
<span class="org-variable-name">grid</span> = np.arange(x.<span class="org-builtin">max</span>() + 1)
plt.hist(x, bins=grid, color=<span class="org-string">'0.5'</span>)
<span class="org-keyword">for</span> i, (k, v) <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(<span class="org-builtin">zip</span>([<span class="org-string">'Ground truth'</span>, <span class="org-string">'MLE'</span>], [log_theta, log_theta_hat])):
  <span class="org-variable-name">p</span> = np.array([poisson_beta_pmf(v, i, 1).mean() <span class="org-keyword">for</span> i <span class="org-keyword">in</span> grid[:-1]])
  plt.plot(.5 + grid[:-1], 1000 * p, lw=1, c=cm(i), label=k)
plt.legend(frameon=<span class="org-constant">False</span>)
plt.xlabel(<span class="org-string">'Number of molecules'</span>)
plt.ylabel(<span class="org-string">'Number of cells'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/pois-beta.org/llik-sim-data.png" alt="llik-sim-data.png">
</p>
</div>

<p>
Draw the log likelihood surface about the true value.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">M</span> = 100
<span class="org-variable-name">grid_kr</span> = np.linspace(log_theta[0] - .5, log_theta[0] + .5, M)
<span class="org-variable-name">grid_kon</span> = np.linspace(log_theta[1] - .5, log_theta[1] + .5, M)
<span class="org-variable-name">grid_koff</span> = np.linspace(log_theta[2] - .5, log_theta[2] + .5, M)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">llik_kr_kon</span> = np.zeros((M, M))
<span class="org-keyword">for</span> i <span class="org-keyword">in</span> <span class="org-builtin">range</span>(M):
  <span class="org-keyword">for</span> j <span class="org-keyword">in</span> <span class="org-builtin">range</span>(M):
    <span class="org-variable-name">llik_kr_kon</span>[i, j] = poisson_beta_neg_llik(np.array([grid_kr[i], grid_kon[j], log_theta[2]]), x, 1)

<span class="org-variable-name">llik_kr_koff</span> = np.zeros((M, M))
<span class="org-keyword">for</span> i <span class="org-keyword">in</span> <span class="org-builtin">range</span>(M):
  <span class="org-keyword">for</span> j <span class="org-keyword">in</span> <span class="org-builtin">range</span>(M):
    <span class="org-variable-name">llik_kr_koff</span>[i, j] = poisson_beta_neg_llik(np.array([grid_kr[i], log_theta[1], grid_koff[j]]), x, 1)

<span class="org-variable-name">llik_kon_koff</span> = np.zeros((M, M))
<span class="org-keyword">for</span> i <span class="org-keyword">in</span> <span class="org-builtin">range</span>(M):
  <span class="org-keyword">for</span> j <span class="org-keyword">in</span> <span class="org-builtin">range</span>(M):
    <span class="org-variable-name">llik_kon_koff</span>[i, j] = poisson_beta_neg_llik(np.array([log_theta[0], grid_kon[i], grid_koff[j]]), x, 1)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = colorcet.cm[<span class="org-string">'fire_r'</span>]
plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(2, 2)
fig.set_size_inches(6, 6)

ax[0,0].contour(grid_kr, grid_kon, llik_kr_kon, cmap=cm, linewidths=1, levels=np.linspace(llik_kr_kon.<span class="org-builtin">min</span>(), llik_kr_kon.<span class="org-builtin">max</span>(), 20))
ax[0,0].scatter(log_theta[0], log_theta[1], marker=<span class="org-string">'x'</span>, s=32, c=<span class="org-string">'k'</span>, label=<span class="org-string">'Ground truth'</span>)
ax[0,0].scatter(log_theta_hat[0], log_theta_hat[1], marker=<span class="org-string">'x'</span>, s=32, c=<span class="org-string">'r'</span>, label=<span class="org-string">'MLE'</span>)
ax[0,0].legend(handletextpad=0)
ax[0,0].set_xlabel(<span class="org-string">'$\ln(k_r)$'</span>)
ax[0,0].set_ylabel(<span class="org-string">'$\ln(k_{\mathrm{on}})$'</span>)

ax[1,0].contour(grid_kr, grid_koff, llik_kr_koff, cmap=cm, levels=np.linspace(llik_kr_koff.<span class="org-builtin">min</span>(), llik_kr_koff.<span class="org-builtin">max</span>(), 20), linewidths=1)
ax[1,0].scatter(log_theta[0], log_theta[2], marker=<span class="org-string">'x'</span>, s=32, c=<span class="org-string">'k'</span>)
ax[1,0].scatter(log_theta_hat[0], log_theta_hat[2], marker=<span class="org-string">'x'</span>, s=32, c=<span class="org-string">'r'</span>)
ax[1,0].set_xlabel(<span class="org-string">'$\ln(k_r)$'</span>)
ax[1,0].set_ylabel(<span class="org-string">'$\ln(k_{\mathrm{off}})$'</span>)

ax[0,1].contour(grid_koff, grid_kon, llik_kon_koff.T, cmap=cm, levels=np.linspace(llik_kon_koff.<span class="org-builtin">min</span>(), llik_kon_koff.<span class="org-builtin">max</span>(), 20), linewidths=1)
ax[0,1].scatter(log_theta[2], log_theta[1], marker=<span class="org-string">'x'</span>, s=32, c=<span class="org-string">'k'</span>)
ax[0,1].scatter(log_theta_hat[2], log_theta_hat[1], marker=<span class="org-string">'x'</span>, s=32, c=<span class="org-string">'r'</span>)
ax[0,1].set_xlabel(<span class="org-string">'$\ln(k_{\mathrm{off}})$'</span>)
ax[0,1].set_ylabel(<span class="org-string">'$\ln(k_{\mathrm{on}})$'</span>)

ax[1,1].set_axis_off()

fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/pois-beta.org/llik-surface.png" alt="llik-surface.png">
</p>
</div>
</div>
</div>

<div id="outline-container-org2d20171" class="outline-3">
<h3 id="org2d20171">Impact of initialization</h3>
<div class="outline-text-3" id="text-org2d20171">
<p>
Look at estimates starting from random initializations.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">thetahat</span> = []
<span class="org-keyword">for</span> i <span class="org-keyword">in</span> <span class="org-builtin">range</span>(100):
  <span class="org-keyword">try</span>:
    <span class="org-variable-name">res</span> = fit_poisson_beta_mle(x, init=np.random.normal(scale=0.5, size=3), max_iters=10000)
  <span class="org-keyword">except</span> <span class="org-type">RuntimeError</span> <span class="org-keyword">as</span> e:
    <span class="org-keyword">print</span>(f<span class="org-string">'Initialization {i} failed: {e.__cause__}'</span>)
  <span class="org-keyword">if</span> <span class="org-keyword">not</span> i % 10:
    <span class="org-keyword">print</span>(f<span class="org-string">'Trial {i}'</span>)
  thetahat.append(res)
<span class="org-variable-name">thetahat</span> = np.array(thetahat)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(2, 2)
fig.set_size_inches(6, 6)
ax[0,0].scatter(thetahat[:,0], thetahat[:,1], s=1, c=<span class="org-string">'k'</span>)
ax[0,0].set_xlabel(<span class="org-string">'Estimated $\ln(k_r)$'</span>)
ax[0,0].set_ylabel(<span class="org-string">'Estimated $\ln(k_{\mathrm{on}})$'</span>)

ax[1,0].scatter(thetahat[:,0], thetahat[:,2], s=1, c=<span class="org-string">'k'</span>)
ax[1,0].set_xlabel(<span class="org-string">'Estimated $\ln(k_r)$'</span>)
ax[1,0].set_ylabel(<span class="org-string">'Estimated $\ln(k_{\mathrm{off}})$'</span>)

ax[0,1].scatter(thetahat[:,2], thetahat[:,1], s=1, c=<span class="org-string">'k'</span>)
ax[0,1].set_xlabel(<span class="org-string">'Estimated $\ln(k_{\mathrm{off}})$'</span>)
ax[0,1].set_ylabel(<span class="org-string">'Estimated $\ln(k_{\mathrm{on}})$'</span>)

ax[1,1].set_axis_off()
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/pois-beta.org/init-ex.png" alt="init-ex.png">
</p>
</div>

<div class="org-src-container">
<pre class="src src-ipython">(pd.DataFrame(thetahat, columns=[<span class="org-string">'log_kr'</span>, <span class="org-string">'log_kon'</span>, <span class="org-string">'log_koff'</span>])
 .to_csv(<span class="org-string">'/scratch/midway2/aksarkar/ideas/poisson-beta-est.txt.gz'</span>, sep=<span class="org-string">'\t'</span>))
</pre>
</div>

<p>
Look at one example where the fit converged to something far from the ground
truth.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">query</span> = thetahat[np.argmin(thetahat[:,0])]

<span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Dark2'</span>)
plt.clf()
plt.gcf().set_size_inches(4, 2)
<span class="org-variable-name">grid</span> = np.arange(x.<span class="org-builtin">max</span>() + 1)
plt.hist(x, bins=grid, color=<span class="org-string">'0.5'</span>)
<span class="org-keyword">for</span> i, (k, v) <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(<span class="org-builtin">zip</span>([<span class="org-string">'Ground truth'</span>, <span class="org-string">'MLE'</span>], [log_theta, query])):
  <span class="org-variable-name">p</span> = np.array([poisson_beta_pmf(v, i, 1).mean() <span class="org-keyword">for</span> i <span class="org-keyword">in</span> grid[:-1]])
  plt.plot(.5 + grid[:-1], 1000 * p, lw=1, c=cm(i), label=k)
plt.legend(frameon=<span class="org-constant">False</span>)
plt.xlabel(<span class="org-string">'Number of molecules'</span>)
plt.ylabel(<span class="org-string">'Number of cells'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/pois-beta.org/init-ex2.png" alt="init-ex2.png">
</p>
</div>
</div>
</div>
<div id="outline-container-orgfb20ae8" class="outline-3">
<h3 id="orgfb20ae8">Slice-within-Gibbs sampling</h3>
<div class="outline-text-3" id="text-orgfb20ae8">
<p>
First, try to sample from a standard Gaussian using slice sampling.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> poisbeta.mcmc
<span class="org-variable-name">f</span> = st.norm().logpdf
<span class="org-variable-name">n_samples</span> = 1000
<span class="org-variable-name">samples</span> = [0]
<span class="org-keyword">for</span> i <span class="org-keyword">in</span> <span class="org-builtin">range</span>(n_samples):
  <span class="org-variable-name">x</span> = poisbeta.mcmc.slice_sample(f, width=5, init=samples[-1])
  samples.append(x)
<span class="org-variable-name">samples</span> = np.array(samples[1:])
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">grid</span> = np.linspace(-4, 4, 1000)
plt.clf()
plt.gcf().set_size_inches(3, 3)
plt.hist(samples, bins=30, density=<span class="org-constant">True</span>, color=<span class="org-string">'0.7'</span>)
plt.plot(grid, np.exp(f(grid)), lw=1, c=<span class="org-string">'k'</span>)
plt.xlabel(<span class="org-string">'Samples'</span>)
plt.ylabel(<span class="org-string">'Density'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/pois-beta.org/slice-sample-norm.png" alt="slice-sample-norm.png">
</p>
</div>

<p>
Now, use slice-within-Gibbs to sample from the posterior \(p(\kr, \kon,
   \koff \mid \cdot)\).
</p>

<div class="org-src-container">
<pre class="src src-ipython">x, *<span class="org-variable-name">theta</span> = simulate_pois_beta(n=100, kr=32, kon=.25, koff=.25, seed=0)
<span class="org-variable-name">log_theta</span> = np.log(np.array(theta))
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">grid</span> = np.arange(x.<span class="org-builtin">max</span>() + 1)
plt.clf()
plt.gcf().set_size_inches(4, 2)
plt.hist(x, bins=grid, color=<span class="org-string">'0.7'</span>)
plt.plot(grid + 0.5, x.shape[0] * np.array([poisbeta.mle.poisson_beta_pmf(log_theta, i, 1) <span class="org-keyword">for</span> i <span class="org-keyword">in</span> grid]).ravel(), lw=1, c=<span class="org-string">'k'</span>)
plt.xlabel(<span class="org-string">'Number of molecules'</span>)
plt.ylabel(<span class="org-string">'Number of cells'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/pois-beta.org/slice-sample-ex.png" alt="slice-sample-ex.png">
</p>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-comment-delimiter"># </span><span class="org-comment">Important: Kim &amp; Marioni 2013 use shape/scale parametrization, but write</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">alpha/beta. We use shape/rate</span>
<span class="org-variable-name">samples</span>, <span class="org-variable-name">trace</span> = poisbeta.mcmc.fit_poisson_beta_mcmc(
  x,
  n_samples=1000,
  ar=1,
  br=1/x.<span class="org-builtin">max</span>(),
  aon=1,
  bon=1/100,
  aoff=1,
  boff=1/100,
  trace=<span class="org-constant">True</span>)
</pre>
</div>

<p>
Plot the evolution of the log joint probability over the samples.
</p>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
plt.gcf().set_size_inches(4, 2)
plt.plot(np.arange(samples.shape[0]), trace, lw=1, c=<span class="org-string">'k'</span>)
plt.xlabel(<span class="org-string">'Sample'</span>)
plt.ylabel(<span class="org-string">'Log joint probability'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/pois-beta.org/slice-sample-log-joint.png" alt="slice-sample-log-joint.png">
</p>
</div>

<p>
Look at the posterior distributions over the parameters. Use 800 samples
after warm-up, based on the evolution of the log joint
probability. Superimpose the moment estimate.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">thetahat</span> = np.exp(poisbeta.fit_poisson_beta_moment(x))
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 3)
fig.set_size_inches(7, 2)
<span class="org-keyword">for</span> a, s, t, h, k <span class="org-keyword">in</span> <span class="org-builtin">zip</span>(ax, samples.T, theta, thetahat, [<span class="org-string">'$k_r$'</span>, <span class="org-string">'$k_{\mathrm{on}}$'</span>, <span class="org-string">'$k_{\mathrm{off}}$'</span>]):
  a.hist(s[-800:], bins=30, density=<span class="org-constant">True</span>, color=<span class="org-string">'0.7'</span>)
  a.axvline(x=t, c=<span class="org-string">'k'</span>, lw=1, label=<span class="org-string">'Ground truth'</span>)
  a.axvline(x=h, c=<span class="org-string">'r'</span>, lw=1, label=<span class="org-string">'Moment'</span>)
  a.set_xlabel(k)
ax[0].set_ylabel(<span class="org-string">'Posterior density'</span>)
ax[-1].legend(frameon=<span class="org-constant">False</span>, loc=<span class="org-string">'center left'</span>, bbox_to_anchor=(1, .5))
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/pois-beta.org/slice-sample-post.png" alt="slice-sample-post.png">
</p>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Abhishek Sarkar</p>
<p class="date">Created: 2020-02-17 Mon 11:53</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
