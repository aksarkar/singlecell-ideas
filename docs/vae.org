#+TITLE: VAE for single-cell expression
#+AUTHOR: Abhishek Sarkar
#+EMAIL: aksarkar@uchicago.edu
#+EXCLUDE_TAGS: noexport
#+HTML_CONTAINER: div
#+HTML_DOCTYPE: html-strict
#+LANGUAGE: en
#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline author:t
#+OPTIONS: broken-links:nil c:nil creator:nil d:(not "LOGBOOK") date:t e:t
#+OPTIONS: email:nil f:t inline:t num:t p:nil pri:nil prop:nil stat:t tags:t
#+OPTIONS: html-link-use-abs-url:nil html-postamble:auto html-preamble:t
#+OPTIONS: html-scripts:t html-style:t html5-fancy:nil tex:t
#+OPTIONS: tasks:t tex:t timestamp:t title:t toc:t todo:t |:t

#+PROPERTY: header-args:ipython+ :session kernel-aksarkar.json :results raw drawer :async t

* Setup :noexport:

  #+BEGIN_SRC emacs-lisp
    (setq python-shell-prompt-detect-failure-warning nil)
  #+END_SRC

  #+RESULTS:

  #+NAME: ipython3-kernel
  #+BEGIN_SRC shell :dir (concat (file-name-as-directory (getenv "SCRATCH")) "singlecell") :var RESOURCES="--mem=36G --partition=gpu2 --gres=gpu:1"
    sbatch $RESOURCES --job-name=ipython3 --output=ipython3.out
    #!/bin/bash
    source activate singlecell
    rm -f $HOME/.local/share/jupyter/runtime/kernel-aksarkar.json
    ipython3 kernel --ip=$(hostname -i) -f kernel-aksarkar.json
  #+END_SRC

  #+RESULTS: ipython3-kernel
  : Submitted batch job 38513527

  #+NAME: imports
  #+BEGIN_SRC ipython
    %matplotlib inline

    import matplotlib.pyplot as plt
    import numpy as np
    import os
    import pandas as pd
    import scipy.linalg as spla
    import scipy.stats as sps
    import tensorflow as tf
    import tensorflow.contrib.bayesflow as bf
    import tensorflow.contrib.distributions as ds
    import tensorflow.contrib.slim as slim

    st = bf.stochastic_tensor
    vi = bf.variational_inference
  #+END_SRC

  #+RESULTS: imports
  :RESULTS:
  :END:

  #+BEGIN_SRC ipython
    from tensorflow.python.client import device_lib as dl
    dl.list_local_devices()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  #+BEGIN_EXAMPLE
  [name: "/cpu:0"
     device_type: "CPU"
     memory_limit: 268435456
     locality {
     }
     incarnation: 6311300002200915715, name: "/gpu:0"
     device_type: "GPU"
     memory_limit: 11324823962
     locality {
       bus_id: 2
     }
     incarnation: 14476157233182787061
     physical_device_desc: "device: 0, name: Tesla K80, pci bus id: 0000:89:00.0"]
  #+END_EXAMPLE
  :END:

* Get the data

  #+BEGIN_SRC shell :dir /home/aksarkar/projects/singlecell/data :async t
    test -f oligodendroma.txt.gz || curl --ftp-pasv 'ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE70nnn/GSE70630/suppl/GSE70630%5FOG%5Fprocessed%5Fdata%5Fv2%2Etxt%2Egz' -o oligodendroma.txt.gz
  #+END_SRC

  #+RESULTS:

  #+NAME: sample-processing
  #+BEGIN_SRC shell :results raw drawer
    curl --ftp-pasv 'ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE70nnn/GSE70630/soft/GSE70630_family.soft.gz' | zgrep -m1 'Supplementary_files_format_and_content'
  #+END_SRC

  #+RESULTS: sample-processing
  :RESULTS:
  !Sample_data_processing = Supplementary_files_format_and_content: tab-delimited text file containing the normalized expression levels (E) for 8008 analyzed genes across 2,594 oligodendroglioma cells that passed QC.
  :END:

  #+NAME: signature-genes
  #+BEGIN_SRC ipython
    signature_genes = pd.read_excel('https://www.nature.com/nature/journal/v539/n7628/extref/nature20123-s1.xlsx', skiprows=8, header=0)
  #+END_SRC

  #+RESULTS: signature-genes
  :RESULTS:
  :END:

  #+NAME: oligodendroma
  #+BEGIN_SRC ipython
    dge = pd.read_table('/home/aksarkar/projects/singlecell/data/oligodendroma.txt.gz', index_col=0, quotechar="'")
    dge = dge.fillna(value=0)
    dge.columns = pd.Series(dge.columns).apply(lambda x: 'MGH{}'.format(x.split('_')[0]) if 'MGH' not in x else x.split('_')[0])
    dge.shape, dge.columns.value_counts()
  #+END_SRC

  #+RESULTS: oligodendroma
  :RESULTS:
  #+BEGIN_EXAMPLE
  ((23686, 4347), MGH54    1225
     MGH53     861
     MGH36     788
     MGH97     598
     MGH93     445
     MGH60     430
     dtype: int64)
  #+END_EXAMPLE
  :END:

* Replicate Tirosh et al:

  Expression levels were quantified as \(E_{i,j} = \log_2 (TPM_{i,j} /10 +
  1)\), where \(TPM_{i,j}\) refers to transcript-per-million for gene \(i\) in
  sample \(j\), as calculated by RSEM.

  For each cell, we quantified two quality measures: the number of genes for which
  at least one read was mapped, and the average expression level of a curated list of
  housekeeping genes. We then conservatively excluded all cells with either fewer
  than 3,000 detected genes or an average housekeeping expression (E, as defined
  above) below 2.5.

  *Presumably this was already done because the number of cells in the data
  matrix matches the reported number (4,347)*

  For the remaining cells we calculated the aggregate expression of each gene
  as (\log 2 (\mathrm{average}(TPM_{i,1...n})+1)\), and excluded genes with an
  aggregate expression below 4, leaving a set of 8,008 analysed genes.

  #+BEGIN_SRC ipython
    tpm = 10 * (np.exp(dge.values * np.log(2)) - 1)
    pass_aggregate_expression = np.log(tpm.mean(axis=1) + 1) / np.log(2) > 4
    pass_aggregate_expression.sum()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  : 8115
  :END:

  For the remaining cells and genes, we defined relative expression by
  centering the expression levels, \(Er_{i,j} = E_{i,j} -
  \mathrm{average}[E_{i,1...n} ]\). Centering was performed within each tumour
  separately in order to decrease the impact of inter-tumoural variability on
  the combined analysis across tumours.

  #+NAME: relative_expression
  #+BEGIN_SRC ipython
    relative_expression = dge[pass_aggregate_expression].groupby(lambda x: x, axis=1).transform(lambda x: x - x.mean())
  #+END_SRC

  #+RESULTS: relative_expression
  :RESULTS:
  :END:

  #+RESULTS:
  :RESULTS:
  :END:

  #+BEGIN_SRC shell :dir (concat (file-name-as-directory (getenv "SCRATCH")) "singlecell")
    curl --ftp-pasv -sO "ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human/release_27/gencode.v27.basic.annotation.gtf.gz"
  #+END_SRC

  #+RESULTS:

  #+NAME: gencode
  #+BEGIN_SRC ipython
    gencode = pd.read_table(os.path.join(os.getenv('SCRATCH'), 'singlecell', 'gencode.v27.basic.annotation.gtf.gz'), comment='#', header=None)
    gencode.columns = ['chr', 'source', 'feature', 'start', 'end', 'score', 'strand', 'frame', 'attribute']
    protein_coding_genes = gencode[(gencode['feature'] == 'gene') & gencode['attribute'].apply(lambda x: 'protein_coding' in x)]
    protein_coding_genes.loc[:,'symbol'] = protein_coding_genes['attribute'].apply(lambda x: dict(info.split() for info in x.split('; '))['gene_name'].strip('"'))
    protein_coding_genes.head()
  #+END_SRC

  #+RESULTS: gencode
  :RESULTS:
  #+BEGIN_EXAMPLE
        chr  source feature   start     end score strand frame  \
    54   chr1  HAVANA    gene   65419   71585     .      +     .   
    175  chr1  HAVANA    gene  450703  451697     .      -     .   
    241  chr1  HAVANA    gene  685679  686673     .      -     .   
    390  chr1  HAVANA    gene  923928  944581     .      +     .   
    658  chr1  HAVANA    gene  944204  959309     .      -     .   

                                                 attribute  symbol  
    54   gene_id "ENSG00000186092.5"; gene_type "protei...   OR4F5  
    175  gene_id "ENSG00000284733.1"; gene_type "protei...  OR4F29  
    241  gene_id "ENSG00000284662.1"; gene_type "protei...  OR4F16  
    390  gene_id "ENSG00000187634.11"; gene_type "prote...  SAMD11  
    658  gene_id "ENSG00000188976.10"; gene_type "prote...   NOC2L  
  #+END_EXAMPLE
  :END:

  #+NAME: cnv
  #+BEGIN_SRC ipython
    relative_expression.merge(protein_coding_genes, left_index=True, right_on='symbol').sort_values(by=['chr', 'start']).reindex().rolling(100).mean()
  #+END_SRC

  #+RESULTS: cnv
  :RESULTS:
  cf2e8de6-4b16-47fc-911a-d6ce45d93025
  :END:

* Model

  \[ q(z \mid x) = N(\mu(x), \sigma(x)) \]

  \[ p(x \mid z) = N(\mu(z), \sigma(z)) \]

  The idea is that \(\mu, \sigma, \pi_0, \lambda\) are outputs of neural
  networks.

  To achieve dimensionality reduction, we want the dimension of \(\mu(x)\) less
  than the dimension of \(x\).

  To visualize the result, we can take \(\mu(x)\).

  #+BEGIN_SRC ipython
    num_epochs = 10
    minibatch_size = 100
    n, p = dge.shape
    num_minibatch = n // 100
    # Last layer is the target latent dimension
    layer_dim = [1024, 512, 256, 3]

    graph = tf.Graph()
    with graph.as_default(), graph.device('/gpu:*'):
      with tf.name_scope('input'):
        x = tf.placeholder(shape=[minibatch_size, p], dtype=tf.float32)
      with slim.arg_scope([slim.fully_connected],
                          normalizer_fn=slim.batch_norm):
        with tf.variable_scope('encoder'):
          # Default activation is relu
          qz = slim.stack(x, slim.fully_connected, layer_dim[:-1], scope='fc')
          loc = slim.linear(qz, num_outputs=layer_dim[-1], activation_fn=None,
                            scope='loc')
          scale = slim.fully_connected(qz, num_outputs=layer_dim[-1],
                                       activation_fn=tf.nn.softplus, scope='scale')
          with st.value_type(st.MeanValue()):
            # Put minimum scale here because it has to be outside the softplus (bias
            # on sigma is inside the softplus)
            qz = st.StochasticTensor(ds.Normal(loc=loc, scale=(1e-6 + scale)))
        with tf.variable_scope('decoder'):
          pz = ds.Normal(loc=tf.zeros(layer_dim[-1]), scale=tf.ones(layer_dim[-1]))
          px = slim.stack(qz, slim.fully_connected, list(reversed(layer_dim[:-1])),
                          scope='fc')
          loc = slim.linear(px, num_outputs=p, activation_fn=None, scope='loc')
          scale = slim.linear(px, num_outputs=p, activation_fn=tf.nn.softplus, scope='scale')
          llik = tf.reduce_sum(ds.Normal(loc=loc, scale=(1e-6 + scale)).log_prob(x))

      vi.register_prior(qz, pz)
      elbo = tf.reduce_sum(vi.elbo(llik))
      opt = tf.train.RMSPropOptimizer(learning_rate=1e-3)
      step = tf.get_variable(
        name='step',
        shape=[],
        initializer=tf.constant_initializer(0.0),
        trainable=False)
      train = slim.learning.create_train_op(
        total_loss=-elbo,
        optimizer=opt,
        global_step=step)

    # This needs to be outside graph.device
    sv = tf.train.Supervisor(
      graph=graph,
      logdir=os.path.join(os.getenv('SCRATCH'), 'vae-model'))
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

  #+BEGIN_SRC shell
  rm -f $SCRATCH/zip-vae-model/checkpoint
  #+END_SRC

  #+RESULTS:

  #+BEGIN_SRC ipython
    with sv.managed_session() as sess:
      for i in range(num_epochs * num_minibatch):
        if sv.should_stop():
          break
        start = (i % num_minibatch) * minibatch_size
        _, *loss = sess.run([train, elbo], feed_dict={x: dge.sample(minibatch_size)})
        if np.isnan(loss[0]):
          print(i, *loss)
          raise tf.train.NanLossDuringTrainingError
        if not i % num_minibatch:
          print(i // num_minibatch, *loss)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

  #+BEGIN_SRC ipython :ipyfile pca.png
    z = np.zeros((dge.shape[0], 3))
    with sv.managed_session() as sess:
      for i in range(num_minibatch):
        start = i * minibatch_size
        z_proj[start:start + minibatch_size] = sess.run(qz, feed_dict={x: dge.iloc[start:start + minibatch_size]})
    plt.clf()
    fig, ax = plt.subplots(2, 1)
    plt.scatter(z_proj[:,0], z_proj[:,1], ax=ax[0])
    ax[0].xlabel('$z_1$')
    ax[0].ylabel('$z_2$')
    plt.scatter(z_proj[:,1], z_proj[:,2], ax=ax[1])
    ax[1].xlabel('$z_2$')
    ax[1].ylabel('$z_3$')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  : <matplotlib.text.Text at 0x7fdd150733c8>
  [[file:pca.png]]
  :END:
